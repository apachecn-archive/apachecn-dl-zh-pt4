<html xmlns:epub="http://www.idpf.org/2007/ops" xmlns:m="http://www.w3.org/1998/Math/MathML" xmlns:pls="http://www.w3.org/2005/01/pronunciation-lexicon" xmlns:ssml="http://www.w3.org/2001/10/synthesis">
<head>
  <meta charset="UTF-8"/>
  <title>Modern Network Architectures</title>
  
  
</head>
<body>
  <div><h1 class="header-title">现代网络架构</h1>
                
            
            
                
<p class="mce-root">在最后一章中，我们探索了深度学习算法如何用于创建艺术图像，基于现有数据集创建新图像，以及生成文本。在这一章中，我们将向您介绍支持现代计算机视觉应用和自然语言系统的不同网络架构。我们将在本章中探讨的一些架构包括:</p>
<ul>
<li>雷斯内特</li>
<li>开始</li>
<li>DenseNet</li>
<li>编码器-解码器架构</li>
</ul>


            

            
        
    </div>


  <div><h1 class="header-title">现代网络架构</h1>
                
            
            
                
<p>当深度学习模型无法学习时，我们所做的一件重要事情是向模型添加更多层。随着图层的增加，模型的精度会提高，然后开始饱和。随着你不断添加更多的层，它开始退化。添加超过一定数量的更多层会增加一定的挑战，例如消失或爆炸渐变，这可以通过仔细初始化权重和引入中间归一化层来部分解决。现代架构，比如<strong>残网</strong> ( <strong> ResNet </strong>)和Inception，试图通过引入不同的技术来解决这个问题，比如残网连接。</p>


            

            
        
    </div>


  <div><h1 class="header-title">雷斯内特</h1>
                
            
            
                
<p>ResNet通过添加快捷方式连接，显式地让网络中的层适合残差映射，从而解决了这些问题。下图显示了ResNet的工作方式:</p>
<div><img height="324" width="352" class="alignnone size-full wp-image-404 image-border" src="img/458fcc47-ed90-46e3-aa8d-15d3a1c6b36b.png"/></div>
<p>在我们见过的所有网络中，我们试图通过堆叠不同的层来找到一个将输入(<em> x </em>)映射到其输出(<em> H(x) </em>)的函数。但是ResNet的作者提出了一个解决方案；我们不是试图学习从<em> x </em>到<em> H(x) </em>的底层映射，而是学习两者之间的差异，或者残差。然后，为了计算<em> H(x) </em>，我们可以将残差加到输入中。说残差是<em>F(x)= H(x)-x</em>；我们不是尝试直接学习<em> H(x) </em>，而是尝试学习<em> F(x) + x </em>。</p>
<p>每个ResNet块由一系列层和一个快捷连接组成，该快捷连接将块的输入添加到块的输出。加法运算是按元素执行的，输入和输出需要具有相同的大小。如果它们的大小不同，那么我们可以使用衬垫。下面的代码演示了一个简单的ResNet块的外观:</p>
<pre>class ResNetBasicBlock(nn.Module):<br/>    <br/>    def __init__(self,in_channels,out_channels,stride):<br/>        <br/>        super().__init__()<br/>        self.conv1 = nn.Conv2d(in_channels,out_channels,kernel_size=3,stride=stride,padding=1,bias=False)<br/>        self.bn1 = nn.BatchNorm2d(out_channels)<br/>        self.conv2 = nn.Conv2d(out_channels,out_channels,kernel_size=3,stride=stride,padding=1,bias=False)<br/>        self.bn2 = nn.BatchNorm2d(out_channels)<br/>        self.stride = stride<br/>        <br/>    def forward(self,x):<br/>        <br/>        residual = x<br/>        out = self.conv1(x)<br/>        out = F.relu(self.bn1(out),inplace=True)<br/>        out = self.conv2(out)<br/>        out = self.bn2(out)<br/>        out += residual<br/>        return F.relu(out)       </pre>
<p><kbd>ResNetBasicBlock</kbd>包含一个<kbd>init</kbd>方法，该方法初始化所有不同的层，例如卷积层、批量标准化和ReLU层。<kbd>forward</kbd>方法几乎类似于我们到目前为止所看到的，除了输入在返回之前被添加回图层的输出。</p>
<p>PyTorch <kbd>torchvision</kbd>包提供了一个带有不同层的现成ResNet模型。一些不同的型号包括:</p>
<ul>
<li>ResNet-18</li>
<li>ResNet-34</li>
<li>ResNet-50</li>
<li>ResNet-101</li>
<li>ResNet-152</li>
</ul>
<p>我们也可以用这些模型来进行迁移学习。<kbd>torchvision</kbd>实例使我们能够简单地创建其中一个模型并使用它们。我们在本书中已经这样做了几次，下面的代码是对此的复习:</p>
<pre>from torchvision.models import resnet18<br/><br/>resnet = resnet18(pretrained=False)</pre>
<p>下图显示了34层ResNet模型的外观:</p>
<div><img height="629" width="92" class="alignnone size-full wp-image-740 image-border" src="img/e36d8826-ab65-403c-aec4-affe9b965fa4.png"/></div>
<p>34层ResNet模型</p>
<p>我们可以看到这个网络是如何由多个ResNet块组成的。在一些实验中，团队尝试了深达1000层的模型。对于大多数真实的用例，我个人建议从较小的网络开始。这些现代网络的另一个关键优势是，与VGG等模型相比，它们需要的参数非常少，因为它们避免了使用需要大量参数来训练的全连接层。另一个用来解决计算机视觉领域问题的流行架构是<strong> Inception </strong>。在继续进行Inception架构之前，让我们在<kbd>Dogs vs. Cats</kbd>数据集上训练一个ResNet模型。我们将使用我们在<a href="4.html" target="_blank">第5章</a> <em>【计算机视觉的深度学习】</em>中使用的数据，并将根据从ResNet计算的特征快速训练一个模型。像往常一样，我们将按照以下步骤来训练模型:</p>
<ul>
<li>创建PyTorch数据集</li>
<li>创建用于培训和验证的加载器</li>
<li>创建ResNet模型</li>
<li>提取卷积特征</li>
<li>为预卷积要素和加载程序创建自定义PyTorch数据集类</li>
<li>创建简单的线性模型</li>
<li>训练和验证模型</li>
</ul>
<p>一旦完成，我们将为Inception和DenseNet重复这个步骤。最后，我们还将探讨集成技术，在这种技术中，我们将这些强大的模型结合起来，构建一个新的模型。</p>


            

            
        
    </div>


  <div><h1 class="header-title">创建PyTorch数据集</h1>
                
            
            
                
<p>我们创建一个包含所需的所有基本转换的转换对象，并使用<kbd xmlns:epub="http://www.idpf.org/2007/ops" xmlns:m="http://www.w3.org/1998/Math/MathML" xmlns:pls="http://www.w3.org/2005/01/pronunciation-lexicon" xmlns:ssml="http://www.w3.org/2001/10/synthesis">ImageFolder</kbd>从我们在<a xmlns:epub="http://www.idpf.org/2007/ops" xmlns:m="http://www.w3.org/1998/Math/MathML" xmlns:pls="http://www.w3.org/2005/01/pronunciation-lexicon" xmlns:ssml="http://www.w3.org/2001/10/synthesis" href="4.html" target="_blank"/> <a xmlns:epub="http://www.idpf.org/2007/ops" xmlns:m="http://www.w3.org/1998/Math/MathML" xmlns:pls="http://www.w3.org/2005/01/pronunciation-lexicon" xmlns:ssml="http://www.w3.org/2001/10/synthesis" href="4.html" target="_blank">章节</a> <em xmlns:epub="http://www.idpf.org/2007/ops" xmlns:m="http://www.w3.org/1998/Math/MathML" xmlns:pls="http://www.w3.org/2005/01/pronunciation-lexicon" xmlns:ssml="http://www.w3.org/2001/10/synthesis"> <a href="4.html" target="_blank"> 5 </a>【计算机视觉的深度学习】中创建的数据目录中加载图像。在下面的代码中，我们创建了数据集:</em></p>
<pre>data_transform = transforms.Compose([<br/>        transforms.Resize((299,299)),<br/>        transforms.ToTensor(),<br/>        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])<br/>    ])<br/><br/># For Dogs &amp; Cats dataset<br/>train_dset = ImageFolder('../../chapter5/dogsandcats/train/',transform=data_transform)<br/>val_dset = ImageFolder('../../chapter5/dogsandcats/valid/',transform=data_transform)<br/>classes=2</pre>
<p>到目前为止，前面的大部分代码都是不言自明的。</p>


            

            
        
    </div>


  <div><h1 class="header-title">创建用于培训和验证的加载器</h1>
                
            
            
                
<p>我们使用PyTorch加载器以批处理的形式加载数据集提供的数据，并利用所有的优势(如混排数据和使用多线程)来加快进程。下面的代码演示了这一点:</p>
<pre>train_loader = DataLoader(train_dset,batch_size=32,shuffle=False,num_workers=3)<br/>val_loader = DataLoader(val_dset,batch_size=32,shuffle=False,num_workers=3)<br/><br/></pre>
<p>在计算预卷积特征时，我们需要保持数据的准确顺序。当我们允许数据被打乱时，我们将无法保持标签。所以，确保<kbd>shuffle</kbd>是<kbd>False</kbd>，否则需要在代码内部处理所需的逻辑。</p>


            

            
        
    </div>


  <div><h1 class="header-title">创建ResNet模型</h1>
                
            
            
                
<p>使用<kbd>resnet34</kbd>预训练模型的层，我们通过丢弃最后一个线性层来创建PyTorch序列模型。我们将使用这个训练好的模型从我们的图像中提取特征。下面的代码演示了这一点:</p>
<pre>#Create ResNet model<br/>my_resnet = resnet34(pretrained=True)<br/><br/>if is_cuda:<br/>    my_resnet = my_resnet.cuda()<br/><br/>my_resnet = nn.Sequential(*list(my_resnet.children())[:-1])<br/><br/>for p in my_resnet.parameters():<br/>    p.requires_grad = False</pre>
<p>在前面的代码中，我们创建了一个在<kbd>torchvision</kbd>模型中可用的<kbd>resnet34</kbd>模型。在下面一行中，我们选择所有的ResNet层，不包括最后一层，并使用<kbd>nn.Sequential</kbd>创建一个新的模型:</p>
<pre>for p in my_resnet.parameters():<br/>    p.requires_grad = False</pre>
<p><kbd>nn.Sequential</kbd>实例允许我们使用一堆PyTorch层快速创建一个模型。一旦创建了模型，不要忘记将<kbd>requires_grad</kbd>参数设置为<kbd>False</kbd>，因为这将允许PyTorch不为保持梯度保留任何空间。</p>


            

            
        
    </div>


  <div><h1 class="header-title">提取卷积特征</h1>
                
            
            
                
<p>我们通过模型传递来自训练和验证数据加载器的数据，并将模型的结果存储在一个列表中以供进一步计算。通过计算预卷积特征，我们可以节省大量训练模型的时间，因为我们不会在每次迭代中计算这些特征。在下面的代码中，我们计算转换前的特征:</p>
<pre>#For training data<br/><br/># Stores the labels of the train data<br/>trn_labels = [] <br/><br/># Stores the pre convoluted features of the train data<br/>trn_features = [] <br/><br/>#Iterate through the train data and store the calculated features and the labels<br/>for d,la in train_loader:<br/>    o = m(Variable(d.cuda()))<br/>    o = o.view(o.size(0),-1)<br/>    trn_labels.extend(la)<br/>    trn_features.extend(o.cpu().data)<br/><br/>#For validation data<br/><br/>#Iterate through the validation data and store the calculated features and the labels<br/>val_labels = []<br/>val_features = []<br/>for d,la in val_loader:<br/>    o = m(Variable(d.cuda()))<br/>    o = o.view(o.size(0),-1)<br/>    val_labels.extend(la)<br/>    val_features.extend(o.cpu().data)</pre>
<p>一旦我们计算了预卷积特征，我们需要创建一个自定义数据集，它可以从我们的预卷积特征中选取数据。让我们为预先复杂的要素创建一个自定义数据集和加载器。</p>


            

            
        
    </div>


  <div><h1 class="header-title">为预卷积要素和加载程序创建自定义PyTorch数据集类</h1>
                
            
            
                
<p class="mce-root">我们已经看到了如何创建PyTorch数据集。它应该是<kbd>torch.utils.data</kbd>数据集类的子类，并且应该实现<kbd>__getitem__(self, index)</kbd>和<kbd>__len__(self)</kbd>方法，这两个方法返回数据集中数据的长度。在以下代码中，我们为预卷积功能实现了一个自定义数据集:</p>
<pre>class FeaturesDataset(Dataset):<br/>    <br/>    def __init__(self,featlst,labellst):<br/>        self.featlst = featlst<br/>        self.labellst = labellst<br/>        <br/>    def __getitem__(self,index):<br/>        return (self.featlst[index],self.labellst[index])<br/>    <br/>    def __len__(self):<br/>        return len(self.labellst)</pre>
<p>一旦创建了自定义数据集类，为预先复杂的要素创建数据加载器就很简单了，如以下代码所示:</p>
<pre>#Creating dataset for train and validation<br/>trn_feat_dset = FeaturesDataset(trn_features,trn_labels)<br/>val_feat_dset = FeaturesDataset(val_features,val_labels)<br/><br/>#Creating data loader for train and validation<br/>trn_feat_loader = DataLoader(trn_feat_dset,batch_size=64,shuffle=True)<br/>val_feat_loader = DataLoader(val_feat_dset,batch_size=64)</pre>
<p>现在我们需要创建一个简单的线性模型，将预先复杂的特性映射到相应的类别。</p>


            

            
        
    </div>


  <div><h1 class="header-title">创建简单的线性模型</h1>
                
            
            
                
<p>我们将创建一个简单的线性模型，将预先复杂的特征映射到相应的类别。在这种情况下，类别的数量是两个:</p>
<pre>class FullyConnectedModel(nn.Module):<br/>    <br/>    def __init__(self,in_size,out_size):<br/>        super().__init__()<br/>        self.fc = nn.Linear(in_size,out_size)<br/><br/>    def forward(self,inp):<br/>        out = self.fc(inp)<br/>        return out<br/><br/>fc_in_size = 8192<br/><br/>fc = FullyConnectedModel(fc_in_size,classes)<br/>if is_cuda:<br/>    fc = fc.cuda()</pre>
<p>现在，我们可以训练新模型并验证数据集了。</p>


            

            
        
    </div>


  <div><h1 class="header-title">训练和验证模型</h1>
                
            
            
                
<p>我们将使用与《T4》第五章 <em xmlns:epub="http://www.idpf.org/2007/ops" xmlns:m="http://www.w3.org/1998/Math/MathML" xmlns:pls="http://www.w3.org/2005/01/pronunciation-lexicon" xmlns:ssml="http://www.w3.org/2001/10/synthesis">中相同的<kbd xmlns:epub="http://www.idpf.org/2007/ops" xmlns:m="http://www.w3.org/1998/Math/MathML" xmlns:pls="http://www.w3.org/2005/01/pronunciation-lexicon" xmlns:ssml="http://www.w3.org/2001/10/synthesis">fit</kbd>函数，用于计算机视觉的深度学习</em>。为了节省篇幅，我没有把它包括在内。以下代码片段包含训练模型的功能并显示结果:</p>
<pre>train_losses , train_accuracy = [],[]<br/>val_losses , val_accuracy = [],[]<br/>for epoch in range(1,10):<br/>    epoch_loss, epoch_accuracy = fit(epoch,fc,trn_feat_loader,phase='training')<br/>    val_epoch_loss , val_epoch_accuracy = fit(epoch,fc,val_feat_loader,phase='validation')<br/>    train_losses.append(epoch_loss)<br/>    train_accuracy.append(epoch_accuracy)<br/>    val_losses.append(val_epoch_loss)<br/>    val_accuracy.append(val_epoch_accuracy)</pre>
<p>上述代码的结果如下:</p>
<pre>#Results<br/>training loss is 0.082 and training accuracy is 22473/23000     97.71
validation loss is   0.1 and validation accuracy is 1934/2000      96.7
training loss is  0.08 and training accuracy is 22456/23000     97.63
validation loss is  0.12 and validation accuracy is 1917/2000     95.85
training loss is 0.077 and training accuracy is 22507/23000     97.86
validation loss is   0.1 and validation accuracy is 1930/2000      96.5
training loss is 0.075 and training accuracy is 22518/23000      97.9
validation loss is 0.096 and validation accuracy is 1938/2000      96.9
training loss is 0.073 and training accuracy is 22539/23000      98.0
validation loss is   0.1 and validation accuracy is 1936/2000      96.8
training loss is 0.073 and training accuracy is 22542/23000     98.01
validation loss is 0.089 and validation accuracy is 1942/2000      97.1
training loss is 0.071 and training accuracy is 22545/23000     98.02
validation loss is  0.09 and validation accuracy is 1941/2000     97.05
training loss is 0.068 and training accuracy is 22591/23000     98.22
validation loss is 0.092 and validation accuracy is 1934/2000      96.7
training loss is 0.067 and training accuracy is 22573/23000     98.14
validation loss is 0.085 and validation accuracy is 1942/2000      97.1</pre>
<p>从结果可以看出，该模型达到了98%的训练准确率和97%的验证准确率。让我们了解另一种现代架构，以及如何使用它来计算预卷积特征，并使用它们来训练模型。</p>


            

            
        
    </div>


  <div><h1 class="header-title">开始</h1>
                
            
            
                
<p>在我们见过的大多数用于计算机视觉模型的深度学习算法中，我们要么选择一个过滤器大小为1 x 1、3 x 3、5 x 5、7 x 7的卷积层，要么选择一个地图池层。初始模块组合不同滤波器大小的卷积，并将所有输出连接在一起。下图使先启模型更加清晰:</p>
<div><img height="287" width="590" class="alignnone size-full wp-image-405 image-border" src="img/f6b94415-ec97-49ff-8c59-775631d0ad41.png"/></div>
<p>图片来源:https://arxiv.org/pdf/1409.4842.pdf</p>
<p>在这个初始块图像中，不同大小的卷积被应用于输入，并且所有这些层的输出被连接。这是一个初始模块的最简单的版本。有另一种形式的初始块，我们先将输入通过1×1卷积，然后再通过3×3和5×5卷积。1×1卷积用于降维。它有助于解决计算瓶颈。1 x 1卷积一次查看一个值，并跨越多个通道。例如，对100 x 64 x 64的输入大小使用10 x 1 x 1的过滤器将得到10 x 64 x 64。下图显示了降维后的初始块:</p>
<div><img height="334" width="641" class="alignnone size-full wp-image-406 image-border" src="img/3d91e925-80cd-4aa7-b94c-26083e8f2534.png"/></div>
<p>图片来源:https://arxiv.org/pdf/1409.4842.pdf</p>
<p>现在，让我们看一个PyTorch示例，看看前面的Inception块是什么样子的:</p>
<pre>class BasicConv2d(nn.Module):<br/><br/>    def __init__(self, in_channels, out_channels, **kwargs):<br/>        super(BasicConv2d, self).__init__()<br/>        self.conv = nn.Conv2d(in_channels, out_channels, bias=False, **kwargs)<br/>        self.bn = nn.BatchNorm2d(out_channels)<br/><br/>    def forward(self, x):<br/>        x = self.conv(x)<br/>        x = self.bn(x)<br/>        return F.relu(x, inplace=True)<br/><br/><br/>class InceptionBasicBlock(nn.Module):<br/><br/>    def __init__(self, in_channels, pool_features):<br/>        super().__init__()<br/>        self.branch1x1 = BasicConv2d(in_channels, 64, kernel_size=1)<br/><br/>        self.branch5x5_1 = BasicConv2d(in_channels, 48, kernel_size=1)<br/>        self.branch5x5_2 = BasicConv2d(48, 64, kernel_size=5, padding=2)<br/><br/>        self.branch3x3dbl_1 = BasicConv2d(in_channels, 64, kernel_size=1)<br/>        self.branch3x3dbl_2 = BasicConv2d(64, 96, kernel_size=3, padding=1)<br/><br/>        self.branch_pool = BasicConv2d(in_channels, pool_features, kernel_size=1)<br/><br/>    def forward(self, x):<br/>        branch1x1 = self.branch1x1(x)<br/><br/>        branch5x5 = self.branch5x5_1(x)<br/>        branch5x5 = self.branch5x5_2(branch5x5)<br/><br/>        branch3x3dbl = self.branch3x3dbl_1(x)<br/>        branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)<br/><br/>        branch_pool = F.avg_pool2d(x, kernel_size=3, stride=1, padding=1)<br/>        branch_pool = self.branch_pool(branch_pool)<br/><br/>        outputs = [branch1x1, branch5x5, branch3x3dbl, branch_pool]<br/>        return torch.cat(outputs, 1)<br/><br/><br/></pre>
<p>前面的代码包含两个类，<kbd>BasicConv2d</kbd>和<kbd>InceptionBasicBlock</kbd>。<kbd>BasicConv2d</kbd>类似于一个自定义层，它将二维卷积层、批量标准化和ReLU层应用于通过的输入。当我们有一个重复的代码结构时，创建一个新的层是一个好的实践，使代码看起来优雅。</p>
<p><kbd>InceptionBasicBlock</kbd>实现了我们在第二个盗梦空间中拥有的东西。让我们浏览每个较小的代码片段，并尝试理解它是如何实现的:</p>
<pre>branch1x1 = self.branch1x1(x)</pre>
<p>上述代码通过应用1 x 1卷积块来转换输入:</p>
<pre>branch5x5 = self.branch5x5_1(x)<br/>branch5x5 = self.branch5x5_2(branch5x5)</pre>
<p>在前面的代码中，我们通过应用1 x 1卷积块和5 x 5卷积块来转换输入:</p>
<pre>branch3x3dbl = self.branch3x3dbl_1(x)<br/>branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)</pre>
<p>在前面的代码中，我们通过应用1 x 1卷积块和3 x 3卷积块来转换输入:</p>
<pre>branch_pool = F.avg_pool2d(x, kernel_size=3, stride=1, padding=1)<br/>branch_pool = self.branch_pool(branch_pool)</pre>
<p>在前面的代码中，我们应用了一个平均池和一个1 x 1卷积块，最后，我们将所有结果连接在一起。一个初始网络将由几个初始块组成。下图显示了一个初始架构的样子:</p>
<div><img height="465" width="1509" class="alignnone size-full wp-image-407 image-border" src="img/8dd6a8d2-c984-49d7-87d9-57cab172b60c.jpg"/></div>
<p>初始架构</p>
<p><kbd>torchvision</kbd>包有一个初始网络，可以像我们使用ResNet网络一样使用它。对最初的Inception块进行了许多改进，PyTorch当前可用的实现是Inception v3。让我们看看如何使用来自<kbd>torchvision</kbd>的Inception v3模型来计算预先计算的特性。我们将不讨论数据加载过程，因为我们将使用前面ResNet部分中的相同数据加载器。我们将关注以下重要主题:</p>
<ul>
<li>创建一个初始模型</li>
<li>使用<kbd>register_forward_hook</kbd>提取卷积特征</li>
<li>为复杂要素创建新数据集</li>
<li>创建完全连接的模型</li>
<li>训练和验证模型</li>
</ul>


            

            
        
    </div>


  <div><h1 class="header-title">创建一个初始模型</h1>
                
            
            
                
<p>Inception v3模型有两个分支，每个分支生成一个输出，在最初的模型训练中，我们会像处理风格转移一样合并损失。到目前为止，我们只对使用一个分支来计算使用Inception的预卷积特征感兴趣。这方面的细节超出了本书的范围。如果你有兴趣了解更多关于它是如何工作的，那么浏览一下论文和初始模型的源代码(<a href="https://github.com/pytorch/vision/blob/master/torchvision/models/inception.py" target="_blank">https://github . com/py torch/vision/blob/master/torch vision/models/Inception . py</a>)会有所帮助。我们可以通过将<kbd>aux_logits</kbd>参数设置为<kbd>False</kbd>来禁用其中一个分支。下面的代码解释了如何创建一个模型并将<kbd>aux_logits</kbd>参数设置为<kbd>False</kbd>:</p>
<pre>my_inception = inception_v3(pretrained=True)<br/>my_inception.aux_logits = False<br/>if is_cuda:<br/>    my_inception = my_inception.cuda()</pre>
<p>与ResNet一样，从初始模型中提取卷积特征并不简单，所以我们将使用<kbd>register_forward_hook</kbd>来提取激活。</p>


            

            
        
    </div>


  <div><h1 class="header-title">使用register_forward_hook提取卷积特征</h1>
                
            
            
                
<p>我们将使用相同的技术，我们用来计算风格转移激活。下面是做了一些小修改的<kbd>LayerActivations</kbd>类，因为我们只对提取特定层的输出感兴趣:</p>
<pre>class LayerActivations():<br/>    features=[]<br/>    <br/>    def __init__(self,model):<br/>        self.features = []<br/>        self.hook = model.register_forward_hook(self.hook_fn)<br/>    <br/>    def hook_fn(self,module,input,output):<br/>        <br/>        self.features.extend(output.view(output.size(0),-1).cpu().data)<br/>    <br/>    def remove(self):<br/>        <br/>        self.hook.remove()</pre>
<p>除了<kbd>hook</kbd>函数，其余的代码与我们用于风格转换的代码相似。由于我们正在捕捉所有图像的输出并存储它们，我们将无法在<strong>图形处理单元</strong> ( <strong> GPU </strong>)内存中保存数据。所以我们从GPU提取张量到CPU，只存储张量而不是<kbd>Variable</kbd>。我们正在将其转换回张量，因为数据加载器只能与张量一起工作。在下面的代码中，我们使用<kbd>LayerActivations</kbd>的对象来提取最后一层的初始模型的输出，不包括平均池层、漏失层和线性层。我们跳过了平均池层，以避免丢失数据中的有用信息:</p>
<pre># Create LayerActivations object to store the output of inception model at a particular layer.<br/>trn_features = LayerActivations(my_inception.Mixed_7c)<br/>trn_labels = []<br/><br/># Passing all the data through the model , as a side effect the outputs will get stored <br/># in the features list of the LayerActivations object. <br/>for da,la in train_loader:<br/>    _ = my_inception(Variable(da.cuda()))<br/>    trn_labels.extend(la)<br/>trn_features.remove()<br/><br/># Repeat the same process for validation dataset .<br/><br/>val_features = LayerActivations(my_inception.Mixed_7c)<br/>val_labels = []<br/>for da,la in val_loader:<br/>    _ = my_inception(Variable(da.cuda()))<br/>    val_labels.extend(la)<br/>val_features.remove()</pre>
<p>让我们创建新的复杂特性所需的数据集和加载器。</p>


            

            
        
    </div>


  <div><h1 class="header-title">为复杂要素创建新数据集</h1>
                
            
            
                
<p>我们可以使用相同的<kbd>FeaturesDataset</kbd>类来创建新的数据集和数据加载器。在下面的代码中，我们创建了数据集和加载器:</p>
<pre>#Dataset for pre computed features for train and validation data sets<br/><br/>trn_feat_dset = FeaturesDataset(trn_features.features,trn_labels)<br/>val_feat_dset = FeaturesDataset(val_features.features,val_labels)<br/><br/>#Data loaders for pre computed features for train and validation data sets<br/><br/>trn_feat_loader = DataLoader(trn_feat_dset,batch_size=64,shuffle=True)<br/>val_feat_loader = DataLoader(val_feat_dset,batch_size=64)</pre>
<p>让我们创建一个新的模型来训练预复杂的特性。</p>


            

            
        
    </div>


  <div><h1 class="header-title">创建完全连接的模型</h1>
                
            
            
                
<p>一个简单的模型可能会以过度拟合而告终，所以让我们在模型中包括辍学。辍学将有助于避免过度拟合。在下面的代码中，我们正在创建我们的模型:</p>
<pre>class FullyConnectedModel(nn.Module):<br/>    <br/>    def __init__(self,in_size,out_size,training=True):<br/>        super().__init__()<br/>        self.fc = nn.Linear(in_size,out_size)<br/><br/>    def forward(self,inp):<br/>        out = F.dropout(inp, training=self.training)<br/>        out = self.fc(out)<br/>        return out<br/><br/># The size of the output from the selected convolution feature <br/>fc_in_size = 131072<br/><br/>fc = FullyConnectedModel(fc_in_size,classes)<br/>if is_cuda:<br/>    fc = fc.cuda()</pre>
<p>一旦创建了模型，我们就可以训练模型了。</p>


            

            
        
    </div>


  <div><h1 class="header-title">训练和验证模型</h1>
                
            
            
                
<p>我们使用与前面的ResNet和其他示例中相同的拟合和训练逻辑。我们将只查看训练代码及其结果:</p>
<pre>for epoch in range(1,10):<br/>    epoch_loss, epoch_accuracy = fit(epoch,fc,trn_feat_loader,phase='training')<br/>    val_epoch_loss , val_epoch_accuracy = fit(epoch,fc,val_feat_loader,phase='validation')<br/>    train_losses.append(epoch_loss)<br/>    train_accuracy.append(epoch_accuracy)<br/>    val_losses.append(val_epoch_loss)<br/>    val_accuracy.append(val_epoch_accuracy)<br/><br/>#Results<br/><br/>training loss is 0.78 and training accuracy is 22825/23000 99.24<br/>validation loss is 5.3 and validation accuracy is 1947/2000 97.35<br/>training loss is 0.84 and training accuracy is 22829/23000 99.26<br/>validation loss is 5.1 and validation accuracy is 1952/2000 97.6<br/>training loss is 0.69 and training accuracy is 22843/23000 99.32<br/>validation loss is 5.1 and validation accuracy is 1951/2000 97.55<br/>training loss is 0.58 and training accuracy is 22852/23000 99.36<br/>validation loss is 4.9 and validation accuracy is 1953/2000 97.65<br/>training loss is 0.67 and training accuracy is 22862/23000 99.4<br/>validation loss is 4.9 and validation accuracy is 1955/2000 97.75<br/>training loss is 0.54 and training accuracy is 22870/23000 99.43<br/>validation loss is 4.8 and validation accuracy is 1953/2000 97.65<br/>training loss is 0.56 and training accuracy is 22856/23000 99.37<br/>validation loss is 4.8 and validation accuracy is 1955/2000 97.75<br/>training loss is 0.7 and training accuracy is 22841/23000 99.31<br/>validation loss is 4.8 and validation accuracy is 1956/2000 97.8<br/>training loss is 0.47 and training accuracy is 22880/23000 99.48<br/>validation loss is 4.7 and validation accuracy is 1956/2000 97.8<br/><br/></pre>
<p>从结果来看，Inception模型在训练数据集上达到了99%的准确率，在验证数据集上达到了97.8%的准确率。由于我们预先计算并将所有特征保存在内存中，因此训练模型只需不到几分钟的时间。如果在机器上运行程序时内存不足，那么您可能需要避免将这些特性保存在内存中。</p>
<p>我们将看看另一个有趣的架构，DenseNet，它在去年变得非常流行。</p>


            

            
        
    </div>


  <div><h1 class="header-title">密集连接的卷积网络–dense net</h1>
                
            
            
                
<p>一些成功和流行的架构，如ResNet和Inception，已经显示了更深和更广的网络的重要性。ResNet利用快捷连接建立更深层次的网络。DenseNet通过引入从每一层到所有其他后续层的连接，将它带到了一个新的水平，在这一层中，人们可以接收来自先前层的所有特征地图。象征性地，它看起来如下:</p>
<p style="padding-left: 150px"><img height="20" width="207" class="fm-editor-equation" src="img/28672c69-1d67-410d-8e49-63158824882c.png"/></p>
<p>下图描述了五层密集块的外观:</p>
<div><img height="390" width="542" src="img/f7f3e5fa-b3b5-49e3-968f-6423cba0e915.png"/></div>
<p>图片来源:https://arxiv.org/abs/1608.06993</p>
<p>torchvision有一个DenseNet实现(<a href="https://github.com/pytorch/vision/blob/master/torchvision/models/densenet.py" target="_blank">https://github . com/py torch/vision/blob/master/torch vision/models/dense net . py</a>)。我们来看两大功能，<kbd>_DenseBlock</kbd>和<kbd>_DenseLayer</kbd>。</p>


            

            
        
    </div>


  <div><h1 class="header-title">DenseBlock</h1>
                
            
            
                
<p>让我们看看<kbd>DenseBlock</kbd>的代码，然后浏览一下:</p>
<pre>class _DenseBlock(nn.Sequential):<br/>    def __init__(self, num_layers, num_input_features, bn_size, growth_rate, drop_rate):<br/>        super(_DenseBlock, self).__init__()<br/>        for i in range(num_layers):<br/>            layer = _DenseLayer(num_input_features + i * growth_rate, growth_rate, bn_size, drop_rate)<br/>            self.add_module('denselayer%d' % (i + 1), layer)</pre>
<p><kbd>DenseBlock</kbd>是一个顺序模块，我们按顺序添加层。基于块中的层数(<kbd>num_layers</kbd>)，我们将<kbd>_Denselayer</kbd>对象的数量和名称添加到其中。所有的魔法都发生在<kbd>DenseLayer</kbd>内部。让我们看看<kbd>DenseLayer</kbd>里面发生了什么。</p>


            

            
        
    </div>


  <div><h1 class="header-title">登塞拉耶</h1>
                
            
            
                
<p>了解特定网络如何工作的一个好方法是查看源代码。PyTorch有一个非常干净的实现，大部分时间都很容易阅读。让我们看看<kbd>DenseLayer</kbd>的实现:</p>
<pre>class _DenseLayer(nn.Sequential):<br/>    def __init__(self, num_input_features, growth_rate, bn_size, drop_rate):<br/>        super(_DenseLayer, self).__init__()<br/>        self.add_module('norm.1', nn.BatchNorm2d(num_input_features)),<br/>        self.add_module('relu.1', nn.ReLU(inplace=True)),<br/>        self.add_module('conv.1', nn.Conv2d(num_input_features, bn_size *<br/>                        growth_rate, kernel_size=1, stride=1, bias=False)),<br/>        self.add_module('norm.2', nn.BatchNorm2d(bn_size * growth_rate)),<br/>        self.add_module('relu.2', nn.ReLU(inplace=True)),<br/>        self.add_module('conv.2', nn.Conv2d(bn_size * growth_rate, growth_rate,<br/>                        kernel_size=3, stride=1, padding=1, bias=False)),<br/>        self.drop_rate = drop_rate<br/><br/>    def forward(self, x):<br/>        new_features = super(_DenseLayer, self).forward(x)<br/>        if self.drop_rate &gt; 0:<br/>            new_features = F.dropout(new_features, p=self.drop_rate, training=self.training)<br/>        return torch.cat([x, new_features], 1)</pre>
<p>如果您不熟悉Python中的继承，那么前面的代码可能看起来不直观。<kbd>_DenseLayer</kbd>是<kbd>nn.Sequential</kbd>的子类；让我们看看每个方法内部发生了什么。</p>
<p>在<kbd>__init__</kbd>方法中，我们添加了输入数据需要传递到的所有层。它与我们见过的所有其他网络架构非常相似。</p>
<p>神奇的事情发生在<kbd>forward</kbd>方法中。我们将输入传递给<kbd>super</kbd>类的<kbd>forward</kbd>方法，也就是<kbd>nn.Sequential</kbd>。我们来看看顺序类的<kbd>forward</kbd>方法中发生了什么(<a href="https://github.com/pytorch/pytorch/blob/409b1c8319ecde4bd62fcf98d0a6658ae7a4ab23/torch/nn/modules/container.py" target="_blank">https://github . com/py torch/py torch/blob/409 B1 c 8319 ECD E4 BD 62 fcf 98 d0a 6658 AE 7 a4 ab 23/torch/nn/modules/container . py</a>):</p>
<pre>def forward(self, input):<br/>    for module in self._modules.values():<br/>        input = module(input)<br/>    return input</pre>
<p>输入通过先前添加到顺序块的所有层，输出连接到输入。对块中所需的层数重复该过程。</p>
<p>了解了<kbd>DenseNet</kbd>块的工作原理后，让我们来探索如何使用DenseNet来计算预卷积特征，并在此基础上构建分类器模型。在高层次上，DenseNet的实现类似于VGG的实现。DenseNet实现还有一个包含所有密集块的特性模块和一个包含全连接模型的分类器模块。我们将通过以下步骤来构建模型。我们将跳过与我们在Inception和ResNet中看到的类似的大部分内容，例如创建数据加载器和数据集。此外，我们将详细讨论以下步骤:</p>
<ul>
<li>创建DenseNet模型</li>
<li>提取DenseNet特征</li>
<li>创建数据集和加载器</li>
<li>创建完全连接的模型和列车</li>
</ul>
<p>到目前为止，大部分代码都是不言自明的。</p>


            

            
        
    </div>


  <div><h1 class="header-title">创建DenseNet模型</h1>
                
            
            
                
<p>Torchvision具有预训练的DenseNet模型，具有不同的层选项(121、169、201、161)。我们选择了有<kbd>121</kbd>层的模型。如前所述，DenseNet有两个模块:特征(包含密集块)和分类器(完全连通块)。由于我们使用DenseNet作为图像特征提取器，我们将只使用特征模块:</p>
<pre>my_densenet = densenet121(pretrained=True).features<br/>if is_cuda:<br/>    my_densenet = my_densenet.cuda()<br/><br/>for p in my_densenet.parameters():<br/>    p.requires_grad = False</pre>
<p>让我们从图像中提取DenseNet特征。</p>


            

            
        
    </div>


  <div><h1 class="header-title">提取DenseNet特征</h1>
                
            
            
                
<p>这与我们为Inception所做的非常相似，除了我们没有使用<kbd>register_forward_hook</kbd>来提取特征。以下代码显示了如何提取DenseNet要素:</p>
<pre>#For training data<br/>trn_labels = []<br/>trn_features = []<br/><br/>#code to store densenet features for train dataset.<br/>for d,la in train_loader:<br/>    o = my_densenet(Variable(d.cuda()))<br/>    o = o.view(o.size(0),-1)<br/>    trn_labels.extend(la)<br/>    trn_features.extend(o.cpu().data)<br/><br/>#For validation data<br/>val_labels = []<br/>val_features = []<br/><br/>#Code to store densenet features for validation dataset. <br/>for d,la in val_loader:<br/>    o = my_densenet(Variable(d.cuda()))<br/>    o = o.view(o.size(0),-1)<br/>    val_labels.extend(la)<br/>    val_features.extend(o.cpu().data)</pre>
<p>前面的代码类似于我们在Inception和ResNet中看到的代码。</p>


            

            
        
    </div>


  <div><h1 class="header-title">创建数据集和加载器</h1>
                
            
            
                
<p>我们将使用为ResNet创建的相同的<kbd>FeaturesDataset</kbd>类，并在下面的代码中使用它为<kbd>train</kbd>和<kbd>validation</kbd>数据集创建数据加载器:</p>
<pre># Create dataset for train and validation convolution features<br/>trn_feat_dset = FeaturesDataset(trn_features,trn_labels)<br/>val_feat_dset = FeaturesDataset(val_features,val_labels)<br/><br/># Create data loaders for batching the train and validation datasets<br/>trn_feat_loader = DataLoader(trn_feat_dset,batch_size=64,shuffle=True,drop_last=True)<br/>val_feat_loader = DataLoader(val_feat_dset,batch_size=64)</pre>
<p>是时候创建模型并训练它了。</p>


            

            
        
    </div>


  <div><h1 class="header-title">创建完全连接的模型和列车</h1>
                
            
            
                
<p>我们将使用一个简单的线性模型，类似于我们在ResNet和Inception中使用的模型。以下代码显示了我们将用来训练模型的网络体系结构:</p>
<pre>class FullyConnectedModel(nn.Module):<br/>    <br/>    def __init__(self,in_size,out_size):<br/>        super().__init__()<br/>        self.fc = nn.Linear(in_size,out_size)<br/><br/>    def forward(self,inp):<br/>        out = self.fc(inp)<br/>        return out<br/><br/>fc = FullyConnectedModel(fc_in_size,classes)<br/>if is_cuda:<br/>    fc = fc.cuda()</pre>
<p>我们将使用相同的<kbd>fit</kbd>方法来训练前面的模型。以下代码片段显示了训练代码以及结果:</p>
<pre>train_losses , train_accuracy = [],[]<br/>val_losses , val_accuracy = [],[]<br/>for epoch in range(1,10):<br/>    epoch_loss, epoch_accuracy = fit(epoch,fc,trn_feat_loader,phase='training')<br/>    val_epoch_loss , val_epoch_accuracy = fit(epoch,fc,val_feat_loader,phase='validation')<br/>    train_losses.append(epoch_loss)<br/>    train_accuracy.append(epoch_accuracy)<br/>    val_losses.append(val_epoch_loss)<br/>    val_accuracy.append(val_epoch_accuracy)<br/>    </pre>
<p>上述代码的结果是:</p>
<pre># Results<br/><br/>training loss is 0.057 and training accuracy is 22506/23000 97.85<br/>validation loss is 0.034 and validation accuracy is 1978/2000 98.9<br/>training loss is 0.0059 and training accuracy is 22953/23000 99.8<br/>validation loss is 0.028 and validation accuracy is 1981/2000 99.05<br/>training loss is 0.0016 and training accuracy is 22974/23000 99.89<br/>validation loss is 0.022 and validation accuracy is 1983/2000 99.15<br/>training loss is 0.00064 and training accuracy is 22976/23000 99.9<br/>validation loss is 0.023 and validation accuracy is 1983/2000 99.15<br/>training loss is 0.00043 and training accuracy is 22976/23000 99.9<br/>validation loss is 0.024 and validation accuracy is 1983/2000 99.15<br/>training loss is 0.00033 and training accuracy is 22976/23000 99.9<br/>validation loss is 0.024 and validation accuracy is 1984/2000 99.2<br/>training loss is 0.00025 and training accuracy is 22976/23000 99.9<br/>validation loss is 0.024 and validation accuracy is 1984/2000 99.2<br/>training loss is 0.0002 and training accuracy is 22976/23000 99.9<br/>validation loss is 0.025 and validation accuracy is 1985/2000 99.25<br/>training loss is 0.00016 and training accuracy is 22976/23000 99.9<br/>validation loss is 0.024 and validation accuracy is 1986/2000 99.3</pre>
<p>前面的算法能够实现99%的最大训练准确度和99%的验证准确度。您的结果可能会改变，因为您创建的<kbd>validation</kbd>数据集可能有不同的图像。</p>
<p>DenseNet的一些优势包括:</p>
<ul>
<li>它大大减少了所需参数的数量</li>
<li>它缓解了消失梯度问题</li>
<li>它鼓励特性重用</li>
</ul>
<p>在下一节中，我们将探索如何使用ResNet、Inception和DenseNet的不同模型来构建一个模型，该模型结合了计算出的复杂特征的优点。</p>


            

            
        
    </div>


  <div><h1 class="header-title">模型组装</h1>
                
            
            
                
<p>有时候，我们可能需要尝试组合多个模型来构建一个非常强大的模型。有许多技术可用于建立集合模型。在本节中，我们将学习如何使用三个不同模型(ResNet、Inception和DenseNet)生成的特性来组合输出，以构建一个强大的模型。我们将使用本章其他例子中使用的相同数据集。</p>
<p>整体模型的架构如下所示:</p>
<div><img height="375" width="472" class="alignnone size-full wp-image-408 image-border" src="img/203dd83d-e8f6-4786-9e55-a5904274c72d.png"/></div>
<p>此图像显示了我们将在集合模型中执行的操作，这些操作可以总结为以下步骤:</p>
<ol>
<li>创建三个模型</li>
<li>使用创建的模型提取图像特征</li>
<li>创建一个自定义数据集，该数据集返回所有三个模型的特征以及标注</li>
<li>创建类似于上图中架构的模型</li>
<li>训练和验证模型</li>
</ol>
<p>让我们详细探讨一下每个步骤。</p>


            

            
        
    </div>


  <div><h1 class="header-title">创建模型</h1>
                
            
            
                
<p>让我们创建所有三个必需的模型，如下面的代码所示:</p>
<pre>#Create ResNet model<br/>my_resnet = resnet34(pretrained=True)<br/><br/>if is_cuda:<br/>    my_resnet = my_resnet.cuda()<br/><br/>my_resnet = nn.Sequential(*list(my_resnet.children())[:-1])<br/><br/>for p in my_resnet.parameters():<br/>    p.requires_grad = False<br/><br/>#Create inception model<br/><br/>my_inception = inception_v3(pretrained=True)<br/>my_inception.aux_logits = False<br/>if is_cuda:<br/>    my_inception = my_inception.cuda()<br/>for p in my_inception.parameters():<br/>    p.requires_grad = False<br/><br/>#Create densenet model<br/><br/>my_densenet = densenet121(pretrained=True).features<br/>if is_cuda:<br/>    my_densenet = my_densenet.cuda()<br/>    <br/>for p in my_densenet.parameters():<br/>    p.requires_grad = False</pre>
<p>现在我们有了所有的模型，让我们从图像中提取特征。</p>


            

            
        
    </div>


  <div><h1 class="header-title">提取图像特征</h1>
                
            
            
                
<p>这里，我们结合了我们在本章中单独看到的算法的所有逻辑:</p>
<pre>### For ResNet<br/><br/>trn_labels = []<br/>trn_resnet_features = []<br/>for d,la in train_loader:<br/>    o = my_resnet(Variable(d.cuda()))<br/>    o = o.view(o.size(0),-1)<br/>    trn_labels.extend(la)<br/>    trn_resnet_features.extend(o.cpu().data)<br/>val_labels = []<br/>val_resnet_features = []<br/>for d,la in val_loader:<br/>    o = my_resnet(Variable(d.cuda()))<br/>    o = o.view(o.size(0),-1)<br/>    val_labels.extend(la)<br/>    val_resnet_features.extend(o.cpu().data)<br/><br/>### For Inception<br/><br/>trn_inception_features = LayerActivations(my_inception.Mixed_7c)<br/>for da,la in train_loader:<br/>    _ = my_inception(Variable(da.cuda()))<br/><br/>trn_inception_features.remove()<br/><br/>val_inception_features = LayerActivations(my_inception.Mixed_7c)<br/>for da,la in val_loader:<br/>    _ = my_inception(Variable(da.cuda()))<br/><br/>val_inception_features.remove()<br/><br/>### For DenseNet<br/><br/><br/>trn_densenet_features = []<br/>for d,la in train_loader:<br/>    o = my_densenet(Variable(d.cuda()))<br/>    o = o.view(o.size(0),-1)<br/>    <br/>    trn_densenet_features.extend(o.cpu().data)<br/>    <br/><br/>val_densenet_features = []<br/>for d,la in val_loader:<br/>    o = my_densenet(Variable(d.cuda()))<br/>    o = o.view(o.size(0),-1)<br/>    val_densenet_features.extend(o.cpu().data)</pre>
<p>到目前为止，我们已经使用所有模型创建了图像特征。如果您面临内存问题，那么您可以删除其中一个模型，或者停止在内存中存储特征，这可能会导致训练速度变慢。如果您在CUDA实例上运行它，那么您可以使用一个更强大的实例。</p>


            

            
        
    </div>


  <div><h1 class="header-title">创建自定义数据集和数据加载器</h1>
                
            
            
                
<p>我们将不能像现在这样使用<kbd>FeaturesDataset</kbd>类，因为它被开发来仅仅从一个模型的输出中挑选。因此，下面的实现包含了对<kbd>FeaturesDataset</kbd>类的微小修改，以适应所有三种不同的生成特性:</p>
<pre>class FeaturesDataset(Dataset):<br/>    <br/>    def __init__(self,featlst1,featlst2,featlst3,labellst):<br/>        self.featlst1 = featlst1<br/>        self.featlst2 = featlst2<br/>        self.featlst3 = featlst3<br/>        self.labellst = labellst<br/>        <br/>    def __getitem__(self,index):<br/>        return (self.featlst1[index],self.featlst2[index],self.featlst3[index],self.labellst[index])<br/>    <br/>    def __len__(self):<br/>        return len(self.labellst)<br/><br/>trn_feat_dset = FeaturesDataset(trn_resnet_features,trn_inception_features.features,trn_densenet_features,trn_labels)<br/>val_feat_dset = FeaturesDataset(val_resnet_features,val_inception_features.features,val_densenet_features,val_labels)</pre>
<p>我们修改了<kbd>__init__</kbd>方法来存储不同模型生成的所有特征，修改了<kbd>__getitem__</kbd>方法来检索图像的特征和标签。使用<kbd>FeatureDataset</kbd>类，我们为训练和验证数据创建了数据集实例。创建数据集后，我们可以使用相同的数据加载器来批处理数据，如以下代码所示:</p>
<pre>trn_feat_loader = DataLoader(trn_feat_dset,batch_size=64,shuffle=True)<br/>val_feat_loader = DataLoader(val_feat_dset,batch_size=64)</pre>


            

            
        
    </div>


  <div><h1 class="header-title">创建集合模型</h1>
                
            
            
                
<p>我们需要创建一个类似于之前展示的架构图的模型。以下代码实现了这一点:</p>
<pre>class EnsembleModel(nn.Module):<br/>    <br/>    def __init__(self,out_size,training=True):<br/>        super().__init__()<br/>        self.fc1 = nn.Linear(8192,512)<br/>        self.fc2 = nn.Linear(131072,512)<br/>        self.fc3 = nn.Linear(82944,512)<br/>        self.fc4 = nn.Linear(512,out_size)<br/><br/>    def forward(self,inp1,inp2,inp3):<br/>        out1 = self.fc1(F.dropout(inp1,training=self.training))<br/>        out2 = self.fc2(F.dropout(inp2,training=self.training))<br/>        out3 = self.fc3(F.dropout(inp3,training=self.training))<br/>        out = out1 + out2 + out3<br/>        out = self.fc4(F.dropout(out,training=self.training))<br/>        return out<br/><br/>em = EnsembleModel(2)<br/>if is_cuda:<br/>    em = em.cuda()</pre>
<p>在上述代码中，我们创建了三个线性图层，这些图层采用不同模型生成的要素。我们汇总这三个线性层的所有输出，并将它们传递给另一个线性层，后者将它们映射到所需的类别。为了防止模型过度拟合，我们使用了辍学。</p>


            

            
        
    </div>


  <div><h1 class="header-title">训练和验证模型</h1>
                
            
            
                
<p>我们需要对<kbd>fit</kbd>方法做一些小的修改，以适应数据加载器生成的三个输入值。下面的代码实现了新的<kbd>fit</kbd>函数:</p>
<pre>def fit(epoch,model,data_loader,phase='training',volatile=False):<br/>    if phase == 'training':<br/>        model.train()<br/>    if phase == 'validation':<br/>        model.eval()<br/>        volatile=True<br/>    running_loss = 0.0<br/>    running_correct = 0<br/>    for batch_idx , (data1,data2,data3,target) in enumerate(data_loader):<br/>        if is_cuda:<br/>            data1,data2,data3,target = data1.cuda(),data2.cuda(),data3.cuda(),target.cuda()<br/>        data1,data2,data3,target = Variable(data1,volatile),Variable(data2,volatile),Variable(data3,volatile),Variable(target)<br/>        if phase == 'training':<br/>            optimizer.zero_grad()<br/>        output = model(data1,data2,data3)<br/>        loss = F.cross_entropy(output,target)<br/>        <br/>        running_loss += F.cross_entropy(output,target,size_average=False).data[0]<br/>        preds = output.data.max(dim=1,keepdim=True)[1]<br/>        running_correct += preds.eq(target.data.view_as(preds)).cpu().sum()<br/>        if phase == 'training':<br/>            loss.backward()<br/>            optimizer.step()<br/>    <br/>    loss = running_loss/len(data_loader.dataset)<br/>    accuracy = 100. * running_correct/len(data_loader.dataset)<br/>    <br/>    print(f'{phase} loss is {loss:{5}.{2}} and {phase} accuracy is {running_correct}/{len(data_loader.dataset)}{accuracy:{10}.{4}}')<br/>    return loss,accuracy</pre>
<p>从前面的代码中可以看出，除了加载器返回三个输入和一个标签之外，大部分内容保持不变。因此，我们对函数进行了修改，这是不言自明的。</p>
<p>下面的代码显示了培训代码:</p>
<pre>train_losses , train_accuracy = [],[]<br/>val_losses , val_accuracy = [],[]<br/>for epoch in range(1,10):<br/>    epoch_loss, epoch_accuracy = fit(epoch,em,trn_feat_loader,phase='training')<br/>    val_epoch_loss , val_epoch_accuracy = fit(epoch,em,val_feat_loader,phase='validation')<br/>    train_losses.append(epoch_loss)<br/>    train_accuracy.append(epoch_accuracy)<br/>    val_losses.append(val_epoch_loss)<br/>    val_accuracy.append(val_epoch_accuracy)<br/>  </pre>
<p>上述代码的结果如下:</p>
<pre>#Results <br/><br/>training loss is 7.2e+01 and training accuracy is 21359/23000 92.87<br/>validation loss is 6.5e+01 and validation accuracy is 1968/2000 98.4<br/>training loss is 9.4e+01 and training accuracy is 22539/23000 98.0<br/>validation loss is 1.1e+02 and validation accuracy is 1980/2000 99.0<br/>training loss is 1e+02 and training accuracy is 22714/23000 98.76<br/>validation loss is 1.4e+02 and validation accuracy is 1976/2000 98.8<br/>training loss is 7.3e+01 and training accuracy is 22825/23000 99.24<br/>validation loss is 1.6e+02 and validation accuracy is 1979/2000 98.95<br/>training loss is 7.2e+01 and training accuracy is 22845/23000 99.33<br/>validation loss is 2e+02 and validation accuracy is 1984/2000 99.2<br/>training loss is 1.1e+02 and training accuracy is 22862/23000 99.4<br/>validation loss is 4.1e+02 and validation accuracy is 1975/2000 98.75<br/>training loss is 1.3e+02 and training accuracy is 22851/23000 99.35<br/>validation loss is 4.2e+02 and validation accuracy is 1981/2000 99.05<br/>training loss is 2e+02 and training accuracy is 22845/23000 99.33<br/>validation loss is 6.1e+02 and validation accuracy is 1982/2000 99.1<br/>training loss is 1e+02 and training accuracy is 22917/23000 99.64<br/>validation loss is 5.3e+02 and validation accuracy is 1986/2000 99.3</pre>
<p>集成模型达到了99.6%的训练准确率和99.3%的验证准确率。尽管集合模型功能强大，但它们的计算开销很大。当你在像Kaggle这样的比赛中解决问题时，它们是很好的技巧。</p>


            

            
        
    </div>


  <div><h1 class="header-title">编码器-解码器架构</h1>
                
            
            
                
<p>我们在书中看到的几乎所有深度学习算法都善于学习如何将训练数据映射到其对应的标签上。对于模型需要从一个序列中学习并生成另一个序列或图像的任务，我们不能直接使用它们。一些示例应用程序如下:</p>
<ul>
<li>语言翻译</li>
<li>图像字幕</li>
<li>图像生成(seq2img)</li>
<li>语音识别</li>
<li>问题回答</li>
</ul>
<p>这些问题中的大多数可以被视为某种形式的序列到序列映射，这些可以使用一系列称为<strong>编码器-解码器架构</strong>的架构来解决。在本节中，我们将了解这些架构背后的直觉。我们不会关注这些网络的实现，因为它们需要更详细的研究。</p>
<p>概括地说，编码器-解码器架构如下所示:</p>
<div><img height="248" width="411" class="alignnone size-full wp-image-409 image-border" src="img/6ec104a4-761f-4cdd-aac0-21e12af8a215.png"/></div>
<p>编码器通常是一个<strong>递归神经网络</strong> ( <strong> RNN </strong>)(用于序列数据)或<strong>卷积神经网络</strong> ( <strong> CNN </strong>)(用于图像)，它接收图像或序列，并将其转换为编码所有信息的固定长度向量。解码器是另一个RNN或CNN，它学习解码编码器生成的向量，并生成新的数据序列。下图显示了编码器-解码器架构如何寻找图像字幕系统:</p>
<div><img height="200" width="261" src="img/bdb83743-f9c2-489e-af1e-c611523bbfb3.png"/></div>
<p>图像字幕系统的编码器-解码器架构<br/>图像来源:https://arxiv.org/pdf/1411.4555.pdf</p>
<p>让我们更详细地看看图像字幕系统的编码器和解码器架构内部发生了什么。</p>


            

            
        
    </div>


  <div><h1 class="header-title">编码器</h1>
                
            
            
                
<p>对于图像字幕系统，我们将优选地使用训练过的架构，例如ResNet或Inception，用于从图像中提取特征。就像我们对集合模型所做的那样，我们可以通过使用线性层来输出固定的向量长度，然后使该线性层可训练。</p>


            

            
        
    </div>


  <div><h1 class="header-title">解码器</h1>
                
            
            
                
<p>解码器是一个<strong>长短期记忆</strong>(<strong>【LSTM】</strong>)层，它将为图像生成字幕。为了构建一个简单的模型，我们可以只将编码器嵌入作为输入传递给LSTM一次。但是对于解码器来说，学习起来可能很有挑战性；相反，通常的做法是在解码器的每一步都嵌入编码器。直观地说，解码器学习生成最好地描述给定图像的标题的文本序列。</p>


            

            
        
    </div>


  <div><h1 class="header-title">摘要</h1>
                
            
            
                
<p>在这一章中，我们探索了一些现代架构，如ResNet、Inception和DenseNet。我们还探讨了如何将这些模型用于迁移学习和集成，并介绍了支持许多系统的编码器-解码器架构，如语言翻译系统。</p>
<p>在下一章中，我们将总结我们在这本书的学习过程中所取得的成就，并讨论你将何去何从。我们将访问PyTorch上的大量资源，以及一些已经使用PyTorch创建或正在进行研究的很酷的深度学习项目。</p>
<p> </p>


            

            
        
    </div>
</body>
</html>