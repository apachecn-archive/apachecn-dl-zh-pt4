<html xmlns:epub="http://www.idpf.org/2007/ops" xmlns:m="http://www.w3.org/1998/Math/MathML" xmlns:pls="http://www.w3.org/2005/01/pronunciation-lexicon" xmlns:ssml="http://www.w3.org/2001/10/synthesis">
<head>
  <meta charset="UTF-8"/>
  <title>Deep Learning for Computer Vision</title>
  
  
</head>
<body>
  <div><h1 class="header-title">计算机视觉的深度学习</h1>
                
            
            
                
<p class="mce-root CDPAlignLeft CDPAlign">在<a href="2.html" target="_blank">第三章</a>、<em>深入研究神经网络、</em>中，我们使用一种流行的<strong>卷积神经网络</strong> ( <strong> CNN </strong>)架构构建了一个图像分类器，称为<strong> ResNet </strong>，但我们将这个模型用作黑盒。在本章中，我们将讨论卷积网络的重要组成部分。本章中我们将涉及的一些重要主题包括:</p>
<ul>
<li>神经网络导论</li>
<li class="mce-root">从头开始构建CNN模型</li>
<li class="mce-root">创建和探索VGG16模型</li>
<li class="mce-root">计算预卷积特征</li>
<li class="mce-root">理解CNN模型学到了什么</li>
<li>可视化CNN图层的权重</li>
</ul>
<p>我们将探索如何从零开始构建一个架构来解决图像分类问题，这是最常见的用例。我们还将学习如何使用迁移学习，这将有助于我们使用非常小的数据集构建图像分类器。</p>
<p>除了学习如何使用CNN，我们还将探索这些卷积网络学习什么。</p>


            

            
        
    </div>


  <div><h1 class="header-title">神经网络导论</h1>
                
            
            
                
<p>在过去的几年中，CNN在图像识别、对象检测、分割和计算机视觉领域中的许多其他任务中变得流行。它们在自然语言处理领域<strong/>(<strong>NLP</strong>)也越来越受欢迎，尽管它们还没有被普遍使用。完全连接层和卷积层之间的根本区别在于中间层中权重相互连接的方式。让我们来看一张图片，其中我们描绘了完全连接的或线性的层是如何工作的:</p>
<div><img height="237" width="198" class="alignnone size-full wp-image-470 image-border" src="img/b11c5a02-626f-4e4c-a4be-d4404fd31f15.png"/></div>
<p>使用线性层或全连接层进行计算机视觉的最大挑战之一是，它们会丢失所有空间信息，并且全连接层使用的权重数量方面的复杂性太大。例如，当我们将224像素的图像表示为平面阵列时，我们最终会得到150，528 (224 x 224 x 3通道)。当图像变平时，我们失去了所有的空间信息。让我们看看简化版的CNN是什么样子的:</p>
<div><img height="262" width="195" class="alignnone size-full wp-image-471 image-border" src="img/612b9675-1581-47c6-8b47-b07d1e20eef2.png"/></div>
<p>卷积层所做的就是在图像上应用一个叫做<strong>滤镜</strong>的权重窗口。在我们试图详细理解卷积和其他构建模块之前，让我们为<kbd>MNIST</kbd>数据集构建一个简单而强大的图像分类器。一旦我们建立了这个，我们将走过网络的每个组成部分。我们将把构建图像分类器分解为以下步骤:</p>
<ul>
<li>获取数据</li>
<li>创建验证数据集</li>
<li>从头开始构建我们的CNN模型</li>
<li>训练和验证模型</li>
</ul>


            

            
        
    </div>


  <div><h1 class="header-title">MNIST—获取数据</h1>
                
            
            
                
<p><kbd>MNIST</kbd>数据集包含60，000个用于训练的从0到9的手写数字，以及10，000个用于测试集的图像。PyTorch <kbd>torchvision</kbd>库为我们提供了一个<kbd>MNIST</kbd>数据集，它下载数据并以一种易于使用的格式提供数据。让我们使用dataset <kbd>MNIST</kbd>函数将数据集拉到我们的本地机器上，然后用一个<kbd>DataLoader</kbd>将它包装起来。我们将使用torchvision变换将数据转换为PyTorch张量，并进行数据归一化。以下代码负责下载、包装<kbd>DataLoader</kbd>并标准化数据:</p>
<pre>transformation = <br/>  transforms.Compose([transforms.ToTensor(),<br/>  transforms.Normalize((0.1307,), (0.3081,))])<br/><br/>train_dataset = <br/>  datasets.MNIST('data/',train=True,transform=transformation,<br/>    download=True)<br/>test_dataset =  <br/>  datasets.MNIST('data/',train=False,transform=transformation,<br/>    download=True)<br/><br/>train_loader =   <br/>  torch.utils.data.DataLoader(train_dataset,batch_size=32,shuffle=True)<br/>test_loader =  <br/>  torch.utils.data.DataLoader(test_dataset,batch_size=32,shuffle=True)</pre>
<p>因此，前面的代码为<kbd>train</kbd>和<kbd>test</kbd>数据集提供了一个<kbd>DataLoader</kbd>。让我们想象一些图像来理解我们正在处理的事情。以下代码将帮助我们可视化MNIST图像:</p>
<pre>def plot_img(image):<br/>    image = image.numpy()[0]<br/>    mean = 0.1307<br/>    std = 0.3081<br/>    image = ((mean * image) + std)<br/>    plt.imshow(image,cmap='gray')</pre>
<p>现在我们可以通过<kbd>plot_img</kbd>方法来可视化我们的数据集。我们将使用以下代码从<kbd>DataLoader</kbd>中提取一批记录，并绘制图像:</p>
<pre>sample_data = next(iter(train_loader))<br/>plot_img(sample_data[0][1])<br/>plot_img(sample_data[0][2])</pre>
<p>图像如下所示:</p>
<div><img height="252" width="255" class="alignnone size-full wp-image-472 image-border" src="img/66ba4835-005a-43e1-9a0c-283ed4b18c65.png"/>   <img height="252" width="255" class="alignnone size-full wp-image-473 image-border" src="img/92b0f189-a014-4814-96d8-395e79fe00ce.png"/></div>


            

            
        
    </div>


  <div><h1 class="header-title">从头开始构建CNN模型</h1>
                
            
            
                
<p>对于这个例子，让我们从头开始构建我们自己的架构。我们的网络架构将包含不同层的组合，即:</p>
<ul>
<li><kbd>Conv2d</kbd></li>
<li><kbd>MaxPool2d</kbd></li>
<li><strong>整流线性单元</strong> ( <strong> ReLU </strong>)</li>
<li>视角</li>
<li>线性层</li>
</ul>
<p>让我们看一下我们将要实现的架构的图示:</p>
<div><img height="616" width="113" class="alignnone size-full wp-image-474 image-border" src="img/813aed09-39f2-46c2-bfd4-7ce8fd92227c.png"/></div>
<p>让我们在PyTorch中实现这个架构，然后看看每个单独的层是做什么的:</p>
<pre>class Net(nn.Module):<br/>    def __init__(self):<br/>        super().__init__()<br/>        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)<br/>        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)<br/>        self.conv2_drop = nn.Dropout2d()<br/>        self.fc1 = nn.Linear(320, 50)<br/>        self.fc2 = nn.Linear(50, 10)<br/><br/>    def forward(self, x):<br/>        x = F.relu(F.max_pool2d(self.conv1(x), 2))<br/>        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))<br/>        x = x.view(-1, 320)<br/>        x = F.relu(self.fc1(x))<br/>        x = F.dropout(x, training=self.training)<br/>        x = self.fc2(x)<br/>        return F.log_softmax(x)</pre>
<p>我们来详细了解一下每一层是做什么的。</p>


            

            
        
    </div>


  <div><h1 class="header-title">Conv2d</h1>
                
            
            
                
<p>负责对我们的MNIST图像应用卷积滤波器。让我们试着理解卷积是如何应用在一维数组上的，然后转移到二维卷积是如何应用到图像上的。我们将看下面的图像，我们将把一个过滤器(或内核)大小的<kbd>3</kbd>的<kbd>Conv1d</kbd>应用于一个长度为<kbd>7</kbd>的张量:</p>
<p><img height="348" width="701" class="alignnone size-full wp-image-475 image-border" src="img/e4495ddb-9bab-465e-ae9b-0d95e1834f13.png"/></p>
<p>底部的方框表示七个值的输入张量，相连的方框表示应用大小为三的卷积滤波器后的输出。在图像的右上角，三个方框代表了<kbd>Conv1d</kbd>层的权重和参数。卷积滤波器的应用就像一个窗口，它通过跳过一个值来移动到下一个值。要跳过的值的数量称为<strong>步距</strong>，默认设置为<kbd>1</kbd>。让我们通过记下第一个和最后一个输出的计算结果来了解输出值是如何计算的:</p>
<p>输出1-&gt;(<em>-0.5209 x 0.2286</em>)+(<em>-0.0147 x 2.4488</em>)+(<em>-0.4281 x-0.9498</em>)</p>
<p>输出5-&gt;(<em>-0.5209 x-0.6791</em>)+(<em>-0.0147 x-0.6535</em>)+(<em>-0.4281 x 0.6437</em>)</p>
<p>现在，卷积的作用应该很清楚了。它通过基于步幅值移动输入来对输入应用过滤器(或内核)，过滤器是一组权重。在前面的例子中，我们一次移动一个过滤器。如果步幅值是<kbd>2</kbd>，那么我们将一次移动两个点。让我们看一个PyTorch实现来理解它是如何工作的:</p>
<pre>conv = nn.Conv1d(1,1,3,bias=False)<br/>sample = torch.randn(1,1,7)<br/>conv(Variable(sample))<br/><br/>#Check the weights of our convolution filter by <br/>conv.weight</pre>
<p>还有另一个重要的参数，称为<strong>填充，</strong>通常用于卷积。如果我们仔细观察前面的例子，我们可能会意识到，如果直到数据结束时才应用过滤器，当没有足够的元素供数据跨越时，它就会停止。填充通过在张量的两端添加零来防止这种情况。让我们再次看看填充如何工作的一维示例:</p>
<div><img height="348" width="1111" class="alignnone size-full wp-image-476 image-border" src="img/17c7f3b3-c505-4b6d-ad7f-2f0981453a3e.png"/></div>
<p>在前面的图像中，我们用填充<kbd>2</kbd>和步幅<kbd>1</kbd>应用了一个<kbd>Conv1d</kbd>层。</p>
<p>让我们看看Conv2d如何处理图像:</p>
<p>在我们了解Conv2d的工作原理之前，我强烈建议你去看看令人惊叹的博客(<a href="http://setosa.io/ev/image-kernels/">http://setosa.io/ev/image-kernels/</a>)，里面有一个卷积工作原理的现场演示。在您花了几分钟时间玩了演示之后，请阅读下一部分。</p>
<p>让我们了解一下演示中发生了什么。在图像的中间方框中，我们有两组不同的数字；一个在盒子里，另一个在盒子下面。方框中表示的是像素值，如左边照片中的白色方框所示。方框下面的数字是用于锐化图像的过滤器(或内核)值。这些数字是为完成某项特定任务而精心挑选的。在这种情况下，它是锐化图像。就像我们前面的例子一样，我们正在做一个元素到元素的乘法，并对所有值求和，以生成右边图像中的像素值。生成的值通过图像右侧的白色框突出显示。</p>
<p>虽然在这个例子中内核中的值是手工选取的，但是在CNN中我们不手工选取这些值，而是我们随机地初始化它们，并且让梯度下降和反向传播来调整内核的值。学习后的内核将负责识别不同的特征，如直线、曲线和眼睛。让我们看另一个图像，我们把它看作一个数字矩阵，看看卷积是如何工作的:</p>
<div><img height="422" width="1304" class="alignnone size-full wp-image-478 image-border" src="img/e96abcad-3542-4a9e-9d85-92ae5770157a.png"/></div>
<p>在前面的屏幕截图中，我们假设6 x 6矩阵代表一幅图像，我们应用了大小为3 x 3的卷积滤波器，然后我们展示了输出是如何生成的。为了简单起见，我们只计算矩阵中突出显示的部分。通过执行以下计算生成输出:</p>
<p>产量-&gt;<em>0.86 x 0+-0.92 x 0+-0.61 x 1+-0.32 x-1+-1.69 x-1........</em></p>
<p>在<kbd>Conv2d</kbd>函数中使用的另一个重要参数是<kbd>kernel_size</kbd>，它决定了内核的大小。一些常用的内核大小有<em> 1 </em>、<em> 3 </em>、<em> 5 </em>和<em> 7 </em>。内核大小越大，滤波器可以覆盖的区域就越大，因此常见的是将<em> 7 </em>或<em> 9 </em>的滤波器应用于早期层中的输入数据。</p>


            

            
        
    </div>


  <div><h1 class="header-title">联营</h1>
                
            
            
                
<p>在卷积图层之后添加池化图层是一种常见的做法，因为它们会减小要素地图的大小和卷积图层的结果。</p>
<p>池提供了两个不同的功能:一个是减少要处理的数据的大小，另一个是迫使算法不要关注图像位置的微小变化。例如，人脸检测算法应该能够检测照片中的人脸，而不管人脸在照片中的位置如何。</p>
<p>我们来看看MaxPool2d是如何工作的。它也有相同的内核大小和步幅的概念。它不同于卷积，因为它没有任何权重，只作用于每个过滤器从前一层生成的数据。如果内核大小是<em> 2 x 2，</em>，那么它会考虑图像中的大小并选择该区域的最大值。让我们看看下面的图像，它将清楚地表明MaxPool2d是如何工作的:</p>
<p><img height="222" width="463" class="alignnone size-full wp-image-479 image-border" src="img/eeda68e5-57bb-4c9e-87f3-41da57355140.png"/></p>
<p>左侧的方框包含特征映射的值。应用最大池后，输出存储在盒子的右侧。让我们看看输出是如何计算的，在输出的第一行中写下值的计算结果:</p>
<div><img height="15" width="211" class="fm-editor-equation" src="img/8f3e3104-a958-4656-958f-6c4f271a4836.png"/></div>
<div><img height="14" width="215" class="fm-editor-equation" src="img/8527235d-3368-410b-9830-5719fc95d708.png"/></div>
<p>另一种常用的汇集技术是<strong>平均汇集</strong>。<kbd>maximum</kbd>功能被<kbd>average</kbd>功能取代。下图解释了平均池的工作原理:</p>
<div><img height="196" width="407" class="alignnone size-full wp-image-480 image-border" src="img/4bfbbbc6-3420-4fab-8ff7-787313489ecb.png"/></div>
<p>在这个例子中，我们不是取四个值的最大值，而是取四个值的平均值。让我们把计算写下来，这样更容易理解:</p>
<div><img height="18" width="242" class="fm-editor-equation" src="img/3fa94b1a-b075-4415-8484-5444b6cae1c4.png"/></div>
<div><img height="16" width="247" class="fm-editor-equation" src="img/564447b0-5d9f-432d-abe0-7bf68bbe1c54.png"/></div>


            

            
        
    </div>


  <div><h1 class="header-title">非线性激活–ReLU</h1>
                
            
            
                
<p>在最大池化之后，或者在应用卷积之后，有一个非线性层是常见的和最佳的实践。大多数网络架构倾向于使用ReLU或不同风格的ReLU。无论我们选择什么非线性函数，它都会应用到特征图的每个元素。为了更直观，让我们看一个例子，在这个例子中，我们对应用了最大池和平均池的相同特征映射应用ReLU:</p>
<div><img height="147" width="385" class="alignnone size-full wp-image-481 image-border" src="img/bd13c8fa-adf1-436f-b1ce-83027ebc2df4.png"/></div>


            

            
        
    </div>


  <div><h1 class="header-title">视角</h1>
                
            
            
                
<p>对于影像分类问题，通常在大多数网络的末端使用全连通或线性图层。我们使用的是二维卷积，它将一个数字矩阵作为输入，输出另一个数字矩阵。要应用线性层，我们需要将二维张量的矩阵展平为一维向量。以下示例将向您展示<kbd>view</kbd>是如何工作的:</p>
<p><img height="100" width="327" class="alignnone size-full wp-image-482 image-border" src="img/b74f354c-6690-4b8a-96fc-fad0537ac50d.png"/></p>
<p>让我们来看一下我们的网络中使用的代码，它具有同样的功能:</p>
<pre>x.view(-1, 320)</pre>
<p>正如我们之前看到的，<kbd>view</kbd>方法将把一个<em> n </em>维张量展平为一个一维张量。在我们的网络中，第一维是每个图像。批处理后的输入数据将具有尺寸<em>32×1×28×28，</em>，其中第一个数字<em> 32，</em>将表示存在尺寸<em> 28 </em>高、<em> 28 </em>宽、<em> 1 </em>通道的<em> 32 </em>图像，因为它是黑白图像。当我们展平时，我们不想展平或混合不同图像的数据。因此，我们传递给<kbd>view</kbd>函数的第一个参数将指示PyTorch避免在第一维上展平数据。让我们在下图中看看这是如何工作的:</p>
<p><img height="180" width="277" class="alignnone size-full wp-image-483 image-border" src="img/f0a8f523-4a29-4ee8-8ef0-a10d54b777df.png"/></p>
<p>在前面的例子中，我们有大小为<em> 2 x 1 x 2 x 2 </em>的数据；在我们应用了<kbd>view</kbd>函数之后，它被转换成大小为<em> 2 x 1 x 4 </em>的张量。让我们来看看另一个我们没有提到<em> - 1 </em>的例子:</p>
<div><img height="162" width="345" class="alignnone size-full wp-image-484 image-border" src="img/73b8a6ca-1268-4c68-a0c2-8a27606a991e.png"/></div>
<p>如果我们忘记提到要展平哪个维度，我们可能会得到意想不到的结果。所以在这一步要格外小心。</p>


            

            
        
    </div>


  <div><h1 class="header-title">线性层</h1>
                
            
            
                
<p>在我们将数据从二维张量转换为一维张量后，我们将数据通过一个线性层，然后是一个非线性激活层。在我们的架构中，我们有两个线性层；一个后跟ReLU，另一个后跟一个<kbd>log_softmax</kbd>，它预测给定图像中包含什么数字。</p>


            

            
        
    </div>


  <div><h1 class="header-title">训练模型</h1>
                
            
            
                
<p class="mce-root">训练模型的过程和我们之前的猫狗图像分类问题是一样的。以下代码片段在提供的数据集上对我们的模型进行了训练:</p>
<pre>def fit(epoch,model,data_loader,phase='training',volatile=False):<br/>    if phase == 'training':<br/>        model.train()<br/>    if phase == 'validation':<br/>        model.eval()<br/>        volatile=True<br/>    running_loss = 0.0<br/>    running_correct = 0<br/>    for batch_idx , (data,target) in enumerate(data_loader):<br/>        if is_cuda:<br/>            data,target = data.cuda(),target.cuda()<br/>        data , target = Variable(data,volatile),Variable(target)<br/>        if phase == 'training':<br/>            optimizer.zero_grad()<br/>        output = model(data)<br/>        loss = F.nll_loss(output,target)<br/>        <br/>        running_loss += F.nll_loss(output,target,size_average=False).data[0]<br/>        preds = output.data.max(dim=1,keepdim=True)[1]<br/>        running_correct += preds.eq(target.data.view_as(preds)).cpu().sum()<br/>        if phase == 'training':<br/>            loss.backward()<br/>            optimizer.step()<br/>    <br/>    loss = running_loss/len(data_loader.dataset)<br/>    accuracy = 100. * running_correct/len(data_loader.dataset)<br/>    <br/>    print(f'{phase} loss is {loss:{5}.{2}} and {phase} accuracy is {running_correct}/{len(data_loader.dataset)}{accuracy:{10}.{4}}')<br/>    return loss,accuracy</pre>
<p>这个方法对于<kbd>training</kbd>和<kbd>validation</kbd>有不同的逻辑。使用不同模式主要有两个原因:</p>
<ul>
<li>在<kbd>train</kbd>模式中，删除会删除一定百分比的值，这在验证或测试阶段是不应该发生的</li>
<li>对于<kbd>training</kbd>模式，我们计算梯度并改变模型的参数值，但是在测试或验证阶段不需要反向传播</li>
</ul>
<p>如前几章所述，前面函数中的大部分代码都是不言自明的。在函数的最后，我们返回该特定时期的模型的<kbd>loss</kbd>和<kbd>accuracy</kbd>。</p>
<p>让我们通过前面的函数运行模型20次迭代，并绘制出<kbd>train</kbd>和<kbd>validation</kbd>的<kbd>loss</kbd>和<kbd>accuracy</kbd>，以了解我们的网络表现如何。以下代码为<kbd>20</kbd>迭代的<kbd>train</kbd>和<kbd>test</kbd>数据集运行<kbd>fit</kbd>方法:</p>
<pre>model = Net()<br/>if is_cuda:<br/>    model.cuda()<br/><br/>optimizer = optim.SGD(model.parameters(),lr=0.01,momentum=0.5)<br/>train_losses , train_accuracy = [],[]<br/>val_losses , val_accuracy = [],[]<br/>for epoch in range(1,20):<br/>    epoch_loss, epoch_accuracy = fit(epoch,model,train_loader,phase='training')<br/>    val_epoch_loss , val_epoch_accuracy = fit(epoch,model,test_loader,phase='validation')<br/>    train_losses.append(epoch_loss)<br/>    train_accuracy.append(epoch_accuracy)<br/>    val_losses.append(val_epoch_loss)<br/>    val_accuracy.append(val_epoch_accuracy)</pre>
<p>以下代码绘制了<kbd>training</kbd>和<kbd>test loss</kbd>:</p>
<pre>plt.plot(range(1,len(train_losses)+1),train_losses,'bo',label = 'training loss')<br/>plt.plot(range(1,len(val_losses)+1),val_losses,'r',label = 'validation loss')<br/>plt.legend()</pre>
<p class="mce-root CDPAlignLeft CDPAlign">上述代码生成了下图:</p>
<div><img height="235" width="349" class="alignnone size-full wp-image-485 image-border" src="img/9a1a319b-9b66-4479-a4d2-014ded3303c5.png"/></div>
<p>以下代码绘制了训练和测试精度:</p>
<pre>plt.plot(range(1,len(train_accuracy)+1),train_accuracy,'bo',label = 'train accuracy')<br/>plt.plot(range(1,len(val_accuracy)+1),val_accuracy,'r',label = 'val accuracy')<br/>plt.legend()</pre>
<p>上述代码生成了下图:</p>
<p><img height="231" width="348" class="alignnone size-full wp-image-486 image-border" src="img/3a360060-04a5-41f8-8dcf-bffc2a6229c0.png"/></p>
<p>在第20个<sup>周期</sup>结束时，我们达到了98.9%的<kbd>test</kbd>准确度。我们已经得到了我们的简单卷积模型的工作，并几乎达到了最先进的结果。让我们看看当我们在以前使用的<kbd>Dogs vs. Cats</kbd>数据集上尝试相同的网络架构时会发生什么。我们将使用上一章的数据，第3章<a href="2.html" target="_blank">、</a>、<em>神经网络的构建模块、</em>以及MNIST示例中的架构，并做一些小的改动。一旦我们训练了模型，让我们评估它以理解我们的简单架构执行得有多好。</p>


            

            
        
    </div>


  <div><h1 class="header-title">给狗和猫分类——CNN从零开始</h1>
                
            
            
                
<p>我们将使用相同的体系结构，但做了一些小的改动，如下所示:</p>
<ul>
<li>第一个线性层的输入尺寸改变了，因为我们的猫和狗图像的尺寸是<em> 256，256 </em></li>
<li>我们添加了另一个线性层，为模型学习提供更多的灵活性</li>
</ul>
<p>让我们看看实现网络架构的代码:</p>
<pre>class Net(nn.Module):<br/>    def __init__(self):<br/>        super().__init__()<br/>        self.conv1 = nn.Conv2d(3, 10, kernel_size=5)<br/>        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)<br/>        self.conv2_drop = nn.Dropout2d()<br/>        self.fc1 = nn.Linear(56180, 500)<br/>        self.fc2 = nn.Linear(500,50)<br/>        self.fc3 = nn.Linear(50, 2)<br/><br/>    def forward(self, x):<br/>        x = F.relu(F.max_pool2d(self.conv1(x), 2))<br/>        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))<br/>        x = x.view(x.size(0),-1)<br/>        x = F.relu(self.fc1(x))<br/>        x = F.dropout(x, training=self.training)<br/>        x = F.relu(self.fc2(x))<br/>        x = F.dropout(x,training=self.training)<br/>        x = self.fc3(x)<br/>        return F.log_softmax(x,dim=1)</pre>
<p>我们将使用用于MNIST例子的相同的<kbd>training</kbd>函数。所以，我不包括这里的代码。但是让我们看看当模型被训练用于<em> 20 </em>次迭代时生成的图。</p>
<p><kbd>training</kbd>和<kbd>validation</kbd>数据集丢失:</p>
<p><img height="252" width="375" class="alignnone size-full wp-image-487 image-border" src="img/68c556c7-c801-401d-b600-3f07b15e9436.png"/></p>
<p><kbd>training</kbd>和<kbd>validation</kbd>数据集的精确度:</p>
<div><img height="252" width="372" class="alignnone size-full wp-image-488 image-border" src="img/25496cf6-3032-4ba7-8a33-22d3e6202e79.png"/></div>
<p>从图中可以清楚地看出，每次迭代的训练损失都在减少，但是验证损失却越来越严重。准确度在训练期间也增加，但是在75%时几乎饱和。这是一个很明显的例子，这个模型并没有推广。我们将看看另一种称为<strong>迁移学习</strong>的技术，它帮助我们训练更精确的模型，并提供一些技巧来加快训练速度。</p>


            

            
        
    </div>


  <div><h1 class="header-title">利用迁移学习对狗和猫进行分类</h1>
                
            
            
                
<p class="mce-root">迁移学习是一种在类似数据集上重用已训练算法的能力，而无需从头开始训练。我们人类不会通过分析数以千计的相似图像来学习识别新图像。作为人类，我们只知道区分特定动物的不同特征，比如狐狸和狗。我们不需要通过理解线条、眼睛和其他更小的特征来了解狐狸是什么。因此，我们将学习如何使用一个预训练的模型，用非常少的数据来构建最先进的图像分类器。</p>
<p>CNN架构的前几层关注较小的特征，例如直线或曲线的外观。CNN后面几层中的过滤器学习更高层次的特征，例如眼睛和手指，最后几层学习识别确切的类别。预训练模型是在类似数据集上训练的算法。大多数流行的算法都在流行的<kbd>ImageNet</kbd>数据集上进行了预训练，以识别1000个不同的类别。这种预先训练的模型将具有被调整以识别各种模式的过滤器权重。那么，让我们来了解如何利用这些预先训练好的重量。我们将研究一种叫做<strong> VGG16 </strong>的算法，这是最早在ImageNet竞赛中获得成功的算法之一。尽管有更多的现代算法，这种算法仍然很受欢迎，因为它易于理解和用于迁移学习。让我们看看VGG16模型的架构，然后尝试理解该架构以及我们如何使用它来训练我们的图像分类器:</p>
<div><img height="632" width="325" class="alignnone size-full wp-image-601 image-border" src="img/f76292fc-f90a-4a8e-92c0-328236a9d09d.png"/></div>
<p>VGG16模型的架构</p>
<p>VGG16架构包含五个VGG模块。一个块是一组卷积层、一个非线性激活函数和一个最大汇集函数。所有算法参数都经过调整，以实现对1，000个类别进行分类的最先进结果。该算法采用批处理形式的输入数据，这些数据通过<kbd>ImageNet</kbd>数据集的平均值和标准偏差进行标准化。在迁移学习中，我们试图通过冻结架构大多数层的学习参数来捕捉算法学习的内容。通常只对网络的最后几层进行微调。在这个例子中，让我们只训练最后几个线性层，并保持卷积层不变，因为卷积特征学习的特征主要用于图像共享相似属性的各种图像问题。让我们用迁移学习训练一个VGG16模型，对狗和猫进行分类。让我们来看看实现这一点所需的不同步骤。</p>


            

            
        
    </div>


  <div><h1 class="header-title">创建和探索VGG16模型</h1>
                
            
            
                
<p>PyTorch在其<kbd>torchvision</kbd>库中提供了一组经过训练的模型。他们中的大多数人接受一个名为<kbd>pretrained</kbd> when <kbd>True</kbd>的论点，该论点下载为<strong> ImageNet </strong>分类问题调整的权重。让我们看看创建VGG16模型的代码片段:</p>
<pre>from torchvision import models<br/>vgg = models.vgg16(pretrained=True)</pre>
<p>现在，我们有了VGG16模型，所有预先训练好的砝码都已准备就绪，可以使用了。第一次运行代码时，可能需要几分钟，这取决于您的网速。权重的大小可能在500 MB左右。我们可以通过打印来快速浏览一下VGG16模型。当我们使用现代架构时，理解这些网络是如何实现的变得非常有用。让我们来看看这个模型:</p>
<pre>VGG (
  (features): Sequential (
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU (inplace)
    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (3): ReLU (inplace)
    (4): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))
    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (6): ReLU (inplace)
    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (8): ReLU (inplace)
    (9): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))
    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (11): ReLU (inplace)
    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (13): ReLU (inplace)
    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (15): ReLU (inplace)
    (16): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))
    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (18): ReLU (inplace)
    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (20): ReLU (inplace)
    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (22): ReLU (inplace)
    (23): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))
    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (25): ReLU (inplace)
    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (27): ReLU (inplace)
    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (29): ReLU (inplace)
    (30): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))
  )
  (classifier): Sequential (
    (0): Linear (25088 -&gt; 4096)
    (1): ReLU (inplace)
    (2): Dropout (p = 0.5)
    (3): Linear (4096 -&gt; 4096)
    (4): ReLU (inplace)
    (5): Dropout (p = 0.5)
    (6): Linear (4096 -&gt; 1000)
  )
)</pre>
<p>模型摘要包含两个顺序模型<kbd>features</kbd>和<kbd>classifiers</kbd>。<kbd>features sequential</kbd>模型有我们将要冻结的层。</p>


            

            
        
    </div>


  <div><h1 class="header-title">冷冻各层</h1>
                
            
            
                
<p>让我们冻结<kbd>features</kbd>模型的所有层，它包含卷积块。冻结层中的权重将防止这些卷积块的权重。由于模型的权重被训练来识别许多重要的特征，我们的算法将能够从第一次迭代开始就做同样的事情。使用模型权重的能力被称为<strong>迁移学习</strong>，这些权重最初是为不同的用例而训练的。现在让我们看看如何冻结层的权重或参数:</p>
<pre>for param in vgg.features.parameters(): param.requires_grad = False</pre>
<p>这段代码阻止优化器更新权重。</p>


            

            
        
    </div>


  <div><h1 class="header-title">微调VGG16</h1>
                
            
            
                
<p>VGG16模型被训练来分类1000个类别，但没有被训练来分类狗和猫。所以，我们需要把最后一层的输出特征从<kbd>1000</kbd>改成<kbd>2</kbd>。下面的代码片段可以做到这一点:</p>
<pre>vgg.classifier[6].out_features = 2</pre>
<p><kbd>vgg.classifier</kbd>提供了对顺序模型中所有层的访问，第六个元素将包含最后一层。当我们训练VGG16模型时，我们只需要训练分类器参数。因此，我们只将<kbd>classifier.parameters</kbd>传递给优化器，如下所示:</p>
<pre>optimizer = <br/>  optim.SGD(vgg.classifier.parameters(),lr=0.0001,momentum=0.5)</pre>


            

            
        
    </div>


  <div><h1 class="header-title">培训VGG16模型</h1>
                
            
            
                
<p>我们已经创建了模型和优化器。由于我们使用的是<kbd>Dogs vs. Cats</kbd>数据集，我们可以使用相同的数据加载器和<kbd>train</kbd>函数来训练我们的模型。请记住，当我们训练模型时，只有分类器内部的参数会发生变化。以下代码片段将为<kbd>20</kbd>时代训练模型，达到98.45%的验证准确率:</p>
<pre>train_losses , train_accuracy = [],[]<br/>val_losses , val_accuracy = [],[]<br/>for epoch in range(1,20):<br/>    epoch_loss, epoch_accuracy = fit(epoch,vgg,train_data_loader,phase='training')<br/>    val_epoch_loss , val_epoch_accuracy = fit(epoch,vgg,valid_data_loader,phase='validation')<br/>    train_losses.append(epoch_loss)<br/>    train_accuracy.append(epoch_accuracy)<br/>    val_losses.append(val_epoch_loss)<br/>    val_accuracy.append(val_epoch_accuracy)</pre>
<p>让我们想象一下培训和验证的损失:</p>
<p><img height="252" width="382" class="alignnone size-full wp-image-490 image-border" src="img/f8d58fda-2db4-4327-9ebc-f4a1a7a02416.png"/></p>
<p>让我们想象一下培训和验证的准确性:</p>
<p><img height="252" width="382" class="alignnone size-full wp-image-491 image-border" src="img/914a6a81-d15d-45db-8e35-bebf67f2ccb8.png"/></p>
<p>我们可以运用一些技巧，比如增加数据和使用不同的脱落值来提高模型的泛化能力。下面的代码片段将VGG分类器模块中的脱落值从<kbd>0.5</kbd>更改为<kbd>0.2</kbd>，并训练模型:</p>
<pre>for layer in vgg.classifier.children():<br/>    if(type(layer) == nn.Dropout):<br/>        layer.p = 0.2<br/><br/>#Training<br/>train_losses , train_accuracy = [],[]<br/>val_losses , val_accuracy = [],[]<br/>for epoch in range(1,3):<br/>    epoch_loss, epoch_accuracy = fit(epoch,vgg,train_data_loader,phase='training')<br/>    val_epoch_loss , val_epoch_accuracy = fit(epoch,vgg,valid_data_loader,phase='validation')<br/>    train_losses.append(epoch_loss)<br/>    train_accuracy.append(epoch_accuracy)<br/>    val_losses.append(val_epoch_loss)<br/>    val_accuracy.append(val_epoch_accuracy)</pre>
<p>把这个训练几个时代让我有了一点进步；你可以尝试使用不同的辍学率值。提高模型泛化能力的另一个重要技巧是添加更多数据或进行数据扩充。我们将通过随机水平翻转图像或以小角度旋转图像来增加数据。<kbd>torchvision</kbd>变换为数据增强提供了不同的功能，而且它们是动态的，每个时代都在变化。我们使用以下代码实现数据增强:</p>
<pre class="mce-root">train_transform =transforms.Compose([transforms.Resize((224,224)),<br/>                                     transforms.RandomHorizontalFlip(),<br/>                                     transforms.RandomRotation(0.2),<br/>                                     transforms.ToTensor(),<br/>                                     transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])<br/>                                      ])<br/><br/>train = ImageFolder('dogsandcats/train/',train_transform)<br/>valid = ImageFolder('dogsandcats/valid/',simple_transform)<br/><br/>#Training <br/><br/>train_losses , train_accuracy = [],[]<br/>val_losses , val_accuracy = [],[]<br/>for epoch in range(1,3):<br/>    epoch_loss, epoch_accuracy = fit(epoch,vgg,train_data_loader,phase='training')<br/>    val_epoch_loss , val_epoch_accuracy = fit(epoch,vgg,valid_data_loader,phase='validation')<br/>    train_losses.append(epoch_loss)<br/>    train_accuracy.append(epoch_accuracy)<br/>    val_losses.append(val_epoch_loss)<br/>    val_accuracy.append(val_epoch_accuracy)</pre>
<p class="mce-root">上述代码的输出生成如下:</p>
<pre>#Results<br/><br/>training loss is 0.041 and training accuracy is 22657/23000 98.51 validation loss is 0.043 and validation accuracy is 1969/2000 98.45 training loss is 0.04 and training accuracy is 22697/23000 98.68 validation loss is 0.043 and validation accuracy is 1970/2000 98.5</pre>
<p>用增广数据训练模型只运行了两个时期，模型精度提高了0.1%；我们可以把它再运行几个时代来进一步改进。如果您在阅读本书的同时一直在训练这些模型，您将会意识到，根据您运行的图形处理器，每个时期的训练可能需要几分钟以上。让我们看看一种技术，它可以在几秒钟内训练每个纪元。</p>


            

            
        
    </div>


  <div><h1 class="header-title">计算预卷积特征</h1>
                
            
            
                
<p>当我们冻结卷积层和训练模型时，对完全连接的层或密集层的输入总是相同的(<kbd>vgg.classifier</kbd>)。为了更好地理解，让我们将卷积块，在我们的示例中为<kbd>vgg.features</kbd>块，视为已学习权重且在训练期间不变的函数。因此，计算卷积特征并存储起来将有助于我们提高训练速度。训练模型的时间减少，因为我们只计算这些特征一次，而不是计算每个纪元。让我们直观地理解并实现相同的内容:</p>
<div><img height="463" width="754" class="alignnone size-full wp-image-492 image-border" src="img/d1cb4fa0-d566-42fb-8b27-f45abadeca02.png"/></div>
<p>第一个框描述了一般如何进行训练，这可能会很慢，因为我们计算每个时期的卷积特征，尽管这些值不会改变。在底部的框中，我们计算一次卷积特征，并且只训练线性层。为了计算预卷积特征，我们将通过卷积块传递所有训练数据并存储它们。为此，我们需要选择VGG模型的卷积模块。幸运的是，VGG16的PyTorch实现有两个顺序模型，因此只需挑选第一个顺序模型的特性就足够了。下面的代码可以做到这一点:</p>
<pre>vgg = models.vgg16(pretrained=True)<br/>vgg = vgg.cuda()<br/>features = vgg.features<br/><br/>train_data_loader = torch.utils.data.DataLoader(train,batch_size=32,num_workers=3,shuffle=False)<br/>valid_data_loader = torch.utils.data.DataLoader(valid,batch_size=32,num_workers=3,shuffle=False)<br/><br/>def preconvfeat(dataset,model):<br/>    conv_features = []<br/>    labels_list = []<br/>    for data in dataset:<br/>        inputs,labels = data<br/>        if is_cuda:<br/>            inputs , labels = inputs.cuda(),labels.cuda() <br/>        inputs , labels = Variable(inputs),Variable(labels)<br/>        output = model(inputs)<br/>        conv_features.extend(output.data.cpu().numpy())<br/>        labels_list.extend(labels.data.cpu().numpy())<br/>    conv_features = np.concatenate([[feat] for feat in conv_features])<br/>    <br/>    return (conv_features,labels_list)<br/><br/>conv_feat_train,labels_train = preconvfeat(train_data_loader,features)<br/>conv_feat_val,labels_val = preconvfeat(valid_data_loader,features)</pre>
<p>在前面的代码中，<kbd>preconvfeat</kbd>方法接收数据集和<kbd>vgg</kbd>模型，并返回复杂的特征以及与之关联的标签。其余的代码与我们在其他示例中创建数据加载器和数据集时使用的代码相似。</p>
<p>一旦我们有了<kbd>train</kbd>和<kbd>validation</kbd>集合的卷积特征，让我们创建一个PyTorch数据集和<kbd>DataLoader</kbd>类，这将简化我们的训练过程。以下代码为我们的卷积特性创建了<kbd>Dataset</kbd>和<kbd>DataLoader</kbd>:</p>
<pre>class My_dataset(Dataset):<br/>    def __init__(self,feat,labels):<br/>        self.conv_feat = feat<br/>        self.labels = labels<br/>    <br/>    def __len__(self):<br/>        return len(self.conv_feat)<br/>    <br/>    def __getitem__(self,idx):<br/>        return self.conv_feat[idx],self.labels[idx]<br/><br/>train_feat_dataset = My_dataset(conv_feat_train,labels_train)<br/>val_feat_dataset = My_dataset(conv_feat_val,labels_val)<br/><br/>train_feat_loader = <br/>  DataLoader(train_feat_dataset,batch_size=64,shuffle=True)<br/>val_feat_loader = <br/>  DataLoader(val_feat_dataset,batch_size=64,shuffle=True)</pre>
<p>因为我们有了新的数据加载器，可以生成批量复杂的特征和标签，所以我们可以使用在其他示例中使用的相同的<kbd>train</kbd>函数。现在我们将使用<kbd>vgg.classifier</kbd>作为创建<kbd>optimizer</kbd>和<kbd>fit</kbd>方法的模型。下面的代码训练分类器模块来识别狗和猫。在Titan X GPU上，每个历元不到5秒，否则需要几分钟:</p>
<pre>train_losses , train_accuracy = [],[]<br/>val_losses , val_accuracy = [],[]<br/>for epoch in range(1,20):<br/>    epoch_loss, epoch_accuracy = fit_numpy(epoch,vgg.classifier,train_feat_loader,phase='training')<br/>    val_epoch_loss , val_epoch_accuracy = fit_numpy(epoch,vgg.classifier,val_feat_loader,phase='validation')<br/>    train_losses.append(epoch_loss)<br/>    train_accuracy.append(epoch_accuracy)<br/>    val_losses.append(val_epoch_loss)<br/>    val_accuracy.append(val_epoch_accuracy)</pre>


            

            
        
    </div>


  <div><h1 class="header-title">理解CNN模型学到了什么</h1>
                
            
            
                
<p>深度学习模型经常被认为是不可解释的。但是人们正在探索不同的技术来解释这些模型内部发生了什么。对于图像，修道院所学的特征是可以解释的。我们将探讨两种流行的技术来理解修道院。</p>


            

            
        
    </div>


  <div><h1 class="header-title">可视化中间层的输出</h1>
                
            
            
                
<p>可视化中间层的输出将有助于我们理解输入图像如何在不同层之间转换。通常，每一层的输出被称为<strong>激活</strong>。要做到这一点，我们应该从中间层提取输出，这可以通过不同的方式来完成。PyTorch提供了一个叫做<kbd>register_forward_hook</kbd>的方法，它允许我们传递一个可以提取特定层的输出的函数。</p>
<p>默认情况下，PyTorch模型只存储最后一层的输出，以优化使用内存。因此，在我们检查中间层的激活看起来像什么之前，让我们理解如何从模型中提取输出。让我们看看下面的代码片段，它提取了，我们将通过它来了解发生了什么:</p>
<pre>vgg = models.vgg16(pretrained=True).cuda()<br/><br/>class LayerActivations():<br/>    features=None<br/>    <br/>    def __init__(self,model,layer_num):<br/>        self.hook = model[layer_num].register_forward_hook(self.hook_fn)<br/>    <br/>    def hook_fn(self,module,input,output):<br/>        self.features = output.cpu()<br/>    <br/>    def remove(self):<br/>        self.hook.remove()<br/>        <br/><br/>conv_out = LayerActivations(vgg.features,0)<br/><br/>o = vgg(Variable(img.cuda()))<br/><br/>conv_out.remove()<br/><br/>act = conv_out.features</pre>
<p>我们从创建预训练的VGG模型开始，从中提取特定层的输出。<kbd>LayerActivations</kbd>类指示PyTorch将层的输出存储到<kbd>features</kbd>变量中。让我们遍历一下<kbd>LayerActivations</kbd>类中的每个函数。</p>
<p><kbd>_init_</kbd>函数将需要提取输出的模型和层数作为参数。我们在层上调用<kbd>register_forward_hook</kbd>方法，并传入一个函数。PyTorch在向前传递时——也就是说，当图像通过各层时——调用传递给<kbd>register_forward_hook</kbd>方法的函数。该方法返回一个句柄，该句柄可用于注销传递给<kbd>register_forward_hook</kbd>方法的函数。</p>
<p><kbd>register_forward_hook</kbd>方法向我们传递给它的函数传递三个值。<kbd>module</kbd>参数允许我们访问层本身。第二个参数是<kbd>input</kbd>，指的是流经该层的数据。第三个参数是<kbd>output</kbd>，它允许访问层的转换输入或激活。我们将输出存储到<kbd>LayerActivations</kbd>类的特征变量中。</p>
<p>第三个函数从<kbd>_init_</kbd>函数中取出<kbd>hook</kbd>并取消注册该函数。现在我们可以传递我们正在寻找激活的模型和层号。让我们看看为不同层的下图创建的激活:</p>
<p><img height="280" width="290" class="alignnone size-full wp-image-493 image-border" src="img/4869b190-ad00-4e28-b4d9-2f6c06447aff.png"/></p>
<p>让我们来看看第一个卷积层创建的一些激活和用于它的代码:</p>
<pre>fig = plt.figure(figsize=(20,50))<br/>fig.subplots_adjust(left=0,right=1,bottom=0,top=0.8,hspace=0,<br/>  wspace=0.2)<br/>for i in range(30):<br/>    ax = fig.add_subplot(12,5,i+1,xticks=[],yticks=[])<br/>    ax.imshow(act[0][i])</pre>
<p>让我们来看看第五卷积层创建的一些激活:</p>
<div><img height="406" width="404" class="alignnone size-full wp-image-494 image-border" src="img/612e41be-0616-4e24-b63e-f875afe02f56.png"/></div>
<p>让我们看看最后一层CNN:</p>
<div><img height="408" width="405" class="alignnone size-full wp-image-495 image-border" src="img/50a60d05-6b3a-4b58-a811-716c608ffe5c.png"/></div>
<p>通过查看不同层生成的内容，我们可以看到，早期的层检测线条和边缘，最后的层倾向于学习更高级别的特征，可解释性更差。在我们继续可视化权重之前，让我们看看在ReLU层之后，功能映射或激活是如何表示其自身的。那么，让我们来看看第二层的输出。</p>
<p>如果您快速查看前面图像第二行中的第五幅图像，看起来滤镜正在检测图像中的眼睛。当模型不执行时，这些可视化的技巧可以帮助我们理解为什么模型可能不工作。</p>


            

            
        
    </div>


  <div><h1 class="header-title">可视化CNN图层的权重</h1>
                
            
            
                
<p>获取特定层的模型权重非常简单。所有的模型重量都可以通过<kbd>state_dict</kbd>功能访问。<kbd>state_dict</kbd>函数返回一个字典，以关键字作为其层，以权重作为其值。以下代码演示了如何提取特定图层的权重并对其进行可视化:</p>
<pre>vgg.state_dict().keys()<br/>cnn_weights = vgg.state_dict()['features.0.weight'].cpu()</pre>
<p>上述代码为我们提供了以下输出:</p>
<div><img height="167" width="454" class="alignnone size-full wp-image-496 image-border" src="img/bf5cc2da-6e75-49fa-9e3a-ee8f83c1ad2c.png"/></div>
<p>每个盒子代表尺寸为<em>3×3</em>的过滤器的重量。每个过滤器被训练来识别图像中的特定模式。</p>


            

            
        
    </div>


  <div><h1 class="header-title">摘要</h1>
                
            
            
                
<p>在这一章中，我们学习了如何使用convents构建图像分类器，以及如何使用预训练的模型。我们讲述了如何通过使用这些预先复杂的特性来加速训练过程的技巧。此外，我们了解了不同的技术，可以用来了解CNN内部发生了什么。</p>
<p>在下一章，我们将学习如何使用递归神经网络处理顺序数据。</p>


            

            
        
    </div>
</body>
</html>