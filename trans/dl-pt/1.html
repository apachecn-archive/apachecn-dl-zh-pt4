<html xmlns:epub="http://www.idpf.org/2007/ops" xmlns:m="http://www.w3.org/1998/Math/MathML" xmlns:pls="http://www.w3.org/2005/01/pronunciation-lexicon" xmlns:ssml="http://www.w3.org/2001/10/synthesis">
<head>
  <meta charset="UTF-8"/>
  <title>Getting Started with Deep Learning Using PyTorch</title>
  
  
</head>
<body>
  <div><h1 class="header-title">使用PyTorch开始深度学习</h1>
                
            
            
                
<p><strong>深度学习</strong> ( <strong> DL </strong>)革新了一个又一个行业。吴恩达在推特上曾有一段著名的描述:</p>
<div><em>Artificial Intelligence is the new electricity!</em></div>
<p>电力改变了无数的行业；<strong>人工智能</strong> ( <strong> AI </strong>)现在也会这么做。</p>
<p>AI和DL像同义词一样使用，但两者之间有实质性的区别。让我们揭开行业术语的神秘面纱，以便作为从业者的你能够区分信号和噪声。</p>
<p>在这一章中，我们将涵盖人工智能的以下不同部分:</p>
<ul>
<li>人工智能本身及其起源</li>
<li>现实世界中的机器学习</li>
<li>深度学习的应用</li>
<li>为什么现在要深度学习？</li>
<li>深度学习框架:PyTorch</li>
</ul>


            

            
        
    </div>


  <div><h1 class="header-title">人工智能</h1>
                
            
            
                
<p>每天都有无数讨论AI的文章发表。这一趋势在过去两年有所增强。网络上流传着几个关于人工智能的定义，我最喜欢的是通常由人类执行的智能任务的自动化。</p>


            

            
        
    </div>


  <div><h1 class="header-title">人工智能的历史</h1>
                
            
            
                
<p>人工智能这个术语是约翰·麦卡锡在1956年首次提出的，当时他召开了关于这个主题的第一次学术会议。机器是否会思考这个问题的旅程比这要早得多。在AI早期，机器能够解决人类难以解决的问题。</p>
<p>例如，恩尼格玛机建于第二次世界大战末期，用于军事通信。艾伦·图灵建立了一个人工智能系统，帮助破解了恩尼格玛密码。破解恩尼格玛密码对一个人来说是一项非常具有挑战性的任务，对一个分析师来说可能需要几周的时间。这台人工智能机器能够在几个小时内破解代码。</p>
<p>计算机很难解决对我们来说很直观的问题，例如区分狗和猫，告诉你的朋友是否因为你在聚会上迟到而生气(情绪)，区分卡车和汽车，在研讨会上做笔记(语音识别)，或者为不懂你的语言的朋友将笔记转换为另一种语言(例如，法语到英语)。这些任务中的大部分对我们来说是直观的，但是我们无法编程或硬编码计算机来完成这些任务。早期人工智能机器中的大多数智能都是硬编码的，例如计算机程序下棋。</p>
<p>在人工智能的早期，许多研究人员认为人工智能可以通过硬编码规则来实现。这种人工智能被称为<strong>符号人工智能</strong>，在解决定义明确的逻辑问题方面非常有用，但它几乎无法解决复杂的问题，如图像识别、对象检测、对象分割、语言翻译和自然语言理解任务。较新的人工智能方法，如机器学习和DL，被开发出来解决这类问题。</p>
<p>为了更好地理解AI、ML和DL之间的关系，让我们将它们想象成与AI同心的圆圈——首先出现的想法(最大)，然后是机器学习(后来开花)，最后是DL——这是推动今天AI爆炸的原因(两者都符合):</p>
<div><img height="267" width="357" class="alignnone size-full wp-image-325 image-border" src="img/5f93757c-b18f-4ffb-b389-0fcfa1cdff41.png"/></div>
<p>人工智能、机器学习和人工智能是如何融合在一起的</p>


            

            
        
    </div>


  <div><h1 class="header-title">机器学习</h1>
                
            
            
                
<p><strong>机器学习</strong> ( <strong> ML </strong>)是人工智能的一个子领域，在过去10年里变得很受欢迎，有时，两者可以互换使用。除了机器学习，人工智能还有很多其他子领域。ML系统是通过展示大量的例子来构建的，不像符号人工智能，我们硬编码规则来构建系统。在高层次上，机器学习系统查看大量数据，并提出规则来预测未知数据的结果:</p>
<div><img height="185" width="264" class="alignnone size-full wp-image-326 image-border" src="img/27c1671a-61d3-46e1-ac66-e3dbd5683ab2.png"/></div>
<p>机器学习与传统编程</p>
<p>大多数ML算法在结构化数据上表现良好，例如销售预测、推荐系统和营销个性化。任何最大似然算法的一个重要因素是特征工程，数据科学家需要花费大量时间来获得最大似然算法要执行的正确特征。在某些领域，如计算机视觉和自然语言处理领域，特征工程很有挑战性，因为它们的维数很高。</p>
<p>直到最近，由于特征工程和高维度等原因，组织使用典型的机器学习技术(如线性回归、随机森林等)来解决这类问题仍具有挑战性。考虑尺寸为224 x 224 x 3(高x宽x通道)的图像，其中图像尺寸中的<em> 3 </em>表示彩色图像中红色、绿色和蓝色通道的值。为了将该图像存储在计算机存储器中，我们的矩阵将包含用于单个图像的150，528个维度。假设您想要在1，000张大小为224 x 224 x 3的图像上构建一个分类器，那么维数将变成1，000乘以150，528。机器学习的一个特殊分支叫做<strong>深度学习</strong>允许你使用现代技术和硬件来处理这些问题。</p>


            

            
        
    </div>


  <div><h1 class="header-title">现实生活中机器学习的例子</h1>
                
            
            
                
<p>以下是一些由机器学习驱动的酷产品:</p>
<ul>
<li><strong>例1 </strong> : Google Photos使用一种叫做<strong>深度学习的特定形式的机器学习对照片进行分组</strong></li>
<li><strong>例2 </strong>:推荐系统是一个ML算法家族，被网飞、亚马逊、iTunes等大公司用于推荐电影、音乐和产品</li>
</ul>


            

            
        
    </div>


  <div><h1 class="header-title">深度学习</h1>
                
            
            
                
<p>传统的ML算法使用手写特征提取来训练算法，而DL算法使用现代技术以自动方式提取这些特征。</p>
<p>例如，预测图像是否包含面部的DL算法提取特征，例如第一层检测边缘，第二层检测鼻子和眼睛等形状，最后一层检测面部形状或更复杂的结构。每一层都基于前一层的数据表示进行训练。如果你觉得这个解释难以理解，没关系，本书后面的章节将帮助你直观地建立和检查这样的网络:</p>
<div><img height="274" width="974" class="alignnone size-full wp-image-327 image-border" src="img/a2d20614-2bcd-40eb-a797-3382a71f2676.png"/></div>
<p>可视化中间层的输出(图片来源:https://www . cs . Princeton . edu/~ rajeshr/papers/cacm 2011-research highlights-conv dbn . pdf)</p>
<p>随着GPU、大数据、云提供商(如<strong>亚马逊网络服务</strong> ( <strong> AWS </strong>)和谷歌云)以及框架(如Torch、TensorFlow、Caffe和PyTorch)的兴起，DL的使用在过去几年中有了巨大的增长。除此之外，大公司分享在巨大数据集上训练的算法，从而帮助创业公司不费吹灰之力就在几个用例上建立最先进的系统。</p>


            

            
        
    </div>


  <div><h1 class="header-title">深度学习的应用</h1>
                
            
            
                
<p>使用DL可能实现的一些流行应用程序如下:</p>
<ul>
<li>接近人类水平的图像分类</li>
<li>接近人类水平的语音识别</li>
<li>机器翻译</li>
<li>自动驾驶汽车</li>
<li>近年来，Siri、谷歌语音和Alexa变得更加准确</li>
<li>一名日本农民正在挑选黄瓜</li>
<li>肺癌检测</li>
<li>语言翻译击败人类水平的准确性</li>
</ul>
<p>下面的屏幕截图显示了一个简短的摘要示例，其中计算机提取了一大段文本，并将其总结为几行:</p>
<div><img height="418" width="599" src="img/4a2ec332-490a-40b0-87ba-2a3290c4aab9.png"/></div>
<p>计算机生成的样本段落摘要</p>
<p class="mce-root CDPAlignLeft CDPAlign">在下图中，一台计算机在没有被告知它显示什么的情况下获得了一个普通图像，通过使用对象检测和字典的一些帮助，您得到了一个图像标题，说明<strong>两个年轻女孩正在玩乐高玩具</strong>。是不是很精彩？</p>
<p><img height="258" width="221" src="img/25961d1a-6689-4d5b-b3e3-b99c80c5aaa8.png"/></p>
<p>物体检测和图像字幕(图片来源:https://cs.stanford.edu/people/karpathy/cvpr2015.pdf)</p>


            

            
        
    </div>


  <div><h1 class="header-title">与深度学习相关的炒作</h1>
                
            
            
                
<p>媒体和人工智能领域以外的人，或者不是人工智能和人工智能真正实践者的人，一直在暗示，随着人工智能/人工智能的发展，像电影<em>终结者2:审判日</em>的故事线这样的事情可能会成为现实。他们中的一些人甚至谈到有一天我们会被机器人控制，由机器人决定什么对人类有益。目前，人工智能的能力被夸大了，远远超出了它的真实能力。目前，大多数DL系统部署在一个非常受控的环境中，并且被给予有限的决策边界。</p>
<p>我的猜测是，当这些系统能够学会做出智能决策，而不仅仅是完成模式匹配时，当数百或数千个DL算法能够一起工作时，我们可能会看到机器人的行为可能会像我们在科幻电影中看到的那样。在现实中，我们并没有更接近一般的人工智能，即机器可以在没有被告知的情况下做任何事情。DL目前的状态更多的是从现有数据中寻找模式来预测未来的结果。作为DL实践者，我们需要区分信号和噪声。</p>


            

            
        
    </div>


  <div><h1 class="header-title">深度学习的历史</h1>
                
            
            
                
<p>尽管深度学习近年来变得流行，但深度学习背后的理论自20世纪50年代以来一直在发展。下表显示了目前在DL应用程序中使用的一些最流行的技术及其大致时间表:</p>
<table style="width: 539px;height: 192px">
<tbody>
<tr>
<td>
<p><strong>技法</strong></p>
</td>
<td>
<p><strong>年份</strong></p>
</td>
</tr>
<tr>
<td>
<p>神经网络</p>
</td>
<td>
<p>1943</p>
</td>
</tr>
<tr>
<td>
<p>反向传播</p>
</td>
<td>
<p>60年代初</p>
</td>
</tr>
<tr>
<td>
<p>卷积神经网络</p>
</td>
<td>
<p>1979</p>
</td>
</tr>
<tr>
<td>
<p>递归神经网络</p>
</td>
<td>
<p>1980</p>
</td>
</tr>
<tr>
<td>
<p>长短期记忆</p>
</td>
<td>
<p>1997</p>
</td>
</tr>
</tbody>
</table>
<p> </p>
<p>多年来，深度学习已经被赋予了几个名称。它在20世纪70年代被称为<strong>控制论</strong>，在80年代被称为连接主义，现在它要么被称为<em>深度学习</em>要么被称为<em>神经网络</em>。我们将交替使用DL和神经网络。神经网络通常被称为一种受人脑工作启发的算法。然而，作为DL的实践者，我们需要理解它主要是由数学(线性代数和微积分)、统计学(概率)和软件工程中强大的理论所启发和支持的。</p>


            

            
        
    </div>


  <div><h1 class="header-title">为什么是现在？</h1>
                
            
            
                
<p>为什么DL现在变得这么流行？一些关键原因如下:</p>
<ul>
<li>硬件可用性</li>
<li>数据和算法</li>
<li>深度学习框架</li>
</ul>


            

            
        
    </div>


  <div><h1 class="header-title">硬件可用性</h1>
                
            
            
                
<p>深度学习需要对数百万，有时数十亿个参数进行复杂的数学运算。现有的CPU需要很长时间来执行这些类型的操作，尽管这在过去几年中有所改善。一种叫做<strong>图形处理器</strong> ( <strong> GPU </strong>)的新型硬件完成了这些庞大的数学运算，比如矩阵乘法，速度快了几个数量级。</p>
<p>GPU最初是由Nvidia和AMD等公司为游戏行业打造的。事实证明，这种硬件非常高效，不仅可以渲染高质量的视频游戏，还可以加快DL算法的速度。Nvidia最近推出的一款GPU<em>1080 ti</em>，花了几天时间在一个<kbd>ImageNet</kbd>数据集上构建一个图像分类系统，而以前可能需要一个月左右。</p>
<p>如果你打算购买运行深度学习的硬件，我会建议你根据预算选择英伟达的GPU。选择一个内存大的。记住，你的电脑内存和GPU内存是两回事。1080ti配有11 GB内存，价格约为700美元。</p>
<p>你也可以使用各种云提供商，比如AWS，Google Cloud，或者Floyd(这家公司提供针对DL优化的GPU机器)。如果您刚刚开始使用DL，或者如果您正在设置供组织使用的机器，您可能有更多的财务自由，那么使用云提供商是经济的。</p>
<p>如果对这些系统进行优化，性能可能会有所不同。</p>
<p>下图显示了比较CPU和GPU性能的一些基准:</p>
<div><img src="img/c49812cd-7561-4820-8fdf-5d88bd969bbf.png"/></div>
<p>神经架构在CPU和GPU上的性能基准测试(图片来源:http://cs 231n . Stanford . edu/slides/2017/cs 231n _ 2017 _ lecture 8 . pdf)</p>


            

            
        
    </div>


  <div><h1 class="header-title">数据和算法</h1>
                
            
            
                
<p>数据是深度学习成功的最重要因素。由于互联网的广泛采用和智能手机的日益普及，一些公司，如脸书和谷歌，已经能够收集各种格式的大量数据，特别是文本、图像、视频和音频。在计算机视觉领域，ImageNet竞赛在提供1000个类别的140万幅图像的数据集方面发挥了巨大作用。</p>
<p>这些类别都是手工标注的，每年都有数百个团队参加比赛。一些在竞赛中成功的算法是VGG，雷斯网，盗梦空间，DenseNet等等。这些算法今天在工业中用于解决各种计算机视觉问题。在深度学习领域中，一些常用于测试各种算法的其他流行数据集如下:</p>
<ul>
<li>MNIST</li>
<li>COCO数据集</li>
<li>西法尔</li>
<li>街景门牌号码</li>
<li>帕斯卡VOC</li>
<li>维基百科转储</li>
<li>20个新闻组</li>
<li>宾州树木银行</li>
<li>卡格尔</li>
</ul>
<p>不同算法的发展，如批量标准化、激活函数、跳过连接、<strong>长短期记忆</strong>(<strong/>)、辍学等等，使得近年来更快、更成功地训练非常深的网络成为可能。在本书接下来的章节中，我们将深入了解每种技术的细节，以及它们如何帮助构建更好的模型。</p>


            

            
        
    </div>


  <div><h1 class="header-title">深度学习框架</h1>
                
            
            
                
<p>在早期，人们需要具备C++和CUDA方面的专业知识来实现DL算法。随着许多组织现在开源他们的深度学习框架，具有脚本语言(如Python)知识的人可以开始构建和使用DL算法。目前业界使用的一些流行的深度学习框架有TensorFlow、Caffe2、Keras、Theano、PyTorch、Chainer、DyNet、MXNet和CNTK。</p>
<p>如果没有这些框架，深度学习的采用不会如此巨大。它们抽象出许多潜在的复杂性，使我们能够专注于应用程序。我们仍然处于DL的早期，在那里，随着大量的研究，突破每天都在公司和组织中发生。因此，各种框架各有利弊。</p>


            

            
        
    </div>


  <div><h1 class="header-title">PyTorch</h1>
                
            
            
                
<p>PyTorch和大多数其他深度学习框架可以用于两种不同的事情:</p>
<ul>
<li>用GPU加速的运算取代类似NumPy的运算</li>
<li>构建深度神经网络</li>
</ul>
<p>PyTorch越来越受欢迎的原因是它的易用性和简单性。与大多数其他流行的使用静态计算图的深度学习框架不同，PyTorch使用动态计算，这允许在构建复杂架构时有更大的灵活性。</p>
<p>PyTorch广泛使用Python概念，如类、结构和条件循环，允许我们以纯面向对象的方式构建DL算法。大多数其他流行的框架都有自己的编程风格，有时会使编写新算法变得复杂，并且不支持直观的调试。在后面的章节中，我们将详细讨论计算图。</p>
<p>虽然PyTorch是最近发布的，仍处于测试版本，但它已经在数据科学家和深度学习研究人员中非常受欢迎，因为它易于使用，性能更好，易于调试，并且得到了SalesForce等各种公司的大力支持。</p>
<p>由于PyTorch主要是为研究而构建的，因此不建议在某些延迟要求非常高的场景中用于生产。然而，随着一个名为<strong>开放神经网络交换</strong>(<strong>ONNX</strong>)(<a href="https://onnx.ai/" target="_blank">https://onnx.ai/</a>)的新项目的出现，这种情况正在发生变化，该项目专注于将PyTorch上开发的模型部署到像Caffe2这样的平台上，以便进行生产。在撰写本文时，谈论这个项目还为时过早，因为它才刚刚启动。该项目得到了脸书和微软的支持。</p>
<p>在本书的其余部分，我们将学习各种乐高积木(更小的概念或技术),用于在计算机视觉和NLP领域构建强大的DL应用程序。</p>


            

            
        
    </div>


  <div><h1 class="header-title">摘要</h1>
                
            
            
                
<p>在这一介绍性章节中，我们探讨了什么是人工智能、机器学习和深度学习，并讨论了这三者之间的差异。我们还研究了日常生活中由它们驱动的应用。我们深入探究为什么DL现在才变得更受欢迎。最后，我们温柔的介绍了PyTorch，这是一个深度学习框架。</p>
<p>在下一章，我们将在PyTorch中训练我们的第一个神经网络。</p>
<p class="mce-root"/>


            

            
        
    </div>
</body>
</html>