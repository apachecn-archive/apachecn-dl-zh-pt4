<html><head/><body>


	
		<title>C13550_05_EPUB_Final_SW</title>
		
	
	
		<div><h1 id="_idParaDest-94"><em class="italics"> <a id="_idTextAnchor114"/>第五章</em></h1>
		</div>
		<div><h1 id="_idParaDest-95"><a id="_idTextAnchor115"/>用于计算机视觉的卷积神经网络</h1>
		</div>
		<div><h2>学习目标</h2>
			<p>本章结束时，您将能够:</p>
			<ul>
				<li class="bullets">解释卷积神经网络如何工作</li>
				<li class="bullets">构建卷积神经网络</li>
				<li class="bullets">通过使用数据扩充来改进构建的模型</li>
				<li class="bullets">通过实施迁移学习来使用最先进的模型</li>
			</ul>
			<p>在这一章中，我们将学习如何使用概率分布作为一种无监督学习的形式。</p>
		</div>
		<div><h2 id="_idParaDest-96"><a id="_idTextAnchor116"/>简介</h2>
			<p><a id="_idTextAnchor117"/><a id="_idTextAnchor118"/><a id="_idTextAnchor119"/><a id="_idTextAnchor120"/><a id="_idTextAnchor121"/><a id="_idTextAnchor122"/><a id="_idTextAnchor123"/><a id="_idTextAnchor124"/><a id="_idTextAnchor125"/><a id="_idTextAnchor126"/><a id="_idTextAnchor127"/><a id="_idTextAnchor128"/><a id="_idTextAnchor129"/><a id="_idTextAnchor130"/><a id="_idTextAnchor131"/>在上一章中，我们了解了如何训练神经网络来预测值，以及如何根据其架构来证明<strong class="bold">递归神经网络(RNN) </strong>在许多情况下都是有用的。在本章中，我们将讨论并观察<strong class="keyword">卷积神经网络</strong>如何以类似于密集神经网络的方式工作(也称为全连接神经网络，如<em class="italics">第2章</em>、<em class="italics">计算机视觉简介</em>中所述)。</p>
			<p>CNN的神经元具有在训练期间更新的权重和偏差。CNN主要用于图像处理。图像被解释为像素，网络输出它认为图像属于的类别，以及陈述每个分类和每个输出的误差的损失函数。</p>
			<p>这些类型的网络假设输入是图像或像图像一样工作，使它们能够更有效地工作(CNN比深度神经网络更快更好)。在接下来的章节中，你会学到更多关于CNN的知识。</p>
			<h2 id="_idParaDest-97"><a id="_idTextAnchor132"/>中枢神经系统的基础</h2>
			<p>在本主题中，我们将了解CNN的工作原理，并解释图像卷积的过程。</p>
			<p>我们知道图像是由像素组成的，例如，如果图像是RGB，它将有三个通道，其中每个字母/颜色(红绿蓝)都有自己的通道，包含一组相同大小的像素。完全连接的神经网络并不在每一层的图像中表示这种深度。相反，他们有一个单一的维度来表示这个深度，这是不够的。此外，它们将一层的每个单个神经元连接到下一层的每个单个神经元，等等。这反过来导致性能下降，这意味着你必须训练一个网络更长的时间，仍然不会得到好的结果。</p>
			<p>CNN是一种神经网络，在分类和图像识别等任务中非常有效。但是，它们也非常适合声音和文本数据。CNN由输入层、隐藏层和输出层组成，就像普通的神经网络一样。输入层和隐藏层通常由<strong class="bold">卷积层</strong>、<strong class="bold">汇集层</strong>(减少输入空间大小的层)和<strong class="bold">全连接层</strong>(全连接层在<em class="italics">第2章</em>、<em class="italics">计算机视觉介绍</em>中解释)。卷积层和汇集层将在本章后面解释。</p>
			<p>CNN赋予每一层深度，从图像的原始深度到更深的隐藏层。下图显示了CNN的工作方式和外观:</p>
			<div><div><img src="img/C13550_05_01.jpg" alt="Figure 5.1: Representation of a CNN"/>
				</div>
			</div>
			<h6>图5.1:CNN的表示</h6>
			<p>在上图中，CNN获取了一个224 x 224 x 3的输入图像，该图像通过卷积过程被转换到下一层，该层压缩了图像大小，但具有更大的深度(我们将在后面解释这些过程是如何工作的)。这些操作一遍又一遍地继续，直到图形表示变平，并且这些密集层被用于以数据集的相应类作为输出而结束。</p>
			<p><strong class="keyword">卷积层:</strong>卷积层由一组固定大小的<strong class="bold">滤波器</strong>组成(通常是小尺寸)，它们是具有特定值/权重的矩阵，通过计算滤波器和输入之间的标量积，应用于整个输入(例如，一幅图像)，这称为卷积。这些过滤器中的每一个都产生二维激活图，该激活图沿着输入的深度堆叠。这些激活图在输入中寻找特征，并将决定网络学习的好坏。你有越多的过滤器，层越深，因此，你的网络学习得越多，但在训练时它变得越慢。例如，在一个特定的图像中，你希望在第一层有3个滤镜，在下一层有96个滤镜，在下一层有256个滤镜，以此类推。请注意，在网络的起点，通常比网络的终点或中间有更少的过滤器。这是因为网络的中间和末端有更多的潜在特征要提取，因此我们需要更多的过滤器，尺寸更小，朝向网络的末端。这是因为我们进入网络越深，我们就越关注图像中的小细节，因此我们希望从这些细节中提取更多的特征，以更好地理解图像。</p>
			<p>例如，卷积层的滤波器大小通常从2x2到7x7不等，这取决于您是在网络的起点(较大的大小)还是在网络的终点(较小的大小)。</p>
			<p>在图5.1中，我们可以看到使用过滤器(浅蓝色)应用卷积，输出将是进入下一步/层的单个值。</p>
			<p>在执行卷积之后，在应用另一个卷积之前，通常应用最大汇集(<strong class="keyword">汇集层</strong>)层，以便减小输入的大小，从而网络可以更深入地理解图像。然而，最近有一种趋势，即避免最大池，而是鼓励大步，这是在执行卷积时自然应用的，因此我们将通过自然应用卷积来解释图像缩小。</p>
			<p><strong class="keyword">步长:</strong>这是应用于整个图像的滤镜步长的长度，以像素为单位。如果选择的步幅为1，将应用过滤器，但一次一个像素。类似地，如果选择跨距为2，则过滤器将一次应用两个像素，输出大小小于输入，依此类推。</p>
			<p>让我们看一个例子。首先，图5.2将用作卷积图像的滤波器，图像是一个2x2矩阵:</p>
			<div><div><img src="img/C13550_05_02.jpg" alt="Figure 5.2: Convolution filter"/>
				</div>
			</div>
			<h6>图5.2:卷积滤波器</h6>
			<p>下面可能是我们正在卷积的图像(矩阵):</p>
			<div><div><img src="img/C13550_05_03.jpg" alt="Figure 5.3: Image to convolve"/>
				</div>
			</div>
			<h6>图5.3:要卷积的图像</h6>
			<p>当然，这不是真实的图像，但为了简单起见，我们采用一个具有随机值的4x4矩阵来演示卷积的工作原理。</p>
			<p>现在，如果我们要应用步幅等于1的卷积，过程如下图所示:</p>
			<div><div><img src="img/C13550_05_04.jpg" alt="Figure 5.4: Convolution process Stride=1"/>
				</div>
			</div>
			<h6>图5.4:卷积过程步幅=1</h6>
			<p>上图显示了对输入图像逐像素应用的2x2滤镜。这个过程从左到右，从上到下。</p>
			<p>该过滤器将其矩阵中每个位置的每个值乘以应用该过滤器的区域(矩阵)中每个位置的每个值。例如，在该过程的第一部分，滤波器被应用于图像[1 2；5 6]而我们拥有的过滤器是[2 1；-1 ^ 2]，那么就是1 * 2+2 * 1+5 *(1)+6 * 2 = 11。</p>
			<p>应用过滤器矩阵后，生成的图像如下所示:</p>
			<div><div><img src="img/C13550_05_05.jpg" alt="Figure 5.5: Convolution result Stride=1"/>
				</div>
			</div>
			<h6>图5.5:卷积结果步幅=1</h6>
			<p>如你所见，生成的图像现在小了一个维度。这是因为还有一个参数，叫做<strong class="bold"> padding </strong>，默认设置为“valid”，表示卷积会正常应用；也就是说，应用卷积自然会使图像变细一个像素。如果设置为“相同”，图像将被一行值等于零的像素包围，因此输出矩阵将具有与输入矩阵相同的大小。</p>
			<p>现在，我们将应用步幅2，以将大小减少2(就像2x2的最大池层一样)。请记住，我们使用的填充等于“有效”</p>
			<p>该流程的步骤会更少，如下图所示:</p>
			<div><div><img src="img/C13550_05_06.jpg" alt="Figure 5.6: Convolution process Stride=2"/>
				</div>
			</div>
			<h6>图5.6:卷积过程步幅=2</h6>
			<p>输出图像/矩阵如下所示:</p>
			<div><div><img src="img/C13550_05_07.jpg" alt="Figure 5.7:  Convolution result Stride=2"/>
				</div>
			</div>
			<h6>图5.7:卷积结果步幅=2</h6>
			<p>得到的图像将是2×2像素的图像。这是由于步幅等于2的卷积的自然过程。</p>
			<p>这些应用于每个卷积层的滤波器具有神经网络调整的权重，以便这些滤波器的输出帮助神经网络学习有价值的特征。如上所述，这些权重通过反向传播过程进行更新。提醒一下，反向传播是这样一个过程，在该过程中，计算在网络的训练步骤中所做的预测相对于预期结果的网络损失(或误差量),更新导致该误差的网络神经元的所有权重，以便它们不会再次犯同样的错误。</p>
			<h2 id="_idParaDest-98"><a id="_idTextAnchor133"/>打造您的第一家CNN</h2>
			<h4>注意</h4>
			<p class="callout">对于这一章，我们将仍然使用TensorFlow之上的Keras作为后端，正如本书第<em class="italics">章“计算机视觉简介</em>中提到的。此外，我们仍将使用Google Colab来训练我们的网络。</p>
			<p>Keras是一个非常好的实现卷积层的库，因为它抽象了用户，所以不必手工实现这些层。</p>
			<p>在<em class="italics">第2章</em>、<em class="italics">计算机视觉介绍、</em>中，我们使用<code>keras.layers</code>包导入了密集层、漏失层和批处理标准化层，为了声明二维卷积层，我们将使用相同的包:</p>
			<pre>from keras.layers import Conv2D</pre>
			<p><code>Conv2D</code>模块和其他模块一样:你必须声明一个顺序模型，这在本书<em class="italics">第二章《计算机视觉导论</em>中有解释，我们还增加了<code>Conv2D</code>:</p>
			<pre>model = Sequential()
model.add(Conv2D(32, kernel_size=(3, 3), padding='same', strides=(2,2), input_shape=input_shape))</pre>
			<p>对于第一个图层，必须指定输入形状，但之后就不再需要了。</p>
			<p>必须指定的第一个参数是网络将在该层学习的过滤器数量。如前所述，在前面的层中，我们将过滤几个将要学习的层，而不是网络中更深的层。</p>
			<p>第二个必须指定的参数是<strong class="bold">内核大小</strong>，它是应用于输入数据的过滤器的大小。通常，设置的内核大小为3x3，甚至是2x2，但有时当图像很大时，会设置更大的内核大小。</p>
			<p>第三个参数是<strong class="bold"> padding </strong>，默认设置为“valid”，但需要设置为“same”，因为我们希望保留输入的大小，以便理解对输入进行下采样的行为。</p>
			<p>第四个参数是<strong class="bold">步幅</strong>，默认设置为(1，1)。我们将把它设置为(2，2)，因为这里有两个数字，并且必须为x轴和y轴都设置它。</p>
			<p>在第一层之后，我们将应用与第2章<em class="italics">计算机视觉介绍</em>中提到的方法相同的方法:</p>
			<pre>model.add(BatchNormalization())
model.add(Activation('relu'))
model.add(Dropout(0.2))</pre>
			<p>提醒一下，<strong class="bold"> BatchNormalization </strong>层用于归一化每一层的输入，这有助于网络更快地收敛，并且总体上可能给出更好的结果。</p>
			<p><code>activation</code>函数是一个接受输入并计算其加权和的函数，加上一个偏差并决定它是否应该被激活(分别输出1和0)。</p>
			<p><strong class="bold"> Dropout </strong>层通过关闭一定比例的神经元，帮助网络避免过度拟合，即训练集的精度远远高于验证集的精度。</p>
			<p>我们可以应用更多组这样的层，根据要解决的问题的大小来改变参数。</p>
			<p>根据问题的不同，最后几层与密集神经网络保持相同。</p>
			<h3 id="_idParaDest-99"><a id="_idTextAnchor134"/>练习17:构建CNN</h3>
			<h4>注意</h4>
			<p class="callout">本练习使用与第2章“计算机视觉简介”相同的包和库。这些库是Keras、Numpy、OpenCV和Matplotlib。</p>
			<p>在这个练习中，我们将带着与<em class="italics">第2章</em>、<em class="italics">活动2 </em>、<em class="italics">相同的问题对时尚MNIST数据库</em>中的10种服装进行分类。</p>
			<p>请记住，在那个活动中，所建立的神经网络不能很好地进行归纳，以对我们传递给它的看不见的数据进行分类。</p>
			<p>提醒一下，这个问题是一个分类问题，模型要正确分类10种衣服:</p>
			<ol>
				<li>打开你的Google Colab界面。</li>
				<li>为这本书创建一个文件夹，并从GitHub下载<code>Datasets</code>文件夹，然后上传到你驱动器的文件夹中。</li>
				<li>Import drive and mount it as follows:<pre>from google.colab import drive
drive.mount('/content/drive')</pre><h4>注意</h4><p class="callout">每次使用新的collaborator时，将驱动器安装到所需的文件夹中。</p></li>
				<li>一旦你第一次安装了你的硬盘，你必须输入授权码，方法是点击谷歌给出的网址，然后按下键盘上的<strong class="bold"> Enter </strong>键:<div> <img src="img/C13550_05_08.jpg" alt="Figure 5.8: Mounting on Google Collab"/> </div> <h6>图5.8:在谷歌协作上安装</h6></li>
				<li>Now that you have mounted the drive, you need to set the path of the directory:<pre>cd /content/drive/My Drive/C13550/Lesson05/</pre><h4>注意</h4><p class="callout">根据您在Google Drive上的文件夹设置，第5步中提到的路径可能会有所变化。路径总是以<code>cd /content/drive/My Drive/</code>开始。</p></li>
				<li>首先，让我们从Keras导入数据，并将随机种子初始化为42，以获得可重复性:<pre>from keras.datasets import fashion_mnist  (x_train, y_train), (x_test, y_test) =fashion_mnist.load_data() import random random.seed(42) </pre></li>
				<li>我们导入NumPy以便对数据进行预处理，并使用Keras工具对标签进行一次性编码:<pre>import numpy as np from keras import utils as np_utils x_train = (x_train.astype(np.float32))/255.0  x_test = (x_test.astype(np.float32))/255.0  x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)  x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)  y_train = np_utils.to_categorical(y_train, 10)  y_test = np_utils.to_categorical(y_test, 10)  input_shape = x_train.shape[1:]</pre></li>
				<li>We declare the <code>Sequential</code> function to make a sequential model in Keras, the callbacks, and, of course, the layers:<pre>from keras.models import Sequential
from keras.callbacks import EarlyStopping, ModelCheckpoint
from keras.layers import Input, Dense, Dropout, Flatten
from keras.layers import Conv2D, Activation, BatchNormalization</pre><h4>注意</h4><p class="callout">我们已经导入了名为<strong class="bold"> EarlyStopping </strong>的回调。这个回调所做的是在许多个时期之后停止训练，此时您选择的度量(例如，验证准确性)已经下降。您可以根据自己的需要来设置这个数字。</p></li>
				<li>Now, we are going to build our first CNN. First, let's declare the model as <code>Sequential</code> and add the first <code>Conv2D</code>:<pre>def CNN(input_shape):
    model = Sequential()
    model.add(Conv2D(32, kernel_size=(3, 3), padding='same', strides=(2,2), input_shape=input_shape))</pre><p>我们添加32个过滤器作为第一层，过滤器大小为3x3。填充被设置为“<code>same</code>”，步距被设置为2，以自然地减少<code>Conv2D</code>模块的维数。</p></li>
				<li>我们在这一层之后添加了<code>Activation</code>和<code>BatchNormalization</code>层:<pre>    model.add(Activation('relu'))     model.add(BatchNormalization())</pre></li>
				<li>我们将添加另外三个层，具有与之前相同的特征，应用dropout并跳转到另一个块:<pre>    model.add(Conv2D(32, kernel_size=(3, 3), padding='same', strides=(2,2)))     model.add(Activation('relu'))     model.add(BatchNormalization())</pre></li>
				<li>现在，我们应用20%的下降，这关闭了网络中20%的神经元:<pre>    model.add(Dropout(0.2))</pre></li>
				<li>我们将用64个过滤器再做一次同样的程序:<pre>    model.add(Conv2D(64, kernel_size=(3, 3), padding='same', strides=(2,2)))     model.add(Activation('relu'))     model.add(BatchNormalization())     model.add(Conv2D(64, kernel_size=(3, 3), padding='same', strides=(2,2)))     model.add(Activation('relu'))     model.add(BatchNormalization())     model.add(Dropout(0.2))</pre></li>
				<li>对于网络的末端，我们应用<code>Flatten</code>层，使得最后一层的输出是一维的。我们应用一个有512个神经元的<code>Dense</code>层。在网络物流发生的地方，我们应用<code>Activation</code>层和<code>BatchNormalization</code>层，然后应用50%的<code>Dropout</code>:<pre>    model.add(Flatten())     model.add(Dense(512))     model.add(Activation('relu'))     model.add(BatchNormalization())     model.add(Dropout(0.5))</pre></li>
				<li>最后，我们将最后一层声明为一个有10个神经元的<code>dense</code>层，这是数据集的类的数量，以及一个<code>Softmax</code>激活函数，它确定图像更可能是哪个类，我们返回模型:<pre>    model.add(Dense(10, activation="softmax"))     return model</pre></li>
				<li>Let's declare the model along with the callbacks and compile it:<pre>model = CNN(input_shape)
 
model.compile(loss='categorical_crossentropy', optimizer='Adadelta', metrics=['accuracy'])
 
ckpt = ModelCheckpoint('Models/model.h5', save_best_only=True,monitor='val_loss', mode='min', save_weights_only=False) 
earlyStopping = EarlyStopping(monitor='val_loss', patience=5, verbose=0,mode='min')</pre><p>对于编译，我们使用相同的优化器。对于声明检查点，我们使用相同的参数。对于声明<code>EarlyStopping</code>，我们使用验证损失作为主要指标，并设置五个时期的耐心。</p></li>
				<li>Let the training begin!<pre>model.fit(x_train, y_train, batch_size=128, epochs=100, verbose=1, validation_data=(x_test, y_test), callbacks=[ckpt,earlyStopping]) </pre><p>我们将批量大小设置为128，因为有足够多的图像，并且这样做将花费更少的时间来训练。纪元的数量被设置为100，因为<code>EarlyStopping</code>会负责停止训练。</p><p>获得的准确度比<em class="italics">第2章</em>、<em class="italics">计算机视觉介绍</em>中的练习更好——我们获得了<strong class="bold"> 92.72% </strong>的准确度。</p><p>看一下下面的输出:</p><div><img src="img/C13550_05_09.jpg" alt=""/></div><h6>图5.9: val_acc显示为0.9240，即92.72%</h6><h4>注意</h4><p class="callout">这个练习的完整代码可以在GitHub上找到:<a href="https://github.com/PacktPublishing/Artificial-Vision-and-Language-Processing-for-Robotics/blob/master/Lesson05/Exercise17/Exercise17.ipynb">https://GitHub . com/packt publishing/Artificial-Vision-and-Language-Processing-for-Robotics/blob/master/lesson 05/exercise 17/exercise 17 . ipynb</a>。</p></li>
				<li>让我们用我们在<em class="italics">活动2 </em>中试过的同样的例子来试一下，<em class="italics">对<em class="italics">第二章</em>的时尚-MNIST数据库</em>中的10种服装进行分类，该数据库位于<code>Dataset/testing/</code> : <pre>import cv2    images = ['ankle-boot.jpg', 'bag.jpg', 'trousers.jpg', 't-shirt.jpg']    for number in range(len(images)):     imgLoaded = cv2.imread('Dataset/testing/%s'%(images[number]),0)      img = cv2.resize(imgLoaded, (28, 28))      img = np.invert(img)      img = (img.astype(np.float32))/255.0      img = img.reshape(1, 28, 28, 1)         plt.subplot(1,5,number+1),plt.imshow(imgLoaded,'gray')      plt.title(np.argmax(model.predict(img)[0]))      plt.xticks([]),plt.yticks([])  plt.show()</pre></li>
			</ol>
			<p>以下是输出结果:</p>
			<div><div><img src="img/C13550_05_10.jpg" alt="Figure 5.10: Prediction of clothes using CNNs"/>
				</div>
			</div>
			<h6>图5.10:使用CNN预测服装</h6>
			<p>提醒一下，下面是对应衣服编号的表格:</p>
			<div><div><img src="img/C13550_05_11.jpg" alt="Figure 5.11: The table with the number of corresponding clothes      "/>
				</div>
			</div>
			<h6>图5.11:对应衣服数量的表格</h6>
			<p>我们可以看到，该模型已经很好地预测了所有的图片，因此我们可以声明，该模型远远优于只有密集层的模型。</p>
			<h2 id="_idParaDest-100"><a id="_idTextAnchor135"/>改善您的模型-数据扩充</h2>
			<p>有时，在某些情况下，您无法通过构建更好的模型来提高模型的准确性。有时候，问题不在于模型，而在于数据。使用机器学习时要考虑的最重要的事情之一是，您使用的数据必须足够好，以便潜在的模型可以概括这些数据。</p>
			<p>数据可以代表现实生活中的事物，但也可能包含表现不佳的不正确数据。当数据不完整或者数据不能很好地表示类时，就会发生这种情况。对于这些情况，数据扩充已经成为最流行的方法之一。</p>
			<p>数据扩充实际上增加了原始数据集的样本数量。对于计算机视觉来说，这可能意味着增加数据集中的图像数量。有几种数据扩充技术，您可能希望根据数据集使用特定的技术。这里提到了其中的一些技术:</p>
			<ul>
				<li><strong class="bold">旋转</strong>:用户设置数据集中图像的旋转角度。</li>
				<li><strong class="bold">翻转</strong>:水平或垂直翻转图像。</li>
				<li><strong class="bold">裁剪</strong>:从图像中随机裁剪一段。</li>
				<li><strong class="bold">改变颜色</strong>:改变图像的颜色。</li>
				<li><strong class="bold">添加噪声</strong>:给图像添加噪声。</li>
			</ul>
			<p>应用这些或其他技术，你最终会生成不同于原始图像的新图像。</p>
			<p>为了在代码中实现这一点，Keras有一个名为<code>ImageDataGenerator</code>的模块，您可以在其中声明要应用于数据集的转换。您可以使用这行代码导入该模块:</p>
			<pre>from keras.preprocessing.image import ImageDataGenerator</pre>
			<p>为了声明将所有这些更改应用到数据集的变量，您必须按照下面的代码片段来声明它:</p>
			<pre>datagen = ImageDataGenerator(
        rotation_range=20,
        zoom_range = 0.2,
        width_shift_range=0.1,
        height_shift_range=0.1,
        horizontal_flip=True
        )</pre>
			<h4>注意</h4>
			<p class="callout">你可以通过查看Keras的这个文档来看看你能传递给<code>ImageDataGenerator</code>什么属性:<a href="https://keras.io/preprocessing/image/">https://keras.io/preprocessing/image/</a>。</p>
			<p>在声明了<code>datagen</code>之后，您必须使用下面的方法计算一些特征标准化的计算:</p>
			<pre>datagen.fit(x_train)</pre>
			<p>这里，<code>x_train</code>是你的训练集。</p>
			<p>为了使用数据扩充来训练模型，应该使用以下代码:</p>
			<pre>model.fit_generator(datagen.flow(x_train, y_train,
                                 batch_size=batch_size),
                    epochs=epochs,
                    validation_data=(x_test, y_test),
                    callbacks=callbacks,
                    steps_per_epoch=len(x_train) // batch_size)</pre>
			<p><code>Datagen.flow()</code>用于应用数据扩充。由于Keras不知道何时停止在给定数据中应用数据扩充，<code>Steps_per_epoch</code>是设置该限制的参数，该限制应该是训练集的长度除以批量大小。</p>
			<p>现在我们将直接进入本章的第二个练习来观察输出。数据扩充承诺更好的结果和更高的准确性。让我们看看这是真是假。</p>
			<h3 id="_idParaDest-101"><a id="_idTextAnchor136"/>练习18:使用数据扩充改进模型</h3>
			<p>在这个练习中，我们将使用Oxford - III宠物数据集，这是不同大小和几个类别的不同猫/狗品种的RGB图像。在这种情况下，为了简单起见，我们将数据集分成两个类:猫和狗。每个类有1000张图片，不多，但是会增量数据增强的效果。这个数据集存储在GitHub上添加的<code>Dataset/dogs-cats/</code>文件夹中。</p>
			<p>我们将构建一个CNN，并在有和没有数据增强的情况下对其进行训练，我们将比较结果:</p>
			<h4>注意</h4>
			<p class="callout">在这个练习中，我们将打开另一个Google Colab笔记本。</p>
			<p class="callout">这个练习的完整代码可以在GitHub上找到:<a href="https://github.com/PacktPublishing/Artificial-Vision-and-Language-Processing-for-Robotics/blob/master/Lesson05/Exercise18/Exercise18.ipynb">https://GitHub . com/packt publishing/Artificial-Vision-and-Language-Processing-for-Robotics/blob/master/lesson 05/exercise 18/exercise 18 . ipynb</a>。</p>
			<ol>
				<li value="1">打开你的Google Colab界面。</li>
				<li>为这本书创建一个文件夹，并从GitHub下载<code>Datasets</code>文件夹，然后上传到你驱动器的文件夹中。</li>
				<li>Import drive and mount it as follows:<pre>from google.colab import drive
drive.mount('/content/drive')</pre><h4>注意</h4><p class="callout">每次使用新的collaborator时，将驱动器安装到所需的文件夹中。</p></li>
				<li>一旦你第一次安装了你的硬盘，你必须点击谷歌提供的网址输入授权码。</li>
				<li>Now that you have mounted the drive, you need to set the path of the directory:<pre>cd /content/drive/My Drive/C13550/Lesson5/Dataset</pre><h4>注意</h4><p class="callout">根据您在Google Drive上的文件夹设置，第5步中提到的路径可能会有所变化。路径总是以<code>cd /content/drive/My Drive/</code>开始。</p></li>
				<li>First, let's use these two methods, which we have already used before, to load the data from disk:<pre>import re, os, cv2
import numpy as np
rows,cols = 128,128
//{…}##the detailed code can be found on Github##
def list_files(directory, ext=None):
//{…}##the detailed code can be found on Github##
def load_images(path,label):
//{…}
    for fname in list_files( path, ext='jpg' ): 
        img = cv2.imread(fname)
        img = cv2.resize(img, (rows, cols))
//{…}##the detailed code can be found on Github##</pre><h4>注意</h4><p class="callout">图像的大小被指定为128x128。这个尺寸比以前使用的尺寸大，因为我们需要这些图像的更多细节，因为类别更难区分，主题呈现在不同的位置，这使得工作更加困难。</p></li>
				<li>我们加载相应的狗和猫的图像，作为图像的<code>X</code>和标签的<code>y</code>，我们打印出这些的形状:<pre>X, y = load_images('Dataset/dogs-cats/dogs',0) X_aux, y_aux = load_images('Dataset/dogs-cats/cats',1) X = np.concatenate((X, X_aux), axis=0) y = np.concatenate((y, y_aux), axis=0) print(X.shape) print(y.shape)</pre> <div> <img src="img/C13550_05_12.jpg" alt="Figure 5.12: Dogs-cats data shape"/> </div> <h6>图5.12:狗和猫的数据形状</h6></li>
				<li>现在我们将导入<code>random</code>，设置种子，并显示一些数据样本:<pre>import random  random.seed(42)  from matplotlib import pyplot as plt   for idx in range(5):      rnd_index = random.randint(0, X.shape[0]-1)     plt.subplot(1,5,idx+1)     plt.imshow(cv2.cvtColor(X[rnd_index],cv2.COLOR_BGR2RGB))      plt.xticks([]),plt.yticks([]) plt.show() </pre> <div> <img src="img/C13550_05_13.jpg" alt="Figure 5.13: Image samples of the Oxford Pet dataset"/> </div> <h6>图5.13:牛津Pet数据集的图像样本</h6></li>
				<li>为了预处理数据，我们将使用与<em class="italics">练习17:构建CNN </em> : <pre>from keras import utils as np_utils X = (X.astype(np.float32))/255.0 X = X.reshape(X.shape[0], rows, cols, 3)  y = np_utils.to_categorical(y, 2) input_shape = X.shape[1:]</pre>中相同的程序</li>
				<li>现在，我们将<code>X</code>和<code>y</code>分为<code>x_train</code>和<code>y_train</code>用于训练集，将<code>x_test</code>和<code>y_test</code>用于测试集，并打印形状:<pre>from sklearn.model_selection import train_test_split x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2) print(x_train.shape) print(y_train.shape) print(x_test.shape) print(y_test.shape)</pre> <div> <img src="img/C13550_05_14.jpg" alt="Figure 5.14: Training and testing set shapes"/> </div> <h6>图5.14:训练和测试集形状</h6></li>
				<li>我们导入相应的数据来构建、编译和训练模型:<pre>from keras.models import Sequential from keras.callbacks import EarlyStopping, ModelCheckpoint from keras.layers import Input, Dense, Dropout, Flatten from keras.layers import Conv2D, Activation, BatchNormalization</pre></li>
				<li>Let's build the model:<pre>def CNN(input_shape):
    model = Sequential()
    
    model.add(Conv2D(16, kernel_size=(5, 5), padding='same', strides=(2,2), input_shape=input_shape))
    model.add(Activation('relu'))
    model.add(BatchNormalization())
    model.add(Conv2D(16, kernel_size=(3, 3), padding='same', strides=(2,2)))
    model.add(Activation('relu'))
    model.add(BatchNormalization())
    model.add(Dropout(0.2))
//{…}##the detailed code can be found on Github##
    
    model.add(Conv2D(128, kernel_size=(2, 2), padding='same', strides=(2,2)))
    model.add(Activation('relu'))
    model.add(BatchNormalization())
    model.add(Dropout(0.2))
    
    model.add(Flatten())
    model.add(Dense(512))
    model.add(Activation('relu'))
    model.add(BatchNormalization())
    model.add(Dropout(0.5))
 
    model.add(Dense(2, activation="softmax"))
 
    return model</pre><p>该模型从第一层的16个过滤器到最后的128个过滤器，每两层的尺寸加倍。</p><p>因为这个问题更难(我们有3个通道和128x128图像的更大的图像)，我们使模型更深，在开始时添加了另外两层16个过滤器(第一层的内核大小为5x5，这在最初阶段更好)，在模型结束时添加了另外两层128个过滤器。</p></li>
				<li>Now, let's compile the model:<pre>model = CNN(input_shape)
model.compile(loss='categorical_crossentropy', optimizer='Adadelta', metrics=['accuracy'])
ckpt = ModelCheckpoint('Models/model_dogs-cats.h5', save_best_only=True,monitor='val_loss', mode='min', save_weights_only=False) 
earlyStopping = EarlyStopping(monitor='val_loss', patience=15, verbose=0,mode='min')</pre><p>我们将EarlyStopping回调的耐心设置为15个历元，因为模型收敛到最佳点需要更多的历元，在此之前验证损失可能会有很大变化。</p></li>
				<li>Then, we train the model:<pre>model.fit(x_train, y_train,
          batch_size=8,
          epochs=100,
          verbose=1, 
          validation_data=(x_test, y_test),
          callbacks=[ckpt,earlyStopping]) </pre><p>批量大小也很低，因为我们没有太多的数据，但它可以很容易地增加到16。</p></li>
				<li>Then, evaluate the model:<pre>from sklearn import metrics
model.load_weights('Models/model_dogs-cats.h5')
y_pred = model.predict(x_test, batch_size=8, verbose=0)
y_pred = np.argmax(y_pred, axis=1)
y_test_aux = y_test.copy()
y_test_pred = list()
for i in y_test_aux:
    y_test_pred.append(np.argmax(i))
 
print (y_pred)
 
# Evaluate the prediction
accuracy = metrics.accuracy_score(y_test_pred, y_pred)
precision, recall, f1, support = metrics.precision_recall_fscore_support(y_test_pred, y_pred, average=None)
print('\nFinal results...')
print(metrics.classification_report(y_test_pred, y_pred))
print('Acc      : %.4f' % accuracy)
print('Precision: %.4f' % np.average(precision))
print('Recall   : %.4f' % np.average(recall))
print('F1       : %.4f' % np.average(f1))
print('Support  :', np.sum(support))</pre><p>您应该会看到以下输出:</p><div><img src="img/C13550_05_15.jpg" alt=""/></div><h6>图5.15:显示模型准确性的输出</h6><p>从上图可以看出，使用该模型在该数据集中达到的准确率为<strong class="bold"> 67.25% </strong>。</p></li>
				<li>We are going to apply data augmentation to this process. We have to import ImageDataGenerator from Keras and declare it with transformations that we are going to make:<pre>from keras.preprocessing.image import ImageDataGenerator
datagen = ImageDataGenerator(
        rotation_range=15,
        width_shift_range=0.2,
        height_shift_range=0.2,
        horizontal_flip=True,
        zoom_range=0.3
        )</pre><p>应用了以下转换:</p><p>我们已经设置了15度的旋转范围，因为图像中的狗和猫可以以稍微不同的方式定位(随意调整这个参数)。</p><p>我们将宽度移动范围和高度移动范围设置为0.2，以水平和垂直移动图像，因为动物可以在图像中的任何位置(也可以调整)。</p><p>我们将水平翻转属性设置为<code>True</code>,因为这些动物可以在数据集中翻转(水平翻转；有了垂直翻转，找到一个动物就困难多了)。</p><p>最后，我们将缩放范围设置为0.3，以随机缩放图像，因为狗和猫可能在图像中更远或更近。</p></li>
				<li>我们拟合用训练数据声明的<code>datagen</code>实例，以便计算特征标准化的数量，并再次声明和编译模型，以确保我们没有使用先前的实例:<pre>datagen.fit(x_train)   model = CNN(input_shape)   model.compile(loss='categorical_crossentropy', optimizer='Adadelta', metrics=['accuracy']) ckpt = ModelCheckpoint('Models/model_dogs-cats.h5', save_best_only=True,monitor='val_loss', mode='min', save_weights_only=False)</pre></li>
				<li>Finally, we train the model with the <code>fit_generator</code> method of the model and the <code>flow()</code> method of the <code>datagen</code> instance generated:<pre>model.fit_generator(
          datagen.flow(x_train, y_train, batch_size=8),
          epochs=100,
          verbose=1, 
          validation_data=(x_test, y_test),
          callbacks=[ckpt,earlyStopping],
          steps_per_epoch=len(x_train) // 8,
          workers=4) </pre><p>我们设置<code>steps_per_epoch</code>参数等于训练集的长度除以批量大小(8)。</p><p>我们还将工作线程的数量设置为4，以利用处理器的4个内核:</p><pre>from sklearn import metrics
# Make a prediction
print ("Making predictions...")
model.load_weights('Models/model_dogs-cats.h5')
#y_pred = model.predict(x_test)
y_pred = model.predict(x_test, batch_size=8, verbose=0)
y_pred = np.argmax(y_pred, axis=1)
y_test_aux = y_test.copy()
y_test_pred = list()
for i in y_test_aux:
    y_test_pred.append(np.argmax(i))
print (y_pred)
# Evaluate the prediction
accuracy = metrics.accuracy_score(y_test_pred, y_pred)
precision, recall, f1, support = metrics.precision_recall_fscore_support(y_test_pred, y_pred, average=None)
print('\nFinal results...')
print(metrics.classification_report(y_test_pred, y_pred))
print('Acc      : %.4f' % accuracy)
print('Precision: %.4f' % np.average(precision))
print('Recall   : %.4f' % np.average(recall))
print('F1       : %.4f' % np.average(f1))
print('Support  :', np.sum(support))</pre><p>您应该会看到以下输出:</p><div><img src="img/C13550_05_16.jpg" alt="Figure 5.16: Output showing the accuracy of the model"/></div><h6>图5.16:显示模型准确性的输出</h6><p>从上图可以看出，通过数据扩充，我们实现了<strong class="bold"> 81% </strong>的准确率，这要好得多。</p></li>
				<li>如果我们想加载我们刚刚训练的模型(狗对猫)，下面的代码实现了这一点:<pre>from keras.models import load_model model = load_model('Models/model_dogs-cats.h5')</pre></li>
				<li>Let's try the model with unseen data. The data can be found in the <code>Dataset/testing</code> folder and the code from <em class="italics">Exercise 17</em>, <em class="italics">Building a CNN</em> will be used (but with different names for the samples):<pre>images = ['dog1.jpg', 'dog2.jpg', 'cat1.jpg', 'cat2.jpg'] 
 
for number in range(len(images)):
    imgLoaded = cv2.imread('testing/%s'%(images[number])) 
    img = cv2.resize(imgLoaded, (rows, cols)) 
    img = (img.astype(np.float32))/255.0 
    img = img.reshape(1, rows, cols, 3) 
    </pre><p>在这几行代码中，我们加载了一个图像，将其调整到预期的大小(128 x 128)，对图像进行了归一化处理(就像我们对训练集所做的那样),并将其整形为(1，128，128，3)以适合作为神经网络的输入。</p><p>我们继续for循环:</p><pre>  plt.subplot(1,5,number+1),plt.imshow(cv2.cvtColor(imgLoad ed,cv2.COLOR_BGR2RGB))
    plt.title(np.argmax(model.predict(img)[0])) 
    plt.xticks([]),plt.yticks([]) 
fig = plt.gcf()
plt.show()</pre></li>
			</ol>
			<div><div><img src="img/C13550_05_17.jpg" alt=""/>
				</div>
			</div>
			<h6>图5.17:使用CNN和数据扩充，用看不见的数据预测牛津Pet数据集</h6>
			<p>我们可以看到，模型已经很好地做出了所有的预测。请注意，并不是所有的品种都存储在数据集中，所以并不是所有的猫和狗都能被正确预测。为了达到这个目标，增加更多种类的品种是必要的。</p>
			<h3 id="_idParaDest-102"><a id="_idTextAnchor137"/>活动5:利用数据扩充对花卉图像进行正确分类</h3>
			<p>在这项活动中，你将把你所学到的东西付诸实践。我们将使用不同的数据集，其中的图像更大(150x150)。该数据集中有5个类:雏菊、蒲公英、玫瑰、向日葵和郁金香。总共有4，323张图片，与我们之前执行的练习相比，数量有所减少。这些类也没有相同数量的图像，但是不用担心。图像是RGB的，所以有三个通道。我们已经将它们存储在每个类的NumPy数组中，所以我们将提供一种正确加载它们的方法。</p>
			<p>以下步骤将指导您完成这一过程:</p>
			<ol>
				<li value="1">使用此代码加载数据集，因为数据以NumPy格式存储:<pre>import numpy as np classes = ['daisy','dandelion','rose','sunflower','tulip'] X = np.load("Dataset/flowers/%s_x.npy"%(classes[0])) y = np.load("Dataset/flowers/%s_y.npy"%(classes[0])) print(X.shape) for flower in classes[1:]:     X_aux = np.load("Dataset/flowers/%s_x.npy"%(flower))     y_aux = np.load("Dataset/flowers/%s_y.npy"%(flower))     print(X_aux.shape)     X = np.concatenate((X, X_aux), axis=0)     y = np.concatenate((y, y_aux), axis=0) print(X.shape) print(y.shape)</pre></li>
				<li>Show some samples from the dataset by importing <code>random</code> and <code>matplotlib</code>, using a random index to access the <code>X</code> set.<h4>注意</h4><p class="callout">NumPy数组以BGR格式(OpenCV格式)存储，因此为了正确显示图像，您需要使用下面的代码将格式更改为RGB(仅显示图像):<code>image=cv2.cvtColor(image,cv2.COLOR_BGR2RGB)</code>。</p><p class="callout">您将需要导入<code>cv2</code>。</p></li>
				<li>标准化<code>X</code>集合，并将标签设置为分类(即<code>y</code>集合)。</li>
				<li>将这些集分成一个训练集和一个测试集。</li>
				<li>Build a CNN.<h4>注意</h4><p class="callout">由于我们有更大的图像，你应该考虑增加更多的层，从而减少图像的大小，第一层应该包含一个更大的内核(内核大于3时应该是奇数)。</p></li>
				<li>从Keras中声明ImageDataGenerator，并进行您认为适合数据集变化的更改。</li>
				<li>训练模型。您可以选择提前停止策略，或者设置大量的时期，并在需要时等待或停止。如果您声明检查点回调，它将总是只保存最佳验证损失模型(如果这是您正在使用的度量)。</li>
				<li>Evaluate the model using this code:<pre>from sklearn import metrics
y_pred = model.predict(x_test, batch_size=batch_size, verbose=0)
y_pred = np.argmax(y_pred, axis=1)
y_test_aux = y_test.copy()
y_test_pred = list()
for i in y_test_aux:
    y_test_pred.append(np.argmax(i))
accuracy = metrics.accuracy_score(y_test_pred, y_pred)
print(accuracy)</pre><h4>注意</h4><p class="callout">这将打印模型的准确性。请注意，batch_size是您为训练集以及测试集<code>x_test</code>和<code>y_test</code>设置的批量大小。</p><p class="callout">您可以使用这段代码来评估任何模型，但是首先您需要加载模型。如果你想从一个<code>.h5</code>文件中加载整个模型，你必须使用这个代码:</p><p class="callout"><code>from keras.models import load_model</code>   <code>model = load_model('model.h5')</code></p></li>
				<li>Try the model with unseen data. In the <code>Dataset/testing/</code> folder, you will find five images of flowers that you can load to try it out. Remember that the classes are in this order: <p>classes=['雏菊'，'蒲公英'，'玫瑰'，'向日葵'，'郁金香']</p><p>因此，结果应该是这样的:</p></li>
			</ol>
			<div><div><img src="img/C13550_05_18.jpg" alt="Figure 5.18: Prediction of roses using CNNs"/>
				</div>
			</div>
			<h6>图5.18:使用CNN预测玫瑰</h6>
			<h4>注意</h4>
			<p class="callout">这项活动的解决方案可在第313页找到。</p>
			<h2 id="_idParaDest-103"><a id="_idTextAnchor138"/>最先进的模型——迁移学习</h2>
			<p>人类不会从头开始学习他们想要完成的每一项任务；他们通常以以前的知识为基础，以便更快地学习任务。</p>
			<p>在训练神经网络时，有些任务对每个人来说都是非常昂贵的，例如，有成千上万的图像用于训练，必须区分两个或更多类似的对象，最终需要花费几天的时间来实现良好的性能。这些神经网络被训练来完成这项昂贵的任务，因为神经网络能够保存这些知识，所以其他模型可以利用这些权重来为类似的任务重新训练特定的模型。</p>
			<p><strong class="keyword">转移学习</strong>就是这么做的——它将预训练模型的知识转移到您的模型中，因此您可以利用这些知识。</p>
			<p>因此，例如，如果您想要制作一个能够识别五个对象的分类器，但这项任务似乎过于昂贵(需要知识和时间)，您可以利用预先训练的模型(通常在著名的<strong class="bold"> ImageNet </strong>数据集上训练)，并重新训练适应您的问题的模型。ImageNet数据集是设计用于视觉对象识别研究的大型视觉数据库，具有超过1400万个图像，超过20，000个类别，这对于个人训练来说是非常昂贵的。</p>
			<p>从技术上来说，您可以使用数据集的权重来加载模型，如果您想解决不同的问题，只需更改模型的最后一层。如果在ImageNet上训练模型，它可以有1000个类，但是你只有5个类，所以你可以将最后一层改为只有5个神经元的密集层。不过，您可以在最后一层之前添加更多层。</p>
			<p>您已导入的模型层(基础模型)可以被冻结，这样它们的权重就不会反映在训练时间上。据此，有两种类型的迁移学习:</p>
			<ul>
				<li><strong class="bold">传统</strong>:冻结基础模型的所有图层</li>
				<li><strong class="bold">微调</strong>:仅冻结基础模型的一部分，通常是第一层</li>
			</ul>
			<p>在Keras中，我们可以导入著名的预训练模型，如Resnet50和VGG16。您可以导入带权重或不带权重的预训练模型(在Keras中，只有ImageNet的权重)，包括或不包括模型的顶部。仅当不包括顶部且最小尺寸为32时，才能指定输入形状。</p>
			<p>使用以下代码行，您将导入不带顶部的Resnet50模型，其重量为<code>imagenet</code>，形状为150x150x3:</p>
			<pre>from keras.applications import resnet50
model = resnet50.ResNet50(include_top=False, weights='imagenet', input_shape=(150,150,3))</pre>
			<p>如果您已经包括了模型的顶部，因为您想要使用模型的最后密集层(假设您的问题类似于ImageNet，但是具有不同的类)，那么您应该编写以下代码:</p>
			<pre>from keras.models import Model
from keras.layers import Dense
 
model.layers.pop()
model.outputs = [model.layers[-1].output]
model.layers[-1].outbound_nodes = []
 
x=Dense(5, activation='softmax')(model.output)
model=Model(model.input,x)</pre>
			<p>这段代码去掉了分类层(最后一个密集层)并准备好模型，以便您可以添加自己的最后一层。当然，您可以在添加分类图层之前添加更多的图层。</p>
			<p>如果您没有添加模型的顶部，那么您应该使用以下代码添加您自己的顶部:</p>
			<pre>from keras.models import Model
from keras.layers import Dense, GlobalAveragePooling2D
x=base_model.output
x=GlobalAveragePooling2D()(x)
x=Dense(512,activation='relu')(x) #dense layer 2
x=Dropout(0.3)(x)
x=Dense(512,activation='relu')(x) #dense layer 3
x=Dropout(0.3)(x)
preds=Dense(5,activation='softmax')(x) #final layer with softmax activation
model=Model(inputs=base_model.input,outputs=preds)</pre>
			<p>在这里，<code>GlobalAveragePooling2D</code>就像一种最大池。</p>
			<p>对于这些类型的模型，您应该像处理训练这些模型的数据一样对数据进行预处理(如果您正在使用权重)。Keras有一个<code>preprocess_input</code>方法，对每个模型都这样做。例如，对于ResNet50，它应该是这样的:</p>
			<pre>from keras.applications.resnet50 import preprocess_input</pre>
			<p>您将图像数组传递给该函数，然后您将为训练准备好数据。</p>
			<p>模型中的<strong class="bold">学习率</strong>是它应该多快将模型转换为局部最小值。通常，你不必担心这一点，但如果你正在重新训练一个神经网络，这是一个你必须调整的参数。重新训练神经网络时，应该减小该参数的值，以便神经网络不会忘记它已经学习过的内容。这个参数在声明优化器时被调整。您可以避免调整该参数，尽管模型可能最终不会收敛或过度拟合。</p>
			<p>使用这种方法，您可以用很少的数据训练您的网络，并获得总体良好的结果，因为您利用了模型的权重。</p>
			<p>你也可以把迁移学习和数据扩充结合起来。</p>
			<h3 id="_idParaDest-104"><a id="_idTextAnchor139"/>练习19:使用极少数据的迁移学习对€5号和€20号钞票进行分类</h3>
			<p>这个问题是关于用很少的数据区分€5元纸币和€20元纸币。我们每节课有30张图片，这比我们之前的练习要少得多。我们将加载数据，声明预训练模型，然后通过数据扩充声明对数据的更改，并训练模型。在此之后，我们将检查模型在看不见数据的情况下表现如何:</p>
			<ol>
				<li value="1">Open up your Google Colab interface.<h4>注意</h4><p class="callout">您需要在您的驱动器上安装<code>Dataset</code>文件夹，并相应地设置路径。</p></li>
				<li>Declare functions to load the data:<pre>import re, os, cv2
import numpy as np
def list_files(directory, ext=None):
//{…}
##the detailed code can be found on Github##
 
def load_images(path,label):
//{…}
##the detailed code can be found on Github##
    for fname in list_files( path, ext='jpg' ): 
        img = cv2.imread(fname)
        img = cv2.resize(img, (224, 224))
//{…}
##the detailed<a id="_idTextAnchor140"/> code can be found on Github##</pre><p>请注意，数据大小调整为224x224。</p></li>
				<li>The data is stored in <code>Dataset/money/</code>, where you have both classes in subfolders. In order to load the data, you have to write the following code:<pre>X, y = load_images('Dataset/money/20',0)
X_aux, y_aux = load_images('Dataset/money/5',1)
X = np.concatenate((X, X_aux), axis=0)
y = np.concatenate((y, y_aux), axis=0)
print(X.shape)
print(y.shape)</pre><p>€20元钞票的标签是0，€5元钞票的标签是1。</p></li>
				<li>我们来展示一下数据:<pre>import random  random.seed(42)  from matplotlib import pyplot as plt   for idx in range(5):      rnd_index = random.randint(0, 59)     plt.subplot(1,5,idx+1),plt.imshow(cv2.cvtColor(X[rnd_index],cv2.COLOR_BGR2RGB))      plt.xticks([]),plt.yticks([]) plt.savefig("money_samples.jpg", bbox_inches='tight') plt.show() </pre> <div> <img src="img/C13550_05_19.jpg" alt="Figure 5.21: Samples of bills"/> </div> <h6>图5.19:票据样本</h6></li>
				<li>Now we are going to declare the pretrained model:<pre>from keras.applications.mobilenet import MobileNet, preprocess_input
from keras.layers import Input, GlobalAveragePooling2D, Dense, Dropout
from keras.models import Model
 
input_tensor = Input(shape=(224, 224, 3))
 
base_model = MobileNet(input_tensor=input_tensor,weights='imagenet',include_top=False)
 
x = base_model.output
x = GlobalAveragePooling2D()(x)
x = Dense(512,activation='relu')(x)
x = Dropout(0.5)(x)
x = Dense(2, activation='softmax')(x)
 
model = Model(base_model.input, x)</pre><p>在本例中，我们使用imagenet的权重加载MobileNet模型。我们不包括顶部，所以我们应该建立自己的顶部。输入形状为224x224x3。</p><p>我们已经通过获取MobileNet最后一层(不是分类层)的输出构建了模型的顶层，并开始在此基础上构建。我们添加了<code>GlobalAveragePooling2D</code>用于图像缩减，一个可以针对我们的具体问题进行训练的密集层，一个<code>Dropout</code>层用于避免过度拟合，最后是分类器层。</p><p>末端的致密层有两个神经元，因为我们只有两个类，它有<code>Softmax</code>激活功能。对于二元分类，也可以使用Sigmoid函数，但是它会改变整个过程，因为您不应该使标签分类，并且预测看起来不同。</p><p>随后，我们创建我们将要训练的模型，将MobileNet的输入作为输入，将分类密集层作为输出。</p></li>
				<li>我们将进行微调。为了做到这一点，我们必须冻结一些输入层，并保持其余的可训练数据不变:<pre>for layer in model.layers[:20]:     layer.trainable=False for layer in model.layers[20:]:     layer.trainable=True</pre></li>
				<li>让我们用<code>Adadelta</code>优化器来编译这个模型:<pre>import keras model.compile(loss='categorical_crossentropy',optimizer=keras.optimizers.Adadelta(), metrics=['accuracy'])</pre></li>
				<li>现在，我们将使用之前导入的<code>preprocess_input</code>方法来预处理MobileNet的<code>X</code>集合，然后我们将标签<code>y</code>转换为独热编码:<pre>from keras import utils as np_utils X = preprocess_input(X) #X = (X.astype(np.float32))/255.0  y = np_utils.to_categorical(y)</pre></li>
				<li>我们使用<code>train_test_split</code>方法将数据分成训练集和测试集:<pre>from sklearn.model_selection import train_test_split x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2) print(x_train.shape) print(y_train.shape) print(x_test.shape) print(y_test.shape)</pre></li>
				<li>We are going to apply data augmentation to our dataset:<pre>from keras.preprocessing.image import ImageDataGenerator
train_datagen = ImageDataGenerator(  
      rotation_range=90,     
      width_shift_range = 0.2,
      height_shift_range = 0.2,
      horizontal_flip=True,    
      vertical_flip=True,
      zoom_range=0.4)
train_datagen.fit(x_train)</pre><p>由于钞票可以处于不同的角度，我们选择旋转90度。对于这项任务来说，其他参数似乎是合理的。</p></li>
				<li>Let's declare a checkpoint to save the model when the validation loss decreases and train the model:<pre>from keras.callbacks import ModelCheckpoint
ckpt = ModelCheckpoint('Models/model_money.h5', save_best_only=True, monitor='val_loss', mode='min', save_weights_only=False)
model.fit_generator(train_datagen.flow(x_train, y_train,
                                batch_size=4),
                    epochs=50,
                    validation_data=(x_test, y_test),
                    callbacks=[ckpt],
                    steps_per_epoch=len(x_train) // 4,
                    workers=4)</pre><p>我们将批大小设置为4，因为我们只有几个数据样本，并且我们不想一次将所有样本传递给神经网络，而是分批传递。我们没有使用EarlyStopping回调，因为由于缺乏数据和使用具有高学习率的Adadelta，损失会上下波动。</p></li>
				<li>Check the results:<div><img src="img/C13550_05_20.jpg" alt="Figure 5.22: Showing the desired output"/></div><h6>图5.20:显示期望的输出</h6><p>在上图中，我们可以看到，在第7个时期，我们已经实现了100%的准确性和低损失。这是因为缺少验证集上的数据，因为只有12个样本，您无法判断模型是否对看不见的数据表现良好。</p></li>
				<li>Let's run the code to calculate the accuracy of this model:<pre>y_pred = model.predict(x_test, batch_size=4, verbose=0)
y_pred = np.argmax(y_pred, axis=1)
y_test_aux = y_test.copy()
y_test_pred = list()
for i in y_test_aux:
    y_test_pred.append(np.argmax(i))
 
accuracy = metrics.accuracy_score(y_test_pred, y_pred)
print('Acc: %.4f' % accuracy)</pre><p>输出如下所示:</p><div><img src="img/C13550_05_21.jpg" alt="Figure 5.23: Accuracy achieved of 100%"/></div><h6>图5.21:准确率达到100%</h6></li>
				<li>Let's try this model with new data. There are test images in the <code>Dataset/testing</code> folder. We have added four examples of bills to check whether the model predicts them well:<h4>注意</h4><pre>images = ['20.jpg','20_1.jpg','5.jpg','5_1.jpg']
model.load_weights('Models/model_money.h5')
for number in range(len(images)):
    imgLoaded = cv2.imread('Dataset/testing/%s'%(images[number])) 
    img = cv2.resize(imgLoaded, (224, 224)) 
    #cv2.imwrite('test.jpg',img) 
    img = (img.astype(np.float32))/255.0 
    img = img.reshape(1, 224, 224, 3) 
    plt.subplot(1,5,number+1),plt.imshow(cv2.cvtColor(imgLoaded,cv2.COLOR_BGR2RGB)) 
    plt.title('20' if np.argmax(model.predict(img)[0]) == 0 else '5') 
    plt.xticks([]),plt.yticks([]) 
plt.show()</pre><p>在这段代码中，我们还加载了不可见的示例，并且我们组合了输出图像，如下所示:</p></li>
			</ol>
			<div><div><img src="img/C13550_05_22.jpg" alt="Figure 5.24: Prediction of bills"/>
				</div>
			</div>
			<h6>图5.22:账单预测</h6>
			<p>模型已经精确地预测了所有的图像！</p>
			<p>恭喜你！现在，由于迁移学习，当数据很少时，你可以用自己的数据集训练模型。</p>
			<h4>注意</h4>
			<p class="callout">这个练习的完整代码上传到GitHub:https://GitHub . com/packt publishing/Artificial-Vision-and-Language-Processing-for-Robotics/blob/master/lesson 05/exercise 19/exercise 19 . ipynb。</p>
			<h2 id="_idParaDest-105"><a id="_idTextAnchor141"/>总结</h2>
			<p>在处理图像时，CNN已经显示出比全连接神经网络好得多的性能。此外，CNN也能够用文本和声音数据实现良好的结果。</p>
			<p>CNN已经得到了深入的解释，卷积是如何工作的，以及随之而来的所有参数。之后，所有这些理论都通过一个练习付诸实践。</p>
			<p>数据扩充是一种通过对原始数据进行简单变换以生成新图像来克服数据集中缺少数据或缺少变化的技术。这个技巧已经被解释过了，并通过一个练习和一个活动付诸实践，在那里你能够用你所获得的知识进行实验。</p>
			<p>迁移学习是一种在缺乏数据或问题非常复杂以至于在正常的神经网络上训练需要太长时间时使用的技术。此外，这种技术根本不需要了解神经网络，因为模型已经实现了。它也可以与数据扩充一起使用。</p>
			<p>迁移学习也包括在内，并在一个数据量很小的练习中付诸实践。</p>
			<p>学习如何构建CNN对于在计算机视觉中识别物体或环境非常有用。当机器人使用其视觉传感器来识别环境时，通常使用CNN，并且使用数据增强来提高CNN的性能。在<em class="italics">第八章</em>、<em class="italics">使用CNN引导机器人的物体识别中，</em>你所学的CNN概念将被应用到现实世界的应用中，你将能够使用深度学习来识别环境。</p>
			<p>在应用这些技术来识别环境之前，首先你需要学习如何管理一个能够识别环境的机器人。在<em class="italics">第6章，机器人操作系统(ROS) </em>中，你将学习如何利用一个叫做ROS的软件，使用模拟器来管理机器人。</p>
		</div>
	

</body></html>