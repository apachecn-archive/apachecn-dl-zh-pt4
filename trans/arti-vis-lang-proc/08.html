<html><head/><body>


	
		<title>C13550_08_EPUB_Final_SW</title>
		
	
	
		<div><h1 id="_idParaDest-142"><em class="italics"> <a id="_idTextAnchor184"/>第八章</em></h1>
		</div>
		<div><h1 id="_idParaDest-143"><a id="_idTextAnchor185"/>用CNN引导机器人进行物体识别</h1>
		</div>
		<div><h2>学习目标</h2>
			<p>本章结束时，您将能够:</p>
			<ul>
				<li class="bullets">解释物体识别的工作原理</li>
				<li class="bullets">建立一个能够识别物体的网络</li>
				<li class="bullets">建立一个物体识别系统</li>
			</ul>
			<p>本章通过构建一个能够基于视频识别物体的网络，讲述了物体识别的工作原理。</p>
		</div>
		<div><h2 id="_idParaDest-144"><a id="_idTextAnchor186"/>简介</h2>
			<p><strong class="keyword">物体识别</strong>是计算机视觉的一个领域，在这个领域中，机器人能够使用能够提取机器人周围图像的摄像机或传感器来检测环境中的物体。从这些图像中，软件检测每个图像中的对象，然后识别对象的类型。机器能够从机器人传感器捕获的图像或视频中识别物体。这使得机器人能够意识到他们的环境。</p>
			<p>如果机器人能够识别其环境并使用物体识别获得这些信息，它将能够执行更复杂的任务，例如抓取物体或在环境中四处移动。在<em class="italics">第9章</em>、<em class="italics">机器人的计算机视觉</em>中，我们将看到一个机器人在虚拟环境中执行这些任务。</p>
			<p>这里要执行的任务是检测图像中的特定对象并识别这些对象。这种类型的计算机视觉问题与我们在本书前面看到的问题有点不同。为了识别特定的对象，我们已经看到了标记这些对象并训练卷积神经网络，这在第5章、<em class="italics">用于计算机视觉的卷积神经网络</em>中讨论过，这将很好地工作，但是首先检测这些对象呢？</p>
			<p>之前，我们了解到，我们想要识别的对象必须用它们所属的相应类来标记。因此，为了检测图像中的这些对象，必须在它们周围绘制矩形边界框，以便正确定位它们在图像中的位置。然后，神经网络将预测这些对象的边界框和标签。</p>
			<p>用边界框标记对象是一项单调乏味的艰巨任务，因此我们不会展示用边界框标记数据集中的图像的过程，也不会展示训练神经网络来识别和检测这些对象的过程。尽管如此，还是有一个名为<code>labelImg</code>的库，你可以在这个https://github.com/tzutalin/labelImg<a href="https://github.com/tzutalin/labelImg">githubrepository:﷟</a>中访问它。这允许您为图像中的每个对象创建边界框。一旦创建了边界框(在数据方面称为坐标)，就可以训练神经网络来预测图像中每个对象的边界框和相应的标签。</p>
			<p>在这一章中，我们将使用YOLO网络的最先进的方法，这些方法随时可以使用，将使你不必构建自己的算法。</p>
			<h2 id="_idParaDest-145"><a id="_idTextAnchor187"/>多目标识别与检测</h2>
			<p>多对象识别和检测涉及检测和识别图像中的几个对象。这项任务包括用边界框标记每个对象，然后识别该对象的类型。</p>
			<p>正因为如此，有许多可用的预训练模型可以检测许多对象。被称为<strong class="keyword"> YOLO </strong>的神经网络是这项特定任务的最佳模型之一，并实时工作。YOLO将在下一章详细解释机器人模拟器的开发。</p>
			<p>对于这一章，我们要使用的YOLO网络经过训练，可以识别和检测80种不同的类别。这些类别是:</p>
			<p>人、自行车、汽车、摩托车、飞机、公共汽车、火车、卡车、船、交通灯、消防栓、停车标志、停车计时器、长凳、鸟、猫、狗、马、羊、牛、大象、熊、斑马、长颈鹿、背包、雨伞、手提包、领带、手提箱、飞盘、滑雪板、运动球、风筝、棒球棍、棒球手套、滑板、冲浪板、网球拍、瓶子、酒杯、杯子、叉子、刀、勺子、碗、香蕉、苹果、三明治、橙子、花椰菜、胡萝卜、热狗、比萨饼、油炸圈饼、蛋糕、椅子</p>
			<p>在图8.1中，您可以看到一条街道的示例，其中使用YOLO检测到了人、汽车和公交车:</p>
			<div><div><img src="img/C13550_08_01.jpg" alt="Figure 8.1: YOLO detection sample"/>
				</div>
			</div>
			<h6>图8.1: YOLO检测样本</h6>
			<p>在这个主题中，我们将建立一个静态图像的多目标识别和检测系统。</p>
			<p>首先，我们将使用一个名为<strong class="bold"> DNN </strong>(深度神经网络)的OpenCV模块来这样做，它包含几行代码。稍后，我们将使用一个名为<strong class="keyword"> ImageAI </strong>的库，它做同样的事情，但代码不到10行，并且允许您选择想要检测和识别的特定对象。</p>
			<p>为了用OpenCV实现YOLO，你需要用OpenCV导入图像，就像我们在本书的其他章节中提到的那样。</p>
			<h3 id="_idParaDest-146"><a id="_idTextAnchor188"/>练习24:构建您的第一个多目标检测和识别算法</h3>
			<h4>注意</h4>
			<p class="callout">我们将使用Google Colab笔记本，因为这项任务不涉及训练算法，而是使用算法。</p>
			<p>在这个练习中，我们将使用YOLO和OpenCV实现一个多目标检测和识别系统。我们将编写一个检测器和一个识别器系统，它将图像作为输入，检测并识别图像中的对象，然后输出包含这些检测的图像:</p>
			<ol>
				<li>打开你的Google Colab界面。</li>
				<li>导入以下库:<pre>import cv2 import numpy as np import matplotlib.pyplot as plt</pre></li>
				<li>To input an image to this network, we need to use the <code>blobFromImage</code> method:<h4>注意</h4><pre>image = cv2.imread('Dataset/obj_det/image6.jpg')
Width = image.shape[1]
Height = image.shape[0]
scale = 0.00392</pre><p>我们需要加载数据集的类，对于YOLO，这些类存储在<code>Models/yolov3.txt</code>中，你可以在GitHub的<code>Chapter 8/Models</code>中找到。我们这样读这些课:</p><pre># read class names from text file
classes = None
with open("Models/yolov3.txt", 'r') as f:
    classes = [line.strip() for line in f.readlines()]</pre></li>
				<li>为不同的类生成不同的颜色:<pre>COLORS = np.random.uniform(0, 255, size=(len(classes), 3))</pre></li>
				<li>读取预先训练好的模型和配置文件:<pre>net = cv2.dnn.readNet('Models/yolov3.weights', 'Models/yolov3.cfg')</pre></li>
				<li>创建输入斑点:<pre>blob = cv2.dnn.blobFromImage(image.copy(), scale, (416,416), (0,0,0), True, crop=False)</pre></li>
				<li>Set the input blob for the network:<pre>net.setInput(blob)</pre><p>为了声明网络，我们使用来自<code>Models/yolov3.weights</code>的<code>readNet</code>方法，这是网络的权重，以及<code>Models/yolov3.cfg</code>，这是模型的架构:</p><h4>注意</h4><p class="callout">方法、类、权重和架构文件可以在GitHub的<code>Lesson08/Models/</code>文件夹中找到。</p><p>现在我们已经设置好了，为了识别和检测图像中的所有对象，剩下的唯一事情就是运行和执行代码，这将在下面解释。</p></li>
				<li>为了获得网络的输出层，我们声明下面代码中提到的方法，然后运行接口以获得输出层的数组，它包含几个检测:<pre># function to get the output layer names in the architecture def get_output_layers(net):          layer_names = net.getLayerNames()          output_layers = [layer_names[i[0] - 1] for i in net.getUnconnectedOutLayers()]     return output_layers</pre></li>
				<li>创建一个函数，用类名:<pre>def draw_bounding_box(img, class_id, confidence, x, y, x_plus_w, y_plus_h):     label = str(classes[class_id])     color = COLORS[class_id]     cv2.rectangle(img, (x,y), (x_plus_w,y_plus_h), color, 2)     cv2.putText(img, label + " " + str(confidence), (x-10,y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)</pre>在检测到的对象周围绘制一个边界框</li>
				<li>Execute the code:<pre># run inference through the network
# and gather predictions from output layers
outs = net.forward(get_output_layers(net))</pre><h4>注意</h4><p class="callout">outs是一组预测。在后面的练习中，我们将看到我们必须循环这个数组，以获得每个检测的边界框和置信度，以及类的类型。</p><p>对象检测算法经常多次检测一个对象，这是一个问题。这个问题可以通过使用<strong class="bold">非最大抑制</strong>来解决，它删除每个具有较低置信度(对象在预测类中的概率)的对象的边界框，之后唯一保留的边界框是具有最高置信度的边界框。在检测包围盒和置信度并声明相应的阈值之后，该算法可以如下运行:</p></li>
				<li>这一步是最重要的一步。这里，我们将从每个输出层(检测到的每个对象)、类ID和边界框的每个检测中收集置信度，但是我们将忽略置信度小于50%的检测:<pre># apply non-max suppression class_ids = [] confidences = [] boxes = [] conf_threshold = 0.5 nms_threshold = 0.4 indexes = cv2.dnn.NMSBoxes(boxes, confidences, conf_threshold, nms_threshold)</pre></li>
				<li>对于来自每个输出层的每个检测，获取置信度、类ID和边界框参数，并忽略弱检测(置信度&lt; 0.5): 【T0】 </li>
				<li>We loop over the list of indexes and use the method that we declared for printing to print every bounding box, every label, and every confidence on the input image: 【T1】 </li>
				<li>Finally, we show and save the resulting image. OpenCV has a method for showing it also; there is no need to use Matplotlib:<pre># display output image    
plt.axis("off")
plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))
# save output image to disk
cv2.imwrite("object-detection6.jpg", image)</pre><p>The output is as follows:</p><div><img src="img/C13550_08_02.jpg" alt="Figure 8.2: YOLO detection sample"/></div><h6>Figure 8.2: YOLO detection sample</h6><p>Finally, we have to draw the bounding boxes, its classes, and the confidence. </p></li>
				<li>Now let's try some other examples using the steps mentioned previously. You can find the images in the  【T2】  folder. The outputs will be as shown in Figure 8.3:</li>
			</ol>
			<div><div><img src="img/Image52926.jpg" alt=""/>
				</div>
			</div>
			<div><div><img src="img/Image52947.jpg" alt="Figure 8.3: YOLO detection sample"/>
				</div>
			</div>
			<h6>Figure 8.3: YOLO detection sample</h6>
			<h3 id="_idParaDest-147"><a id="_idTextAnchor189"/> ImageAI</h3>
			<p>还有另一种方法可以轻松实现这一点。您可以使用<strong class="keyword"> ImageAI </strong>库，它能够用几行代码执行对象检测和识别。</p>
			<p>这个库的GitHub库的链接可以在这里找到:</p>
			<p><a href="https://github.com/OlafenwaMoses/ImageAI">https://github.com/OlafenwaMoses/ImageAI</a></p>
			<p>为了安装该库，您可以通过使用pip和以下命令来完成:</p>
			<pre>pip install <a href="https://github.com/OlafenwaMoses/ImageAI/releases/download/2.0.2/imageai-2.0.2-py3-none-any.whl">https://github.com/OlafenwaMoses/ImageAI/releases/download/2.0.2/imageai-2.0.2-py3-none-any.whl</a></pre>
			<p>要使用这个库，我们需要导入一个类:</p>
			<pre>from imageai.Detection import ObjectDetection</pre>
			<p>我们导入了<code>ObjectDetection</code>类，它将作为神经网络工作。</p>
			<p>之后，我们声明将要进行预测的类的对象:</p>
			<pre>detector = ObjectDetection()</pre>
			<p>我们将要使用的模型必须声明。对于这个库，我们只能使用三种模型:RetinaNet、YOLOV3和TinyYOLOV3。YOLOV3与我们之前使用的型号相同，具有中等的性能和准确性，检测时间也适中。</p>
			<p>至于RetinaNet，它具有更高的性能和准确性，但检测时间较长。</p>
			<p>TinyYOLOV3针对速度进行了优化，具有中等的性能和准确性，但检测时间要快得多。这个模型将在下一个主题中使用，因为它的速度。</p>
			<p>您只需要修改几行代码就可以使用这些模型了。对于YOLOV3，需要以下几行:</p>
			<pre>detector.setModelTypeAsYOLOv3()
detector.setModelPath("Models/yolo.h5")
detector.loadModel()</pre>
			<p><code>.h5</code>文件包含了YOLOV3神经网络的权重和架构。</p>
			<p>要运行推理并获得相应的检测，只需要一行代码:</p>
			<pre>detections = detector.detectObjectsFromImage(input_image="Dataset/obj_det/sample.jpg", output_image_path="samplenew.jpg")</pre>
			<p>这行代码的作用是将一幅图像作为输入，检测图像中的对象及其类的边界框。它输出用这些检测绘制的新图像，以及检测到的对象的列表。</p>
			<p>让我们看看它如何检测我们在上一个练习中使用的<code>sample.jpg</code>图像:</p>
			<div><div><img src="img/Image52955.jpg" alt="Figure 8.4: ImageAI YOLOV3 image detection"/>
				</div>
			</div>
			<h6>图8.4: ImageAI YOLOV3图像检测</h6>
			<p>ImageAI还允许你定制你想要识别的对象。默认情况下，它还能够检测与YOLO相同的类，这是使用OpenCV构建的，即80个类。</p>
			<p>您可以通过将一个对象作为名为<code>CustomObjects</code>的参数进行传递来定制它，以便只检测您想要的对象，在这里您可以指定您想要模型检测哪些对象。此外，检测器识别这些物体的方法从<code>detectObjectsFromImage()</code>变为<code>detectCustomObjectsFromImage()</code>。它是这样使用的:</p>
			<pre>custom_objects = detector.CustomObjects(car=True)
detections = detector.detectCustomObjectsFromImage(custom_objects=custom_objects, input_image="Dataset/obj_det/sample.jpg", output_image_path="samplenew.jpg")</pre>
			<div><div><img src="img/C13550_08_05.jpg" alt="Figure 8.5: ImageAI YOLOV3 custom image detection"/>
				</div>
			</div>
			<h6>图8.5: ImageAI YOLOV3自定义图像检测</h6>
			<h2 id="_idParaDest-148"><a id="_idTextAnchor190"/>视频中的多目标识别与检测</h2>
			<p>静态图像中的多物体识别和检测听起来很神奇，但在视频中检测和识别物体呢？</p>
			<p>你可以从互联网上下载任何视频，并尝试检测和识别视频中出现的所有对象。</p>
			<p>接下来的过程是获取视频的每一帧，并为每一帧检测相应的对象及其标签。</p>
			<p>首先声明相应的库:</p>
			<pre>from imageai.Detection import VideoObjectDetection
from matplotlib import pyplot as plt</pre>
			<p><code>imageai</code>库包含一个对象，允许用户对视频应用对象检测和识别:</p>
			<pre>video_detector = VideoObjectDetection()</pre>
			<p>我们需要V <code>ideoObjectDetection</code>来检测视频中的物体。此外，需要Matplotlib来显示每一帧的检测过程:</p>
			<div><div><img src="img/C13550_08_06.jpg" alt="Figure 8.6: ImageAI one-frame object detection process"/>
				</div>
			</div>
			<h6>图8.6: ImageAI单帧对象检测过程</h6>
			<p>现在我们首先需要加载模型。您可以根据您需要的视频处理速度和精度来决定加载什么模型。YOLOV3处于中间，在RetinaNet和TinyYOLOV3之间，RetinaNet最精确但最慢，TinyYOLOV3最不精确但最快。我们将坚持使用YOLOV3型号，但也可以随意使用其他两种型号。声明视频对象检测后的声明与上一个主题中的声明相同:</p>
			<pre>video_detector.setModelTypeAsYOLOv3()
video_detector.setModelPath("Models/yolo.h5")
video_detector.loadModel()</pre>
			<p>在运行视频检测器之前，我们需要声明一个函数，该函数将应用于处理的每一帧。该函数不执行检测算法，但它处理每一帧的检测过程。为什么我们必须在物体检测过程之后处理每一帧的输出？这是因为我们希望使用Matplotlib一帧一帧地显示检测过程..</p>
			<p>在声明该方法之前，我们需要声明对象将要打印的颜色:</p>
			<pre>color_index = {'bus': 'red', 'handbag': 'steelblue', 'giraffe': 'orange', 'spoon': 'gray', 'cup': 'yellow', 'chair': 'green', 'elephant': 'pink', 'truck': 'indigo', 'motorcycle': 'azure', 'refrigerator': 'gold', 'keyboard': 'violet', 'cow': 'magenta', 'mouse': 'crimson', 'sports ball': 'raspberry', 'horse': 'maroon', 'cat': 'orchid', 'boat': 'slateblue', 'hot dog': 'navy', 'apple': 'cobalt', 'parking meter': 'aliceblue', 'sandwich': 'skyblue', 'skis': 'deepskyblue', 'microwave': 'peacock', 'knife': 'cadetblue', 'baseball bat': 'cyan', 'oven': 'lightcyan', 'carrot': 'coldgrey', 'scissors': 'seagreen', 'sheep': 'deepgreen', 'toothbrush': 'cobaltgreen', 'fire hydrant': 'limegreen', 'remote': 'forestgreen', 'bicycle': 'olivedrab', 'toilet': 'ivory', 'tv': 'khaki', 'skateboard': 'palegoldenrod', 'train': 'cornsilk', 'zebra': 'wheat', 'tie': 'burlywood', 'orange': 'melon', 'bird': 'bisque', 'dining table': 'chocolate', 'hair drier': 'sandybrown', 'cell phone': 'sienna', 'sink': 'coral', 'bench': 'salmon', 'bottle': 'brown', 'car': 'silver', 'bowl': 'maroon', 'tennis racket': 'palevilotered', 'airplane': 'lavenderblush', 'pizza': 'hotpink', 'umbrella': 'deeppink', 'bear': 'plum', 'fork': 'purple', 'laptop': 'indigo', 'vase': 'mediumpurple', 'baseball glove': 'slateblue', 'traffic light': 'mediumblue', 'bed': 'navy', 'broccoli': 'royalblue', 'backpack': 'slategray', 'snowboard': 'skyblue', 'kite': 'cadetblue', 'teddy bear': 'peacock', 'clock': 'lightcyan', 'wine glass': 'teal', 'frisbee': 'aquamarine', 'donut': 'mincream', 'suitcase': 'seagreen', 'dog': 'springgreen', 'banana': 'emeraldgreen', 'person': 'honeydew', 'surfboard': 'palegreen', 'cake': 'sapgreen', 'book': 'lawngreen', 'potted plant': 'greenyellow', 'toaster': 'ivory', 'stop sign': 'beige', 'couch': 'khaki'}</pre>
			<p>现在我们要声明应用于每一帧的方法:</p>
			<pre>def forFrame(frame_number, output_array, output_count, returned_frame):
    plt.clf()
    this_colors = []
    labels = []
    sizes = []
    counter = 0</pre>
			<p>首先，如图所示，声明该函数，并将帧数、检测数组、检测到的每个对象的出现次数以及帧数传递给它。此外，我们声明了相应的变量，我们将使用这些变量来打印每一帧上的所有检测:</p>
			<pre>    for eachItem in output_count:
        counter += 1
        labels.append(eachItem + " = " + str(output_count[eachItem]))
        sizes.append(output_count[eachItem])
        this_colors.append(color_index[eachItem])</pre>
			<p>在这个循环中，存储对象及其相应的出现。还存储了代表每个对象的颜色:</p>
			<pre>    plt.subplot(1, 2, 1)
    plt.title("Frame : " + str(frame_number))
    plt.axis("off")
    plt.imshow(returned_frame, interpolation="none")
    plt.subplot(1, 2, 2)
    plt.title("Analysis: " + str(frame_number))
    plt.pie(sizes, labels=labels, colors=this_colors, shadow=True, startangle=140, autopct="%1.1f%%")
    plt.pause(0.01)</pre>
			<p>在最后这段代码中，为每一帧打印了两个图:一个显示带有相应检测的图像，另一个显示包含每个检测到的对象的出现次数及其占总出现次数的百分比的图表。</p>
			<p>该输出如图8.6所示。</p>
			<p>在最后一个单元格中，为了执行视频检测器，我们编写了这几行代码:</p>
			<pre>plt.show()
video_detector.detectObjectsFromVideo(input_file_path="path_to_video.mp4", output_file_path="output-video" ,  frames_per_second=20, per_frame_function=forFrame,  minimum_percentage_probability=30, return_detected_frame=True, log_progress=True)</pre>
			<p>第一行初始化Matplotlib图。</p>
			<p>第二行开始视频检测。传递给该函数的参数如下:</p>
			<ul>
				<li><code>input_file_path</code>:输入视频路径</li>
				<li><code>output_file_path</code>:输出视频路径</li>
				<li><code>frames_per_second</code>:输出视频的每秒帧数</li>
				<li><code>per_frame_function</code>:每次检测帧内物体后的回调函数</li>
				<li><code>minimum_percentage_probability</code>:最小概率值阈值，仅考虑置信度最高的检测</li>
				<li><code>return_detected_frame</code>:如果设置为真，回调函数接收帧作为参数</li>
				<li><code>log_progress</code>:如果设置为True，进程将记录在控制台中</li>
			</ul>
			<h3 id="_idParaDest-149"><a id="_idTextAnchor192"/>活动8:视频中的多目标检测和识别</h3>
			<p>在本练习中，我们将逐帧处理视频，检测每帧中所有可能的对象，并将输出视频保存到磁盘:</p>
			<h4>注意</h4>
			<p class="callout">我们将在本次活动中使用的视频上传到GitHub的<code>Dataset/videos/street.mp4</code>文件夹中:</p>
			<p class="callout">网址:<a href="https://github.com/PacktPublishing/Artificial-Vision-and-Language-Processing-for-Robotics/blob/master/Lesson08/Dataset/videos/street.mp4">https://github . com/packt publishing/Artificial-Vision-and-Language-Processing-for-Robotics/blob/master/lesson 08/Dataset/videos/street . MP4</a></p>
			<ol>
				<li value="1">打开一个Google Colab笔记本，挂载磁盘，导航到第8章所在的位置。</li>
				<li>在笔记本中安装这个库，因为它没有预先安装，使用这个命令:<pre>!pip3 install https://github.com/OlafenwaMoses/ImageAI/releases/download/2.0.2/imageai-2.0.2-py3-none-any.whl</pre></li>
				<li>导入开发该活动所需的库，并设置<code>matplotlib</code>。</li>
				<li>Declare the model that you are going to use for detecting and recognizing objects.<h4>注意</h4><p class="callout">您可以在这里找到该信息:<a href="https://github.com/OlafenwaMoses/ImageAI/blob/master/imageai/Detection/VIDEO.md">https://github . com/OlafenwaMoses/ImageAI/blob/master/ImageAI/Detection/video . MD</a></p><p class="callout">还要注意，所有模型都存储在<code>Models</code>文件夹中。</p></li>
				<li>声明在处理每一帧后将要调用的回调方法。</li>
				<li>Run Matplotlib and the video detection processes on the <code>street.mp4</code> video that is inside the <code>Dataset/videos/</code> folder. You can also try out the <code>park.mp4</code> video, which is in the same directory.<h4>注意</h4><p class="callout">这项活动的解决方案在第326页。</p></li>
			</ol>
			<h2 id="_idParaDest-150"><a id="_idTextAnchor193"/>总结</h2>
			<p>对象识别和检测能够识别图像中的几个对象，在这些对象周围绘制边界框，并预测它们是什么类型的对象。</p>
			<p>已经解释了标记边界框和它们的标签的过程，但是不深入，因为需要巨大的过程。相反，我们使用最先进的模型来识别和检测这些物体。</p>
			<p>YOLOV3是本章中使用的主要型号。OpenCV被用来解释如何使用它的DNN模块运行对象检测流水线。ImageAI是一个用于对象检测和识别的替代库，它显示了用几行代码和简单的对象定制编写对象检测管道的潜力。</p>
			<p>最后，通过使用一个视频将ImageAI对象检测管道付诸实践，其中从视频中获得的每一帧都通过该管道来检测和识别这些帧中的对象，并使用Matplotlib显示它们。</p>
		</div>
	

</body></html>