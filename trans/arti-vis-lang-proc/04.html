<html><head/><body>


	
		<title>C13550_04_EPUB_Final_SW</title>
		
	
	
		<div><h1 id="_idParaDest-76"><em class="italics"> <a id="_idTextAnchor095"/>第四章</em></h1>
		</div>
		<div><h1 id="_idParaDest-77"><a id="_idTextAnchor096"/>具有NLP的神经网络</h1>
		</div>
		<div><h2>学习目标</h2>
			<p>本章结束时，您将能够:</p>
			<ul>
				<li class="bullets">解释什么是递归神经网络</li>
				<li class="bullets">设计并构建一个递归神经网络</li>
				<li class="bullets">评估非数字数据</li>
				<li class="bullets">使用RNNs评估不同的最新语言模型</li>
				<li class="bullets">用时间序列数据预测一个值</li>
			</ul>
			<p>本章涵盖了RNNs的各个方面。它处理<a id="_idTextAnchor097"/>解释、设计和建造各种RNN模型。</p>
		</div>
		<div><h2 id="_idParaDest-78"><a id="_idTextAnchor098"/>简介</h2>
			<p>如前一章所述，自然语言处理(NLP)是人工智能(AI)的一个领域，涵盖了计算机如何理解和操纵人类语言以执行有用的任务。现在，随着深度学习技术的发展，深度自然语言处理已经成为一个新的研究领域。</p>
			<p>那么，什么是深度NLP呢？它是NLP技术和深度学习的结合。这些技术相结合的结果是以下领域的进步:</p>
			<ul>
				<li>语言学:语音到文本</li>
				<li>工具:词性标注、实体识别和句子解析</li>
				<li>应用:情感分析、问题回答、对话代理和机器翻译</li>
			</ul>
			<p>深度自然语言处理最重要的方法之一是单词和句子的表示。单词可以表示为位于充满其他单词的平面中的向量。根据每个单词与另一个单词的相似性，其在平面中的距离将被相应地设置为更大或更小。</p>
			<div><div><img src="img/C13550_04_01.jpg" alt="Figure 4.1: Representation of words in multiple dimensions"/>
				</div>
			</div>
			<h6>图4.1:多维度的单词表示</h6>
			<p>上图展示了一个单词嵌入的例子。<strong class="keyword">单词嵌入</strong>是将语料库中的单词和句子映射成向量或实数的技术和方法的集合。它根据单词出现的上下文生成每个单词的表示。然后，单词嵌入可以发现单词之间的相似之处。例如，与dog最接近的单词如下:</p>
			<ol>
				<li>狗</li>
				<li>猫</li>
				<li>母牛</li>
				<li>鼠</li>
				<li>伯德</li>
			</ol>
			<p>生成嵌入有不同的方法，比如Word2Vec，这将在<em class="italics">第7章</em>、<em class="italics">构建一个对话代理来管理机器人</em>中涉及。</p>
			<p>这并不是深度学习在形态学层面上给NLP带来的唯一大的改变。通过深度学习，一个单词可以表示为向量的组合。</p>
			<p>每个语素都是一个向量，一个词是几个语素向量组合的结果。</p>
			<p>这种组合向量的技术也在语义层面上使用，但是用于单词的创建和句子的创建。每个短语由许多词向量组合而成，因此一个句子可以表示为一个向量。</p>
			<p>另一个改进是在句子解析方面。这个任务很难，因为它是模糊的。神经网络可以准确地确定句子的语法结构。</p>
			<p>就全面应用而言，这些领域如下:</p>
			<ul>
				<li><strong class="keyword">情绪分析</strong>:传统上，这是由一袋标有积极或消极情绪的单词组成。然后，把这几个词组合起来，就返回了整句话的情绪。现在使用深度学习和单词表示模型，效果更好。</li>
				<li><strong class="keyword">问题回答</strong>:为了找到问题的答案，向量表示可以将文档、段落或句子与输入的问题进行匹配。</li>
				<li><strong class="keyword">对话代理</strong>:通过神经语言模型，模型可以理解查询并创建响应。</li>
				<li>机器翻译:机器翻译是自然语言处理中最困难的任务之一。已经尝试了许多方法和模型。传统模型非常庞大和复杂，但深度学习神经机器翻译解决了这个问题。句子用向量编码，输出解码。</li>
			</ul>
			<p>单词的向量表示是深度自然语言处理的基础。制造一架飞机，可以完成很多任务。在分析深度NLP技术之前，我们将回顾什么是递归神经网络(RNN)，它在深度学习中的应用，以及如何创建我们的第一个RNN。</p>
			<p>我们未来的对话代理将检测对话的意图，并以预定义的答案做出响应。但是有了一个好的对话数据集，我们可以创建一个递归神经网络来训练一个语言模型(LM)，它能够对对话中的给定主题产生响应。该任务可以由其他神经网络架构来执行，例如seq2seq模型。</p>
			<h2 id="_idParaDest-79"><a id="_idTextAnchor099"/>递归神经网络</h2>
			<p>在本节中，我们将回顾<strong class="keyword">递归神经网络</strong> ( <strong class="keyword"> RNNs </strong>)。这个题目先来看看RNNs的理论。它将回顾该模型中的许多架构，并帮助您确定使用哪个模型来解决某个问题，它还将研究几种类型的RNN及其优缺点。此外，我们将看看如何创建一个简单的RNN，训练它，并作出预测。</p>
			<h3 id="_idParaDest-80"><a id="_idTextAnchor100"/>递归神经网络简介(RNN)</h3>
			<p>人类行为表现出一系列有序的行为序列。人类能够基于一组先前的动作或序列来学习动态路径。这意味着人们不是从零开始学习；我们有一些以前的知识，这对我们有帮助。例如，如果你不理解句子中的前一个单词，你就不能理解这个单词！</p>
			<p>传统上，神经网络不能解决这类问题，因为它们不能学习以前的信息。但是，如果仅仅用当前的信息无法解决问题，会发生什么呢？</p>
			<p>1986年，Michael I. Jordan提出了一个处理时间组织这一经典问题的模型。该模型能够通过研究动态对象先前的运动来学习其轨迹。约旦创造了第一个RNN。</p>
			<h6>   </h6>
			<div><div><img src="img/C13550_04_02.jpg" alt="Figure 4.2: Example of non-previous information versus temporal sequences"/>
				</div>
			</div>
			<h6>图4.2:非先前信息与时间序列的例子</h6>
			<p>在上图中，左边的图像向我们显示，如果没有任何信息，我们无法知道黑点的下一个动作会是什么，但如果我们假设它以前的运动被记录为图表右侧的红线，我们就可以预测它的下一个动作会是什么。</p>
			<h3 id="_idParaDest-81"><a id="_idTextAnchor101"/>递归神经网络内部</h3>
			<p>到目前为止，我们已经看到RNNs不同于神经网络(NNs)。RNN神经元就像正常的神经元一样，但其中有环路，允许它们存储时间状态。存储某一时刻的状态，他们可以根据以前的时间状态进行预测。</p>
			<div><div><img src="img/C13550_04_03.jpg" alt="Figure 4.3: Traditional neuron"/>
				</div>
			</div>
			<h6>图4.3:传统神经元</h6>
			<p>上图显示了在神经网络中使用的传统神经元。<em class="italics">X</em>T8】n为神经元的输入，激活函数后，产生响应。RNN神经元的模式是不同的:</p>
			<div><div><img src="img/C13550_04_04_(1).jpg" alt="Figure 4.4: Recurrent neuron"/>
				</div>
			</div>
			<h6>图4.4:循环神经元</h6>
			<p>上图中的循环允许神经元存储时间状态。<em class="italics"> h </em> <em class="italics"> n </em>是输入的输出，<em class="italics"> X </em> <em class="italics"> n </em>，以及之前的状态。神经元随着时间而变化和进化。</p>
			<p>如果神经元的输入是一个序列，展开的RNN应该是这样的:</p>
			<div><div><img src="img/C13550_04_05.jpg" alt="Figure 4.5: Unrolled recurrent neuron"/>
				</div>
			</div>
			<h6>图4.5:展开的循环神经元</h6>
			<p>图4.5中的链状模式表明，rnn与序列和列表密切相关。所以，我们有和输入一样多的神经元，每个神经元把它的状态传递给下一个。</p>
			<h3 id="_idParaDest-82"><a id="_idTextAnchor102"/> RNN建筑</h3>
			<p>根据RNN中输入和输出的数量，有许多具有不同数量神经元的体系结构。每种体系结构都专门用于某项任务。到目前为止，有许多类型的网络:</p>
			<div><div><img src="img/C13550_04_06.jpg" alt="Figure 4.6: Structures of RNNs"/>
				</div>
			</div>
			<h6>图4.6:rnn的结构</h6>
			<p>上图显示了rnn的各种分类。在本书的前面，我们回顾了一对一的架构。在本章中，我们将学习多对一体系结构。</p>
			<ul>
				<li><strong class="keyword">一对一</strong>:来自一个输入的分类或回归任务(图像分类)。</li>
				<li><strong class="keyword">一对多</strong>:图像字幕任务。这些都是深度学习中的硬任务。例如，传递图像作为输入的模型可以描述图片中的元素。</li>
				<li><strong class="keyword">多对一</strong>:时序、情感分析……每项任务只有一个输出，但基于一系列不同的输入。</li>
				<li>多对多:机器自动翻译系统。</li>
				<li><strong class="keyword">同步多对多</strong>:视频分类。</li>
			</ul>
			<h3 id="_idParaDest-83">长期依赖问题</h3>
			<p>在某些任务中，只需要使用最新的信息来预测模型的下一步。对于时间序列，有必要检查较旧的元素来学习或预测句子中的下一个元素或单词。比如，看看这句话:</p>
			<ul>
				<li>云在天空中。</li>
			</ul>
			<p>现在想象一下这句话:</p>
			<ul>
				<li>云朵在[？]</li>
			</ul>
			<p>您可能会假设所需的单词是sky，您知道这一点是因为前面的信息:</p>
			<ul>
				<li>云在天空中</li>
			</ul>
			<p>但是在其他任务中，模型需要先前的信息来获得更好的预测。比如，看看这句话:</p>
			<ul>
				<li>我出生在意大利，但是当我3岁的时候，我搬到了法国…这就是我说话的原因[？]</li>
			</ul>
			<p>为了预测单词，模型需要从句子的开头获取信息，这可能是一个问题。这是RNNs的一个问题:当与信息的距离较大时，学习难度更大。这个问题叫做<strong class="keyword">消失渐变</strong>。</p>
			<p><strong class="bold">消失渐变问题</strong></p>
			<p>信息以RNN的形式在时间中传播，因此来自先前步骤的信息被用作下一步骤的输入。在每一步，模型计算成本函数，因此每次，模型可以获得误差度量。当通过网络传播计算的误差，并试图在更新权重时最小化该误差时，该操作的结果是更接近于零的数字(如果将两个小数字相乘，结果是更小的数字)。这意味着模型的梯度随着每次乘法变得越来越小。这里的问题是，网络将无法正常训练。RNNs解决这个问题的方法是使用长短期记忆(LSTM)。</p>
			<h3 id="_idParaDest-84"><a id="_idTextAnchor104"/>练习14:用RNN预测房价</h3>
			<p>我们将使用Keras创建我们的第一个RNN。这个练习不是时间序列问题。我们将使用回归数据集来介绍RNNs。</p>
			<p>我们可以使用Keras库中包含的几种方法作为模型或图层类型:</p>
			<ul>
				<li>Keras模型:这些让我们可以使用Keras中不同的可用模型。我们将使用顺序模型。</li>
				<li>Keras层:我们可以向我们的神经网络添加不同类型的层。在这个练习中，我们将使用LSTM和密集层。密集层是神经网络中神经元的规则层。每个神经元接收来自前一层所有神经元的输入，但它们紧密相连。</li>
			</ul>
			<p>本练习的主要目标是预测波士顿一所房子的价值，因此我们的数据集将包含每所房子的信息，例如房产的总面积或房间数量:</p>
			<ol>
				<li value="1">从<code>sklearn</code>导入波士顿房价的数据集，看一下数据:<pre>from sklearn.datasets import load_boston boston = load_boston() boston.data</pre> <div> <img src="img/C13550_04_07.jpg" alt="Figure 4.7: Boston house-prices data"/> </div> <h6>图4.7:波士顿房价数据</h6></li>
				<li>您可以看到数据具有很高的值，因此最好的办法是对数据进行标准化。通过<code>sklearn</code>的<code>MinMaxScaler</code>函数，我们将把数据转换成0到1之间的值:<pre>from sklearn.preprocessing import MinMaxScaler import numpy as np   scaler = MinMaxScaler() x = scaler.fit_transform(boston.data)   aux = boston.target.reshape(boston.target.shape[0], 1) y = scaler.fit_transform(aux)</pre></li>
				<li>将数据分为训练集和测试集。测试集的一个好的百分比是数据的20%:<pre>from sklearn.model_selection import train_test_split   x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, shuffle=False) print('Shape of x_train {}'.format(x_train.shape)) print('Shape of y_train {}'.format(y_train.shape)) print('Shape of x_test {}'.format(x_test.shape)) print('Shape of y_test {}'.format(y_test.shape))</pre><div><img src="img/C13550_04_08.jpg" alt="Figure 4.8: Shape of the train and test data"/></div><h6>图4.8:列车的形状和测试数据</h6></li>
				<li>导入Keras库并设置一个种子来初始化权重:<pre>import tensorflow as tf from keras.models import Sequential from keras.layers import Dense tf.set_random_seed(1)</pre></li>
				<li>创建一个简单的模型。致密层只是一组神经元。最后一个密集层只有一个神经元返回输出:<pre>model = Sequential()   model.add(Dense(64, activation='relu')) model.add(Dense(32, activation='relu')) model.add(Dense(1))   model.compile(loss='mean_squared_error', optimizer='adam')</pre></li>
				<li>训练网络:<pre>history = model.fit(x_train, y_train, batch_size=32, epochs=5, verbose=2)</pre> <div> <img src="img/C13550_04_09.jpg" alt="Figure 4.9: Training the network"/> </div> <h6>图4.9:训练网络</h6></li>
				<li>计算模型误差:<pre>error = model.evaluate(x_test, y_test) print('MSE: {:.5f}'.format(error))</pre> <div> <img src="img/C13550_04_10.jpg" alt="Figure 4.10: Computing the error in the model"/> </div> <h6>图4.10:计算模型误差</h6></li>
				<li>绘制预测:<pre>import matplotlib.pyplot as plt   prediction = model.predict(x_test) print('Prediction shape: {}'.format(prediction.shape))   plt.plot(range(len(x_test)), prediction.reshape(prediction.shape[0]), '--r') plt.plot(range(len(y_test)), y_test) plt.show()</pre></li>
			</ol>
			<div><div><img src="img/C13550_04_11.jpg" alt="Figure 4.11: Predictions of our model"/>
				</div>
			</div>
			<h6>图4.11:我们模型的预测</h6>
			<p>现在你有了一个回归问题的RNN！你可以尝试修改参数，增加更多的层，或者改变神经元的数量，看看会发生什么。在下一个练习中，我们将使用LSTM图层解决时间序列问题。</p>
			<h2 id="_idParaDest-85"><a id="_idTextAnchor105"/>长短期记忆</h2>
			<p>LSTM是一种RNN，旨在解决长期依赖问题。它能记住长时间或短时间的数值。它与传统rnn的主要区别在于，它们包含一个单元或一个循环来内部存储内存。</p>
			<p>Hochreiter和Schmidhuber在1997年创建了这种类型的神经网络。这是LSTM神经元的基本模式:</p>
			<div><div><img src="img/C13550_04_12.jpg" alt="Figure 4.12: LSTM neuron structure"/>
				</div>
			</div>
			<h6>图4.12: LSTM神经元结构</h6>
			<p>正如你在上图中看到的，LSTM神经元的模式是复杂的。它有三种类型的门:</p>
			<ul>
				<li>输入门:允许我们控制输入值来更新存储单元的状态。</li>
				<li>遗忘门:允许我们删除记忆单元的内容。</li>
				<li>输出门:允许我们控制输入和单元存储内容的返回值。</li>
			</ul>
			<p>Keras中的LSTM模型具有三维输入:</p>
			<ul>
				<li>样本:是您拥有的数据量(序列的数量)。</li>
				<li>时间步长:是你网络的内存。换句话说，它存储以前的信息，以便做出更好的预测。</li>
				<li>Features: Is the number of features in every time step. For example, if you are processing pictures, the features are the number of pixels.<h4>注意</h4><p class="callout">这种复杂的设计会导致形成另一种类型的网络。这种新型的神经网络是一个<strong class="keyword">门控递归单元(GRU) </strong>，它解决了消失梯度问题。</p></li>
			</ul>
			<h3 id="_idParaDest-86"><a id="_idTextAnchor106"/>练习15:预测一个数学函数的下一个解</h3>
			<p>在本练习中，我们将构建一个LSTM来预测正弦函数的值。在本练习中，您将学习如何使用LSTM模型对Keras模型进行定型和预测。此外，本练习将涵盖数据生成以及如何将数据分为训练样本和测试样本:</p>
			<ol>
				<li value="1">通过Keras，我们可以使用Sequential类创建一个RNN，还可以创建一个LSTM来添加新的递归神经元。导入LSTM模型的Keras库、设置数据的NumPy库和打印图表的matplotlib库:<pre>import tensorflow as tf from keras.models import Sequential from keras.layers import LSTM, Dense import numpy as np import matplotlib.pyplot as plt</pre></li>
				<li>创建数据集来训练和评估模型。我们将生成一个包含1000个值的数组，作为正弦函数的结果:<pre>serie = 1000 x_aux = [] #Natural numbers until serie x_aux = np.arange(serie) serie = (np.sin(2 * np.pi * 4 * x_aux / serie) + 1) / 2</pre></li>
				<li>看数据好不好，我们来作图:<pre>plt.plot(x_aux, serie) plt.show()</pre> <div> <img src="img/C13550_04_13.jpg" alt="Figure 4.13: Output with the plotted data"/> </div> <h6>图4.13:用作图的数据输出</h6></li>
				<li>As this chapter explains, RNN works with sequences of data, so we need to split our data into sequences. In our case, the maximum length of the sequences will be 5. This is necessary because the RNNs need sequences as input.<p>这个模型将是<strong class="bold">多对一</strong>，因为输入是一个序列，而输出只是一个值。要了解我们为什么要使用多对一结构创建RNN，我们只需要知道输入和输出数据的维度:</p><pre>#Prepare input data
maxlen = 5
seq = []
res = []
for i in range(0, len(serie) - maxlen):
    seq.append(serie[i:maxlen+i])
    res.append(serie[maxlen+i])
print(seq[:5])
print(res[:5])</pre></li>
				<li>Prepare the data to introduce it to the LSTM model. Pay attention to the shape of the <code>x</code> and <code>y</code> variables. RNNs need a three-dimensional vector as input and a two-dimensional vector as output. That's why we will reshape the variables:<pre>x = np.array(seq)
y = np.array(res)
x = x.reshape(x.shape[0], x.shape[1], 1)
y = y.reshape(y.shape[0], 1)
print('Shape of x {}'.format(x.shape))
print('Shape of y {}'.format(y.shape))</pre><div><img src="img/C13550_04_14.jpg" alt="Figure 4.14: Reshaping the variables"/></div><h6>图4.14:重塑变量</h6><h4>注意</h4><p class="callout">LSTM的输入维数是3。</p></li>
				<li>将数据拆分成训练集和测试集:<pre>from sklearn.model_selection import train_test_split   x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, shuffle=False) print('Shape of x_train {}'.format(x_train.shape)) print('Shape of y_train {}'.format(y_train.shape)) print('Shape of x_test {}'.format(x_test.shape)) print('Shape of y_test {}'.format(y_test.shape))</pre> <div> <img src="img/C13550_04_15.jpg" alt="Figure 4.15: Splitting data as train and test"/> </div> <h6>图4.15:将数据拆分成训练集和测试集</h6></li>
				<li>建立一个简单的模型，该模型具有一个LSTM单元和一个具有一个神经元和线性激活的密集层。密集层只是一个规则的神经元层，它接收来自前一层的输入，并生成许多神经元作为输出。正因为如此，我们的密集层只有一个神经元，因为我们需要一个标量值作为输出:<pre>tf.set_random_seed(1) model = Sequential() model.add(LSTM(1, input_shape=(maxlen, 1)))    model.add(Dense(1, activation='linear'))       model.compile(loss='mse', optimizer='rmsprop')</pre></li>
				<li>以5个时期(一个时期是当整个数据集被神经网络处理时)和32的批量大小训练模型并评估它:<pre>history = model.fit(x_train, y_train, batch_size=32, epochs=5, verbose=2) error = model.evaluate(x_test, y_test) print('MSE: {:.5f}'.format(error))</pre> <div> <img src="img/C13550_04_16.jpg" alt="Figure 4.16: Training with 5 epochs with batch size 32"/> </div> <h6>图4.16:以32的批量大小训练5个时期</h6></li>
				<li>绘制测试预测，看看是否有效:<pre>prediction = model.predict(x_test) print('Prediction shape: {}'.format(prediction.shape)) plt.plot(range(len(x_test)), prediction.reshape(prediction.shape[0]), '--r') plt.plot(range(len(y_test)), y_test) plt.show()</pre> <div> <img src="img/C13550_04_17.jpg" alt="Figure 4.17: Plotting the predicted shape"/> </div> <h6>图4.17:绘制预测形状</h6></li>
				<li>让我们改进我们的模型。创建一个新的，在LSTM层有四个单元，一个密集层有一个神经元，但是有sigmoid激活:<pre>model2 = Sequential() model2.add(LSTM(4,input_shape=(maxlen,1))) model2.add(Dense(1, activation='sigmoid')) model2.compile(loss='mse', optimizer='rmsprop')</pre></li>
				<li>训练评估25个历元，批量为8: <pre>history = model2.fit(x_train, y_train,                      batch_size=8,                      epochs=25,                       verbose=1) error = model2.evaluate(x_test, y_test) print('MSE: {:.5f}'.format(error))</pre> <div> <img src="img/C13550_04_18.jpg" alt="Figure 4.18: Training for 25 epochs with batch size 8"/> </div> <h6>图4.18:训练25个历元，批量为8 </h6></li>
				<li>绘制模型的预测:<pre>predict_2 = model2.predict(x_test) predict_2 = predict_2.reshape(predict_2.shape[0])  print(x_test.shape) plt.plot(range(len(x_test)),predict_2, '--r') plt.plot(range(len(y_test)), y_test) plt.show()</pre></li>
			</ol>
			<div><div><img src="img/C13550_04_19.jpg" alt="Figure 4.19: Prediction of our neural network"/>
				</div>
			</div>
			<h6>图4.19:我们神经网络的预测</h6>
			<p>你现在可以对比一下每个模型的剧情，我们可以看到第二个模型更好。通过这个练习，您已经学习了LSTM的基本知识，如何训练和评估您创建的模型，以及如何确定它是好是坏。</p>
			<h2 id="_idParaDest-87"><a id="_idTextAnchor107"/>神经语言模型</h2>
			<p><em class="italics">第3章</em>，<em class="italics">自然语言处理基础</em>向我们介绍了统计语言模型(LMs)，它是一个单词序列的概率分布。我们知道LMs可以用来预测句子中的下一个单词，或者计算下一个单词的概率分布。</p>
			<div><div><img src="img/C13550_04_20.jpg" alt="Figure 4.20: LM formula to compute the probability distribution of an upcoming word"/>
				</div>
			</div>
			<h6>图4.20:计算即将出现的单词的概率分布的LM公式</h6>
			<p>单词顺序是<em class="italics"> x1 </em>、<em class="italics">x2</em>……下一个单词是<em class="italics">x</em>t+1。<em class="italics">w</em>T29】j是词汇中的一个词。<em class="italics"> V </em>是词汇表，<em class="italics"> j </em>是单词在词汇表中的位置。<em class="italics">w</em>j<em class="italics">j</em>是位于<em class="italics"> V </em>内<em class="italics"> j </em>位置的字。</p>
			<p>你每天都用LMs。手机键盘使用这种技术来预测一个句子的下一个单词，谷歌等搜索引擎使用它来预测你想在他们的搜索引擎中搜索什么。</p>
			<p>我们讨论了n-gram模型和计算语料库中单词的二元模型，但是这种解决方案有一些限制，比如长依赖。深度NLP和神经LMs将有助于避开这些限制。</p>
			<h3 id="_idParaDest-88"><a id="_idTextAnchor108"/>神经语言模型简介</h3>
			<p>神经LMs遵循与统计LMs相同的结构。他们的目标是预测句子中的下一个单词，但方式不同。由于使用序列作为输入，神经LM受RNN的激励。</p>
			<p><em class="italics">练习15 </em>，<em class="italics">预测一个数学函数的下一个解</em>从前面五个步骤的序列中预测正弦函数的下一个结果。在这种情况下，数据不是正弦函数结果的序列，而是单词，模型将预测下一个单词。</p>
			<p>这些神经LMs是从改进统计方法的必要性中出现的。较新的模型可以解决传统LMs的一些限制和问题。</p>
			<p><strong class="bold">统计LMs的问题</strong></p>
			<p>在前一章中，我们回顾了LMs和N元模型、二元模型以及马尔可夫模型的概念。这些方法通过计算文本中的出现次数来执行。这就是为什么这些方法被称为统计LMs。</p>
			<p>LMs的主要问题是数据限制。如果我们要计算的句子的概率分布在数据中不存在，我们该怎么办？这里的部分解决方案是平滑方法，但这是不够的。</p>
			<p>另一种解决方案是使用马尔可夫假设(每个概率只取决于前一步，简化链式规则)来简化句子，但这不会给出很好的预测。这意味着，我们可以用3个字母来简化我们的模型。</p>
			<p>这个问题的解决方案是增加语料库的大小，但是语料库最终会变得太大。n元模型中的这些限制被称为<strong class="bold">稀疏问题</strong>。</p>
			<p><strong class="bold">基于窗口的神经模型</strong></p>
			<p>这个新模型的第一个近似是使用滑动窗口来计算下一个单词的概率。这个解决方案的概念来自于窗口分类。</p>
			<p>在单词方面，没有任何上下文，很难理解单个单词的意思。如果这个词不在一个句子或一个段落中，就会有很多问题，例如，两个相似的词或自动反义词之间的歧义。自动反义词是有多重含义的词。根据上下文的不同，残障一词既可以表示优势(例如在体育运动中)，也可以表示劣势(有时是攻击性的，是身体上的问题)。</p>
			<p>窗口分类将单词在其相邻单词的上下文(由窗口创建)中分类。滑动窗口的方法可以用于生成LM。下面是一个图形示例:</p>
			<div><div><img src="img/C13550_04_21.jpg" alt="Figure 4.21: Window-based neural LM"/>
				</div>
			</div>
			<h6>图4.21:基于窗口的神经LM</h6>
			<p>在上图中，有一个基于窗口的神经模型如何工作的示例。窗口大小为5(字1到字5)。它创建一个向量来连接每个单词的嵌入向量，并在一个隐藏层中对此进行计算:</p>
			<div><div><img src="img/C13550_04_22.jpg" alt="Figure 4.22: Hidden layer formula"/>
				</div>
			</div>
			<h6>图4.22:隐藏层公式</h6>
			<p>最后，为了预测一个单词，该模型返回一个可用于对该单词的概率进行分类的值:</p>
			<div><div><img src="img/C13550_04_23.jpg" alt="Figure 4.23: Softmax function"/>
				</div>
			</div>
			<h6>图4.23: Softmax函数</h6>
			<p>然后，具有最高值的单词将是预测的单词。</p>
			<h4>注意</h4>
			<p class="callout">我们不打算深入研究这些术语，因为我们将使用LSTM来创建LM。</p>
			<p>与传统方法相比，这种方法的优势如下:</p>
			<ul>
				<li>更少的计算工作。基于窗口的神经模型需要较少的计算资源，因为它们不需要迭代通过语料库计算概率。</li>
				<li>它避免了改变N-gram的维数以找到好的概率分布的问题。</li>
				<li>生成的文本在意义上更有意义，因为这种方法解决了稀疏性问题。</li>
			</ul>
			<p>但是也有一些问题:</p>
			<ul>
				<li>窗口限制:窗口的大小不能太大，所以一些单词的意思可能是错误的。</li>
				<li>每个窗口都有自己的权重值，因此会导致模糊。</li>
				<li>如果窗口变大，模型也会变大。</li>
			</ul>
			<p>通过分析窗口模型的问题，RNN可以提高其性能。</p>
			<h3 id="_idParaDest-89"><a id="_idTextAnchor109"/> RNN语言模型</h3>
			<p>RNN能够在前面的一系列步骤中计算即将出现的单词的概率。这种方法的核心思想是在整个训练过程中重复使用相同的重量。</p>
			<p>与基于窗口的模型相比，使用RNN LM有一些优势:</p>
			<ul>
				<li>这种架构可以处理任何长度的句子；与基于窗口的方法不同，它没有固定的大小。</li>
				<li>每个输入尺寸的模型都是相同的。投入再大也不会增长。</li>
				<li>根据神经网络结构，它可以使用来自前面步骤和前面步骤的信息。</li>
				<li>权重是跨时间步长共享的。</li>
			</ul>
			<p>到目前为止，我们已经讨论了改进统计LM的不同方法以及每种方法的优缺点。在开发RNN逻辑模型之前，我们需要知道如何引入一个句子作为神经网络的输入。</p>
			<p><strong class="bold">一键编码</strong></p>
			<p>神经网络和机器学习是关于数字的。正如我们在本书中看到的，输入元素是数字，输出是编码标签。但是，如果神经网络有一个句子或一组字符作为输入，它如何将其转换为数值？</p>
			<p>独热编码是离散变量的数字表示。它假设在一组离散的变量中，不同的值具有相同大小的特征向量。这意味着如果有一个大小为10的语料库，每个单词将被编码为长度为10的向量。因此，每个维度都对应于集合中的一个独特元素。</p>
			<div><div><img src="img/C13550_04_24.jpg" alt="Figure 4.24: RNN pre-processing data flow"/>
				</div>
			</div>
			<h6>图4.24: RNN预处理数据流</h6>
			<p>上图显示了独热编码的工作原理。理解每个向量的形状很重要，因为神经网络需要理解我们有什么输入数据以及我们想要获得什么输出。接下来，<em class="italics">练习16，编码一个小语料库</em>将帮助你更详细地检查一键编码的基础。</p>
			<h3 id="_idParaDest-90"><a id="_idTextAnchor110"/>练习16:对小型语料库进行编码</h3>
			<p>在这个练习中，我们将学习如何使用一键编码对一组单词进行编码。这是最基本的编码方法，它给出了离散变量的表示。</p>
			<p>本练习将涵盖执行此任务的不同方式。一种方法是手动执行编码，另一种方法是使用库。完成练习后，我们将获得每个单词的向量表示，准备用作神经网络的输入:</p>
			<ol>
				<li value="1">定义一个语料库。这个语料库与我们在<em class="italics">第三章</em>、<em class="italics">自然语言处理基础</em> : <pre>corpus = [      'My cat is white',      'I am the major of this city',      'I love eating toasted cheese',      'The lazy cat is sleeping', ]</pre>中使用的是同一个语料库</li>
				<li>使用<code>spaCy</code>对其进行标记。我们不打算使用停用词(删除无用的词，如文章)的方法，因为我们有一个小的语料库。我们要所有的代币:<pre>import spacy import en_core_web_sm nlp = en_core_web_sm.load()   corpus_tokens = [] for c in corpus:     doc = nlp(c)     tokens = []     for t in doc:         tokens.append(t.text)     corpus_tokens.append(tokens) corpus_tokens</pre></li>
				<li>创建一个包含语料库中每个唯一标记的列表:<pre>processed_corpus = [t for sentence in corpus_tokens for t in sentence] processed_corpus = set(processed_corpus) processed_corpus</pre> <div> <img src="img/C13550_04_25.jpg" alt="Figure 4.25: List with each unique token in the corpus"/> </div> <h6>图4.25:包含语料库中每个唯一标记的列表</h6></li>
				<li>创建一个字典，将语料库中的每个单词作为关键字，一个唯一的数字作为值。这个字典看起来会像{word:value}，这个值在一键编码向量中的索引为1:<pre>word2int = dict([(tok, pos) for pos, tok in enumerate(processed_corpus)]) word2int</pre><div><img src="img/C13550_04_26.jpg" alt="Figure 4.26: Each word as a key and a unique number as value"/></div><h6>图4.26:每个单词作为一个键，一个唯一的数字作为一个值</h6></li>
				<li>编码一个句子。这种执行编码的方式是手动的。有一些库，比如sklearn，提供了自动编码的方法:<pre>Import numpy as np sentence = 'My cat is lazy' tokenized_sentence = sentence.split() encoded_sentence = np.zeros([len(tokenized_sentence),len(processed_corpus)]) encoded_sentence for i,c in enumerate(sentence.split()):     encoded_sentence[i][ word2int[c] ] = 1 encoded_sentence</pre> <div> <img src="img/C13550_04_27.jpg" alt=""/> </div> <h6>图4.27:手动一键编码矢量。</h6> <pre>print("Shape of the encoded sentence:", encoded_sentence.shape)</pre></li>
				<li>导入<code>sklearn</code>方法。sklearn首先用<code>LabelEncoder</code>对语料库中的每个唯一记号进行编码，然后用<code>OneHotEncoder</code>创建向量:<pre>from sklearn.preprocessing import LabelEncoder from sklearn.preprocessing import OneHotEncoder Declare the LabelEncoder() class. le = LabelEncoder() Encode the corpus with this class. labeled_corpus = le.fit_transform(list(processed_corpus)) labeled_corpus</pre> <div> <img src="img/C13550_04_28.jpg" alt=""/> </div> <h6>图4.28:用OneHotEncoder创建的向量</h6></li>
				<li>现在，拿我们之前编码的同一个句子，应用我们创建的<code>LabelEncoder</code>变换方法:<pre>sentence = 'My cat is lazy' tokenized_sentence = sentence.split() integer_encoded = le.transform(tokenized_sentence) integer_encoded</pre> <div> <img src="img/C13550_04_29.jpg" alt="Figure 4.29: LabelEncoder transform applied"/> </div> <h6>图4.29:应用的LabelEncoder变换</h6></li>
				<li>我们可以解码<code>LabelEncoder</code>在初始句:<pre>le.inverse_transform(integer_encoded)</pre> <div> <img src="img/C13550_04_30.jpg" alt="Figure 4.30: Decoded LabelEncoder"/> </div> <h6>图4.30:解码的LabelEncoder </h6></li>
				<li>用<code>sparse=False</code>声明<code>OneHotEncoder</code>(如果不指定，将返回一个稀疏矩阵):<pre>onehot_encoder = OneHotEncoder(sparse=False)</pre></li>
				<li>为了用我们创建的标签编码器对我们的句子进行编码，我们需要重塑我们的标签语料库，使其适合<code>onehot_encoder</code>方法:<pre>labeled_corpus = labeled_corpus.reshape(len(labeled_corpus), 1) onehot_encoded = onehot_encoder.fit(labeled_corpus)</pre></li>
				<li>最后，我们可以将我们的句子(用LabelEncoder编码)转换成一个独热向量。这种编码方式和手工编码的结果不会相同，但它们会有相同的形状:<pre>sentence_encoded = onehot_encoded.transform(integer_encoded.reshape(len(integer_encoded), 1)) print(sentence_encoded)</pre></li>
			</ol>
			<div><div><img src="img/C13550_04_31.jpg" alt="Figure 4.31: One-hot encoded vectors using Sklearn methods"/>
				</div>
			</div>
			<h6>图4.31:使用Sklearn方法的一键编码向量</h6>
			<h4>注意</h4>
			<p class="callout">这个练习非常重要。如果你不理解矩阵的形状，就很难理解rnn的输入。</p>
			<p>干得好！您已经完成了<em class="italics">练习16 </em>。现在你可以把离散变量编码成向量。这是训练和评估神经网络的预处理数据的一部分。接下来，我们进行本章的活动，其目的是使用RNNs和一键编码创建一个LM。</p>
			<h4>注意</h4>
			<p class="callout">对于较大的语料库，一键编码不是很有用，因为它会为单词创建巨大的向量。相反，使用嵌入向量是正常的。这一概念将在本章的后面介绍。</p>
			<h3 id="_idParaDest-91"><a id="_idTextAnchor111"/>RNNs的输入维度</h3>
			<p>在开始RNN活动之前，您可能不了解输入维度。在本节中，我们将重点了解n维数组的形状，以及如何添加新的维度或删除一个维度。</p>
			<p><strong class="bold">序列数据格式</strong></p>
			<p>我们已经提到了多对一架构，其中每个样本由一个固定的序列和一个标签组成。该标签对应于序列中即将到来的值。大概是这样的:</p>
			<div><div><img src="img/C13550_04_32.jpg" alt="Figure 4.32: Format of sequence data"/>
				</div>
			</div>
			<h6>图4.32:序列数据的格式</h6>
			<p>在本例中，矩阵X中有两个序列，y中有两个输出标签。因此，形状如下:</p>
			<p>X = (2，4)</p>
			<p>Y = (2)</p>
			<p>但是，如果您尝试将这些数据插入到RNN中，将不会成功，因为它没有正确的维度。</p>
			<p><strong class="bold"> RNN数据格式</strong></p>
			<p>为了在Keras中实现具有时间序列的RNN，该模型将需要一个三维的输入向量和一个二维的输出向量。</p>
			<p>因此，对于X矩阵，我们将得到以下结果:</p>
			<ul>
				<li>样本数目</li>
				<li>序列长度</li>
				<li>值长度</li>
			</ul>
			<div><div><img src="img/C13550_04_33.jpg" alt="Figure 4.33: RNN data format"/>
				</div>
			</div>
			<h6>图4.33: RNN数据格式</h6>
			<p>这里的形状如下:</p>
			<p>X = (2，4，1)</p>
			<p>Y = (2，1)</p>
			<p><strong class="bold">一热格式</strong></p>
			<p>使用一键编码，我们拥有与输入相同的维度，但是值的长度改变了。在上图中，我们可以看到一维的值([1]，[2]，…)。但是使用一键编码，这些值将变成向量，所以形状如下:</p>
			<div><div><img src="img/C13550_04_34.jpg" alt="Figure 4.34: One-hot format"/>
				</div>
			</div>
			<h6>图4.34:一键格式</h6>
			<p>X = (2，4，3)</p>
			<p>Y = (2，3)</p>
			<p>为了对尺寸进行所有这些更改，将使用NumPy库中的<strong class="bold"> reshape </strong>方法。</p>
			<h4>注意</h4>
			<p class="callout">了解了维度之后，您就可以开始活动了。请记住，LSTM的输入维度是三维的，输出维度是二维的。因此，如果您连续创建两个LSTM图层，如何将第三个维度添加到第一个图层的输出中？将返回状态更改为True。</p>
			<h3 id="_idParaDest-92"><a id="_idTextAnchor112"/>活动4:预测序列中的下一个字符</h3>
			<p>在这个活动中，我们将预测一个长序列中即将出现的角色。必须使用一键编码来执行活动，以创建输入和输出向量。该模型的架构将是一个LSTM，正如我们在<em class="italics">练习14 </em>、<em class="italics">中看到的用RNN </em>预测房价。</p>
			<p>场景:您在一家全球性公司担任安全经理。一天早上，您注意到一名黑客发现并更改了公司数据库的所有密码。你和你的工程师团队开始尝试解码黑客的密码进入系统并修复一切。分析所有新密码后，您会看到一个共同的结构。</p>
			<p>你只需要解码密码中的一个字符，但你不知道这个字符是什么，你只有一次机会获得正确的密码。</p>
			<p>然后，您决定创建一个程序来分析长数据序列和您已经知道的五个字符的密码。有了这些信息，它可以预测密码的最后一个字符。</p>
			<p>密码的前五个字符是:tyuio。最后一个角色会是什么？</p>
			<h4>注意</h4>
			<p class="callout">你必须使用一次性编码和LSTM。您将使用一次性编码向量来训练您的模型。</p>
			<ol>
				<li value="1">This is the sequence of data: qwertyuiopasdfghjklñzxcvbnm<h4>注意</h4><p class="callout">这个序列重复100次，这样做:sequence = ' qwertyuiopasdfghjkl-zxcvbnm ' * 100。</p></li>
				<li>将数据分成五个字符的序列，并准备输出数据。</li>
				<li>将输入和输出序列编码为独热编码向量。</li>
				<li>设置训练和测试数据。</li>
				<li>Design the model.<h4>注意</h4><p class="callout">输出中有许多零，因此很难获得精确的结果。使用alpha为0.01的LeakyRelu激活函数，在进行预测时，对该向量的值进行四舍五入。</p></li>
				<li>训练和评估它。</li>
				<li>Create a function that, when given five characters, predicts the next one in order to work out the last character of the password.<h4>注意</h4><p class="callout">这项活动的解决方案可在第308页找到。</p></li>
			</ol>
			<h2 id="_idParaDest-93"><a id="_idTextAnchor113"/>总结</h2>
			<p>得益于卷积网络，人工智能和深度学习在图像和人工视觉方面取得了巨大进步。但是rnn也有很大的权力。</p>
			<p>在本章中，我们回顾了神经网络如何使用时间序列来预测正弦函数的值。如果您更改训练数据，该体系结构可以了解每个分布的股票走势。此外，rnn有许多架构，每种架构都针对特定任务进行了优化。但是rnn有一个渐变消失的问题。这个问题的解决方案是一种新的模型，称为LSTM，它改变神经元的结构来记忆时间步长。</p>
			<p>着眼于语言学，统计LMs有许多与计算负荷和分布概率相关的问题。为了解决稀疏性问题，n-gram模型的大小被降低到4或3克，但这不足以预测即将到来的单词。如果我们使用这种方法，就会出现稀疏性问题。具有固定窗口大小的神经LM可以防止稀疏性问题，但是仍然存在窗口大小和权重有限的问题。有了RNNs，这些问题就不会出现，而且根据架构的不同，它可以获得更好的结果，向前向后看很多步。但是深度学习是关于向量和数字的。当你想预测单词时，你需要对数据进行编码来训练模型。有各种不同的方法，如一键编码器或标签编码器。现在，您可以从经过训练的语料库和RNN中生成文本。</p>
			<p>在下一章，我们将讨论卷积神经网络(CNN)。我们将回顾细胞神经网络的基本技术和架构，也将着眼于更复杂的实现，如迁移学习。</p>
		</div>
	

</body></html>