<html><head/><body>


	
		<title>Chapter_7_SMP_ePub</title>
		
	
	
		<div><div/>
		</div>
		<div><h1 id="_idParaDest-171"><a id="_idTextAnchor180"/> 7。卷积神经网络</h1>
		</div>
		<div><p>本章介绍<strong class="bold">卷积神经网络</strong>(<strong class="bold">CNN</strong>)。CNN在AI中无处不在，包括图像识别和语音识别。本章将详细介绍CNN的机制以及如何用Python实现它们。</p>
			<h2 id="_idParaDest-172"><a id="_idTextAnchor181"/>整体架构</h2>
			<p>首先，我们来看看CNN的网络架构。你可以通过组合不同的层来创建一个CNN，就像我们目前看到的神经网络一样。然而，CNN也有其他层:卷积层和池层。我们将在下面的章节中详细介绍卷积层和池层。本节描述了如何组合各层来创建CNN。</p>
			<p>在我们目前看到的神经网络中，相邻层的神经元都是相连的。这些层被称为<strong class="bold">全连接</strong>层，我们将它们实现为仿射层。例如，您可以使用仿射层创建一个由五个完全连接的层组成的神经网络，如图<em class="italics">图7.1 </em>所示。</p>
			<p>如<em class="italics">图7.1 </em>所示，激活函数的ReLU层(或Sigmoid层)遵循全连接神经网络中的仿射层。这里，在四对<strong class="bold">仿射–ReLU</strong>层之后，是仿射层，这是第五层。最后，Softmax层输出最终结果(概率):</p>
			<div><div><img src="img/fig07_1.jpg" alt="Figure 7.1: Sample network consisting of fully connected layers (Affine layers)&#13;&#10;"/>
				</div>
			</div>
			<h6>图7.1:由完全连接的层(仿射层)组成的样本网络</h6>
			<p>那么，CNN有什么样的架构呢？<em class="italics">图7.2 </em>显示了一个CNN示例:</p>
			<div><div><img src="img/fig07_2.jpg" alt="Figure 7.2: Sample CNN – convolution and pooling layers are added (they are shown as gray rectangles)&#13;&#10;"/>
				</div>
			</div>
			<h6>图7.2:示例CNN–添加卷积和池层(它们显示为灰色矩形)</h6>
			<p>如图<em class="italics">图7.2 </em>所示，CNN有额外的卷积和池层。在CNN中，层按照<strong class="bold">卷积ReLU(池化)</strong>的顺序连接(池化层有时会被省略)。我们可以把之前的<strong class="bold">仿射-ReLU</strong>联系，看成是用“卷积-ReLU-(池化)”代替</p>
			<p>在<em class="italics">图7.2 </em>的CNN中，注意靠近输出的层是之前的“Affine–ReLU”对，而最后输出的层是之前的“Affine–soft max”对。这是一个普通CNN经常看到的结构。</p>
			<h2 id="_idParaDest-173"><a id="_idTextAnchor182"/>卷积层</h2>
			<p>有一些CNN特有的术语，比如padding和stride。与以前的全连接网络不同，流经CNN中每一层的数据都是有形状的数据(如三维数据)。所以，第一次了解CNN，你可能会觉得很难。这里，我们将研究CNN中使用的卷积层的机制。</p>
			<h3 id="_idParaDest-174"><a id="_idTextAnchor183"/>全连接层的问题</h3>
			<p>到目前为止我们看到的全连接神经网络使用的都是全连接层(仿射层)。在全连接层中，相邻层的所有神经元都是连接的，输出的数目可以任意确定。</p>
			<p>然而，全连接层的问题是数据的形状被<em class="italics">忽略了</em>。例如，当输入数据是图像时，它通常具有三维形状，由高度、宽度和通道尺寸确定。但是，当三维数据被提供给完全连接的层时，它必须被转换成一维平面数据。在之前用于MNIST数据集的示例中，输入图像的形状为1、28、28 (1个通道，28个像素高，28个像素宽)，但元素排列成一行，生成的784条数据提供给第一个仿射图层。</p>
			<p>假设图像具有三维形状，并且该形状包含重要的空间信息。识别这些信息的基本模式可能隐藏在三维形状中。空间上靠近的像素具有相似的值，RBG通道彼此紧密相关，而远处的像素不相关。但是，完全连接的层会忽略形状，并将所有输入数据视为等效神经元(具有相同维数的神经元)，因此它不能使用关于形状的信息。</p>
			<p>另一方面，卷积层保持形状。对于图像，它将输入数据作为三维数据接收，并将三维数据输出到下一层。因此，CNN可以正确理解具有形状的数据，例如图像。</p>
			<p>在CNN中，卷积层的输入/输出数据有时被称为<strong class="bold">特征图</strong>。卷积层的输入数据称为<strong class="bold">输入特征图</strong>，而卷积层的输出数据称为<strong class="bold">输出特征图</strong>。在本书中，<em class="italics">输入/输出数据</em>和<em class="italics">特征图</em>将互换使用。</p>
			<h3 id="_idParaDest-175"><a id="_idTextAnchor184"/>卷积运算</h3>
			<p>在卷积层中执行的处理被称为“卷积运算”,相当于图像处理中的“滤波运算”。让我们看一个例子(<em class="italics">图7.3 </em>)来理解卷积运算:</p>
			<div><div><img src="img/fig07_3.jpg" alt="Figure 7.3: Convolution operation – the ⊛ symbol indicates a convolution operation&#13;&#10;"/>
				</div>
			</div>
			<h6>图7.3:卷积运算–⊛符号表示卷积运算</h6>
			<p>如图<em class="italics">图7.3 </em>所示，卷积运算对输入数据进行过滤。在此示例中，输入数据的形状具有高度和宽度，过滤器的形状也是如此。在本例中，当我们将数据和过滤器的形状指定为(高度，宽度)时，输入大小为(4，4)，过滤器大小为(3，3)，输出大小为(2，2)。一些文献使用“内核”一词来表示“过滤器”</p>
			<p>现在，让我们分解在<em class="italics">图7.3 </em>中所示的卷积运算中执行的计算。<em class="italics">图7.4 </em>显示了卷积运算的计算过程。</p>
			<p>当滤波器窗口以固定间隔移动时，卷积运算被应用于输入数据。这里的窗口表示图7.4 中<em class="italics">所示的灰色3x3截面。如图<em class="italics">图7.4 </em>所示，滤波器的元素和输入的相应元素在每个位置相乘并求和(这种计算有时被称为<strong class="bold">乘累加运算</strong>)。结果存储在输出的相应位置。卷积运算的输出可以通过在所有位置执行该过程来获得。</em></p>
			<p>完全连接的神经网络具有偏差和权重参数。在CNN中，过滤器参数对应于先前的“权重”它也有偏见。<em class="italics">的卷积运算图7.3 </em>显示了应用滤波器的阶段。<em class="italics">图7.5 </em>显示了卷积运算的处理流程，包括偏差:</p>
			<div><div><img src="img/fig07_4.jpg" alt="Figure 7.4: Calculation procedure of a convolution operation&#13;&#10;"/>
				</div>
			</div>
			<h6>图7.4:卷积运算的计算过程</h6>
			<div><div><img src="img/fig07_5.jpg" alt="Figure 7.5: Bias in a convolution operation – a fixed value (bias) is added to the element &#13;&#10;after the filter is applied&#13;&#10;"/>
				</div>
			</div>
			<h6>图7.5:卷积运算中的偏差–应用过滤器后，固定值(偏差)被添加到元素中</h6>
			<p>如图<em class="italics">图7.5 </em>所示，应用滤波器后，数据中会增加一个偏置项。这里，偏差总是只有一个(1x1 ),其中在应用滤波器之后，对于四个数据片段存在一个偏差。在应用过滤器后，这个值被添加到所有元素中。</p>
			<h3 id="_idParaDest-176"><a id="_idTextAnchor185"/>填充</h3>
			<p>在处理卷积图层之前，输入数据周围有时会填充固定数据(如0)。这称为<strong class="bold">填充</strong>，常用于卷积运算。例如，在<em class="italics">图7.6 </em>中，填充1应用于(4，4)输入数据。填充1表示用宽度为一个像素的零填充圆周:</p>
			<div><div><img src="img/fig07_6.jpg" alt="Figure 7.6: Padding in a convolution operation – add zeros around the input data (padding is shown by dashed lines here, and the zeros are omitted)&#13;&#10;"/>
				</div>
			</div>
			<h6>图7.6:卷积运算中的填充–在输入数据周围添加零(填充在这里用虚线表示，零被省略)</h6>
			<p>如图<em class="italics">图7.6 </em>所示，填充将(4，4)输入数据转换为(6，6)数据。在应用(3，3)滤波器之后，产生(4，4)输出数据。在这个例子中，使用了填充1。您可以将任何整数(如2或3)设置为填充值。如果填充值为2，则输入数据的大小为(8，8)。如果填充为3，则大小为(10，10)。</p>
			<h4>注意</h4>
			<p class="callout">填充主要用于调整输出大小。例如，当(3，3)滤波器应用于(4，4)输入数据时，输出大小为(2，2)。输出大小比输入大小小两个元素。这在深度网络中引起了问题，在深度网络中卷积运算被重复多次。如果每个卷积运算在空间上减小尺寸，则输出尺寸将在某个时间达到1，并且不再有卷积运算可用。为了避免这种情况，可以使用填充。在前面的示例中，当填充宽度为1时，输出大小(4，4)与输入大小(4，4)保持相同。因此，在执行卷积运算后，可以将相同空间大小的数据传递给下一个图层。</p>
			<h3 id="_idParaDest-177"><a id="_idTextAnchor186"/>大步流星</h3>
			<p>应用过滤器的位置间隔被称为<strong class="bold">步距</strong>。在所有前面的例子中，步幅是1。例如，当步幅为2时，应用过滤器的窗口的间隔将是两个元素，如图<em class="italics">图7.7 </em>所示。</p>
			<p>在<em class="italics">图7.7 </em>中，滤波器被应用于(7，7)输入数据，步长为2。当步幅为2时，输出大小变为(3，3)。因此，步幅指定了应用过滤器的间隔。</p>
			<div><div><img src="img/fig07_7.jpg" alt="Figure 7.7: Sample convolution operation where the stride is 2&#13;&#10;"/>
				</div>
			</div>
			<h6>图7.7:步长为2的卷积运算示例</h6>
			<p>正如我们到目前为止所看到的，步幅越大，输出大小越小，填充越大，输出大小越大。怎样才能用方程表示这样的关系呢？让我们看看如何根据填充和步幅计算输出大小。</p>
			<p>这里输入大小为(<em class="italics"> H </em>，<em class="italics"> W </em>)，滤波器大小为(<em class="italics"> FH </em>，<em class="italics"> FW </em>)，输出大小为(<em class="italics"> OH </em>，<em class="italics"> OW </em>)，填充为<em class="italics"> P </em>，步距为<em class="italics"> S </em>。在这种情况下，您可以使用下面的公式计算输出大小，即公式(7.1):</p>
			<table id="table001-5" class="No-Table-Style">
				<colgroup>
					<col/>
					<col/>
				</colgroup>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><img src="img/Figure_7.7a.png" alt="90"/></p>
						</td>
						<td class="No-Table-Style">
							<p>(7.1)</p>
						</td>
					</tr>
				</tbody>
			</table>
			<p>现在，让我们用这个等式来做一些计算:</p>
			<ol>
				<li><strong class="bold">Example 1: Example is shown in Figure 7.6</strong><p>输入大小:(4，4)，填充:1，步幅:1，过滤器大小:(3，3):</p><div><img src="img/Figure_7.7c.jpg" alt="91"/></div></li>
				<li><strong class="bold">Example 2: Example is shown in Figure 7.7</strong><p>输入大小:(7，7)，填充:0，步幅:2，过滤器大小:(3，3):</p><div><img src="img/Figure_7.7e.jpg" alt="92"/></div></li>
				<li><strong class="bold">Example 3</strong><p>输入大小:(28，31)，填充:2，步幅:3，过滤器大小:(5，5):</p><div><img src="img/Figure_7.7g.jpg" alt="93"/></div></li>
			</ol>
			<p>如这些例子所示，你可以通过给方程(7.1)赋值来计算输出大小。只能通过赋值得到输出大小，但注意必须赋值，使方程(7.1)中的<img src="img/Figure_7.7i.png" alt="94"/>和<img src="img/Figure_7.7j.png" alt="95"/>可整除。如果输出大小是不可分的(即结果是小数)，您必须通过生成一个错误来处理它。一些深度学习框架在不产生错误的情况下推进了这个过程；例如，当值不能被整除时，它们将该值四舍五入到最接近的整数。</p>
			<h3 id="_idParaDest-178"><a id="_idTextAnchor187"/>对三维数据执行卷积运算</h3>
			<p>到目前为止，我们所看到的例子针对的是具有高度和宽度的二维形状。对于图像，我们必须处理具有通道维度以及高度和宽度的三维数据。这里，我们将查看一个对三维数据进行卷积运算的示例，使用的技术与我们在前面的示例中使用的技术相同。</p>
			<p><em class="italics">图7.8 </em>显示了卷积运算的例子，而<em class="italics">图7.9 </em>显示了计算过程。这里，我们可以看到对三维数据执行卷积运算的结果。您可以看到，与二维数据相比，特征图的深度(通道尺寸)有所增加(图7.3 中的示例)。如果在通道维度中有多个特征映射，则对每个通道执行使用输入数据和滤波器的卷积运算，并将结果相加以获得一个输出:</p>
			<div><div><img src="img/fig07_8.jpg" alt="Figure 7.8: Convolution operation for three-dimensional data&#13;&#10;"/>
				</div>
			</div>
			<h6>图7.8:三维数据的卷积运算</h6>
			<div><div><img src="img/fig07_9.jpg" alt="Figure 7.9: Calculation procedure of the convolution operation for three-dimensional data&#13;&#10;"/>
				</div>
			</div>
			<h6>图7.9:三维数据卷积运算的计算过程</h6>
			<h4>注意</h4>
			<p class="callout">在三维卷积运算中，如本例所示，输入数据和过滤器在通道数量方面必须相同。在这个例子中，输入数据和滤波器中的通道数是相同的；有三个。另一方面，您可以根据自己的喜好设置过滤器的大小。在本例中，过滤器大小为(3，3)。您可以将其设置为任意大小，例如(2，2)、(1，1)或(5，5)。但是，如前所述，通道的数量必须与输入数据的数量相同。在本例中，必须有三个。</p>
			<h3 id="_idParaDest-179"><a id="_idTextAnchor188"/>用积木思考</h3>
			<p>在三维卷积运算中，您可以将数据和过滤器视为矩形块。这里的一块是立体的长方体，如图<em class="italics">图7.10 </em>所示。我们将按照通道、高度、宽度的顺序将三维数据表示为多维数组。所以，当形状的通道数为C，高度为H，宽度为W时，表示为(C，H，W)。我们将以同样的顺序表示一个滤波器，这样当通道数为C时，高度为<strong class="bold"> FH </strong> ( <strong class="bold">滤波器高度</strong>)，宽度为<strong class="bold"> FW </strong> ( <strong class="bold">滤波器宽度</strong>)对于一个滤波器，表示为(C，FH，FW):</p>
			<div><div><img src="img/fig07_10.jpg" alt="Figure 7.10: Using blocks to consider a convolution operation&#13;&#10;"/>
				</div>
			</div>
			<h6>图7.10:使用块来考虑卷积运算</h6>
			<p>在本例中，数据的输出是一个要素地图。一个特征映射意味着输出通道的大小为一。那么，如何在信道维度上提供卷积运算的多种输出呢？为此，我们使用多个过滤器(权重)。<em class="italics">图7.11 </em>以图形方式显示了这一点:</p>
			<div><div><img src="img/fig07_11.jpg" alt="Figure 7.11: Sample convolution operation with multiple filters&#13;&#10;"/>
				</div>
			</div>
			<h6>图7.11:多个滤波器的卷积运算示例</h6>
			<p>如图<em class="italics">图7.11 </em>所示，当应用的滤波器数为FN时，生成的输出图数也为FN。通过组合FN贴图，您可以创建一个形状块(FN，OH，OW)。将这个完成的块传递给下一层是CNN的过程。</p>
			<p>您还必须考虑卷积运算中过滤器的数量。为此，我们将把过滤器权重数据写成四维数据(output_channel，input_channel，height，width)。例如，当有20个大小为5×5的具有三个通道的滤波器时，它被表示为(20，3，5，5)。</p>
			<p>卷积运算有偏差(就像完全连接的层)。<em class="italics">图7.12 </em>显示了添加偏差时<em class="italics">图7.11 </em>中提供的示例。</p>
			<p>我们可以看到，每个通道只有一个偏置数据。这里，偏置的形状是(FN，1，1)，而滤波器输出的形状是(FN，OH，OW)。将这两个模块相加，会将相同的偏置值添加到滤波器输出结果中的每个通道(FN，OH，OW)。NumPy的广播方便不同形状的块(请参考<em class="italics">第一章</em>、<em class="italics">Python介绍</em>中的<em class="italics">广播</em>部分):</p>
			<div><div><img src="img/fig07_12.jpg" alt="Figure 7.12: Process flow of a convolution operation (the bias term is also added)&#13;&#10;"/>
				</div>
			</div>
			<h6>图7.12:卷积运算的处理流程(也添加了偏差项)</h6>
			<h3 id="_idParaDest-180"><a id="_idTextAnchor189"/>批量处理</h3>
			<p>在神经网络处理中，输入数据是成批处理的。到目前为止，我们看到的全连接神经网络的实现支持批处理，这使得处理更加有效，并支持训练过程中的小批量。</p>
			<p>我们还可以通过将流经每一层的数据存储为四维数据来支持卷积运算中的批处理。具体来说，数据按顺序存储(batch_num，channel，height，width)。例如，批量对N个数据进行图7.12<em class="italics">所示的处理时，数据的形状如下。</em></p>
			<p>在此处显示的批处理数据流中，批处理的维度被添加到每条数据的开头。因此，数据作为四维数据通过每一层。请注意，在网络中流动的四维数据表示对N个数据执行卷积运算；也就是说，一次进行N个过程:</p>
			<div><div><img src="img/Figure_7.13.jpg" alt="Figure 7.13: Process flow of a convolution operation (batch processing)&#13;&#10;"/>
				</div>
			</div>
			<h6>图7.13:卷积运算的处理流程(批处理)</h6>
			<h2 id="_idParaDest-181"><a id="_idTextAnchor190"/>汇集层</h2>
			<p>汇集操作使高度和宽度的空间变小。如<em class="italics">图7.14所示，</em>它将一个2 x 2的区域转换成一个元素，以减少空间的大小:</p>
			<div><div><img src="img/fig07_14.jpg" alt="Figure 7.14: Procedure of max pooling&#13;&#10;"/>
				</div>
			</div>
			<h6>图7.14:最大池化过程</h6>
			<p>此示例显示了在步长为2的情况下执行2 x 2最大池时的过程。“最大池”取一个区域的最大值，而“2 x 2”表示目标区域的大小。正如我们所看到的，它在一个2 x 2的区域中取最大元素。在本例中，跨距为2，因此2 x 2窗口一次移动两个元素。通常，池窗口大小和跨度使用相同的值。例如，对于3×3的窗口，步距是3，对于4×4的窗口，步距是4。</p>
			<h4>注意</h4>
			<p class="callout">除了最大池，平均池也可以使用。最大池取目标区域中的最大值，而平均池取目标区域中的平均值。在图像识别中，主要使用max pooling。因此，本书中的“池层”表示最大池化。</p>
			<h3 id="_idParaDest-182"><a id="_idTextAnchor191"/>一个汇集层的特征</h3>
			<p>池层具有各种特征，如下所述。</p>
			<p><strong class="bold">没有需要学习的参数</strong></p>
			<p>与卷积层不同，池层不需要学习任何参数。Pooling没有需要学习的参数，因为它只取目标区域中的最大值(或平均值)。</p>
			<p><strong class="bold">通道数量不变</strong></p>
			<p>在池中，输出数据中的通道数与输入数据中的通道数相同。如<em class="italics">图7.15 </em>所示，该计算对每个通道独立进行:</p>
			<div><div><img src="img/fig07_15.jpg" alt="Figure 7.15: Pooling does not change the number of channels&#13;&#10;"/>
				</div>
			</div>
			<h6>图7.15:池化不会改变通道的数量</h6>
			<p><strong class="bold">对微小的位置变化具有鲁棒性</strong></p>
			<p>即使输入数据略有变化，池化也会返回相同的结果。因此，它对输入数据的微小变化具有鲁棒性。例如，在3×3池化中，池化吸收了输入数据的移位，如图<em class="italics">图7.16 </em>所示:</p>
			<div><div><img src="img/fig07_16.jpg" alt="Figure 7.16: Even when the input data is shifted by one element in terms of width, the output is the same (it may not be the same, depending on the data)&#13;&#10;"/>
				</div>
			</div>
			<h6>图7.16:即使输入数据在宽度上移动了一个元素，输出也是一样的(可能不一样，这取决于数据)</h6>
			<h2 id="_idParaDest-183">实施<a id="_idTextAnchor192"/>卷积和池层</h2>
			<p>到目前为止，我们已经看到了卷积和池层的细节。在本节中，我们将使用Python实现这两个层。如<em class="italics">第五章</em>、<em class="italics">反向传播</em>所述，这里将要实现的类也提供了向前和向后的方法，这样它就可以作为一个模块使用。</p>
			<p>您可能会觉得实现卷积和池化层很复杂，但如果您使用某种“技巧”，您可以轻松实现它们本节描述了这个技巧，并使手头的任务变得容易。然后，我们将实现一个卷积层。</p>
			<h3 id="_idParaDest-184"><a id="_idTextAnchor193"/>四维阵列</h3>
			<p>如前所述，四维数据在CNN的每一层中流动。例如，当数据的形状是(10，1，28，28)时，它表示存在高度为28、宽度为28、通道为1的十条数据。您可以用Python实现这一点，如下所示:</p>
			<pre>&gt;&gt;&gt; x = np.random.rand(10, 1, 28, 28) # Generate data randomly
&gt;&gt;&gt; x.shape
(10, 1, 28, 28)</pre>
			<p>要访问第一段数据，可以写<code>x[0]</code>(在Python中索引从0开始)。类似地，您可以编写<code>x[1]</code>来访问第二段数据:</p>
			<pre>&gt;&gt;&gt; x[0].shape # (1, 28, 28)
&gt;&gt;&gt; x[1].shape # (1, 28, 28)</pre>
			<p>要访问第一条数据的第一个通道中的空间数据，可以编写以下代码:</p>
			<pre>&gt;&gt;&gt; x[0, 0] # or x[0][0]</pre>
			<p>你可以在CNN中以这种方式处理四维数据。因此，实现卷积运算可能是复杂的。然而，一个叫做<code>im2col</code>的“窍门”让这个任务变得简单了。</p>
			<h3 id="_idParaDest-185"><a id="_idTextAnchor194"/>用异丙醇膨胀</h3>
			<p>要实现卷积运算，通常需要多次嵌套<code>for</code>语句。这样的实现有点麻烦，而且NumPy中的<code>for</code>语句会降低处理速度(在NumPy中，最好不要使用任何<code>for</code>语句来访问元素)。这里，我们将不使用任何<code>for</code>语句。相反，我们将使用一个简单的函数<code>im2col</code>进行简单的实现。</p>
			<p><code>im2col</code>功能为过滤器(重量)方便地扩展输入数据。如图<em class="italics">图7.17 </em>所示，<code>im2col</code>将三维输入数据转换为二维矩阵(确切的说是将包括批次号在内的四维数据转换为二维数据)。</p>
			<p><code>im2col</code>为过滤器(重量)方便地扩展输入数据。具体来说，它将输入数据(一个三维块)中将要应用过滤器的区域扩展成一行，如图<em class="italics">图7.18 </em>所示。<code>im2col</code>扩展所有要应用过滤器的位置。</p>
			<p>在图7.18的<em class="italics">中，</em>使用了一个大的步幅，这样过滤区域就不会重叠。这样做是出于可见性的原因。在实际卷积运算中，大多数情况下滤波区域会重叠，在这种情况下，<code>im2col</code>扩展后的元素数量会比原始块中的元素数量多。因此，使用<code>im2col</code>的实现具有比通常消耗更多内存的缺点。然而，将数据放入大型矩阵有利于用计算机进行计算。比如矩阵计算库(线性代数库)高度优化矩阵计算，使其可以快速乘法大型矩阵。因此，您可以通过将输入数据转换为矩阵来有效地使用线性代数库:</p>
			<div><div><img src="img/fig07_17.jpg" alt="Figure 7.17: Overview of im2col&#13;&#10;"/>
				</div>
			</div>
			<h6>图7.17:im2 col概述</h6>
			<div><div><img src="img/fig07_18.jpg" alt="Figure 7.18: Expanding the filter target area from the beginning in a row&#13;&#10;"/>
				</div>
			</div>
			<h6>图7.18:从一行的开始处扩展过滤器目标区域</h6>
			<h4>注意</h4>
			<p class="callout">名称<code>im2col</code>是“图像到列”的缩写，意思是将图像转换成矩阵。Caffe和Chainer等深度学习框架提供了<code>im2col</code>函数，用于实现一个卷积层。</p>
			<p>使用<code>im2col</code>扩展输入数据后，你所要做的就是将卷积层的滤波器(权重)扩展成一行，并将两个矩阵相乘(见<em class="italics">图7.19 </em>)。此过程与完全连接的仿射层几乎相同:</p>
			<div><div><img src="img/fig07_19.jpg" alt="Figure 7.19: Details of filtering in a convolution operation – expand the filter into a column and multiply the matrix by the data expanded by im2col. Lastly, reshape the result of the size of the output data.&#13;&#10;"/>
				</div>
			</div>
			<h6>图7.19:卷积运算中过滤的细节–将过滤器展开为一列，并将矩阵乘以由im2col展开的数据。最后，调整输出数据大小的结果。</h6>
			<p>如图<em class="italics">图7.19 </em>所示，使用<code>im2col</code>函数的输出是一个二维矩阵。您必须将二维输出数据转换成适当的形状，因为CNN将数据存储为四维数组。下一节将介绍卷积层的实现流程。</p>
			<h3 id="_idParaDest-186"><a id="_idTextAnchor195"/>实现卷积层</h3>
			<p>这本书用的是<code>im2col</code>函数，我们就当黑盒用，不考虑它的实现。<code>im2col</code>实施地点位于<code>common/util.py</code>。这是一个长度大约为10行的简单函数。有兴趣可以参考一下。</p>
			<p>该<code>im2col</code>功能有如下界面:</p>
			<pre>im2col (input_data, filter_h, filter_w, stride=1, pad=0)</pre>
			<ul>
				<li><code>input_data</code>:输入由四维数组组成的数据(数据量、通道、高度、宽度)</li>
				<li><code>filter_h</code>:过滤器的高度</li>
				<li><code>filter_w</code>:滤镜宽度</li>
				<li><code>stride</code>:跨步</li>
				<li><code>pad</code>:填充</li>
			</ul>
			<p><code>im2col</code>函数考虑“过滤器大小”、“步幅”和“填充”，将输入数据扩展为二维数组，如下所示:</p>
			<pre>import sys, os
sys.path.append(os.pardir)
from common.util import im2col
x1 = np.random.rand(1, 3, 7, 7)
col1 = im2col(x1, 5, 5, stride=1, pad=0)
print(col1.shape) # (9, 75)
x2 = np.random.rand(10, 3, 7, 7)
col2 = im2col(x2, 5, 5, stride=1, pad=0)
print(col2.shape) # (90, 75)</pre>
			<p>前面的代码显示了两个示例。第一个使用7x7数据，批量大小为1，其中通道数为3。第二个使用相同形状的数据，批量大小为10。当我们使用<code>im2col</code>函数时，两种情况下第二维中的元素数量都是75。这是过滤器中元素的总数(3个通道，大小5x5)。当批量大小为1时，<code>im2col</code>的结果大小为(9，75)。另一方面，在第二个例子中它是(90，75 ),因为批量大小是10。它可以存储10倍的数据。</p>
			<p>现在，我们将使用<code>im2col</code>实现一个卷积层，作为一个名为<code>Convolution</code>的类:</p>
			<pre>class Convolution:
    def __init__(self, W, b, stride=1, pad=0):
        self.W = W
        self.b = b
        self.stride = stride
        self.pad = pad
    def forward(self, x):
        FN, C, FH, FW = self.W.shape
        N, C, H, W = x.shape
        out_h = int(1 + (H + 2*self.pad - FH) / self.stride)
        out_w = int(1 + (W + 2*self.pad - FW) / self.stride)
<code>        col = im2col(x, FH, FW, self.stride, self.pad)</code>
<code>        col_W = self.W.reshape(FN, -1).T # Expand the filter</code>
<code>        out = np.dot(col, col_W) + self.b</code>
        out = out.reshape(N, out_h, out_w, -1).transpose(0, 3, 1, 2)
        return out</pre>
			<p>卷积层的初始化方法将过滤器(权重)、偏移、步幅和填充作为参数。过滤器是四维的，(<code>FN</code>、<code>C</code>、<code>FH</code>和<code>FW</code>)。<code>FN</code>代表滤片数(滤片数)<code>C</code>代表通道，<code>FH</code>代表滤片高度，<code>FW</code>代表滤片宽度。</p>
			<p>在卷积层的实现中，重要部分以粗体显示。这里，<code>im2col</code>用于扩展输入数据，而<code>reshape</code>用于将滤波器扩展成二维数组。将扩展的矩阵相乘。</p>
			<p>扩展过滤器的代码部分(前面代码中的粗体部分)将每个过滤器的块扩展成一行，如图<em class="italics">图7.19 </em>所示。这里将<code>-1</code>指定为<code>reshape (FN, -1)</code>，这是<code>reshape</code>的便捷特性之一。当<code>reshape</code>被指定为<code>-1</code>时，元素的数量被调整，以使其与多维数组中的元素数量相匹配。例如，形状为(10，3，5，5)的数组总共有750个元素。当在这里指定<code>reshape(10, -1)</code>时，它被整形为(10，75)形状的数组。</p>
			<p><code>forward</code>功能在结束时适当调整输出尺寸。那里用的是NumPy的<code>transpose</code>函数。<code>transpose</code>函数改变多维数组中轴的顺序。如图<em class="italics">图7.20 </em>所示，您可以指定从0开始的索引(数字)顺序来改变轴的顺序。</p>
			<p>因此，您可以通过使用<code>im2col</code>进行扩展，以几乎与全连接仿射层相同的方式实现卷积层的正向过程(参见<em class="italics">第5章</em>、<em class="italics">反向传播</em>中的<em class="italics">实现仿射和Softmax层</em>部分)。接下来，我们将在卷积层实现反向传播。请注意，卷积层中的反向传播必须与<code>im2col</code>相反。这由本书中提供的col2im函数来处理(位于<code>common/util.py</code>)。除了使用col2im时，您可以在卷积层中以与仿射层相同的方式实现向后传播。卷积层中反向传播的实现位于<code>common/layer.py</code>。</p>
			<div><div><img src="img/fig07_20.jpg" alt="Figure 7.20: Using NumPy's transpose to change the order of the axes – specifying the indices &#13;&#10;(numbers) to change the order of axes&#13;&#10;"/>
				</div>
			</div>
			<h6>图7.20:使用NumPy的转置来改变轴的顺序——指定索引(数字)来改变轴的顺序</h6>
			<h3 id="_idParaDest-187"><a id="_idTextAnchor196"/>实现一个池层</h3>
			<p>实施汇集层时，可使用<code>im2col</code>扩展输入数据，如卷积层的情况。不同的是，与卷积层不同，池与通道维度无关。如图<em class="italics">图7.21 </em>所示，每个渠道独立扩展目标汇集区域。</p>
			<p>展开后，你只需取展开矩阵每一行的最大值，并将结果转换成合适的形状(<em class="italics">图7.22 </em>)。</p>
			<p>这就是池层中转发过程的实现方式。下面显示了Python中的一个示例实现:</p>
			<pre>class Pooling:
    def __init__(self, pool_h, pool_w, stride=1, pad=0):
        self.pool_h = pool_h
        self.pool_w = pool_w
        self.stride = stride
        self.pad = pad
    def forward(self, x):
        N, C, H, W = x.shape
        out_h = int(1 + (H - self.pool_h) / self.stride)
        out_w = int(1 + (W - self.pool_w) / self.stride)
        # Expansion (1)
        col = im2col(x, self.pool_h, self.pool_w, self.stride, self.pad)
        col = col.reshape(-1, self.pool_h*self.pool_w)
        # Maximum value (2)
<code>        out = np.max(col, axis=1)</code>
        # Reshape (3)
        out = out.reshape(N, out_h, out_w, C).transpose(0, 3, 1, 2)
            return out</pre>
			<div><div><img src="img/fig07_21.jpg" alt="Figure 7.21: Expanding the target pooling area of the input data (pooling of 2x2)&#13;&#10;"/>
				</div>
			</div>
			<h6>图7.21:扩展输入数据的目标池区域(2x2的池)</h6>
			<p>如图<em class="italics">图7.22 </em>所示，实现一个池层有三个步骤:</p>
			<ol>
				<li value="1">展开输入数据。</li>
				<li>取每行的最大值。</li>
				<li>适当地改变输出的形状。</li>
			</ol>
			<p>每一步的实现都很简单，只有一两行:</p>
			<div><div><img src="img/fig07_22.jpg" alt="Figure 7.22: Flow of implementation of a pooling layer – the maximum elements &#13;&#10;in the pooling area are shown in gray&#13;&#10;"/>
				</div>
			</div>
			<h6>图7.22:池化层的实现流程——池化区域中的最大元素以灰色显示</h6>
			<h4>注意</h4>
			<p class="callout">可以使用NumPy的<code>np.max</code>方法取最大值。通过在np.max中指定axis参数，可以沿着指定的轴取最大值。例如，<code>np.max(x, axis=1)</code>返回第一维度的每个轴上<code>x</code>的最大值。</p>
			<p>这就是池层中的转发过程。如此处所示，在将输入数据扩展成适合池化的形状之后，它的后续实现就非常简单了。</p>
			<p>对于池层中的反向过程，<code>max</code>的反向传播(用于<em class="italics">第5章</em>、<em class="italics">反向传播</em>中<em class="italics"> ReLU层</em>小节中ReLU层的实现)提供了这方面的更多信息。池层的实现位于<code>common/layer.py</code>。</p>
			<h2 id="_idParaDest-188"><a id="_idTextAnchor197"/>实现有线电视新闻网</h2>
			<p>到目前为止，我们已经实现了卷积和池层。现在，我们将这些层组合起来，创建一个识别手写数字的CNN并实现，如图<em class="italics">图7.23 </em>所示。</p>
			<p>如图<em class="italics">图7.23 </em>所示，网络由“卷积-ReLU-池化-仿射-ReLU-仿射Softmax”层组成。我们将把它实现为一个名为<code>SimpleConvNet</code>的类:</p>
			<div><div><img src="img/fig07_23.jpg" alt="Figure 7.23: Network configuration of a simple CNN&#13;&#10;"/>
				</div>
			</div>
			<h6>图7.23:简单CNN的网络配置</h6>
			<p>现在，我们来看看<code>SimpleConvNet (__init__)</code>的初始化。它采用以下参数:</p>
			<ul>
				<li><code>input_dim</code>:输入数据的尺寸(<strong class="bold">通道</strong>、<strong class="bold">高度</strong>、<strong class="bold">宽度</strong>)。</li>
				<li><code>conv_param</code>:卷积层的超参数(字典)。以下是字典关键字:</li>
				<li><code>filter_num</code>:过滤器数量</li>
				<li><code>filter_size</code>:过滤器的尺寸</li>
				<li><code>stride</code>:步幅</li>
				<li><code>pad</code>:填充</li>
				<li><code>hidden_size</code>:隐藏层神经元数量(全连接)</li>
				<li><code>output_size</code>:输出层神经元数量(全连接)</li>
				<li><code>weight_init_std</code>:初始化时重量的标准偏差</li>
			</ul>
			<p>这里，卷积层的超参数作为名为<code>conv_param</code>的字典提供。我们假设使用<code>{'filter_num':30, 'filter_size':5, 'pad':0, 'stride':1}</code>存储所需的超参数值。</p>
			<p><code>SimpleConvNet</code>初始化的实现有点长，所以这里分成三个部分以便于理解。以下代码显示了初始化过程的第一部分:</p>
			<pre>class SimpleConvNet:
    def __init__(self, input_dim=(1, 28, 28),
                conv_param={'filter_num':30, 'filter_size':5,
                    'pad':0, 'stride':1},
                hidden_size=100, output_size=10, weight_init_std=0.01):
        filter_num = conv_param['filter_num']
        filter_size = conv_param['filter_size']
        filter_pad = conv_param['pad']
        filter_stride = conv_param['stride']
        input_size = input_dim[1]
        conv_output_size = (input_size - filter_size + 2*filter_pad) / \
                        filter_stride + 1
        pool_output_size = int(filter_num * (conv_output_size/2) *(conv_output_size/2))</pre>
			<p>这里，由初始化参数提供的卷积层的超参数从字典中取出(以便我们稍后可以使用它们)。然后，计算卷积层的输出大小。以下代码初始化权重参数:</p>
			<pre>    self.params = {}
    self.params['W1'] = weight_init_std * \
    np.random.randn(filter_num, input_dim[0],
    filter_size, filter_size)
    self.params['b1'] = np.zeros(filter_num)
    self.params['W2'] = weight_init_std * \
    np.random.randn(pool_output_size,hidden_size)
    self.params['b2'] = np.zeros(hidden_size)
    self.params['W3'] = weight_init_std * \
    np.random.randn(hidden_size, output_size)
    self.params['b3'] = np.zeros(output_size)</pre>
			<p>训练所需的参数是第一(卷积)层和其余两个完全连接层的权重和偏差。参数存储在实例字典变量<code>params</code>中。<code>W1</code>键用于权重，而<code>b1</code>键用于第一(卷积)层的偏差。同样，<code>W2</code>和<code>b2</code>键用于第二层(完全连接)的权重和偏置，<code>W3</code>和<code>b3</code>键分别用于第三层(完全连接)的权重和偏置。最后，生成所需的层，如下所示:</p>
			<pre>    self.layers = OrderedDict( )
    self.layers['Conv1'] = Convolution(self.params['W1'],
                                self.params['b1'],
                                conv_param['stride'],
                                conv_param['pad'])
    self.layers['Relu1'] = Relu( )
    self.layers['Pool1'] = Pooling(pool_h=2, pool_w=2, stride=2)
    self.layers['Affine1'] = Affine(self.params['W2'],
                                self.params['b2'])
    self.layers['Relu2'] = Relu( )
    self.layers['Affine2'] = Affine(self.params['W3'],
                                self.params['b3'])
    self.last_layer = SoftmaxWithLoss( )</pre>
			<p>以适当的顺序将层添加到有序字典(<code>OrderedDict</code>)中。只有最后一层<code>SoftmaxWithLoss</code>被添加到另一个变量<code>last-layer</code>中。</p>
			<p>这是<code>SimpleConvNet</code>的初始化。初始化后，您可以执行<code>predict</code>预测方法和<code>loss</code>计算损失函数值的方法，如下所示:</p>
			<pre>def predict(self, x):
    for layer in self.layers.values( ):
        x = layer.forward(x)
    return x
def loss(self, x, t):
    y = self.predict(x)
return self.lastLayer.forward(y, t)</pre>
			<p>这里，<code>x</code>参数是输入数据，<code>t</code>参数是标签。<code>predict</code>方法只从顶层开始依次调用添加的层，并将结果传递给下一层。除了在<code>predict</code>方法中的正向处理外，<code>loss</code>方法执行正向处理直到最后一层<code>SoftmaxWithLoss</code>。</p>
			<p>以下实现通过反向传播获得梯度，如下所示:</p>
			<pre>def gradient(self, x, t):
    # forward
    self.loss(x, t)
    # backward
    dout = 1
    dout = self.lastLayer.backward(dout)
    layers = list(self.layers.values( ))
    layers.reverse( )
    for layer in layers:
        dout = layer.backward(dout)
    # Settings
    grads = {}
    grads['W1'] = self.layers['Conv1'].dW
    grads['b1'] = self.layers['Conv1'].db
    grads['W2'] = self.layers['Affine1'].dW
    grads['b2'] = self.layers['Affine1'].db
    grads['W3'] = self.layers['Affine2'].dW
    grads['b3'] = self.layers['Affine2'].db
    return grads</pre>
			<p>反向传播用于获得参数的梯度。为此，前向传播和后向传播一个接一个地进行。因为前向和后向传播在每一层中都正确地实现了，所以在这里我们只需要以适当的顺序调用它们。最后，每个权重参数的梯度存储在<code>grads</code>字典中。因此，您可以实现<code>SimpleConvNet</code>。</p>
			<p>现在，让我们使用MNIST数据集训练<code>SimpleConvNet</code>类。训练的代码与<em class="italics">第4章</em>、<em class="italics">神经网络训练</em>中<em class="italics">实现训练算法</em>部分描述的几乎相同。所以这里就不展示代码了(源代码位于<code>ch07/train_convnet.py</code>)。</p>
			<p>当使用<code>SimpleConvNet</code>对MNIST数据集进行训练时，训练数据的识别准确率为99.82%，而测试数据的识别准确率为98.96%(不同训练的识别准确率略有不同)。对于相对较小的网络的测试数据，99%是非常高的识别准确率。在下一章中，我们将添加层来创建一个网络，其中测试数据的识别准确率超过99%。</p>
			<p>正如我们在这里看到的，卷积和池层是图像识别中不可或缺的模块。CNN可以读取图像的空间特征，并在手写数字识别中实现高精度。</p>
			<h2 id="_idParaDest-189"><a id="_idTextAnchor198"/>可视化CNN</h2>
			<p>CNN中使用的卷积层“看到”了什么？在这里，我们将可视化一个卷积层，以探索CNN中发生的事情。</p>
			<h3 id="_idParaDest-190"><a id="_idTextAnchor199"/>可视化第一层的权重</h3>
			<p>早些时候，我们对MNIST数据集进行了简单的CNN训练。第一(卷积)层的权重形状是(30，1，5，5)。它的大小是5x5，有一个通道和30个过滤器。当滤镜的大小为5x5并且具有1个通道时，它可以被可视化为1个通道的灰度图像。现在，让我们将卷积层(第一层)的滤镜显示为图像。在这里，我们将比较训练前后的重量。<em class="italics">图7.24 </em>显示了结果(源代码位于<code>ch07/visualize_filter.py</code>):</p>
			<div><div><img src="img/fig07_24.jpg" alt="Figure 7.24: Weight of the first (convolution) layer before and after training. The elements of the weight are real numbers, but they are normalized between 0 and 255 to show the images so that the smallest value is black (0) and the largest value is white (255)&#13;&#10;"/>
				</div>
			</div>
			<h6>图7.24:训练前后第一(卷积)层的权重。权重的元素是实数，但它们在0和255之间被归一化以显示图像，因此最小值是黑色(0)，最大值是白色(255)</h6>
			<p>如图<em class="italics">图7.24 </em>所示，训练前的滤波器是随机初始化的。黑白色调没有图案。另一方面，训练后的滤波器是具有图案的图像。一些过滤器具有从白色到黑色的渐变，而一些过滤器具有小的颜色区域(称为“斑点”)，这表明训练为过滤器提供了模式。</p>
			<p>在<em class="italics">图7.24 </em>右侧带有图案的滤镜“查看”边缘(颜色的边界)和斑点。例如，当一个滤镜在左半部分为白色，在右半部分为黑色时，它会对垂直边缘做出反应，如图<em class="italics">图7.25 </em>所示。</p>
			<p><em class="italics">图7.25 </em>显示了选择两个学习过的滤波器，对输入图像进行卷积处理的结果。您可以看到“过滤器1”对垂直边缘做出反应，而“过滤器2”对水平边缘做出反应:</p>
			<div><div><img src="img/fig07_25.jpg" alt="Figure 7.25: Filters reacting to horizontal and vertical edges. White pixels appear at a vertical edge in output image 1. Meanwhile, many white pixels appear at a horizontal edge in output image 2.&#13;&#10;"/>
				</div>
			</div>
			<h6>图7.25:过滤器对水平和垂直边缘的反应。白色像素出现在输出图像1的垂直边缘。同时，许多白色像素出现在输出图像2的水平边缘。</h6>
			<p>因此，您可以看到卷积层中的过滤器提取了基本信息，如边缘和斑点。早先实现的CNN将这种原始信息传递给随后的层。</p>
			<h3 id="_idParaDest-191"><a id="_idTextAnchor200"/>利用层次结构提取信息</h3>
			<p>前面的结果来自第一(卷积)层。它提取边缘和斑点等低级信息。那么，多层CNN中的每一层都提取了什么类型的信息呢？深度学习中的可视化研究[( <em class="italics">马修·d·泽勒和罗布·弗格斯(2014):可视化和理解卷积网络。在大卫·弗利特，托马斯·帕德拉，伯恩特·席勒，&amp;廷恩·图特拉尔，编辑。计算机视觉-ECCV 2014。计算机科学讲义。施普林格国际出版公司，818–833</em>)和(<em class="italics"> A. Mahendran和A. Vedaldi (2015):通过倒置理解深层图像表征。2015年IEEE计算机视觉和模式识别会议(CVPR)。5188 – 5196.DOI:</em>(<a href="http://dx.doi.org/10.1109/CVPR.2015.7299155">http://dx.doi.org/10.1109/CVPR.2015.7299155</a>)】曾陈述过，越深入一层，提取的信息越抽象(准确的说是反应强烈的神经元)。</p>
			<h2 id="_idParaDest-192"><a id="_idTextAnchor201"/>典型的CNN</h2>
			<p>迄今为止，已经提出了各种架构的CNN。在本节中，我们将了解两个重要的网络。一个是LeNet ( <em class="italics"> Y. Lecun，L. Bottou，Y. Bengio和P. Haffner (1998):基于梯度的学习应用于文档识别。IEEE 86，11(1998年11月)，2278–2324会议录。土井</em>:(<a href="http://dx.doi.org/10.1109/5.726791">http://dx.doi.org/10.1109/5.726791</a>))。它是最早的CNN之一，于1998年首次提出。另一个是AlexNet ( <em class="italics"> Alex Krizhevsky，Ilya Sutskever，Geoffrey E. Hinton (2012):深度卷积神经网络的ImageNet分类。在f .佩雷拉、C. J. C. Burges、L. Bottou、&amp; K. Q. Weinberger编辑的。神经信息处理系统进展。柯伦联合公司，1097–1105</em>。它是在2012年提出的，引起了人们对深度学习的关注。</p>
			<h3 id="_idParaDest-193">LeNet</h3>
			<p>LeNet是1998年提出的用于手写数字识别的网络。在网络中，卷积层和汇集层(即，仅“稀释元素”的子采样层)被重复，最后，全连接层输出结果。</p>
			<p>LeNet和“当前CNN”有一些不同一是有一个激活功能。LeNet用的是sigmoid函数，现在主要用ReLU。在原始LeNet中使用子采样来减少中间数据的大小，而现在主要使用最大池:</p>
			<p>在这方面，LeNet和“当前的CNN”之间有一些差异，但这些差异并不显著。当我们考虑到LeNet是近20年前提出的“第一个CNN”时，这是令人惊讶的。</p>
			<h3 id="_idParaDest-194">AlexNet</h3>
			<p>AlexNet是在LeNet提出近20年后出版的。虽然AlexNet在深度学习方面创造了一个繁荣，但其网络架构与LeNet相比并没有太大变化:</p>
			<p>AlexNet堆叠了一个卷积层和一个池层，并通过一个全连接层输出结果。它的架构与LeNet没有太大的不同，但也有一些不同，如下所示:</p>
			<ul>
				<li>ReLU用作激活功能</li>
				<li>使用称为<strong class="bold">局部响应标准化</strong> ( <strong class="bold"> LRN </strong>)的局部标准化层</li>
				<li>使用辍学(参见第6章、<em class="italics">培训技巧</em>中<em class="italics">辍学</em>小节)</li>
			</ul>
			<p>LeNet和AlexNet在网络架构方面没有太大的不同。然而，周围的环境和计算机技术已经有了很大的进步。现在，每个人都可以获得大量的数据，而擅长大型并行计算的广泛存在的GPU使高速的海量运算成为可能。大数据和GPU极大地推动了深度学习的发展。</p>
			<h4>注意</h4>
			<p class="callout">深度学习(有很多层的网络)中往往存在很多参数。训练需要很多计算，需要大量数据来“满足”这些参数。我们可以说，GPU和大数据揭示了这些挑战。</p>
			<h2 id="_idParaDest-195"><a id="_idTextAnchor204"/>总结</h2>
			<p>在这一章中，我们学习了CNN。具体来说，我们详细讨论了卷积层和池层(构成CNN的基本模块),以便在实现级别理解它们。CNN主要用于查看有关图像的数据。请确保在继续之前理解本章的内容。</p>
			<p>在本章中，我们了解了以下内容:</p>
			<ul>
				<li>在CNN中，卷积层和池层被添加到先前的网络，该网络由完全连接的层组成。</li>
				<li>可以使用<code>im2col</code>(一个将图像展开成数组的函数)简单高效地实现卷积和合并图层。</li>
				<li>可视化CNN使您能够看到随着层变得更深，高级信息是如何被提取的。</li>
				<li>典型的CNN包括LeNet和AlexNet。</li>
				<li>大数据和GPU对深度学习的发展贡献巨大。</li>
			</ul>
		</div>
		<div><div/>
		</div>
		<div><div/>
		</div>
	

</body></html>