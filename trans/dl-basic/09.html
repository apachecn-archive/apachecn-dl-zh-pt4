<html><head/><body>


	
		<title>Chapter_8_SMP_ePub</title>
		
	
	
		<div><div/>
		</div>
		<div><h1 id="_idParaDest-196"><a id="_idTextAnchor205"/> 8。深度学习</h1>
		</div>
		<div><p>深度学习是一种基于深度神经网络的机器学习方法。您可以通过向我们目前描述的网络添加层来创建深度网络。然而，深层网络存在问题。本章将描述深度学习的特征、问题和可能性，以及当前深度学习实践的概述。</p>
			<h2 id="_idParaDest-197"><a id="_idTextAnchor206"/>加深关系网</h2>
			<p>通过这本书，我们了解了许多关于神经网络的知识，包括构成神经网络的各个层，训练中使用的有效技术，对处理图像特别有效的CNN，以及如何优化参数。这些都是深度学习的重要技术。在这里，我们将整合我们迄今为止所学的技术来创建一个深度网络。然后，我们将尝试使用MNIST数据集进行手写数字识别。</p>
			<h3 id="_idParaDest-198"><a id="_idTextAnchor207"/>更深层次的关系网</h3>
			<p>首先，我们将创建一个CNN，其网络架构如图8.1所示。该网络基于VGG网络，将在下一节描述。</p>
			<p>如<em class="italics">图8.1 </em>所示，该网络比我们迄今实施的网络更深。这里使用的所有卷积层都是小型3x3滤镜。这里，通道的数量随着网络的加深而变大(随着卷积层中的通道数量从第一层中的16个增加到16、32、32、64和64个)。如您所见，插入池层是为了逐渐减小中间数据的空间大小，而丢弃层用于后面的完全连接层:</p>
			<div><div><img src="img/fig08_1.jpg" alt="Figure 8.1: Deep CNN for handwritten digit recognition&#13;&#10;"/>
				</div>
			</div>
			<h6>图8.1:用于手写数字识别的深度CNN</h6>
			<p>该网络使用“he初始化器”来初始化权重，并使用Adam来更新权重参数，从而产生以下特征:</p>
			<ul>
				<li>使用小型3×3滤波器的卷积层</li>
				<li>ReLU作为激活函数</li>
				<li>在完全连接层之后使用的脱落层</li>
				<li>优化由Adam完成</li>
				<li>初始权重值的“初始化程序”</li>
			</ul>
			<p>正如这些特征所表明的，图8.1 中的网络使用了许多我们到目前为止学到的神经网络技术。现在，让我们使用这个网络进行训练。结果表明，该网络的识别准确率为99.38%(最终识别准确率略有差异，但该网络一般会超过99%)。</p>
			<h4>注意</h4>
			<p class="callout">实现图8.1 所示网络的源代码位于<code>ch08/deep_convnet.py</code>。培训代码在<code>ch08/train_deepnet.py</code>提供。您可以使用此代码来重现将在此进行的培训。深网训练需要很多时间(大概半天以上)。这本书提供了在<code>ch08/deep_conv_net_params.pkl</code>训练过的体重参数。<code>deep_convnet.py</code>代码文件提供了加载训练参数的功能。你可以根据需要使用它。</p>
			<p><em class="italics">图8.1 </em>所示网络的错误率仅为0.62%。在这里，我们可以看到哪些图像被错误地识别。<em class="italics">图8.2 </em>显示了识别错误示例:</p>
			<div><div><img src="img/fig08_2.jpg" alt="Figure 8.2: Sample images that were recognized incorrectly – the upper left of each image shows the correct label, while the lower right shows the result of prediction by this network&#13;&#10;"/>
				</div>
			</div>
			<h6>图8.2:被错误识别的样本图像–每幅图像的左上角显示了正确的标签，而右下角显示了该网络的预测结果</h6>
			<p>如<em class="italics">图8.2 </em>所示，这些图像即使是我们人类也很难识别。左上角的图像看起来像一个“0”(正确答案是“6”)，旁边的一个看起来肯定是一个“5”(正确答案是“3”)。一般来说，“1”和“7”，“0”和“6”，“3”和“5”之间的区别是很难的。这些例子解释了为什么它们被错误地识别。</p>
			<p>虽然这个深度CNN非常精确，但它会像人类一样错误地识别图像。这也向我们展示了一个深度CNN的巨大潜力。</p>
			<h3 id="_idParaDest-199">Im <a id="_idTextAnchor208"/>证明识别准确性</h3>
			<p>该网站名为“这个图像是什么类的？”<em class="italics"> (Rodrigo Benenson的博客</em> " <em class="italics">分类数据集结果</em>"(<a href="http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html">http://rodrigob . github . io/are _ we _ there _ yet/build/Classification _ datasets _ results . html</a>))通过相关文献中公布的技术对各种数据集的识别准确率进行排名(<em class="italics">图8.3 </em>):</p>
			<div><div><img src="img/fig08_3.jpg" alt="Figure 8.3: Ranking techniques for the MNIST dataset&#13;&#10;"/>
				</div>
			</div>
			<h6>图8.3:MNIST数据集的排名技术</h6>
			<h4>注意</h4>
			<p class="callout"><em class="italics">图8.3 </em>引自参考，<em class="italics"> Rodrigo Benenson的博客</em>“<em class="italics">分类数据集结果</em>”(<a href="http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html">http://rodrigob . github . io/are _ we _ there _ yet/build/Classification _ datasets _ results . html</a>)截止2016年6月。</p>
			<p>在<em class="italics">图8.3 </em>所示的排名中，“神经网络”、“深度”和“卷积”等关键词很明显。许多排名靠前的技术都是基于CNN的。截至2016年6月，MNIST数据集的最高识别准确率为99.79%(错误率为0.21%)，该技术也是基于CNN的(<em class="italics">李万，马修泽勒，张思欣，Yann L. Cun，和Rob Fergus (2013):使用DropConnect正则化神经网络。大卫·麦卡勒斯特编辑。第30届机器学习国际会议论文集(ICML2013)。JMLR研讨会和会议记录，1058–1066</em>。这里使用的CNN不是很深(两个卷积层和两个全连接层)。</p>
			<h4>注意</h4>
			<p class="callout">对于MNIST数据集，即使网络不是很深，也可以立即获得最高的精度。对于相对简单的问题如手写数字识别，网络的表示不需要很高。所以加层不是很有好处。在大规模通用对象识别过程中，添加层大大提高了识别精度，因为这是一个复杂的问题。</p>
			<p>通过检查前面提到的高级技术，我们可以找到进一步提高识别准确性的技术和提示。例如，我们可以看到集成学习、学习率衰减和<strong class="bold">数据扩充</strong>有助于识别准确率的提高。数据扩充是一种简单但特别有效的提高识别准确度的方法。</p>
			<p>数据增强使用一种算法来人工扩展输入图像(训练图像)。如图<em class="italics">图8.4 </em>所示，通过旋转或垂直/水平移动稍微改变输入图像来添加图像。当数据集中的图像数量有限时，这一点尤其有效:</p>
			<div><div><img src="img/Figure_8.4.jpg" alt="Figure 8.4: Sample data augmentation&#13;&#10;"/>
				</div>
			</div>
			<h6>图8.4:样本数据扩充</h6>
			<p>除了<em class="italics">图8.4 </em>中所示的修改之外，您可以使用数据增强以各种方式扩展图像。例如，您可以剪切图像的一部分(或裁剪)或水平反转图像(称为翻转，尽管这仅在不需要考虑图像的对称性时有效)。对于普通图像，改变它们的外观(例如，通过增加亮度和放大或缩小它们)也是有效的。如果可以使用数据增强来增加训练图像的数量，就可以通过使用深度学习来提高识别精度。这似乎是一个简单的技巧，但它往往会带来良好的效果。我们不会在这里实现数据扩充。既然实现这个很容易，那么如果你有兴趣的话，请自己尝试一下。</p>
			<h3 id="_idParaDest-200">默蒂<a id="_idTextAnchor209"/>开拓更深层次的网络</h3>
			<p>关于加深人际网络的重要性，还有很多不为人知的地方。虽然现在理论发现不足，但过去的研究和实验可以解释一些事情(相当直观)。这一部分将提供一些数据和解释来支持“加深关系网”的重要性</p>
			<p>首先，围绕大规模图像识别(如ILSVRC)的比赛结果表明了“使网络变得更深”的重要性(详情请参见下一节)。他们指出，最近许多排名靠前的技术都是基于深度学习的，网络往往会更深入。网络越深，识别性能越好。</p>
			<p>这样做的一个好处是可以减少网络中的参数数量。当网络更深时，它可以用更少的参数实现相似(或更高)的表示。当您考虑卷积运算中的滤波器大小时，这很容易理解。<em class="italics">图8.5 </em>显示了一个带有5x5过滤器的卷积层。</p>
			<p>请注意计算输出数据的每个节点的输入数据的面积。当然，在<em class="italics">图8.5 </em>所示的例子中，每个输出节点都是基于输入数据的5x5区域。现在，我们来考虑一个3×3卷积运算重复两次的情况，如图<em class="italics">图8.6 </em>所示。在这种情况下，中间数据基于每个输出节点的3x3区域。那么，中间数据的3x3区域是基于之前输入数据的哪个区域呢？当你仔细观察<em class="italics">图8.6 </em>时，你会注意到它是基于一个5×5的区域。因此，<em class="italics">图8.6 </em>的输出数据“看着”一个5×5区域的输入数据进行计算:</p>
			<div><div><img src="img/Figure_8.5.jpg" alt="Figure 8.5: Example of a 5x5 convolution operation&#13;&#10;"/>
				</div>
			</div>
			<h6>图8.5:一个5x5卷积运算的例子</h6>
			<div><div><img src="img/Figure_8.6.jpg" alt="Figure 8:6: Example of when 3x3 convolution operations are repeated twice&#13;&#10;"/>
				</div>
			</div>
			<h6>图8:6:3×3卷积运算重复两次的示例</h6>
			<p>一次5x5卷积运算的面积相当于两次3x3卷积运算的面积。前者使用25个参数(5x5)，后者总共使用18个参数(2x3x3)。因此，多个卷积层减少了参数的数量。随着网络越来越深，减少的参数数量变得越来越大。例如，当3×3卷积运算重复三次时，参数的数量总共是27。为了用一次卷积运算“观察”相同的区域，需要一个7×7滤波器，这意味着参数的数量达到49。</p>
			<h4>注意</h4>
			<p class="callout">通过多次应用小滤波器使网络变得更深的好处是可以减少参数数量，扩大<strong class="bold">感受野</strong>(改变神经元的局部空间区域)。添加图层时，会在卷积图层之间放置一个激活函数(如ReLU ),从而改善网络表示。这是因为激活函数对网络施加了一个“非线性”力。多个非线性函数支持更复杂的表达式。</p>
			<p>训练效率是把网络做深的另一个优势。更深的网络可以减少训练数据，快速进行训练。你可以通过记住<em class="italics">中提供的描述来直观地理解这一点，可视化<em class="italics">第7章</em>、<em class="italics">卷积神经网络</em>中的一个CNN </em>部分。在这一节中，您了解了CNN中的卷积层分层提取信息。在前卷积层，神经元对边缘等简单形状做出反应。随着层变得更深，神经元对层次更复杂的形状做出反应，如纹理和物体部分。</p>
			<p>考虑到网络的这种层次结构，考虑识别“狗”的问题为了在浅层网络中解决这个问题，卷积层必须一次“理解”一只狗的许多特征。有各种各样的狗，它们的样子也各不相同，这取决于图像拍摄的环境。因此，了解一只狗的特征需要多种多样的训练数据和大量的训练时间。</p>
			<p>但是，你可以通过把一个网络做得更深来把问题分层次学习。然后，每一层要学习的问题就变得简单了。比如第一层可以集中学习边缘。因此，网络可以用少量的训练数据有效地学习。这是因为包含边缘的图像的数量大于狗的图像的数量，并且边缘的图案比狗的图案简单。</p>
			<p>同样重要的是，你可以通过加深关系网来传递信息。例如，提取边缘的层旁边的层可以使用边缘信息，因此我们可以期望它高效地学习更高级的模式。简而言之，通过将网络变得更深，你可以将每一层要学习的问题分成“容易解决的简单问题”，这样你就可以期待高效的培训。</p>
			<p>这就是支持“让网络更深”重要性的解释。请注意，近年来，新的技术和环境(如大数据和计算机能力)提供了更深的网络，这使得能够在深度网络中进行正确的训练。</p>
			<h2 id="_idParaDest-201"><a id="_idTextAnchor211"/>深度学习简史</h2>
			<p>据说深度学习开始在大规模图像识别的比赛中引起广泛关注是因为2012年举办的<strong class="bold"> ImageNet大规模视觉识别挑战赛</strong> ( <strong class="bold"> ILSRVC </strong>)。在比赛中，一种名为AlexNet的深度学习技术取得了压倒性的胜利，颠覆了传统的图像识别方法。自2012年深度学习发起反击以来，一直在后续比赛中扮演主角。在这里，我们将围绕大规模图像识别的竞争来看当前深度学习的趋势，称为ILSVRC。</p>
			<h3 id="_idParaDest-202"><a id="_idTextAnchor212"/> ImageNet</h3>
			<p>ImageNet ( <em class="italics">邓，董，索彻，李，，，2009): ImageNet:一个大规模的层次图像数据库</em>。在2009年IEEE计算机视觉和模式识别会议上。CVPR 2009。248 – 255.DOI:(<a href="http://dx.doi.org/10.1109/CVPR.2009.5206848">http://dx.doi.org/10.1109/CVPR.2009.5206848</a>))是一个包含超过一百万张图片的数据集。如图<em class="italics">图8.7 </em>所示，它包含了各种类型的图像，每个图像都与一个标签(类名)相关联。每年都会举办一场名为ILSVRC的图像识别比赛，使用的就是这个庞大的数据集:</p>
			<div><div><img src="img/Figure_8.7.jpg" alt="Figure 8.7: Sample data in the large-scale ImageNet dataset&#13;&#10;"/>
				</div>
			</div>
			<h6>图8.7:大规模ImageNet数据集中的样本数据</h6>
			<h4>注意</h4>
			<p class="callout"><em class="italics">图8.7 </em>引自参考文献，<em class="italics">邓，董，索彻，李，，李菲菲(2009): ImageNet:一个大规模分层图像数据库</em>。在2009年IEEE】计算机视觉和模式识别会议上。CVPR 2009。248 – 255.土井:(<a href="http://dx.doi.org/10.1109/CVPR.2009.5206848">http://dx.doi.org/10.1109/CVPR.2009.5206848</a>)。</p>
			<p>ILSVRC竞赛提供了一些测试项目，其中一项是“分类”(在“分类”部分，对1000个类进行分类，以竞争识别准确率)。<em class="italics">图8.8 </em>显示了自2010年以来ILSVRC分类部门获胜团队的结果。这里，如果前5个预测包含正确的类别，则认为分类是“正确的”。以下条形图显示了错误率:</p>
			<div><div><img src="img/Figure_8.8.jpg" alt="Figure 8.8: The results of the winning teams in ILSVRC – the vertical axis shows error rates, &#13;&#10;while the horizontal axis shows years. Team names or technique names are shown in the &#13;&#10;parentheses on the horizontal axis.&#13;&#10;"/>
				</div>
			</div>
			<h6>图8.8:ils vrc中获胜团队的结果——纵轴显示错误率，横轴显示年份。团队名称或技术名称显示在横轴的括号中。</h6>
			<p>请注意，从上图中可以看出，自2012年以来，深度学习技术一直处于领先地位。实际上，我们可以看到，在2012年，AlexNet显著降低了错误率。从那以后，深度学习技术在准确性方面稳步提高。这一点在2015年的ResNet中尤为明显，这是一个超过150层的深度网络，并将错误率降低到了3.5%。甚至有人说这个结果超过了普通人类的识别能力。</p>
			<p>在过去几年取得巨大成果的深度学习网络中，VGG、谷歌网和ResNet是最著名的。你会在各种与深度学习相关的地方碰到它们。接下来我简单介绍一下这三个著名的网络。</p>
			<h3 id="_idParaDest-203"><a id="_idTextAnchor213"/> VGG</h3>
			<p>VGG是一个“基本”CNN，由卷积层和池层组成。如<em class="italics">图8.9 </em>所示，它可以有多达16层(或19层)的权重(卷积层和全连通层)使其自身变深，根据层数有时被称为“VGG16”或“VGG19”:</p>
			<div><div><img src="img/fig08_9.jpg" alt="Figure 8.9: VGG&#13;&#10;"/>
				</div>
			</div>
			<h6>图8.9: VGG</h6>
			<h4>注意</h4>
			<p class="callout"><em class="italics">图8.9 </em>引自参考文献，<em class="italics">卡伦·西蒙扬和安德鲁·齐泽曼(2014):用于大规模图像识别的极深卷积网络。arXiv:1409.1556[cs](2014年9月)</em>。</p>
			<p>VGG包含连续的卷积层与一个小的3x3过滤器。如上图所示，两个或四个连续的卷积层和一个合并层将大小减半，并且重复此过程。最后，通过完全连接的层提供结果。</p>
			<h4>注意</h4>
			<p class="callout">VGG在2014年的比赛中获得了二等奖(GoogLeNet，将在下面介绍，在2014年获得)。它的性能不如第一名的网络，但许多工程师更喜欢使用基于VGG的网络，因为它们的结构非常简单，功能多样。</p>
			<h3 id="_idParaDest-204">谷歌网</h3>
			<p><em class="italics">图8.10 </em>显示了GoogLeNet的网络架构。矩形表示各种层，如卷积层和池层:</p>
			<div><div><img src="img/fig08_10.jpg" alt="Figure 8.10: GoogLeNet&#13; &#10;"/>
				</div>
			</div>
			<h6>图8.10:谷歌网络</h6>
			<h4>注意</h4>
			<p class="callout"><em class="italics">图8.10 </em>和<em class="italics">图8.11 </em>引自<em class="italics"> Christian Szegedy等人(2015):用卷积更深入。在IEEE计算机视觉和模式识别会议(CVPR) </em>。</p>
			<p>它的网络架构看起来似乎很复杂，但基本上和一个CNN是一样的。GoogLeNet的与众不同之处在于，网络不仅在垂直方向上有深度，在水平方向上也有深度(spread)。</p>
			<p>GoogLeNet在水平方向有“宽度”。它被称为“初始架构”，基于图8.11所示的结构:</p>
			<div><div><img src="img/Figure_8.11.jpg" alt="Figure 8.11: Inception architecture of GoogLeNet&#13;&#10;"/>
				</div>
			</div>
			<h6>图8.11:Google net的初始架构</h6>
			<p>如<em class="italics">图8.11 </em>所示，初始架构应用了多个不同大小的过滤器(和池)并组合结果。使用这个初始架构作为一个构建块(组件)是GoogLeNet的主要特点。</p>
			<p>GoogLeNet在许多地方使用带有1x1滤镜的卷积图层。这种1x1卷积运算减小了通道方向上的尺寸，从而减少了参数数量并加速了处理。</p>
			<h3 id="_idParaDest-205"><a id="_idTextAnchor215"/> ResNet</h3>
			<p>ResNet ( <em class="italics">何，，，任，(2015):深度残差学习用于图像识别。arXiv:1512.03385[cs](2015年12月)</em>)是微软的一个团队开发的网络。它的特点是有一种“机制”，可以让网络变得比以往更深。</p>
			<p>加深网络对提高网络性能很重要。然而，当一个网络变得太深时，深度学习就会失败，最终的性能往往很差。为了解决这个问题，ResNet引入了“跳过架构”(也称为“捷径”或“旁路”)。通过引入这种跳过架构，随着网络变得更深，性能可以得到提高(尽管允许的深度是有限制的)。</p>
			<p>skip架构跳过输入数据中的卷积层，将输入数据添加到输出中，如图<em class="italics">图8.12 </em>所示:</p>
			<div><div><img src="img/fig08_12.jpg" alt="Figure 8.12: Components of ResNet – the &quot;weight layer&quot; here indicates a convolution layer&#13;&#10;"/>
				</div>
			</div>
			<h6>图8.12:ResNet的组件–这里的“权重层”表示卷积层</h6>
			<h4>注意</h4>
			<p class="callout"><em class="italics">图8.12 </em>、<em class="italics">图8.13 </em>引自参考文献，<em class="italics">何、、任、(2015):深度残差学习用于图像识别。arXiv:1512.03385[cs](2015年12月)</em>。</p>
			<p>在<em class="italics">图8.12 </em>中，输入<em class="italics"> x </em>通过跳过两个连续卷积层连接到输出。两个卷积层的输出原本是<em class="italics"> F(x) </em>，而skip架构将其改为<em class="italics"> F(x) + x </em>。</p>
			<p>采用这种skip架构可以实现高效的学习，即使网络很深。这是因为skip架构在反向传播期间无衰减地传输信号。</p>
			<h4>注意</h4>
			<p class="callout">skip架构只“按原样”传递输入数据在反向传播中，它还将梯度从上游“按原样”传递到下游，而不改变它们。因此，使用skip架构，您不必担心梯度变小(或变大)。你可以期待“有意义的梯度”被传输到前面的层。您还可以期待skip架构缓解传统的梯度消失问题，该问题会随着网络变深而降低梯度。</p>
			<p>ResNet基于我们之前描述的VGG网络，采用了skip架构，使网络更加深入。<em class="italics">图8.13 </em>显示了这样的结果:</p>
			<div><div><img src="img/fig08_13.jpg" alt="Figure 8.13: ResNet – blocks support 3x3 convolution layers. Its characteristic is the skip &#13;&#10;architecture, which skips layers.&#13;&#10;"/>
				</div>
			</div>
			<h6>图8.13:ResNet–块支持3x3卷积层。它的特点是跳过层的skip架构。</h6>
			<p>如图<em class="italics">图8.13 </em>所示，ResNet跳过两个卷积层，使网络更深。实验表明，即使当网络包含150层或更多层时，识别精度也在继续提高。在ILSVRC的比赛中，它取得了错误率(没有被列入前5名预测的正确类别的百分比)3.5%的惊人成绩。</p>
			<h4>注意</h4>
			<p class="callout">通过使用庞大的ImageNet数据集训练的重量数据经常被有效地使用。这叫做<strong class="bold">迁移学习</strong>。一部分训练好的权重被复制到另一个神经网络进行微调。例如，提供了具有与VGG相同结构的网络。训练的权重被用作初始值，并且对新的数据集进行微调。当你手头有几个数据集时，迁移学习尤其有效。</p>
			<h2 id="_idParaDest-206"><a id="_idTextAnchor216"/>加速深度学习</h2>
			<p>大数据和大规模网络需要深度学习的海量运算。到目前为止，我们已经使用CPU进行计算，但仅靠CPU不足以解决深度学习。其实很多深度学习框架都支持<strong class="bold">图形处理单元</strong>(<strong class="bold">GPU</strong>)快速处理大量运算。最近的框架开始通过使用多个GPU或机器来支持分布式学习。本节描述了在深度学习中加速计算。我们深度学习的实现在8.1节结束。这里描述的加速(比如支持GPU)我们就不实现了。</p>
			<h3 id="_idParaDest-207">需要克服的挑战</h3>
			<p>在讨论深度学习的加速之前，我们先来看看深度学习中哪些过程是需要时间的。<em class="italics">图8.14 </em>中的饼状图显示了AlexNet转发处理中每个类花费的时间:</p>
			<div><div><img src="img/Figure_8.14.jpg" alt="Figure 8.14: Percentage of time that each layer spends in the forward processing of AlexNet – the left-hand chart shows GPU time, while the right-hand one shows CPU time&#13;&#10;"/>
				</div>
			</div>
			<h6>图8.14:每一层花费在AlexNet的前向处理中的时间百分比——左边的图表显示GPU时间，而右边的图表显示CPU时间</h6>
			<p>这里，“conv”表示卷积层，“pool”表示汇集层，“fc”表示全连通层，“norm”表示归一化层(引自<em class="italics">贾(2014):大规模学习语义图像表征)。加州大学伯克利分校EECS系博士论文，2014年5月</em>，(<a href="http://www.eecs.berkeley.edu/Pubs/TechRpts/2014/EECS-2014-93.html">http://www . eecs . Berkeley . edu/Pubs/tech rpts/2014/EECS-2014-93 . html</a>))。</p>
			<p>如你所见，卷积层在AlexNet中花费了大量的时间。实际上卷积层的总处理时间达到了GPU时间的95%和CPU时间的89%！因此，在卷积层中进行快速高效的操作是深度学习的主要挑战。<em class="italics">图8.14 </em>显示了推断阶段的结果，但卷积层在训练阶段也花费了大量时间。</p>
			<h4>注意</h4>
			<p class="callout">如第7章、<em class="italics">卷积神经网络</em>中<em class="italics">卷积层</em>主题所述，卷积层中的运算基本上是“乘-累加运算”因此，加速深度学习取决于如何快速高效地计算海量的“乘累加运算”。</p>
			<h3 id="_idParaDest-208"><a id="_idTextAnchor218"/>使用GPU进行加速</h3>
			<p>最初，GPU专门用于图形。最近，它们已被用于一般的数值计算，以及图形处理。由于GPU可以快速进行并行算术运算，GPU计算将其压倒性的能力用于各种目的。</p>
			<p>深度学习需要大规模的乘累加运算(或者大矩阵的乘积)。GPU擅长这种海量并行运算，而CPU擅长连续复杂的计算。与只使用CPU相比，你可以使用GPU来加速深度学习操作，这令人惊讶。<em class="italics">图8.15 </em>比较了AlexNet在CPU和GPU之间学习所用的时间:</p>
			<div><div><img src="img/Figure_8.15.jpg" alt="Figure 8.15: Comparing the time that AlexNet took for learning between a &quot;16-core Xeon CPU&quot; &#13;&#10;and a &quot;Titan series&quot; GPU&#13;&#10;"/>
				</div>
			</div>
			<h6>图8.15:比较AlexNet在“16核至强CPU”和“泰坦系列”GPU之间的学习时间</h6>
			<h4>注意</h4>
			<p class="callout"><em class="italics">图8.15 </em>引自参考，<em class="italics"> NVIDIA博客《NVIDIA用TITAN X、New DIGITS训练系统和DevBox推进深度学习》</em>(<a href="https://blogs.nvidia.com/blog/2015/03/17/digits-devbox/">https://blogs.nvidia.com/blog/2015/03/17/digits-devbox/</a>)。</p>
			<p>可以看到，CPU用了40多天，而GPU只用了6天。我们还可以看到，使用针对深度学习优化的cuDNN库，进一步加速了训练。</p>
			<p>GPU主要由英伟达和AMD两家公司提供。虽然你可以使用他们的两个GPU进行一般的算术运算，但英伟达的GPU更“熟悉”深度学习。实际上，许多深度学习框架只能受益于英伟达的GPU。这是因为深度学习框架中使用了NVIDIA提供的GPU计算集成开发环境CUDA。cuDNN，可以在<em class="italics">图8.15 </em>中看到，是一个运行在CUDA上的库，其中实现了针对深度学习优化的功能。</p>
			<h4>注意</h4>
			<p class="callout">我们使用<code>im2col</code>将卷积层中的运算转换成大矩阵的乘积。实现这个<code>im2col</code>方法适合GPU。GPU擅长一口气计算一个大批量，而不是一个个计算小批量。使用<code>im2col</code>来计算巨大矩阵的乘积，可以很容易地展示GPU的真正力量。</p>
			<h3 id="_idParaDest-209"><a id="_idTextAnchor219"/>分布式培训</h3>
			<p>你可以通过使用GPU来加速深度学习操作，但深度网络仍然需要几天或几周的训练。正如我们到目前为止所看到的，深度学习涉及大量的尝试和错误。你必须尝试许多事情来创建一个良好的网络。你自然想尽可能减少训练所需的时间。然后，扩展深度学习或“分布式训练”变得很重要。</p>
			<p>为了进一步加速深度学习所需的计算，您可能希望将它们分布在多个GPU或机器中。现在，一些深度学习框架支持多个GPU或机器进行分布式训练。其中，Google的TensorFlow和微软的<strong class="bold">计算网络工具包</strong> ( <strong class="bold"> CNTK </strong>)都被开发出来专注于分布式训练。基于大型数据中心的低延迟和高吞吐量网络，这些框架的分布式训练取得了令人惊讶的结果。</p>
			<p>分布式训练能在多大程度上加速深度学习？答案是GPU数量越大，训练速度越快。事实上，100个GPU(多台机器上总共安装了100个GPU)比一个GPU实现了56倍的加速。举例来说，这意味着通常需要7天的培训在3小时内就完成了，这表明了分布式培训的惊人效果。</p>
			<p>分布式训练中的“如何分配计算”是一个非常困难的问题。它包含了很多不容易解决的问题，比如机器之间的通信和数据同步。可以把这么难的问题留给TensorFlow等优秀的框架。这里就不讨论分布式训练的细节了。分布式训练的技术细节请参见关于TensorFlow的技术论文(白皮书)(<em class="italics">Martín Abadi et al .(2016):tensor flow:异构分布式系统上的大规模机器学习。arXiv:1603.04467[cs](2016年3月)</em>。</p>
			<h3 id="_idParaDest-210"><a id="_idTextAnchor220"/>为算术精度减少位数</h3>
			<p>内存空间和总线带宽，以及计算复杂度，都可能成为加速深度学习的瓶颈。对于内存空间，必须在内存中存储大量的权重参数和中间数据。对于总线带宽，当流经GPU(或CPU)总线的数据增加超过限制时，就会出现瓶颈。在这些情况下，您希望网络中流动的数据位数尽可能小。</p>
			<p>计算机主要使用64位或32位浮点数来表示实数。使用许多位来表示一个数减少了数值计算中误差的影响，但是增加了处理成本和存储器的使用，给总线带宽带来了负担。</p>
			<p>从我们所知道的深度学习关于数值精度(用多少位来表示一个数值)，它不需要非常高的精度。由于其鲁棒性，这是神经网络最重要的特征之一。这里的鲁棒性意味着，例如，即使输入图像包含少量噪声，输出结果在神经网络中也不会改变。考虑到健壮性，即使网络中流动的数据“恶化”，它对输出结果的影响也很小</p>
			<p>计算机通常使用32位单精度浮点表示法或64位双精度浮点表示法来表示小数。实验表明，16位<code>float</code>在深度学习中已经足够(<em class="italics"> Suyog Gupta，Ankur Agrawal，Kailash Gopalakrishnan，Pritish Narayanan (2015):数值精度有限的深度学习。更正，abs/1502.02551 392 (2015) </em>。其实NVIDIA那一代GPU用的Pascal架构是支持半精度浮点数运算的。据认为，半格式将被用作未来的标准。</p>
			<h4>注意</h4>
			<p class="callout">NVIDIA的Maxwell一代GPU支持半精度浮点数的存储(维护数据)，但不进行16位运算。下一代Pascal架构也进行16位运算。我们可以预计，只使用半精度浮点数进行计算将加快处理速度，使其速度大约是上一代GPU的两倍。</p>
			<p>在深度学习的前面实现中，我们没有涉及数字精度。Python一般使用64位浮点数。NumPy提供了一种16位半精度浮点数据类型(不过，它只用于存储，不用于运算)。我们可以很容易地证明，使用NumPy的半精度浮点数不会降低识别精度。有兴趣的请看<code>ch08/half_float_network.py</code>。</p>
			<p>已经进行了一些研究来减少深度学习中的位数。在最近的研究中，提出了一种称为“二值化神经网络”的技术(<em class="italics">Matthieu Courbariaux and yo shua beng io(2016):二值化神经网络:训练深度神经网络，权重和激活约束为+1或-1。arXiv预印本arXiv:1602.02830 (2016) </em>。它用1位表示权重和中间数据。减少比特数来加速深度学习是我们应该关注的话题。当我们考虑将深度学习用于嵌入式设备时，这一点尤为重要。</p>
			<h2 id="_idParaDest-211"><a id="_idTextAnchor221"/>深度学习的实际用途</h2>
			<p>作为使用深度学习的一个例子，我们主要讨论了图像分类，例如手写数字识别，这被称为“对象识别”。然而，我们可以将深度学习应用于物体识别之外的许多问题。深度学习在图像识别、声音(语音识别)和自然语言处理等许多问题上表现出色。本节将介绍深度学习在计算机视觉领域能做什么(它的应用)。</p>
			<h3 id="_idParaDest-212"><a id="_idTextAnchor222"/>物体检测</h3>
			<p>对象检测识别图像中对象的位置并对它们进行分类。物体检测比物体识别更难。虽然对象识别以整个图像为目标，但是对象检测必须识别图像中类别的位置，并且可能存在多个对象。</p>
			<p>一些基于CNN的技术已经被提出用于目标检测。它们表现出优异的性能，这表明深度学习对于对象检测也是有效的。</p>
			<p>在基于CNN的对象检测技术中，一种称为R-CNN的技术(<em class="italics"> Ross Girshick、Jeff Donahue、Trevor Darrell和Jitendra Malik (2014):用于精确对象检测和语义分割的丰富特征层次。在580–587年</em>)比较出名。<em class="italics">图8.16 </em>显示了R-CNN的工艺流程:</p>
			<div><div><img src="img/Figure_8.16.jpg" alt="Figure 8.16: Process flow of R-CNN&#13;&#10;"/>
				</div>
			</div>
			<h6>图8.16:R-CNN的流程图</h6>
			<h4>注意</h4>
			<p class="callout"><em class="italics">图8.16 </em>引自参考文献，<em class="italics"> Ross Girshick、Jeff Donahue、Trevor Darrell和Jitendra Malik (2014):用于精确对象检测和语义分割的丰富特征层次。在580–587</em>中。</p>
			<p>在<em class="italics">图8.16 </em>中，注意<em class="italics"> 2。提取区域建议</em>和<em class="italics"> 3。计算CNN特征</em>部分。第一种技术检测似乎是物体的区域(以某种方式)，然后将CNN应用于提取的区域以对它们进行分类。R-CNN将图像转换成正方形，并使用<strong class="bold">支持向量机</strong> ( <strong class="bold"> SVMs </strong>)进行分类。它的实际处理流程稍微复杂一些，但是主要由前述过程组成:候选区域的提取和计算CNN特征。</p>
			<p>在R-CNN的“提取区域提议”过程中，检测目标的候选对象，这是可以使用计算机视觉中开发的各种技术的地方。在关于R-CNN的论文中，使用了一种称为选择性搜索的技术。最近，一种称为“更快的R-CNN”的技术(<em class="italics">任，何，Ross Girshick，和(2015):更快的R-CNN:利用区域提议网络实现实时对象检测。在c .科尔特斯、N. D .劳伦斯、D. D .李、m .杉山、神经信息处理系统进展。Curran Associates，Inc .，91–99</em>)已被提出。它甚至使用CNN来提取地区提案。更快的R-CNN在整个过程中使用一个CNN，这实现了快速处理。</p>
			<h3 id="_idParaDest-213"><a id="_idTextAnchor223"/>分割</h3>
			<p>分割以像素为基础对图像进行分类。它通过使用训练数据来学习，其中对象以像素为基础着色，并在推断期间对输入图像的所有像素进行分类。到目前为止，我们实现的神经网络对整个图像进行分类。那么，如何才能以像素为单位进行分类呢？</p>
			<p>用神经网络执行分割的最简单方法是对每个像素进行预测。例如，您可以提供一个对矩形区域中心的像素进行分类的网络，以便对所有像素进行预测。如您所见，这需要与像素数一样多的前向过程，因此需要大量时间来完成(问题是卷积运算会无用地重新计算许多区域)。为了减少这种无用的计算，一种被称为<strong class="bold">完全卷积网络</strong> ( <strong class="bold"> FCN </strong>)的技术已经被提出(<em class="italics"> Jonathan Long，Evan Shelhamer，和Trevor Darrell (2015):用于语义分割的完全卷积网络。在IEEE计算机视觉和模式识别会议(CVPR) </em>。它在一个正向过程中对所有像素进行分类(参见<em class="italics">图8.20 </em>)。</p>
			<p>FCN是仅由卷积图层组成的网络。虽然普通的CNN包含完全连接的层，但FCN用扮演相同角色的<em class="italics">卷积层来代替完全连接的层</em>。在用于对象识别的网络的完全连接的层中，中间数据的空间体积被处理为排成一行的节点。另一方面，在仅包含卷积图层的网络中，空间体积可以在处理过程中保持不变，直到最后一次输出。</p>
			<p>FCN的主要特点是空间尺寸在末端扩大。这种扩展可以放大缩小的中间数据，使其立刻与输入图像的大小相同。FCN末端的扩展是通过双线性插值的扩展(双线性扩展)。FCN使用去卷积来进行双线性扩展(详细信息，请参见论文(<em class="italics"> Jonathan Long，Evan Shelhamer，和Trevor Darrell (2015):语义分割的完全卷积网络)。在IEEE计算机视觉和模式识别会议(CVPR) </em>关于FCN)。</p>
			<h4>注意</h4>
			<p class="callout">在全连接层中，输出连接到所有输入。您还可以在卷积图层中创建结构相同的连接。例如，输入数据大小为32x10x10(通道数为32，高度为10，宽度为10)的全连接图层可以替换为过滤器大小为32x10x10的卷积图层。如果完全连接的层具有100个输出节点，则卷积层可以通过提供100个32x10x10滤波器来实现完全相同的处理。通过这种方式，完全连接的层可以替换为执行等效处理的卷积层。</p>
			<h3 id="_idParaDest-214"><a id="_idTextAnchor224"/>生成图像字幕</h3>
			<p>有一些有趣的研究正在进行，结合了自然语言和计算机视觉。当提供图像时，解释图像的文本(图像标题)被自动生成。</p>
			<p>例如，越野自行车比赛中的摩托车图像可以包含标题:“一个人在土路上骑摩托车”(该文本是从图像中自动生成的)。令人惊讶的是，该系统甚至“理解”它正在一条土路上，一个人正在骑摩托车。</p>
			<p>一种称为<strong class="bold">神经图像字幕</strong> ( <strong class="bold"> NIC </strong>)的模型通常用于生成深度学习的图像字幕。NIC由一个深度CNN和一个用于处理自然语言的<strong class="bold">递归神经网络</strong> ( <strong class="bold"> RNN </strong>)组成。RNN具有递归连接，通常用于自然语言和时序数据等序列数据。</p>
			<p>NIC使用CNN从图像中提取特征，并将它们传递给RNN。RNN使用CNN提取的特征作为初始值来“递归地”生成文本这里就不讨论技术细节了。基本上，NIC有一个简单的架构，结合了两个神经网络:一个CNN和一个RNN。它可以生成惊人精确的图像标题。处理各种类型的信息，如图像和自然语言，称为<strong class="bold">多模态处理</strong>。近年来，多模态处理获得了很多关注:</p>
			<h4>注意</h4>
			<p class="callout">RNN其余部分代表经常性。“递归”表示神经网络的递归网络架构。由于重复出现的架构，RNN受到之前产生的信息的影响——换句话说，它记得过去的信息。这是RNN的主要特征。例如，在生成单词“I”之后，它会受到该单词的影响，并生成下一个单词“am”。然后，它受到先前生成的“我是”的词的影响，生成“睡觉”的词。对于连续数据，如自然语言和时间序列数据，RNN的行为就好像它记住了过去的信息。</p>
			<h2 id="_idParaDest-215"><a id="_idTextAnchor225"/>深度学习的未来</h2>
			<p>深度学习现在被用在各个领域，传统领域也是如此。这一部分描述了深度学习的可能性和一些显示深度学习未来的研究。</p>
			<h3 id="_idParaDest-216"><a id="_idTextAnchor226"/>转换图像样式</h3>
			<p>正在进行的研究使用深度学习像艺术家一样“画”一幅画。神经网络的一个流行用例是基于两个提供的图像创建一个新图像。其中一个称为“内容图像”，而另一个称为“样式图像”基于这两个图像创建新的图像。</p>
			<p>在一个示例中，您可以指定梵高的绘画风格作为将应用于内容图像的风格，深度学习按照指定绘制新的图片。这项研究发表在论文《艺术风格的神经算法》(<em class="italics"> Leon A. Gatys，Alexander S. Ecker，and Matthias Bethge (2015):艺术风格的神经算法)。arXiv:1508.06576[cs，q-bio](2015年8月)</em>)并且一出版就受到了全世界的大量关注。</p>
			<p>粗略地说，在该技术中，网络中的中间数据进行学习，以便接近“内容图像”的中间数据通过这样做，可以转换输入图像，使得它在形状上与内容图像相似。为了从“风格图像”中吸收风格，引入了风格矩阵的概念。通过训练使得风格矩阵的间隙较小，输入图像可以接近梵高的风格。</p>
			<h3 id="_idParaDest-217"><a id="_idTextAnchor227"/>生成图像</h3>
			<p>前面的图像样式转换示例需要两个图像来生成一个新图像。另一方面，一些研究试图在不需要任何图像的情况下生成新图像(该技术通过预先使用许多图像来训练，但是不需要图像来“绘制”新图像。)例如，你可以使用深度学习从零开始生成“卧室”的图像</p>
			<p>它们可能看起来是真实的照片，但它们是由DCGAN新生成的。由DCGAN生成的图像是没有人见过的图像(那些不存在于训练数据中的图像)，并且是从零开始新创建的。</p>
			<p>当DCGAN生成看起来像真的图像时，它会创建一个图像生成过程的模型。该模型通过使用许多图像(例如卧室的图像)来学习。训练完成后，您可以使用该模型生成新图像。</p>
			<p>DCGANs使用深度学习。DCGAN技术的要点是它使用两个神经网络:一个生成器和一个鉴别器。生成器生成看似真实的图像，而鉴别器确定它是否真实，即它是由生成器生成的还是真的被拍摄的。这样，通过使两个网络相互竞争来训练它们。</p>
			<p>生成器学习一种更精细的技术来创建假图像，而鉴别器像鉴定师一样成长，能够以更高的精度检测假货。有趣的是，在一种叫做<strong class="bold">生成对抗网络</strong> ( <strong class="bold">甘</strong>)的技术中，两者都在竞争中成长。最后，通过竞争成长起来的生成器可以绘制出看起来真实的图像(或者可能成长得更快)。</p>
			<h4>注意</h4>
			<p class="callout">到目前为止我们看到的机器学习问题被称为<strong class="bold">监督学习</strong>问题。他们使用包含成对图像数据和标签的数据集，例如在手写数字识别中。同时，这里的问题没有提供标签数据。仅提供图像(一组图像)。这叫做<strong class="bold">无监督学习</strong>。无监督学习已经研究了相对长的时间(<strong class="bold">深度信念网络</strong>和<strong class="bold">深度玻尔兹曼机器</strong>很有名)，但似乎这些天来，对它的研究不是很积极。由于使用深度学习的技术，如DCGANs，正在吸引越来越多的关注，预计无监督学习将在未来得到进一步发展。</p>
			<h3 id="_idParaDest-218"><a id="_idTextAnchor228"/>自动驾驶</h3>
			<p>“自动驾驶”技术，即计算机代替人驾驶汽车，可能很快就会实现。IT公司、大学和研究机构以及汽车制造商都在竞相实现自动驾驶。这只有在各种技术(如确定交通路线的路径规划技术和包括相机和激光在内的传感技术)相结合时才会发生。据说用来正确识别周围环境的技术是最重要的。很难识别每天每时每刻都在变化的环境，以及自由移动的汽车和人。</p>
			<p>如果系统能够鲁棒、可靠地正确识别行驶区域，甚至在各种环境中，自动驾驶可能会在不久的将来实现——这是一项深度学习应该证明是非常宝贵的任务。</p>
			<p>比如一个基于CNN的网络叫做SegNet(<em class="italics">Vijay Badrinarayanan，Kendall，Roberto Cipolla (2015): SegNet:一个用于图像分割的深度卷积编码器-解码器架构。arXiv预印本arXiv:1511.00561 (2015) </em>)可以准确识别道路环境，如图<em class="italics">图8.17 </em>:</p>
			<div><div><img src="img/fig08_17.jpg" alt="Figure 8.17: Example of segmenting an image by using deep learning – the road, cars, buildings, and sidewalks are recognized accurately&#13;&#10;"/>
				</div>
			</div>
			<h6>图8.17:使用深度学习分割图像的示例-道路、汽车、建筑物和人行道被准确识别</h6>
			<h4>注意</h4>
			<p class="callout"><em class="italics">图8.17 </em>引自参考文献，<em class="italics"> SegNet演示页</em>(【http://mi.eng.cam.ac.uk/projects/segnet/】T21)。</p>
			<p>对输入图像进行分割(像素级评估)，如图<em class="italics">图8.17 </em>所示。结果表明，道路、建筑物、人行道、树木、汽车和摩托车得到了一定程度的准确区分。如果深度学习从现在开始提高这些识别技术的准确性和速度，自动驾驶可能会在不太遥远的将来投入实际使用。</p>
			<h3 id="_idParaDest-219"><a id="_idTextAnchor229"/>深度Q网络(强化学习)</h3>
			<p>有一个研究领域叫做<strong class="bold">强化学习</strong>在这个领域中，计算机通过试错来独立学习，就像人类学习骑自行车一样，例如。这与“监督学习”不同，在“监督学习”中，“监督者”面对面授课。</p>
			<p>强化学习的基本框架是一个主体根据环境的情况选择动作，它的动作改变环境。采取行动后，环境会给代理人一些奖励。强化学习的目的是确定代理人的行动策略，使其能够获得更好的回报，如下所示:</p>
			<div><div><img src="img/Figure_8.18.jpg" alt="Figure 8.18: Basic framework of reinforcement learning – the agent learns independently &#13;&#10;to obtain a better reward&#13;&#10;"/>
				</div>
			</div>
			<h6>图8.18:强化学习的基本框架——代理人独立学习以获得更好的回报</h6>
			<p><em class="italics">图8.18 </em>中的图表显示了强化学习的基本框架。请注意，奖励不是标记为数据，因为它是在监督学习。例如，在电子游戏“超级马里奥兄弟”中，将马里奥向右移动所获得的确切奖励数量不一定清楚。在这种情况下，“预期”奖励必须由明确的指标决定，如游戏分数(获得硬币、击败敌人等)和游戏结束逻辑。在监督学习中，每个动作都可以被“监督者”正确地评估</p>
			<p>一种<strong class="bold">深度Q-网络</strong> ( <strong class="bold"> DQN </strong>)是一种强化学习技术(<em class="italics"> Volodymyr Mnih et al (2015):通过深度强化学习实现人类水平的控制。使用深度学习的Nature 518，7540 (2015)，529–533</em>)。它基于被称为Q-learning的强化学习算法。Q-learning确定了一个称为最优动作值函数的函数来确定最优动作。DQN使用深度学习(CNN)来逼近该函数。</p>
			<p>一些研究表明，dqn可以自动学习视频游戏，以获得比人类更成功的游戏。如图<em class="italics">图8.19 </em>所示，CNN在DQN中使用时，接收四帧连续的游戏图像作为输入，输出游戏控制器运动的“值”(操纵杆的移动和按钮操作)。</p>
			<p>传统上，当网络学习视频游戏时，通常预先提取并提供游戏的状态(例如角色的位置)。同时，DQN只接收视频游戏的图像作为输入数据，如图<em class="italics">图8.19 </em>所示。这是DQN值得注意的地方，大大提高了它的适用性。这是因为您不需要更改每个游戏的设置，您只需要向DQN提供游戏图像。事实上，DQNs已经学习了很多游戏，比如同样配置的“吃豆人”和“雅达利2600”并取得了比人类更好的成绩:</p>
			<div><div><img src="img/fig08_19.jpg" alt="Figure 8.19: Using a Deep Q-Network to learn the operations of a video game. Here, the &#13;&#10;network receives the images of a video game as an input and learns the operation of the game &#13;&#10;controller (joystick) through trial and error&#13;&#10;"/>
				</div>
			</div>
			<h6>图8.19:使用深度Q网络学习视频游戏的操作。这里，网络接收视频游戏的图像作为输入，并通过反复试验来学习游戏控制器(操纵杆)的操作</h6>
			<h4>注意</h4>
			<p class="callout"><em class="italics">图8.17 </em>引自参考文献，<em class="italics"> Volodymyr Mnih et al. (2015):通过深度强化学习的人类级控制。性质518、7540 (2015)、529–533</em>。</p>
			<h4>注意</h4>
			<p class="callout">一个AI叫AlphaGo的新闻(<em class="italics"> David Silver等人(2016):用深度神经网络和树搜索掌握围棋的博弈。自然529，7587 (2016)，484–489</em>)击败围棋冠军备受关注。AlphaGo中也使用了深度学习和强化学习。它从专业人士创造的3000万场比赛记录中学习，并与自己多次对战，积累了足够的知识。AlphaGo和DQNs都被谷歌的DeepMind研究过。我们今后必须密切注意他们的活动。</p>
			<h2 id="_idParaDest-220"><a id="_idTextAnchor230"/>摘要</h2>
			<p>在本章中，我们实现了一个深度细胞神经网络，并获得了超过99%的手写数字识别率。我们还讨论了使网络更深的动机以及当前向更深网络发展的趋势。我们还研究了深度学习的趋势和应用，这项研究正在加速它，这将推动这项技术走向未来。</p>
			<p>在深度学习领域，还有许多未知的东西，新的研究一直在发表。世界各地的研究人员和工程师继续积极研究，并将实现我们甚至无法想象的技术。</p>
			<p>本章包括以下几点:</p>
			<ul>
				<li>使网络更深将提高许多深度学习问题的性能。</li>
				<li>在图像识别比赛中，使用深度学习的技术获得了很高的排名，并且当前的网络比它们的前辈更深</li>
				<li>著名的网络包括VGG、谷歌网和瑞思网。</li>
				<li>GPU，分布式训练，比特精度的降低，可以加速深度学习。</li>
				<li>深度学习(神经网络)可以用于对象检测和分割，也可以用于对象识别。</li>
				<li>使用深度学习的应用包括图像标题的生成、图像的生成和强化学习。如今，深度学习用于自动驾驶也在意料之中。</li>
			</ul>
			<p>感谢您阅读这本书。我们希望你对深度学习有了更好的理解，并发现这是一次有趣的旅程。</p>
		</div>
	

</body></html>