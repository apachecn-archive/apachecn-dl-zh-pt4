<html><head/><body><html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Deploying Models to Production</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:f69ed529-6576-4ff7-a760-3ddf81ca9def" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">将模型部署到生产中</h1>

                

            

            

                

<p>在本书的前几章中，我们已经学习了开发、测试和使用各种深度学习模型的技能。我们还没有在更广泛的软件工程背景下谈论深度学习的作用。在这最后一章中，我们将利用这段时间来讨论持续交付，以及机器学习在这一背景下的作用。然后，我们将看看如何以持续交付的心态将模型部署到生产中。最后，我们将查看Azure机器学习服务，以正确管理您开发的模型。</p>

<p>本章将涵盖以下主题:</p>

<ul>

<li>在DevOps环境中使用机器学习</li>

<li>存储模型</li>

<li>使用Azure机器学习服务管理模型</li>

</ul>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Technical requirements</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:f69ed529-6576-4ff7-a760-3ddf81ca9def" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">技术要求</h1>

                

            

            

                

<p>我们假设您的计算机上安装了Anaconda的最新版本，并按照<a href="9a2c8c46-f9a0-4e05-86ef-31300a28a7ba.xhtml">第1章</a>、<em>CNTK入门</em>中的步骤在您的计算机上安装CNTK。本章的示例代码可以在我们的GitHub资源库中找到:<a href="https://github.com/PacktPublishing/Deep-Learning-with-Microsoft-Cognitive-Toolkit-Quick-Start-Guide/tree/master/ch7">https://GitHub . com/packt publishing/Deep-Learning-with-Microsoft-Cognitive-Toolkit-Quick-Start-Guide/tree/master/ch7</a>。</p>

<p>在这一章中，我们将学习一些储存在Jupyter笔记本中的例子。要访问示例代码，请在下载代码的目录中的Anaconda提示符下运行以下命令:</p>

<pre><strong>cd ch7</strong><br/><strong>jupyter notebook</strong></pre>

<p>本章还包含一个C#代码示例，演示如何加载开源ONNX格式的模型。如果你想运行C#代码，你需要。你机器上安装的NET Core 2.2。您可以下载最新版本的。网芯来自:<a href="https://dotnet.microsoft.com/download">https://dotnet.microsoft.com/download</a>。</p>

<p class="mce-root">请观看以下视频，了解实际运行的代码:</p>

<p><a href="http://bit.ly/2U8YkZf">http://bit.ly/2U8YkZf</a></p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Using machine learning in a DevOps environment</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:f69ed529-6576-4ff7-a760-3ddf81ca9def" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">在DevOps环境中使用机器学习</h1>

                

            

            

                

<p>大多数现代软件开发都是以敏捷的方式进行的，在这种环境下，开发人员和IT专家从事同一个项目。我们正在构建的软件通常通过持续集成和持续部署管道部署到生产中。我们如何在这个现代环境中整合机器学习？这是否意味着当我们开始构建人工智能解决方案时，我们必须做出很多改变？这些是当你将人工智能和机器学习引入工作流时，你可能会遇到的一些常见问题。</p>

<p>幸运的是，您不必改变整个构建环境或部署工具堆栈来将机器学习集成到软件中。我们将讨论的大多数内容都适合您现有的环境。</p>

<p>让我们来看看您在常规敏捷软件项目中可能遇到的典型连续交付场景:</p>

<div><img class="aligncenter size-full wp-image-632 image-border" src="img/31eff2da-92c2-470c-8e45-8b98931548be.png" style=""/></div>

<p>如果您以前在DevOps环境中工作过，这个概述看起来会很熟悉。它从连接到持续集成管道的源代码控制开始。持续集成管道产生可以部署到生产中的工件。这些工件通常存储在某个地方，用于备份和回滚。这个工件存储库连接到一个发布管道，该管道将软件部署到一个测试、验收以及最终的生产环境中。</p>

<p>您不需要对这个标准设置进行太多更改就可以将机器学习集成到其中。然而，当你开始使用机器学习时，有几件关键的事情是很重要的。让我们关注四个阶段，并探索如何扩展标准连续交付设置:</p>

<ul>

<li>如何跟踪你用于机器学习的数据？</li>

<li>持续集成管道中的培训模型。</li>

<li>将模型部署到生产中。</li>

<li>收集生产反馈。</li>

</ul>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Keeping track of your data</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:f69ed529-6576-4ff7-a760-3ddf81ca9def" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">跟踪您的数据</h1>

                

            

            

                

<p>让我们从机器学习开始:你用来训练模型的数据。机器学习很难得到好的数据。几乎80%的工作都将放在数据管理和数据处理上。如果你每次想训练一个模特都要重做所有的工作，那真的很可悲。</p>

<p>这就是拥有某种形式的数据管理非常重要的原因。这可以是一个中央服务器，您可以在其中存储适合用于训练模型的数据集。如果你有超过几千兆字节的数据，它也可以是一个数据仓库。一些公司选择使用Hadoop或Azure Data Lake等工具来管理他们的数据。无论您使用什么，最重要的是保持数据集的整洁，并保持一种可用于训练的格式。</p>

<p>要为您的解决方案创建数据管道，您可以使用传统的<strong>提取</strong> <strong>转换</strong> <strong>加载</strong> ( <strong> ETL </strong>)工具，如SQL server integration services，或者您可以在Python中构建自定义脚本，并在Jenkins、Azure DevOps或Team Foundation Server中将它们作为专用持续集成管道的一部分来执行。</p>

<p>数据管道将是您从各种业务来源收集数据并对其进行处理的工具，这样您就可以获得一个具有足够质量的数据集来存储为您的模型的主数据集。这里需要注意的是，尽管您可以跨不同的模型重用数据集，但最好不要一开始就考虑这个目标。当您试图在太多的使用场景中使用数据集时，您将很快发现您的主数据集将变得不干净和难以管理。</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Training models in a continuous integration pipeline</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:f69ed529-6576-4ff7-a760-3ddf81ca9def" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">持续集成管道中的培训模型</h1>

                

            

            

                

<p>一旦你有了一个基本的数据管道，是时候考虑在你的持续集成环境中集成人工智能模型的训练了。到目前为止，我们只使用Python笔记本来创建我们的模型。遗憾的是，Python笔记本并不能很好地部署到生产中。您不能在构建期间自动运行它们。</p>

<p>在连续交付环境中，您仍然可以使用Python笔记本来执行初始实验，以便发现数据中的模式并构建模型的初始版本。一旦你有了一个候选模型，你将不得不把你的代码从笔记本转移到一个合适的Python程序中。</p>

<p>您可以将Python训练代码作为持续集成管道的一部分来运行。例如，如果您正在使用Azure DevOps、Team Foundation Server或Jenkins，您已经拥有了将培训代码作为持续集成管道运行的所有工具。</p>

<p>我们建议将培训代码作为独立于软件其余部分的管道运行。训练一个深度学习模型通常需要很长时间，你不想把你的构建基础设施锁定在那上面。通常，你会看到人们使用专用虚拟机，甚至专用硬件来为他们的机器学习模型建立训练管道，因为训练一个模型需要大量的计算能力。</p>

<p>持续集成管道将基于您使用数据管道生成的数据集生成一个模型。就像代码一样，您也应该对您的模型和您用来训练它们的设置进行版本化。</p>

<p>跟踪您的模型和用于训练它们的设置非常重要，因为这允许您在生产中试验同一模型的不同版本并收集反馈。保留训练模型的备份也有助于在灾难(例如生产服务器崩溃)后快速恢复生产。</p>

<p>由于模型是二进制文件，并且可能变得非常大，所以最好将模型视为二进制工件，就像。NET，或者Java中的Maven工件。</p>

<p>像Nexus或Artifactory这样的工具非常适合存储模型。在Nexus或Artifactory中发布您的模型只需几行代码，将为您节省数百小时的模型再培训工作。</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Deploying models to production</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:f69ed529-6576-4ff7-a760-3ddf81ca9def" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">将模型部署到生产中</h1>

                

            

            

                

<p>一旦您有了一个模型，您需要能够将它部署到生产中。如果您已经将模型存储在存储库中，如Artifactory或Nexus，这就变得更容易了。您可以像创建持续集成管道一样创建专用的发布管道。在Azure DevOps和Team Foundation Server中，有一个专门的功能。在Jenkins中，您可以使用单独的管道将模型部署到服务器。</p>

<p>在发布管道中，您可以从工件存储库中下载您的模型，并将其部署到生产中。机器学习模型主要有两种部署方式。您可以将它作为一个额外的文件部署到您的应用程序中，也可以将其作为一个专用的服务组件进行部署。</p>

<p>如果您将您的模型作为应用程序的一部分进行部署，那么您通常只将模型存储在您的工件存储库中。模型现在变成了一个额外的工件，需要下载到部署您的解决方案的现有发布管道中。</p>

<p>如果您正在为您的模型部署一个专用的服务组件，您通常会将模型、使用模型进行预测的脚本以及模型所需的其他文件存储在工件存储库中，并将其部署到生产中。</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Gathering feedback on your models</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:f69ed529-6576-4ff7-a760-3ddf81ca9def" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">收集对您的模型的反馈</h1>

                

            

            

                

<p>在生产中使用深度学习或机器学习模型时，还有最后一点需要记住。你已经用某个数据集训练了模型。您希望这个数据集能够很好地代表您的生产环境中实际发生的情况。但事实并非如此，因为当你建立模型时，你周围的世界也在变化。</p>

<p>这就是为什么向用户征求反馈并相应地更新你的模型是很重要的。虽然不是持续部署环境的正式组成部分，但如果您想成功使用机器学习解决方案，正确设置它仍然是一个重要的方面。</p>

<p>建立一个反馈回路并不需要非常复杂。例如，当您对交易进行分类以进行欺诈检测时，您可以通过要求员工验证模型的结果来建立反馈循环。然后，您可以将员工的验证结果与已分类的输入一起存储。通过这样做，您可以确保您的模型不会错误地指责客户欺诈，并且它可以帮助您收集新的观察结果来改进您的模型。稍后，当您想要改进模型时，可以使用新收集的观察来扩展您的训练集。</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Storing your models</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:f69ed529-6576-4ff7-a760-3ddf81ca9def" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">存储您的模型</h1>

                

            

            

                

<p>为了能够将您的模型部署到生产中，您需要能够在磁盘上存储一个经过训练的模型。CNTK提供了两种在磁盘上存储模型的方法。您可以存储检查点以便以后继续训练，也可以存储模型的便携版本。这些存储方法中的每一种都有自己的用途。</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Storing model checkpoints to continue training at a later point</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:f69ed529-6576-4ff7-a760-3ddf81ca9def" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">存储模型检查点，以便在以后继续训练</h1>

                

            

            

                

<p>有些模型需要很长时间来训练，有时一次长达数周。当你的机器在训练中崩溃，或者停电时，你不想失去你所有的进步。</p>

<p>这就是检查点有用的地方。您可以在训练期间使用<kbd>CheckpointConfig</kbd>对象创建一个检查点。通过修改回调列表，可以将这个附加回调添加到训练代码中，如下所示:</p>

<pre>checkpoint_config = CheckpointConfig('solar.dnn', frequency=100, restore=True, preserve_all=False)<br/><br/>history = loss.train(<br/>    train_datasource, <br/>    epoch_size=EPOCH_SIZE,<br/>    parameter_learners=[learner], <br/>    model_inputs_to_streams=input_map,<br/>    callbacks=[progress_writer, test_config, checkpoint_config],<br/>    minibatch_size=BATCH_SIZE,<br/>    max_epochs=EPOCHS)</pre>

<p>遵循给定的步骤:</p>

<ol>

<li>首先，创建一个新的<kbd>CheckpointConfig</kbd>并为检查点模型文件提供一个文件名，在一个新的检查点之前的迷你批处理的数量应该被创建为<kbd>frequency</kbd>，并将<kbd>preserve_all</kbd>设置为<kbd>False</kbd>。</li>

<li>接下来，在<kbd>loss</kbd>上使用train方法，并在<kbd>callbacks</kbd>关键字参数中提供<kbd>checkpoint_config</kbd>来使用检查点。</li>

</ol>

<p>当您在训练期间使用检查点时，您将开始在磁盘上看到名为<kbd>solar.dnn</kbd>和<kbd>solar.dnn.ckp</kbd>的附加文件。<kbd>solar.dnn</kbd>文件包含以二进制格式存储的训练模型。<kbd>solar.dnn.ckp</kbd>文件包含训练期间使用的微型批次源的检查点信息。</p>

<p>当您将<kbd>CheckpointConfig</kbd>对象的恢复参数设置为<kbd>True</kbd>时，会自动恢复最近的检查点。这使得在训练代码中集成检查点变得容易。</p>

<p>拥有一个检查点模型不仅在训练中遇到计算机问题时有用。如果您想在从生产中收集了额外的数据后继续培训，检查点也很有用。您可以简单地恢复最新的检查点，并从那里开始向模型中输入新的样本。</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Storing portable models for use in other applications</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:f69ed529-6576-4ff7-a760-3ddf81ca9def" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">存储便携式模型以供其他应用程序使用</h1>

                

            

            

                

<p class="mce-root">尽管您可以在生产中使用检查点模型，但这样做并不明智。检查点模型以只有CNTK理解的格式存储。现在，可以使用二进制格式，因为CNTK已经存在，而且模型格式将在相当长的时间内保持兼容。但是，和所有的软件一样，CNTK不会永远存在。</p>

<p>这正是ONNX被发明的原因。ONNX是开放式神经网络交换格式。当您使用ONNX时，您以protobuf兼容的格式存储您的模型，许多其他框架都可以理解这种格式。甚至还有一个针对Java和C#的本地ONNX运行时，它允许您从。NET或Java应用程序。</p>

<p>ONNX得到了许多大公司的支持，如脸书、英特尔、英伟达、微软、AMD、IBM和惠普。其中一些公司为ONNX提供转换器，而其他公司甚至支持直接在硬件上运行ONNX模型，而无需使用额外的软件。NVIDIA现在有很多芯片可以直接读取ONNX文件并执行这些模型。</p>

<p>作为一个例子，我们将首先探索如何以ONNX格式存储一个模型，并使用C#再次从磁盘加载它来进行预测。首先，我们将看看如何以ONNX格式保存模型，然后我们将探索如何加载ONNX模型。</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Storing a model in ONNX format</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:f69ed529-6576-4ff7-a760-3ddf81ca9def" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">以ONNX格式存储模型</h1>

                

            

            

                

<p>要以ONNX格式存储模型，您可以在<kbd>model</kbd>函数上使用<kbd>save</kbd>方法。当您没有提供任何额外的参数时，它将以用于检查点的相同格式存储模型。但是，您可以提供一个附加参数来指定模型格式，如下所示:</p>

<pre>from cntk import ModelFormat<br/><br/>model.save('solar.onnx', format=ModelFormat.ONNX)</pre>

<p>遵循给定的步骤:</p>

<ol>

<li>首先，从<kbd>cntk</kbd>包中导入<kbd>ModelFormat</kbd>枚举。</li>

<li>接下来，用输出文件名对训练好的模型调用<kbd>save</kbd>方法，并将<kbd>ModelFormat.ONNX</kbd>指定为<kbd>format</kbd>关键字参数。</li>

</ol>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Using ONNX models in C#</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:f69ed529-6576-4ff7-a760-3ddf81ca9def" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">在C#中使用ONNX模型</h1>

                

            

            

                

<p>一旦模型存储在磁盘上，我们就可以使用C#来加载和使用它。CNTK版包含一个非常完整的C # API，您可以用它来训练和评估模型。</p>

<p>要在C#中使用CNTK模型，您需要使用一个名为<kbd>CNTK.GPU</kbd>或<kbd>CNTK.CPUOnly</kbd>的库，该库可以从. NET的包管理器NuGet中检索到。CNTK的纯CPU版本包括一个版本的CNTK二进制文件，这些文件已经编译为在CPU上运行模型，而GPU版本可以使用GPU和CPU。</p>

<p>在C#中加载CNTK模型是通过使用以下代码片段来完成的:</p>

<div><pre>var deviceDescriptor = DeviceDescriptor.CPUDevice;<br/>var function = Function.Load("model.onnx", deviceDescriptor, ModelFormat.ONNX);</pre>

<p>遵循给定的步骤:</p>

<ol>

<li>首先，创建一个设备描述符，以便针对CPU执行模型。</li>

<li>接下来，使用<kbd>Function.Load</kbd>方法加载先前存储的模型。提供<kbd>deviceDescriptor</kbd>并使用<kbd>ModelFormat.ONNX</kbd>将文件加载为ONNX模型。</li>

</ol>

</div>

<p>现在我们已经加载了模型，让我们用它来做一个预测。为此，我们需要编写另一段代码:</p>

<pre>public IList&lt;float&gt; Predict(float petalWidth, float petalLength, float sepalWidth, float sepalLength)<br/>{<br/>    var features = _modelFunction.Inputs[0];<br/>    var output = _modelFunction.Outputs[0];<br/><br/>    var inputMapping = new Dictionary&lt;Variable, Value&gt;();<br/>    var outputMapping = new Dictionary&lt;Variable, Value&gt;();<br/><br/>    var batch = Value.CreateBatch(<br/>        features.Shape,<br/>        new float[] { sepalLength, sepalWidth, petalLength, petalWidth },<br/>        _deviceDescriptor);<br/><br/>    inputMapping.Add(features, batch);<br/>    outputMapping.Add(output, null);<br/><br/>    _modelFunction.Evaluate(inputMapping, outputMapping, _deviceDescriptor);<br/><br/>    var outputValues = outputMapping[output].GetDenseData&lt;float&gt;(output);<br/>    return outputValues[0];<br/>}</pre>

<p>遵循给定的步骤:</p>

<ol>

<li>创建一个新方法<kbd>Predict</kbd>，它接受模型的输入特性。</li>

<li>在<kbd>Predict</kbd>方法中，将模型的输入和输出变量存储在两个独立的变量中，以便于访问。</li>

<li>接下来，创建一个字典，将数据映射到模型的输入和输出变量。</li>

<li>然后，创建一个新的批次，其中包含一个带有模型输入要素的样本。</li>

<li>向输入映射添加一个新条目，以将批处理映射到输入变量。</li>

<li>接下来，为输出变量的输出映射添加一个新条目。</li>

<li>现在，使用输入、输出映射和设备描述符在加载的模型上调用<kbd>Evaluate</kbd>方法。</li>

<li>最后，从输出映射中提取输出变量并检索数据。</li>

</ol>

<p>本章的示例代码包括一个基本的C#项目。NET core，它演示了. NET Core项目中CNTK的使用。您可以在本章的代码示例目录的<kbd>csharp-client</kbd>文件夹中找到示例代码。</p>

<p>使用以ONNX格式存储的模型，可以使用Python来训练模型，使用C#或其他语言来运行生产模型。这尤其有用，因为像C#这样的语言的运行时性能比Python好得多。</p>

<p>在下一节中，我们将看看如何使用Azure机器学习服务来管理训练和存储模型的过程，这样我们就有了一种更加结构化的方法来处理模型。</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Using Azure Machine Learning service to manage models</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:f69ed529-6576-4ff7-a760-3ddf81ca9def" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">使用Azure机器学习服务管理模型</h1>

                

            

            

                

<p>虽然您可以完全手工构建一个持续的集成管道，但这仍然需要相当多的工作。你需要专用硬件来运行深度学习培训工作，这可能会增加成本。云中有很好的替代方案。谷歌提供TensorFlow服务。微软提供Azure机器学习服务作为管理模型的一种方式。这两个工具都很棒，我们强烈推荐。</p>

<p>让我们来看看Azure机器学习服务，以了解当您想要建立一个完整的机器学习管道时，它可以为您做些什么:</p>

<div><img class="aligncenter size-full wp-image-633 image-border" src="img/931612d7-2dae-4252-b2ba-0ea9d6bf7d19.png" style=""/></div>

<p>Azure机器学习服务是一种云服务，为您的机器学习项目的每个阶段提供完整的解决方案。它有实验的概念，允许你管理实验。它具有一个模型注册表，允许您存储经过训练的模型和这些模型的Docker图像。您可以使用Azure机器学习服务工具在几分钟内将这些模型部署到生产中。</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Deploying Azure Machine Learning service</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:f69ed529-6576-4ff7-a760-3ddf81ca9def" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">部署Azure机器学习服务</h1>

                

            

            

                

<p>为了使用此服务，您需要在Azure上拥有一个有效帐户。如果你还没有一个帐户，你可以去:<a href="https://azure.microsoft.com/en-gb/free/">https://azure.microsoft.com/en-gb/free/</a>使用一个试用帐户。这将给你一个12个月的免费帐户，价值150美元的信用点数，以探索不同的Azure服务。</p>

<p class="mce-root"/>

<p>有很多方法可以部署Azure机器学习服务。您可以通过门户创建一个新的实例，但是也可以使用云shell来创建服务的实例。让我们看看如何通过门户创建一个新的Azure机器学习服务实例。</p>

<p>用你最喜欢的浏览器，导航到网址:【https://portal.azure.com/<a xmlns:epub="http://www.idpf.org/2007/ops" href="https://portal.azure.com/">。使用您的凭据登录，您将看到一个门户，向您显示所有可用的Azure资源，以及一个类似于以下屏幕截图的仪表板:</a></p>

<div><img class="aligncenter size-full wp-image-634 image-border" src="img/d6be4d9a-532b-43d3-873e-7f298c5fd304.png" style=""/></div>

<p>Azure资源和仪表板</p>

<p>从这个门户你可以创建新的Azure资源，比如Azure机器学习工作区。点击屏幕左上角的大+按钮开始。这将显示以下页面，允许您创建新资源:</p>

<div><img class="aligncenter size-full wp-image-635 image-border" src="img/d473560b-00c5-4ba2-88ef-2cb881565369.png" style=""/></div>

<p>创建新资源</p>

<p>您可以在此搜索栏中搜索不同类型的资源。搜索Azure Machine Learning，并从列表中选择Azure Machine Learning工作区资源类型。这将显示以下详细信息面板，允许您启动创建向导:</p>

<div><img class="aligncenter size-full wp-image-636 image-border" src="img/ea5b88f6-7e13-4d81-8726-59f65d9c68c3.png" style=""/></div>

<p>启动创建向导</p>

<p>该详细信息面板将解释资源的用途，并指向关于该资源的文档和其他重要信息，例如定价详细信息。要创建此资源类型的新实例，请单击“创建”按钮。这将启动向导来创建Azure机器学习工作区的新实例，如下所示:</p>

<div><img class="aligncenter size-full wp-image-637 image-border" src="img/c36e7609-d67b-4cd8-8700-c6d3da70200d.png" style=""/></div>

<p>创建Azure机器学习工作区的新实例</p>

<p>在创建向导中，您可以配置工作空间的名称、它所属的资源组以及它应该创建的数据中心。Azure资源是作为资源组的一部分创建的。这些资源组帮助您组织事物，并将相关的基础结构集中在一个地方。如果要删除一组资源，可以只删除资源组，而不是单独删除每个资源。如果您想在测试完机器学习工作区后删除所有内容，这将非常有用。</p>

<p>为机器学习工作区使用一个专用的资源组是一个好主意，因为它将包含不止一个资源。将这些资源与其他资源混合在一起，会使您在完成后或出于某种原因需要移动资源时更难清理。</p>

<p class="mce-root"/>

<p>单击屏幕底部的“创建”按钮后，机器学习工作区就创建好了。这需要几分钟时间。在后台，Azure资源管理器将根据创建向导中的选择创建许多资源。部署完成后，您将在门户中收到通知。</p>

<p>创建机器学习工作区时，您可以通过门户导航到工作区，方法是首先转到屏幕左侧导航栏中门户上的资源组。接下来，单击您刚刚创建的资源组，以获得机器学习工作区和相关资源的概述，如下面的屏幕截图所示:</p>

<div><img class="aligncenter size-full wp-image-639 image-border" src="img/d9c88065-5451-4366-9750-fb73570dbb4b.png" style=""/></div>

<p>获得机器学习工作空间和相关资源的概述</p>

<p>有工作空间本身，有一个仪表板，允许你探索实验和管理你的机器学习解决方案的某些方面。工作区还包括一个Docker注册表，用于将模型存储为Docker图像，以及使用模型进行预测所需的脚本。当您在Azure Portal上检查工作区时，您还会发现一个存储帐户，您可以使用它来存储您的实验生成的数据集和数据。</p>

<p>Azure机器学习服务环境中包含的一个好东西是Application Insights实例。您可以使用Application Insights来监控生产中的模型，并收集有价值的反馈以在以后改进模型。这是默认包含的，因此您不必为您的机器学习解决方案手动创建监控解决方案。</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Exploring the machine learning workspace</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:f69ed529-6576-4ff7-a760-3ddf81ca9def" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">探索机器学习工作空间</h1>

                

            

            

                

<p>Azure机器学习工作区包含许多元素。让我们来探索一下，当您开始使用它时，感受一下您能得到什么:</p>

<div><img class="aligncenter size-full wp-image-640 image-border" src="img/560a062f-5309-48c3-a1b7-0f4bdef7b38e.png" style=""/></div>

<p>机器学习工作空间</p>

<p>要进入机器学习工作区，请单击屏幕左侧导航栏中的资源组项目。选择包含机器学习工作区项目的资源组，然后单击机器学习工作区。它将具有您之前在创建向导中配置的名称。</p>

<p>在工作区，有一个专门的实验区。这个部分将提供对您在工作区中运行的实验的访问，以及关于作为实验的一部分而执行的运行的细节。</p>

<p>机器学习工作区的另一个有用元素是模型部分。当您训练了一个模型之后，您可以将它存储在模型注册表中，以便以后可以将其部署到生产环境中。模型会自动连接到生成它的实验运行，因此您可以随时追溯到使用了哪些代码来生成模型，以及使用了哪些设置来训练它。</p>

<p>模型部分下面是图像部分。这一部分向您展示了从您的模型中创建的Docker图像。您可以将Docker映像中的模型与评分脚本打包在一起，以使部署到生产环境中变得更加容易和更加可预测。</p>

<p>最后，部署部分包含所有基于映像的部署。如果您需要扩展您的模型部署，您可以使用Azure Machine Learning service使用单个容器实例、虚拟机甚至Kubernetes集群将模型部署到生产环境中。</p>

<p>Azure机器学习服务还提供了一种技术，允许您建立一个管道来准备数据，训练模型，并将其部署到生产中。如果您希望构建一个包含预处理步骤和训练步骤的单一流程，这个特性会很有用。在您需要执行许多步骤来获得一个训练好的模型的情况下，它尤其强大。现在，我们将限制自己运行基本的实验，并将结果模型部署到生产Docker容器实例中。</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Running your first experiment</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:f69ed529-6576-4ff7-a760-3ddf81ca9def" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">运行您的第一个实验</h1>

                

            

            

                

<p>现在您有了一个工作空间，让我们看看如何从Python笔记本中使用它。我们将修改一些深度学习代码，以便将训练好的模型保存到Azure机器学习服务工作区，作为实验的输出，并跟踪模型的指标。</p>

<p>首先，我们需要如下安装<kbd>azureml</kbd>包:</p>

<pre><strong>pip install --upgrade azureml-sdk[notebooks]</strong></pre>

<p><kbd>azureml</kbd>包包含运行实验所需的组件。为了让它工作，你需要在你的机器学习项目的根目录下创建一个名为<kbd>config.json</kbd>的文件。如果您正在使用本章的示例代码，您可以修改<kbd>azure-ml-service</kbd>文件夹中的<kbd>config.json</kbd>文件。它包含以下内容:</p>

<pre>{<br/>    "workspace_name": "&lt;workspace name&gt;",<br/>    "resource_group": "&lt;resource group&gt;",<br/>    "subscription_id": "&lt;your subscription id&gt;"<br/>}</pre>

<p>该文件包含Python代码将使用的工作空间、包含您正在使用的工作空间的资源组以及创建工作空间的订阅。工作区名称应该与您之前在向导中选择的创建工作区的名称相匹配。资源组应该与包含工作区的资源组相匹配。最后，您需要找到订阅ID。</p>

<p>导航到门户上机器学习工作区的资源组时，您将在资源组详细信息面板的顶部看到订阅ID，如以下屏幕截图所示:</p>

<div><img class="aligncenter size-full wp-image-641 image-border" src="img/e46ec086-f4c5-4d0b-b3f5-dffbfabc7860.png" style=""/></div>

<p>资源组详细信息面板顶部的订阅ID</p>

<p>当您将鼠标悬停在订阅ID的值上时，门户将显示一个按钮，将该值复制到您的剪贴板。将该值粘贴到配置文件的subscriptionId字段中并保存。现在，您可以使用以下小段代码从任何Python笔记本或Python程序连接到您的工作空间:</p>

<pre>from azureml.core import Workspace, Experiment<br/><br/>ws = Workspace.from_config()<br/>experiment = Experiment(name='classify-flowers', workspace=ws)</pre>

<p class="mce-root"/>

<p>遵循给定的步骤:</p>

<ol>

<li>首先，我们基于刚刚创建的配置文件创建一个新的工作区。这连接到Azure中的工作区。一旦连接上，您就可以用自己选择的名称创建一个新实验，并将其连接到工作区。</li>

<li>接下来，创建一个新的实验，并将其连接到工作区。</li>

</ol>

<p>Azure机器学习服务中的一个实验可以用来跟踪你用CNTK测试的架构。例如，您可以为卷积神经网络创建一个实验，并创建第二个实验来尝试用递归神经网络解决相同的问题。</p>

<p>让我们探索如何跟踪度量和实验的其他输出。我们将使用前面章节中的iris花卉分类模型，并扩展训练逻辑来跟踪指标，如下所示:</p>

<pre>from cntk import default_options, input_variable<br/>from cntk.layers import Dense, Sequential<br/>from cntk.ops import log_softmax, sigmoid<br/><br/>model = Sequential([<br/>    Dense(4, activation=sigmoid),<br/>    Dense(3, activation=log_softmax)<br/>])<br/><br/>features = input_variable(4)<br/>z = model(features)</pre>

<p>遵循给定的步骤:</p>

<ol>

<li>首先，导入<kbd>default_options</kbd>和<kbd>input_variable</kbd>函数。</li>

<li>接下来，从<kbd>cntk.layers</kbd>模块导入模型的图层类型。</li>

<li>之后，从<kbd>cntk.ops</kbd>模块导入<kbd>log_softmax</kbd>和<kbd>sigmoid</kbd>激活功能。</li>

<li>创建一个新的<kbd>Sequential</kbd>图层组。</li>

<li>在设置了4个神经元和<kbd>sigmoid</kbd>激活函数的<kbd>Sequential</kbd>层上添加一个新的<kbd>Dense</kbd>层。</li>

<li>添加另一个具有3个输出和一个<kbd>log_softmax</kbd>激活功能的<kbd>Dense</kbd>层。</li>

<li>创建一个大小为4的新<kbd>input_variable</kbd>。</li>

<li>用<kbd>features</kbd>变量调用模型来完成模型。</li>

</ol>

<p class="mce-root"/>

<p class="mce-root"/>

<p>为了训练模型，我们将使用手动迷你批次循环。首先，我们必须加载并预处理iris数据集，以便它匹配我们的模型所期望的格式，如下面的代码片段所示:</p>

<pre>import pandas as pd<br/>import numpy as np<br/><br/>df_source = pd.read_csv('iris.csv', <br/>    names=['sepal_length', 'sepal_width','petal_length','petal_width', 'species'], <br/>    index_col=False)<br/><br/>X = df_source.iloc[:, :4].values<br/>y = df_source['species'].values</pre>

<p>遵循给定的步骤:</p>

<ol>

<li>导入<kbd>pandas</kbd>和<kbd>numpy</kbd>模块，加载包含训练样本的CSV文件。</li>

<li>使用read_csv函数加载包含训练数据的输入文件。</li>

<li>接下来，提取前4列作为输入特征</li>

<li>最后，提取物种列作为标签</li>

</ol>

<p>标签存储为字符串，因此我们必须将它们转换为一组热点向量，以便与模型匹配，如下所示:</p>

<pre>label_mapping = {<br/>    'Iris-setosa': 0,<br/>    'Iris-versicolor': 1,<br/>    'Iris-virginica': 2<br/>}<br/><br/>def one_hot(index, length):<br/>    result = np.zeros(length)<br/>    result[index] = 1.<br/>    <br/>y = [one_hot(label_mapping[v], 3) for v in y]</pre>

<p>遵循给定的步骤:</p>

<ol>

<li>创建从标签到其数字表示的映射。</li>

<li>接下来，定义一个新的效用函数<kbd>one_hot</kbd>，将一个数值编码成一个独热向量。</li>

<li>最后，使用python list comprehension来迭代labels集合中的值，并将它们转换成一次性编码的向量。</li>

</ol>

<p class="mce-root"/>

<p>我们需要再执行一个步骤来准备用于训练的数据集。为了能够验证模型确实得到了正确的优化，我们想要创建一个拒绝集，我们将针对它运行一个测试:</p>

<pre>from sklearn.model_selection import train_test_split<br/><br/>X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, stratify=y)</pre>

<p>使用<kbd>train_test_split</kbd>方法，创建一个包含20%训练样本的小型保留集。使用<kbd>stratify</kbd>关键字并提供标签来平衡分割。</p>

<p>一旦我们准备好数据，我们就可以专注于训练模型。首先，我们需要设置一个<kbd>loss</kbd>函数、<kbd>learner</kbd>和<kbd>trainer</kbd>，如下所示:</p>

<pre>from cntk.losses import cross_entropy_with_softmax<br/>from cntk.metrics import classification_error<br/>from cntk.learners import sgd<br/>from cntk.train.trainer import Trainer<br/><br/>label = input_variable(3)<br/><br/>loss = cross_entropy_with_softmax(z, label)<br/>error_rate = classification_error(z, label)<br/><br/>learner = sgd(z.parameters, 0.001)<br/>trainer = Trainer(z, (loss, error_rate), [learner])</pre>

<ol>

<li>从<kbd>cntk.losses</kbd>模块导入cross_entropy_with_softmax函数。</li>

<li>接下来，从<kbd>cnkt.metrics</kbd>模块导入classificatin _ error函数。</li>

<li>然后，从<kbd>cntk.learners</kbd>模块导入<kbd>sgd</kbd>学习器。</li>

<li>创建一个形状为3的新<kbd>input_variable</kbd>来存储标签</li>

<li>接下来，创建cross_entropy_with_softmax loss的新实例，并为其提供模型变量<kbd>z</kbd>和<kbd>label</kbd>变量。</li>

<li>然后，使用classification_error函数创建一个新的度量，并为其提供网络和<kbd>label</kbd>变量。</li>

<li>现在，用网络的参数初始化<kbd>sgd</kbd>学习器，将其学习率设置为0.001。</li>

<li>最后，用网络初始化<kbd>Trainer</kbd>、<kbd>loss</kbd>、<kbd>metric</kbd>和<kbd>learner</kbd>。</li>

</ol>

<p class="mce-root"/>

<p class="mce-root"/>

<p>通常，我们可以在<kbd>loss</kbd>函数上使用<kbd>train</kbd>方法来优化我们模型中的参数。然而，这一次，我们希望控制培训过程，这样我们就可以在Azure Machine Learning workspace中注入逻辑来跟踪指标，如以下代码片段所示:</p>

<pre>import os<br/>from cntk import ModelFormat<br/><br/>with experiment.start_logging() as run:<br/>    for _ in range(10):<br/>        trainer.train_minibatch({ features: X_train, label: y_train })<br/><br/>        run.log('average_loss', trainer.previous_minibatch_loss_average)<br/>        run.log('average_metric', trainer.previous_minibatch_evaluation_average)<br/><br/>    test_metric = trainer.test_minibatch( {features: X_test, label: y_test })</pre>

<p>遵循给定的步骤:</p>

<ol>

<li>要开始新的运行，调用实验中的<kbd>start_logging</kbd>方法。这将创建一个新的<kbd>run</kbd>。在运行范围内，我们可以执行训练逻辑。</li>

<li>创建一个新的for循环来训练10个时期。</li>

<li>在for循环中，调用<kbd>trainer</kbd>上的<kbd>train_minibatch</kbd>方法来训练模型。为它提供输入变量和训练数据之间的映射。</li>

<li>之后，使用教练对象中的<kbd>previous_minibatch_loss_average</kbd>值记录跑步的<kbd>average_loss</kbd>指标。</li>

<li>除了平均损失，使用trainer对象上的<kbd>previous_minibatch_evaluation_average</kbd>属性记录跑步中的平均指标。</li>

</ol>

<p>一旦我们训练了模型，我们就可以使用<kbd>test_minibatch</kbd>方法对测试集执行测试。这个方法返回我们之前创建的<kbd>metric</kbd>函数的输出。我们也会将此记录到机器学习工作区。</p>

<p>运行允许我们跟踪与模型的单个训练会话相关的数据。我们可以在<kbd>run</kbd>对象上使用<kbd>log</kbd>方法记录度量。该方法接受指标的名称和指标的值。您可以使用这个方法来记录诸如<kbd>loss</kbd>函数的输出之类的东西，以监控您的模型是如何收敛到一组最佳参数的。</p>

<p>还可以记录其他内容，例如用于训练模型的历元数、程序中使用的随机种子以及其他有用的设置，您可能需要这些设置以便在以后重现实验。</p>

<p>当您导航到实验选项卡下的机器学习工作区中的实验时，运行期间记录的指标会自动显示在门户上，如下图所示。</p>

<div><img class="aligncenter size-full wp-image-642 image-border" src="img/2cbb7274-466a-4831-943b-ab5c0d1ad1e0.png" style=""/></div>

<p>导航到实验选项卡下机器学习工作区中的实验</p>

<p>除了<kbd>log</kbd>方法之外，还有一个<kbd>upload_file</kbd>方法来上传训练期间生成的文件，如下面的代码片段所示。您可以使用此方法来存储培训完成后保存的模型文件:</p>

<pre>z.save('outputs/model.onnx') # The z variable is the trained model<br/>run.upload_file('model.onnx', 'outputs/model.onnx')</pre>

<p><kbd>upload_file</kbd>方法需要一个文件名，因为它可以在工作区中找到，还需要一个本地路径，在那里可以找到源文件。请注意文件的位置。由于Azure Machine Learning workspace的限制，它只会从outputs文件夹中提取文件。这一限制将来可能会取消。</p>

<p>确保在运行范围内执行<kbd>upload_file</kbd>方法，以便AzureML库将模型链接到您的实验运行，从而使其可跟踪。</p>

<p>将文件上传到工作空间后，您可以在运行的outputs部分下的门户中找到它。要获得运行细节，请在Azure Portal中打开机器学习工作区，导航到实验，然后选择您想要查看其细节的运行，如下所示:</p>

<div><img class="aligncenter size-full wp-image-643 image-border" src="img/663a6299-7df9-4aac-b503-0005bb600bc0.png" style=""/></div>

<p>选择管路</p>

<p>最后，当您完成运行并想要发布模型时，您可以在模型注册表中注册它，如下所示:</p>

<pre>stored_model = run.register_model(model_name='classify_flowers', model_path='model.onnx')</pre>

<p><kbd>register_model</kbd>方法将模型存储在模型注册中心，这样您就可以将它部署到生产中。当模型先前存储在注册表中时，它将自动存储为新版本。现在，如果需要，您可以随时返回到以前的版本，如下所示:</p>

<div><img class="aligncenter size-full wp-image-644 image-border" src="img/1747e79d-4e26-473d-abbf-941254425c2f.png" style=""/></div>

<p>存储为新版本的模型</p>

<p>你可以在工作区的模型注册表中找到模型，方法是转到Azure Portal上的机器学习工作区，然后在工作区的导航菜单中点击栏中的模型项。</p>

<p>模型会自动与实验运行相关联，因此您总是可以找到用于训练模型的设置。这很重要，因为它增加了您在需要时重现结果的机会。</p>

<p>我们仅限于在本地进行实验。如果你愿意，你可以使用Azure机器学习在专用硬件上运行实验。你可以在Azure机器学习文档网站上了解更多信息:<a href="https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-set-up-training-targets">https://docs . Microsoft . com/en-us/Azure/Machine-Learning/service/how-to-set-up-training-targets</a>。</p>

<p>一旦您完成了一次实验运行，您就可以将训练好的模型部署到生产中。在下一节中，我们将探索如何做到这一点。</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Deploying your model to production</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:f69ed529-6576-4ff7-a760-3ddf81ca9def" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">将您的模型部署到生产中</h1>

                

            

            

                

<p>Azure机器学习的最后一个有趣部分是它包含的部署工具。部署工具允许您从模型注册中心获取模型，并将其部署到生产环境中。</p>

<p>在将模型部署到生产环境之前，您需要有一个包含模型和评分脚本的图像。该图像是一个Docker图像，它包含一个web服务器，当对它发出请求时，它将调用评分脚本。评分脚本接受JSON有效负载形式的输入，并使用它通过模型进行预测。我们的虹膜分类模型的评分脚本如下所示:</p>

<pre>import os<br/>import json<br/>import numpy as np<br/>from azureml.core.model import Model<br/>import onnxruntime<br/><br/>model = None<br/><br/>def init():<br/>    global model<br/>    <br/>    model_path = Model.get_model_path('classify_flowers')<br/>    model = onnxruntime.InferenceSession(model_path)<br/>    <br/><br/>def run(raw_data):<br/>    data = json.loads(raw_data)<br/>    data = np.array(data).astype(np.float32)<br/>    <br/>    input_name = model.get_inputs()[0].name<br/>    output_name = model.get_outputs()[0].name<br/><br/>    prediction = model.run([output_name], { input_name: data})<br/>    <br/>    # Select the first output from the ONNX model.<br/>    # Then select the first row from the returned numpy array.<br/>    prediction = prediction[0][0]<br/><br/>    return json.dumps({'scores': prediction.tolist() })</pre>

<p>遵循给定的步骤:</p>

<ol>

<li>首先，导入构建脚本所需的组件。</li>

<li>接下来，定义一个包含加载模型的全局模型变量。</li>

<li>之后，定义init函数来初始化脚本中的模型。</li>

<li>在init函数中，使用<kbd>Model.get_model_path</kbd>函数检索模型的路径。这将自动定位Docker映像中的模型文件。</li>

<li>接下来，通过初始化<kbd>onnxruntime.InferenceSession</kbd>类的新实例来加载模型。</li>

<li>定义另一个函数，<kbd>run</kbd>，它接受单个参数<kbd>raw_data</kbd>。</li>

<li>在<kbd>run</kbd>函数中，将<kbd>raw_data</kbd>变量的内容从JSON转换为Python数组。</li>

<li>接下来，将<kbd>data</kbd>数组转换成Numpy数组，这样我们就可以用它来进行预测。</li>

<li>之后，对加载的模型使用<kbd>run</kbd>方法，并输入输入特征。包括一个字典，告诉ONNX运行时如何将输入数据映射到模型的输入变量。</li>

<li>该模型返回一个输出数组，其中1个元素用于模型的输出。该输出包含一行数据。从输出数组中选择第一个元素，从所选输出变量中选择第一行，并将其存储在<kbd>prediction</kbd>变量中。</li>

<li>最后，将预测的输出作为JSON对象返回。</li>

</ol>

<p>Azure Machine Learning service将在您创建容器映像时自动包含您为特定模型注册的任何模型文件。因此，<kbd>get_model_path</kbd>也将在部署的图像中工作，并解析到托管模型和评分脚本的容器中的一个目录。</p>

<p> </p>

<p>现在我们有了一个评分脚本，让我们创建一个图像，并将该图像作为web服务部署到云中。要部署web服务，您可以显式创建一个映像。或者，您可以让Azure机器学习服务基于您提供的配置创建一个，如下所示:</p>

<pre>from azureml.core.image import ContainerImage<br/><br/>image_config = ContainerImage.image_configuration(<br/>    execution_script="score.py", <br/>    runtime="python", <br/>    conda_file="conda_env.yml")</pre>

<p>遵循给定的步骤:</p>

<ol>

<li>首先，从<kbd>azureml.core.image</kbd>模块导入ContainerImage类。</li>

<li>接下来，使用<kbd>ContainerImage.image_configuration</kbd>方法创建一个新的映像配置。提供score.py作为参数<kbd>execution_script</kbd>，python <kbd>runtime</kbd>，最后提供conda_env.yml作为图片的<kbd>conda_file</kbd>。</li>

</ol>

<p>我们将容器映像配置为使用Python作为运行时。我们还为Anaconda配置了一个特殊的环境文件，这样我们就可以配置像CNTK这样的定制模块，如下所示:</p>

<pre>name: project_environment<br/>dependencies:<br/>  # The python interpreter version.<br/>  # Currently Azure ML only supports 3.5.2 and later.<br/>- python=3.6.2<br/><br/>- pip:<br/>    # Required packages for AzureML execution, history, and data preparation.<br/><br/>  - azureml-defaults<br/>  - onnxruntime</pre>

<p>遵循给定的步骤:</p>

<ol>

<li>首先，给环境一个名字。这是可选的，但是当您在本地从这个文件创建一个环境进行测试时，这是非常有用的。</li>

<li>接下来，为您的评分脚本提供python版本3.6.2。</li>

<li>最后，用包含<kbd>azureml-default</kbd>和<kbd>onnxruntime</kbd>的子列表向列表添加一个pip依赖项。</li>

</ol>

<p><kbd>azureml-default</kbd>包包含了在docker容器映像中处理实验和模型所需的一切。它包括像Numpy和Pandas这样的标准包，以便于安装。需要<kbd>onnxruntime</kbd>包，这样我们就可以将模型加载到我们正在使用的评分脚本中。</p>

<p>还需要一个步骤来将训练好的模型部署为web服务。我们需要设置一个web服务配置，并将模型作为服务进行部署。机器学习服务支持部署到虚拟机、Kubernetes集群和Azure容器实例，它们是运行在云中的基本Docker容器。这是将模型部署到Azure容器实例的方法:</p>

<pre>from azureml.core.webservice import AciWebservice, Webservice<br/><br/>aciconfig = AciWebservice.deploy_configuration(cpu_cores=1, memory_gb=1)<br/><br/>service = Webservice.deploy_from_model(workspace=ws,<br/>                                       name='classify-flowers-svc',<br/>                                       deployment_config=aciconfig,<br/>                                       models=[stored_model],<br/>                                       image_config=image_config)</pre>

<p>遵循给定的步骤:</p>

<ol>

<li>首先，从azureml.core.webservice模块导入AciWebservice和Webservice类。</li>

<li>然后，使用AziWebservice类上的deploy_configuration方法创建一个新的<kbd>AciWebservice</kbd>配置。为it部门提供一组软件资源限制。一个CPU和1GB内存。</li>

<li>一旦有了web服务的配置，就可以通过调用<kbd>deploy_from_model</kbd>将注册的模型部署到生产环境中，其中包含要部署的工作区、服务名和您想要部署的模型。提供您之前创建的映像配置。</li>

</ol>

<p>一旦创建了容器映像，它将作为容器实例部署在Azure上。这将在机器学习工作区的资源组中创建新资源。</p>

<p>一旦新服务启动，您将在机器学习工作区的Azure Portal上看到一个新的部署，如下面的截图所示:</p>

<div><img class="aligncenter size-full wp-image-646 image-border" src="img/8c048b5c-208c-4e6a-917c-b4043faf8d70.png" style=""/></div>

<p>机器学习工作区中Azure门户上的新部署</p>

<p>该部署包括一个评分URL，您可以从应用程序中调用该URL来使用该模型。因为您使用REST来调用模型，所以您与它在幕后运行CNTK的事实是隔离的。您还可以使用任何您能想到的编程语言，只要它能执行HTTP请求。</p>

<p class="mce-root"/>

<p class="mce-root"/>

<p class="mce-root"/>

<p>例如，在Python中，我们可以使用<kbd>requests</kbd>包作为基本的REST客户端，使用您刚刚创建的服务进行预测。让我们首先安装<kbd>requests</kbd>模块，如下所示:</p>

<pre><strong>pip install --upgrade requests</strong></pre>

<p>安装了<kbd>requests</kbd>包后，我们可以编写一个小脚本来执行针对已部署服务的请求，如下所示:</p>

<pre>import requests<br/>import json<br/><br/>service_url = "&lt;service-url&gt;"<br/>data = [[1.4, 0.2, 4.9, 3.0]]<br/><br/>response = requests.post(service_url, json=data)<br/><br/>print(response.json())</pre>

<p>遵循给定的步骤:</p>

<ol>

<li>首先，导入请求和json包。</li>

<li>接下来，为service_url创建一个新变量，并用webservice的url填充它。</li>

<li>然后，创建另一个变量，以存储您要进行预测的数据。</li>

<li>之后，使用requests.post函数将数据发送到已部署的服务并存储响应。</li>

<li>最后，读取响应中返回的JSON数据以获得预测值。</li>

</ol>

<p>可以通过执行以下步骤来获取service_url:</p>

<ol>

<li>首先，导航到包含机器学习工作区的资源组。</li>

<li>然后，选择工作区，并选择细节面板左侧的部署部分。</li>

<li>选择要查看其详细信息的部署，并从详细信息页面复制URL。</li>

</ol>

<div><img class="aligncenter size-full wp-image-647 image-border" src="img/98ea4d64-d656-4c2d-a74e-7a76b9ace6fc.png" style=""/></div>

<p>选择部署</p>

<p>当您运行刚刚创建的脚本时，您将收到一个响应，其中包含输入样本的预测类。输出如下所示:</p>

<pre>{"scores": [-2.27234148979187, -2.486853837966919, -0.20609207451343536]}</pre>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Summary</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:f69ed529-6576-4ff7-a760-3ddf81ca9def" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">摘要</h1>

                

            

            

                

<p>在这一章中，我们已经了解了如何将深度学习和机器学习模型应用到生产中。我们已经探索了一些基本原则，这些原则将帮助您在持续交付环境中成功地进行深度学习。</p>

<p>由于ONNX格式的可移植性，我们已经了解了如何将模型导出到ONNX，以便更容易地将训练好的模型部署到生产中，并保持它们运行多年。然后，我们探索了如何在其他语言(如C#)中使用CNTK API进行预测。</p>

<p class="mce-root"/>

<p class="mce-root"/>

<p>最后，我们看了如何使用Azure机器学习服务来提升您在实验管理、模型管理和部署工具方面的DevOps体验。虽然你不需要这样的工具来开始，但当你计划运行一个更大的生产项目时，在你的军火库中拥有像Azure Machine Learning service这样的东西真的很有帮助。</p>

<p>到了这一章，我们已经到了这本书的结尾。第一章，我们开始探索CNTK。然后，我们看了如何建立模型，向它们提供数据，并测量它们的性能。在了解了基础知识之后，我们探索了两个有趣的用例，查看图像和时间序列数据。最后，我们将模型投入生产。现在你应该有足够的信息开始用CNTK构建你自己的模型！</p>





            



            

        

    </body>



</html></body></html>