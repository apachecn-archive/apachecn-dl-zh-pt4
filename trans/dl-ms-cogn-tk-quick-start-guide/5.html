<html><head/><body><html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Working with Images</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:f69ed529-6576-4ff7-a760-3ddf81ca9def" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">使用图像</h1>

                

            

            

                

<p>在这一章中，我们将利用CNTK探索一些更深入的学习模型。具体来说，我们将研究使用神经网络对图像数据进行分类。当我们讨论如何训练卷积神经网络时，您在过去章节中学习的所有内容都将在本章中回来。</p>

<p>本章将涵盖以下主题:</p>

<ul>

<li>卷积神经网络体系结构</li>

<li>如何构建卷积神经网络</li>

<li>如何将图像数据输入卷积网络</li>

<li>如何通过数据扩充提高网络性能</li>

</ul>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Technical requirements</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:f69ed529-6576-4ff7-a760-3ddf81ca9def" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">技术要求</h1>

                

            

            

                

<p>我们假设您的计算机上安装了Anaconda的最新版本，并按照<a href="9a2c8c46-f9a0-4e05-86ef-31300a28a7ba.xhtml">第1章</a>、<em>CNTK</em>入门中的步骤在您的计算机上安装CNTK。本章的示例代码可以在我们的GitHub资源库中找到:<a href="https://github.com/PacktPublishing/Deep-Learning-with-Microsoft-Cognitive-Toolkit-Quick-Start-Guide/tree/master/ch5">https://GitHub . com/packt publishing/Deep-Learning-with-Microsoft-Cognitive-Toolkit-Quick-Start-Guide/tree/master/ch5</a>。</p>

<p>在本章中，我们将学习存储在Jupyter笔记本中的一个示例。要访问示例代码，请在下载代码的目录中的Anaconda提示符下运行以下命令:</p>

<pre><strong>cd ch5</strong><br/><strong>jupyter notebook</strong></pre>

<p>我们将在每一节中提到相关的笔记本，这样您就可以跟随并亲自尝试不同的技术。</p>

<p>GitHub存储库中没有本章的数据集。它太大了，不能放在那里。请打开<kbd>Prepare the dataset.ipynb</kbd>笔记本，按照其中的说明获取本章的数据。</p>

<p class="mce-root">请观看以下视频，了解实际运行的代码:</p>

<p><a href="http://bit.ly/2Wm6U49">http://bit.ly/2Wm6U49</a></p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Convolutional neural network architecture</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:f69ed529-6576-4ff7-a760-3ddf81ca9def" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">卷积神经网络体系结构</h1>

                

            

            

                

<p class="mce-root">在前几章中，我们已经学习了如何使用常规的前馈网络架构来构建神经网络。在前向神经网络中，我们假设不同的输入特征之间存在相互作用。但是我们不会对这些相互作用的本质做任何假设。然而，这并不总是正确的做法。</p>

<p>当您处理图像等复杂数据时，前馈神经网络不会做得很好。这是因为我们假设网络的输入之间存在相互作用。但是我们没有考虑到它们是以空间方式组织起来的。当你观察图像中的像素时，它们之间有水平和垂直的关系。图像中的颜色和图像中特定颜色像素的位置之间也有关系。</p>

<p>卷积网络是一种特殊的神经网络，它明确假设我们正在处理与它有空间关系的数据。这使得它非常擅长识别图像。但是其他空间组织的数据也可以。让我们探索用于图像分类任务的卷积神经网络的架构。</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Network architecture used for image classification</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:f69ed529-6576-4ff7-a760-3ddf81ca9def" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">用于图像分类的网络体系结构</h1>

                

            

            

                

<p>用于影像分类的卷积网络通常包含一个或多个卷积层，后面是池层，通常以规则的全连接层结束，以提供最终输出，如以下屏幕截图所示:</p>

<div><img class="aligncenter size-full wp-image-581 image-border" src="img/3bf0afaf-b58a-413f-ab2f-4caa7b6c68ee.png" style=""/></div>

<p>这张图片来自:https://en.wikipedia.org/wiki/File:Typical_cnn.png</p>

<p>当你仔细观察卷积网络的结构时，你会发现它是从一组<strong>卷积</strong>和<strong>汇集</strong>层开始的。你可以认为这是一个复杂的，可训练的照片过滤器。<strong>卷积层</strong>过滤出对图像进行分类所需的有趣细节，而<strong>汇集</strong> <strong>层</strong>总结了这些特征，因此在网络末端附近有更少的数据点需要处理。</p>

<p>通常，您会在一个用于图像分类的神经网络中找到几组卷积层和池层。这样做是为了能够从图像中提取更复杂的细节。网络的第一层从图像中提取简单的细节，例如线条。然后，下一组层结合前一组层的输出来学习更复杂的特征，例如拐角或曲线。可以想象，之后的几层用来学习越来越复杂的特性。</p>

<p>通常，当你建立一个神经网络时，你想要对图像中的内容进行分类。这就是经典致密层发挥重要作用的地方。通常，用于图像识别的模型将以一个输出图层和一个或多个密集图层结束。</p>

<p>让我们看看如何使用卷积层和池层来创建卷积神经网络。</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Working with convolution layers</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:f69ed529-6576-4ff7-a760-3ddf81ca9def" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">使用卷积图层</h1>

                

            

            

                

<p>现在，您已经了解了卷积网络的样子，让我们看看卷积网络中使用的卷积层:</p>

<div><img class="aligncenter size-full wp-image-580 image-border" src="img/ad85d578-6750-4eaf-8626-068c79382417.png" style=""/></div>

<p>这张图片来自:https://en.wikipedia.org/wiki/File:Conv_layer.png</p>

<p>卷积层是卷积网络的核心构建模块。您可以将卷积图层视为可训练的过滤器，用于从输入中提取重要的细节并移除被视为噪声的数据。卷积层包含一组权重，这些权重覆盖了一个小区域(宽度和高度)，但覆盖了该层输入的所有通道。创建卷积层时，需要在神经元中指定其深度。你会发现大部分框架，包括CNTK，在谈到层的深度时都会谈到滤镜。</p>

<p>当我们执行正向传递时，我们将图层的过滤器滑过输入，并在输入和每个过滤器的权重之间执行点积运算。滑动运动由步幅设置控制。当您将跨距指定为1时，您将得到与输入具有相同宽度和高度的输出矩阵，但深度与图层中过滤器的数量相同。您可以设置不同的步幅，这将减少输出矩阵的宽度和高度。</p>

<p>除了过滤器的输入大小和数量，您还可以配置图层的填充。向卷积图层添加填充将在处理后的输入数据周围添加零边框。这听起来可能有点奇怪，但在某些情况下会非常方便。</p>

<p>当您查看卷积图层的输出大小时，它将根据输入大小、过滤器数量、步幅和填充来确定。该公式如下所示:</p>

<div><img class="fm-editor-equation" src="img/29379922-0057-4588-808c-60406285ff02.png" style="width:13.75em;height:2.50em;"/></div>

<p><em> W </em>是输入的大小，<em> F </em>是滤镜的数量或者层的深度，<em> P </em>填充，<em> S </em>步幅。</p>

<p>但是，并非所有输入大小、过滤器、步幅和填充的组合都有效。例如，当您的输入大小<em> W </em> = 10，层深度<em> F </em> = 3，跨距<em> S </em> = 2时，您将最终得到5.5的输出音量。不是所有的输入都会完美地映射到这个输出大小，所以CNTK会抛出一个异常。这就是填充设置的用武之地。通过指定填充，我们可以确保所有输入都映射到输出神经元。</p>

<p>我们刚刚讨论的输入大小和过滤器的设置可能感觉有点抽象，但对他们来说是有意义的。设置较大的输入大小将导致图层从输入中捕获较粗糙的模式。设置较小的输入大小将使图层更好地检测更精细的模式。过滤器的深度或数量控制着在输入图像中可以检测到多少不同的图案。在高层次上，你可以说一个卷积滤波器检测一个模式；比如水平线。带有两个滤镜的图层可以检测两种不同的图案:水平线和垂直线。</p>

<p>为卷积网络提供正确的设置可能需要相当多的工作。幸运的是，CNTK的设置有助于简化这个过程。</p>

<p>训练卷积层的方式与常规密集层相同。这意味着我们将执行向前传递，计算梯度，并使用学习者在向后传递中提出更好的参数值。</p>

<p>卷积层之后通常是汇集层，用于压缩卷积层学习到的特征。接下来让我们来看看池层。</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Working with pooling layers</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:f69ed529-6576-4ff7-a760-3ddf81ca9def" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">使用池层</h1>

                

            

            

                

<p>在上一节中，我们已经了解了卷积层以及如何使用它们从像素数据中提取细节。汇集层用于汇总提取的详细信息。池化图层有助于减少数据量，从而使数据分类变得更加容易。</p>

<p>当样本具有许多不同的输入特征时，神经网络很难对样本进行分类，理解这一点很重要。这就是为什么我们使用卷积层和汇集层的组合来提取细节并进行总结:</p>

<div><img class="aligncenter size-full wp-image-579 image-border" src="img/14a89752-593c-4dcb-a9ad-ecbc785bfe33.png" style=""/></div>

<p>这张图片来自:https://en.wikipedia.org/wiki/File:Max_pooling.png</p>

<p>池图层具有缩减采样算法，可通过输入大小和步幅进行配置。我们将把前一卷积层中的每个滤波器的输出馈送到汇集层中。池层跨数据切片移动，并采用与配置的输入大小相等的小窗口。它获取小区域的值，并从中获取最大值作为该区域的输出。就像卷积层一样，它使用步幅来控制在输入中移动的速度。例如，大小为1，步幅为2，将使数据维数减半。通过仅使用最高的输入值，它丢弃了75%的输入数据。</p>

<p>这种池化技术被称为最大采样，它并不是池化图层降低输入数据维度的唯一方式。你也可以使用平均池。在这种情况下，输入区域的平均值用作池化图层中的输出。</p>

<p>请注意，池化层只会减小宽度和高度上的输入大小。深度保持不变，因此您可以放心，要素只是缩减像素采样，而不会完全移除。</p>

<p class="mce-root">由于池图层具有固定的算法来对输入数据进行缩减采样，因此其中没有可训练的参数。这意味着不需要花费时间来训练池层。</p>

<p>卷积网络的其他用途</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Other uses for convolutional networks</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:f69ed529-6576-4ff7-a760-3ddf81ca9def" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">我们正致力于使用卷积网络进行图像分类，但您可以将这种神经网络用于更多场景，例如:</h1>

                

            

            

                

<p>图像中的目标检测。CNTK网站上有一个很好的例子，展示了如何构建一个对象检测模型:<a href="https://docs.microsoft.com/en-us/cognitive-toolkit/Object-Detection-using-Fast-R-CNN">https://docs . Microsoft . com/en-us/cognitive-toolkit/Object-Detection-using-Fast-R-CNN</a></p>

<ul>

<li>检测照片中的人脸并预测照片中人的年龄</li>

<li>使用我们在<a href="a5da9ef2-399a-4c30-b751-318d64939369.xhtml">第6章</a>、<em>中讨论的卷积和递归神经网络的组合对图像进行说明，处理时间序列数据</em></li>

<li>从声纳图像预测到湖底的距离</li>

<li>当你开始为不同的任务组合卷积网络时，你可以构建一些非常强大的应用程序；例如，一个安全摄像头可以检测视频流中的人，并警告保安有入侵者。</li>

</ul>

<p>中国等国家正在大力投资这类技术。卷积网络在智能城市应用中用于监控交叉路口。使用深度学习模型，当局可以检测红绿灯处的事故，并自动改变交通路线，以便警察可以更轻松地工作。</p>

<p>构建卷积网络</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Building convolutional networks</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:f69ed529-6576-4ff7-a760-3ddf81ca9def" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">既然您已经了解了卷积网络背后的基础知识和一些常见用例，那么让我们来看看如何使用CNTK构建一个卷积网络。</h1>

                

            

            

                

<p>我们将建立一个可以识别图像中手写数字的模型。有一个免费的数据集叫做MNIST数据集，包含60，000个手写数字样本。对于MNIST数据集，还有一个包含10，000个样本的测试集。</p>

<p>让我们开始，看看在CNTK中构建卷积网络是什么样子的。首先，我们将了解如何构建卷积神经网络的结构，然后我们将了解如何训练卷积神经网络的参数。最后，我们将探索如何通过改变不同层设置的结构来改进神经网络。</p>

<p>构建网络结构</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Building the network structure</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:f69ed529-6576-4ff7-a760-3ddf81ca9def" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">通常，当您构建用于识别图像中的模式的神经网络时，您将使用卷积层和池层的组合。网络的末端应包含一个或多个隐藏图层，以softmax图层结束，用于分类目的。</h1>

                

            

            

                

<p>让我们建立网络结构:</p>

<p>遵循给定的步骤:</p>

<pre>from cntk.layers import Convolution2D, Sequential, Dense, MaxPooling<br/>from cntk.ops import log_softmax, relu<br/>from cntk.initializer import glorot_uniform<br/>from cntk import input_variable, default_options<br/><br/>features = input_variable((3,28,28))<br/>labels = input_variable(10)<br/><br/>with default_options(initialization=glorot_uniform, activation=relu):<br/>    model = Sequential([<br/>        Convolution2D(filter_shape=(5,5), strides=(1,1), num_filters=8, pad=True),<br/>        MaxPooling(filter_shape=(2,2), strides=(2,2)),<br/>        Convolution2D(filter_shape=(5,5), strides=(1,1), num_filters=16, pad=True),<br/>        MaxPooling(filter_shape=(3,3), strides=(3,3)),<br/>        Dense(10, activation=log_softmax)<br/>    ])<br/><br/>z = model(features)</pre>

<p>首先，导入神经网络所需的层。</p>

<ol>

<li>然后，导入网络的激活功能。</li>

<li>接下来，导入<kbd>glorot_uniform initializer</kbd>函数，稍后初始化卷积层。</li>

<li>之后，导入<kbd>input_variable</kbd>函数来创建输入变量，导入<kbd>default_options</kbd>函数来简化神经网络的配置。</li>

<li>创建一个新的<kbd>input_variable</kbd>存储输入图像，它们将包含<kbd>3</kbd>通道(红色、绿色和蓝色)，大小为<kbd>28</kbd>乘<kbd>28</kbd>像素。</li>

<li>创建另一个<kbd>input_variable</kbd>来存储要预测的标签。</li>

<li>接下来，为网络创建<kbd>default_options</kbd>，并将<kbd>glorot_uniform</kbd>用作初始化函数。</li>

<li>然后，创建一个新的<kbd>Sequential</kbd>层集来构建神经网络</li>

<li>在<kbd>Sequential</kbd>图层组中，添加一个<kbd>Convolutional2D</kbd>图层，图层<kbd>filter_shape</kbd>为<kbd>5</kbd>，图层<kbd>strides</kbd>设置为<kbd>1</kbd>，并将滤镜数量设置为<kbd>8</kbd>。启用<kbd>padding</kbd>使图像被填充以保持原始尺寸。</li>

<li>用<kbd>2</kbd>的<kbd>filter_shape</kbd>和<kbd>2</kbd>的<kbd>strides</kbd>设置添加一个<kbd>MaxPooling</kbd>图层，将图像压缩一半。</li>

<li>添加另一个<kbd>Convolution2D</kbd>图层，其<kbd>filter_shape</kbd>为5，<kbd>strides</kbd>设置为1，使用16个滤镜。添加<kbd>padding</kbd>以保留由之前的池层生成的图像的大小。</li>

<li>接下来，添加另一个<kbd>MaxPooling</kbd>图层，其<kbd>filter_shape</kbd>为3，<kbd>strides</kbd>设置为3，将图像缩小到三分之一。</li>

<li>最后，为网络可以预测的10个可能的类别添加一个有10个神经元的<kbd>Dense</kbd>层。使用一个<kbd>log_softmax</kbd>激活函数将网络转换成一个分类模型。</li>

<li>我们使用28x28像素的图像作为模型的输入。这个大小是固定的，所以当您想要使用这个模型进行预测时，您需要提供相同大小的图像作为输入。</li>

</ol>

<p>注意，这个模型仍然非常基础，不会产生完美的结果，但这是一个好的开始。稍后，我们可以开始调整它，如果我们需要的话。</p>

<p>用图像训练网络</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Training the network with images</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:f69ed529-6576-4ff7-a760-3ddf81ca9def" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">现在我们有了卷积神经网络的结构，让我们来探索如何训练它。训练一个处理图像的神经网络需要比大多数计算机可用的内存更多的内存。这就是第三章、<em xmlns:epub="http://www.idpf.org/2007/ops">的<a xmlns:epub="http://www.idpf.org/2007/ops" href="f7cd9148-99e8-427c-acf4-d74c3e52df58.xhtml">小批量来源进入你的神经网络</a></em>的地方。我们将设置一组两个迷你批次源来训练和评估我们刚刚创建的神经网络。让我们先来看看如何为图像构建一个迷你批处理源:</h1>

                

            

            

                

<p>遵循给定的步骤:</p>

<pre>import os<br/>from cntk.io import MinibatchSource, StreamDef, StreamDefs, ImageDeserializer, INFINITELY_REPEAT<br/>import cntk.io.transforms as xforms<br/><br/>def create_datasource(folder, max_sweeps=INFINITELY_REPEAT):<br/>    mapping_file = os.path.join(folder, 'mapping.bin')<br/>    <br/>    stream_definitions = StreamDefs(<br/>        features=StreamDef(field='image', transforms=[]),<br/>        labels=StreamDef(field='label', shape=10)<br/>    )<br/>    <br/>    deserializer = ImageDeserializer(mapping_file, stream_definitions)<br/>    <br/>    return MinibatchSource(deserializer, max_sweeps=max_sweeps)</pre>

<p>首先，导入<kbd>os</kbd>包来访问一些有用的文件系统函数。</p>

<ol>

<li>接下来，导入必要的组件来创建一个新的<kbd>MinibatchSource</kbd>。</li>

<li>创建一个新函数<kbd>create_datasource</kbd>,它采用输入文件夹的路径和一个<kbd>max_sweeps</kbd>设置来控制我们迭代数据集的频率。</li>

<li>在<kbd>create_datasource</kbd>函数中，在源文件夹中找到mapping.bin文件。该文件将包含磁盘上的图像及其关联标签之间的映射。</li>

<li>然后创建一组流定义，从mapping.bin文件中读取。</li>

<li>为图像文件添加一个<kbd>StreamDef</kbd>。确保包含<kbd>transforms</kbd>关键字参数，并用空数组初始化它。</li>

<li>为具有10个特征的标签字段添加另一个<kbd>StreamDef</kbd>。</li>

<li>创建一个新的<kbd>ImageDeserializer</kbd>，并为其提供<kbd>mapping_file</kbd>和<kbd>stream_definitions</kbd>变量。</li>

<li>最后，创建一个<kbd>MinibatchSource</kbd>，并为其提供反序列化器和<kbd>max_sweeps</kbd>设置。</li>

<li>注意，您可以使用<kbd>Preparing the dataset.ipynb</kbd> Python笔记本中的代码创建培训所需的文件。确保您的硬盘上有足够的空间来存储图像。1 GB的硬盘空间足以存储用于训练和验证的所有样本。</li>

</ol>

<p>一旦我们有了<kbd>create_datasource</kbd>函数，我们就可以创建两个单独的数据源来训练模型:</p>

<p>首先，用<kbd>mnist_train</kbd>文件夹调用<kbd>create_datasource</kbd>函数来创建用于训练的数据源。</p>

<pre>train_datasource = create_datasource('mnist_train')<br/>test_datasource = create_datasource('mnist_test', max_sweeps=1, train=False)</pre>

<ol>

<li>接下来，使用<kbd>mnist_test</kbd>文件夹调用<kbd>create_datasource</kbd>函数，并将<kbd>max_sweeps</kbd>设置为1，以创建用于验证神经网络的数据源。</li>

<li>一旦你准备好了图像，就该开始训练神经网络了。我们可以使用<kbd>loss</kbd>函数上的<kbd>train</kbd>方法开始训练过程:</li>

</ol>

<p>遵循给定的步骤:</p>

<pre>from cntk import Function<br/>from cntk.losses import cross_entropy_with_softmax<br/>from cntk.metrics import classification_error<br/>from cntk.learners import sgd<br/><br/>@Function<br/>def criterion_factory(output, targets):<br/>    loss = cross_entropy_with_softmax(output, targets)<br/>    metric = classification_error(output, targets)<br/>    <br/>    return loss, metric<br/><br/>loss = criterion_factory(z, labels)<br/>learner = sgd(z.parameters, lr=0.2)</pre>

<p>首先，从cntk包中导入函数装饰器。</p>

<ol>

<li>接下来，从losses模块导入<kbd>cross_entropy_with_softmax</kbd>函数。</li>

<li>然后，从度量模块导入<kbd>classification_error</kbd>函数。</li>

<li>之后，从学习者模块导入<kbd>sgd</kbd>学习者。</li>

<li>创建一个带有两个参数的新函数<kbd>criterion_factory</kbd>，输出和目标。</li>

<li>用<kbd>@Function</kbd>装饰器标记该函数，将其转换成一个CNTK函数对象。</li>

<li>在该函数中，创建一个<kbd>cross_entropy_with_softmax</kbd>函数的新实例。</li>

<li>接下来，创建一个<kbd>classification_error</kbd>指标的新实例。</li>

<li>返回损失和度量作为函数的结果。</li>

<li>创建<kbd>criterion_factory</kbd>函数后，用它初始化一个新的损耗。</li>

<li>最后，用模型的参数和0.2的学习率设置<kbd>sgd</kbd>学习器。</li>

<li>现在，我们已经为神经网络设置了损失和学习器，让我们看看如何训练和验证神经网络:</li>

</ol>

<p>遵循给定的步骤:</p>

<pre>from cntk.logging import ProgressPrinter<br/>from cntk.train import TestConfig<br/><br/>progress_writer = ProgressPrinter(0)<br/>test_config = TestConfig(test_datasource)<br/><br/>input_map = {<br/>    features: train_datasource.streams.features,<br/>    labels: train_datasource.streams.labels<br/>}<br/><br/>loss.train(train_datasource, <br/>           max_epochs=1,<br/>           minibatch_size=64,<br/>           epoch_size=60000, <br/>           parameter_learners=[learner], <br/>           model_inputs_to_streams=input_map, <br/>           callbacks=[progress_writer, test_config])</pre>

<p>首先从<kbd>logging</kbd>模块导入<kbd>ProgressPrinter</kbd>类</p>

<ol>

<li>接下来，从<kbd>train</kbd>模块导入<kbd>TestConfig</kbd>类。</li>

<li>创建一个<kbd>ProgressPrinter</kbd>的新实例，这样我们就可以记录培训过程的输出。</li>

<li>然后，使用我们之前制作的<kbd>test_datasource</kbd>作为输入，为神经网络创建<kbd>TestConfig</kbd>。</li>

<li>创建一个新字典，将来自<kbd>train_datasource</kbd>的数据流映射到神经网络的输入变量。</li>

<li>最后，调用<kbd>loss</kbd>上的<kbd>train</kbd>方法，并提供<kbd>train_datasource</kbd>、训练器的设置、<kbd>learner</kbd>、<kbd>input_map</kbd>以及在训练期间使用的回调。</li>

<li>当您执行python代码时，您将得到类似如下的输出:</li>

</ol>

<p>注意损失是如何随时间减少的。确实需要相当长的时间来达到模型可用的足够低的值。训练图像分类模型将需要很长时间，因此这是使用GPU将对训练模型所需的时间产生很大影响的情况之一。</p>

<pre><strong>average      since    average      since      examples

    loss       last     metric       last              

 ------------------------------------------------------

Learning rate per minibatch: 0.2

      105        105      0.938      0.938            64

 1.01e+07   1.51e+07      0.901      0.883           192

 4.31e+06          2      0.897      0.895           448

 2.01e+06          2      0.902      0.906           960

 9.73e+05          2      0.897      0.893          1984

 4.79e+05          2      0.894      0.891          4032</strong><br/><strong>[...]</strong></pre>

<p>选择合适的图层组合</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Picking the right combination of layers</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:f69ed529-6576-4ff7-a760-3ddf81ca9def" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">在前面的章节中，我们已经看到了如何使用卷积层和池层来构建神经网络。</h1>

                

            

            

                

<p>我们刚刚看到，训练一个用于图像识别的模型需要相当长的时间。除了训练时间长之外，为卷积网络选择正确的设置非常困难，而且需要很长时间。通常你需要几个小时的实验来找到一个有效的网络结构。这对有抱负的人工智能开发人员来说是非常消极的。</p>

<p>幸运的是，有几个研究小组正在努力寻找用于图像分类任务的神经网络的最佳架构。有几种不同的架构已经在竞赛和现实生活场景中成功使用:</p>

<p>VGG-16</p>

<ul>

<li>雷斯内特</li>

<li>开始</li>

<li>而且还有好几个。虽然我们无法详细介绍如何构建每种架构，但让我们从功能的角度来探究它们是如何工作的，这样您就可以更明智地选择在自己的应用中尝试哪种网络架构。</li>

</ul>

<p>VGG网络体系结构是由视觉几何小组发明的，作为一种将图像分类到1000个不同类别的方法。这很难做到，但该团队设法获得了70.1%的准确率，考虑到区分1000个不同类别有多难，这已经很不错了</p>

<p>VGG网络架构使用输入大小为3x3的卷积层堆栈。这些层的深度不断增加。从32个过滤器的层开始，继续到48个过滤器，一直到512个过滤器。使用2x2池过滤器来减少数据量。VGG网络架构在2015年发明时是最先进的，因为它比以前发明的模型具有更好的准确性。</p>

<p>不过，还有其他方法可以为图像识别建立神经网络。ResNet架构使用所谓的微架构。它仍然使用卷积层，但这次它们是按块排列的。该体系结构仍然与其他卷积网络非常相似，但是VGG网络具有长链层，而ResNet体系结构在卷积层的块周围具有跳跃连接:</p>

<p>ResNet架构</p>

<div><img class="aligncenter size-full wp-image-577 image-border" src="img/b8cff3e4-456c-49b2-934b-f9e9cc603f31.png" style=""/></div>

<p>这就是微架构这个术语的来源。每个模块都是一个微型网络，能够从输入中学习模式。每个块有几个卷积层和一个残差连接。该连接绕过卷积层块，来自该剩余连接的数据被添加到卷积层的输出中。这背后的想法是，剩余连接动摇了网络中的学习过程，以便它学习得更好更快。</p>

<p>与VGG网络体系结构相比，ResNet体系结构更深入，但更容易训练，因为它需要优化的参数更少。VGG网络架构占用599 MB内存，而ResNet架构仅占用102 MB内存。</p>

<p>我们将探索的最后一个网络架构是初始架构。这种架构也属于微架构范畴。初始网络使用初始块，而不是ResNet架构中使用的剩余块:</p>

<p>初始网络</p>

<div><img class="aligncenter size-full wp-image-578 image-border" src="img/d0d66db2-9de6-4355-a709-34d0e87c754d.png" style=""/></div>

<p>inception架构中的Inception模块使用不同输入大小的卷积层，输入大小为1x1、3x3和5x5，然后沿通道轴连接。这将生成一个与输入具有相同宽度和高度的矩阵，但其通道数比输入多。这样做的目的是，从输入中提取的要素分布范围更广，因此执行分类任务的数据质量也更高。这里描述的初始架构非常肤浅；通常使用的完整版本可以有两个以上的初始块。</p>

<p>当你开始研究其他卷积网络架构时，你会很快发现你需要更多的计算能力来训练它们。通常，数据集放不进内存，并且您的计算机太慢，无法在合理的时间内训练模型。这就是分布式培训可以提供帮助的地方。如果你对使用多台机器训练模型感兴趣，你一定要看看CNTK手册中的这一章:<a href="https://docs.microsoft.com/en-us/cognitive-toolkit/Multiple-GPUs-and-machines">https://docs . Microsoft . com/en-us/cognitive-toolkit/Multiple-GPU-and-machines</a>。</p>

<p>通过数据扩充提高模型性能</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Improving model performance with data augmentation</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:f69ed529-6576-4ff7-a760-3ddf81ca9def" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">用于图像识别的神经网络不仅难以建立和训练，还需要大量的数据来训练。此外，他们倾向于过度适应训练中使用的图像。例如，当您仅使用处于直立位置的人脸照片时，您的模型将很难识别向另一个方向旋转的人脸。</h1>

                

            

            

                

<p>为了帮助克服某些方向的旋转和移动问题，您可以使用图像增强。为图像创建微型批处理源时，CNTK支持特定的变换。</p>

<p>我们为这一章附加了一个笔记本，演示如何使用转换。您可以在本章示例的<kbd>Recognizing hand-written digits with augmented data.ipynb</kbd>文件中找到本节的示例代码。</p>

<p>有几种变换可供您使用。例如，只需几行代码，您就可以随机裁剪用于训练的图像。您可以使用的其他变换是缩放和颜色。你可以在CNTK网站上找到更多关于这些转变的信息:<a href="https://cntk.ai/pythondocs/cntk.io.transforms.html">https://cntk.ai/pythondocs/cntk.io.transforms.html</a>。</p>

<p>在本章前面用于创建minibatch源的函数中，我们可以通过包含裁剪转换来更改转换列表，如以下代码所示:</p>

<p>我们增强了这个函数，增加了一组图像变换。当我们训练时，我们将随机裁剪图像，这样我们可以得到更多的图像变化。然而，这改变了图像的尺寸，所以我们还需要包括比例变换，以确保它符合我们的神经网络的输入层所期望的大小。</p>

<pre>import os<br/>from cntk.io import MinibatchSource, StreamDef, StreamDefs, ImageDeserializer, INFINITELY_REPEAT<br/>import cntk.io.transforms as xforms<br/><br/>def create_datasource(folder, train=True, max_sweeps=INFINITELY_REPEAT):<br/>    mapping_file = os.path.join(folder, 'mapping.bin')<br/>    <br/>    image_transforms = []<br/>    <br/>    if train:<br/>        <strong>image_transforms += [</strong><br/><strong>            xforms.crop(crop_type='randomside', side_ratio=0.8),</strong><br/><strong>            xforms.scale(width=28, height=28, channels=3, interpolations='linear')</strong><br/><strong>        ]</strong><br/>            <br/>    stream_definitions = StreamDefs(<br/>        features=StreamDef(field='image', <strong>transforms=image_transforms</strong>),<br/>        labels=StreamDef(field='label', shape=10)<br/>    )<br/>    <br/>    deserializer = ImageDeserializer(mapping_file, stream_definitions)<br/>    <br/>    return MinibatchSource(deserializer, max_sweeps=max_sweeps)<br/><br/></pre>

<p>在训练过程中使用这些变换将增加训练数据的变化，从而减少神经网络在颜色、旋转或大小略有不同的图像上卡住的机会。</p>

<p>但是，请注意，这些变换不会生成新的样本。他们只是在数据输入训练器之前改变数据。您可能希望增加最大历元数，以便在应用这些变换时生成足够多的随机样本。你需要多少额外的训练将取决于你的数据集的大小。</p>

<p>记住输入层和中间层的尺寸对卷积网络的性能有很大影响，这一点也很重要。当你想检测小物体时，大的图像自然会更好。将图像缩放回小得多的尺寸会使较小的对象消失或丢失太多细节而无法被网络识别。</p>

<p>然而，支持更大图像的卷积网络将需要更多的计算能力来优化，因此需要更长的时间来训练它们，并且更难获得最佳结果。</p>

<p>最终，您将需要平衡图像大小、图层尺寸和使用的数据扩充的组合，以获得最佳结果。</p>

<p>摘要</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Summary</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:f69ed529-6576-4ff7-a760-3ddf81ca9def" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">在这一章中，我们已经学习了使用神经网络对图像进行分类。这与处理正常数据非常不同。我们不仅需要更多的训练数据来获得正确的结果，我们还需要不同的架构来处理更适合这项工作的图像。</h1>

                

            

            

                

<p>我们已经了解了如何使用卷积层和池层来创建高级照片过滤器，该过滤器从数据中提取重要的细节并汇总这些细节，以将输入的维度减少到可管理的大小。</p>

<p>一旦我们使用了卷积过滤器和池过滤器的高级属性，就可以像往常一样使用密集图层生成分类网络了。</p>

<p>为图像分类模型提出一个好的结构可能相当困难，所以在冒险进入图像分类之前检查一个现有的架构总是一个好主意。此外，使用正确的增强技术可以帮助获得更好的性能。</p>

<p>处理图像只是深度学习强大的场景之一。在下一章中，我们将研究如何使用深度学习来训练时间序列数据的模型，如股票交易信息，或比特币等事物的课程信息。我们将学习如何在CNTK中使用序列，以及如何构建一个可以随着时间推移进行推理的神经网络。下一章见。</p>

<p>Working with images is just one of the scenarios where deep learning is powerful. In the next chapter, we'll look at how to use deep learning to train models on time-series data, such as stock exchange information, or course information for things such as Bitcoin. We'll learn how to use sequences in CNTK and how to build a neural network that can reason over time. See you in the next chapter.</p>





            



            

        

    </body>



</html></body></html>