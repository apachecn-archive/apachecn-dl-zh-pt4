<html><head/><body>


    
        <title>Training a Single Neuron</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    Training a Single Neuron
                
            
            
                
<p class="mce-root">在修改了关于从数据中学习的概念之后，我们现在将密切关注一种算法，该算法训练一种最基本的基于神经的模型:感知机。我们将看看算法运行所需的步骤，以及停止条件。本章将介绍感知器模型，它是第一个代表神经元的模型，旨在以简单的方式从数据中学习。感知器模型是理解从数据中学习的基本和高级神经模型的关键。在这一章中，我们还将讨论与非线性可分数据相关的问题和注意事项。</p>
<p>完成这一章后，你应该对讨论感知器模型和应用它的学习算法感到舒服了。您将能够在线性和非线性可分离数据上实现该算法。</p>
<p>具体来说，本章涵盖以下主题:</p>
<ul>
<li>感知器模型</li>
<li>感知器学习算法</li>
<li>基于非线性可分数据的感知器</li>
</ul>
<h1 id="uuid-80afeaef-4aa6-436b-8bf2-a322fedeac3c">感知器模型</h1>
<p>回到<a href="e3181710-1bb7-4069-825a-a235355bc116.xhtml">第一章</a>、<em>机器学习介绍</em>，我们简单介绍了一个神经元的基本模型和<strong>感知器学习算法</strong> ( <strong> PLA </strong>)。在这一章中，我们将重温和扩展这个概念，并展示如何用Python来编码。我们将从基本定义开始。</p>
<h2 id="uuid-491b7962-497d-43fa-98a2-b1d05e5afcc3">视觉概念</h2>
<p>感知器是对人类启发的信息处理单元的类比，最初由F. Rosenblatt构想，并在<em>图5.1 </em>中描述(Rosenblatt，F. (1958))。在该模型中，输入用向量<img class="fm-editor-equation" src="img/dd2553bf-c673-4ade-b77f-c9edb436ed12.png" style="width:0.92em;height:1.00em;"/>表示，神经元的激活由函数<img class="fm-editor-equation" src="img/018df1a9-86c5-40ef-afd0-12da768026e0.png" style="width:2.33em;height:1.75em;"/>给出，输出为<img class="fm-editor-equation" src="img/e9f481bc-0639-4987-8be9-75dd4d377193.png" style="width:0.58em;height:1.00em;"/>。神经元的参数是<img class="fm-editor-equation" src="img/d16c70d8-9f05-4c25-91f9-61fa60a89974.png" style="width:1.17em;height:0.92em;"/>和<img class="fm-editor-equation" src="img/430e33f2-b6a0-44d0-b21a-c16e32da02b8.png" style="width:0.42em;height:0.83em;"/>:</p>
<div><img src="img/bac392e3-087b-485d-aa22-287a161238ce.png" style="width:34.50em;height:18.42em;"/></div>
<p>图5.1-感知器的基本模型</p>
<p>感知器的<em>可训练</em>参数为<img src="img/b7dce5c4-c06a-4c49-99ac-44a00c3dde4a.png" style="width:2.92em;height:1.33em;"/>，未知。因此，我们可以使用输入训练数据<img class="fm-editor-equation" src="img/d22b2bbd-59f3-4f40-a4ab-6df6e95bf1ff.png" style="width:6.75em;height:1.33em;"/>通过PLA来确定这些参数。从<em>图5.1 </em> , <img class="fm-editor-equation" src="img/5ebb08ed-23c4-4a97-ad97-192adaf9fa16.png" style="width:1.58em;height:1.17em;"/> <sub> </sub>乘以<img class="fm-editor-equation" src="img/590e73b6-a479-4c5c-9285-4551930c7b91.png" style="width:1.83em;height:1.17em;"/>，然后<img class="fm-editor-equation" src="img/8b81f157-95ad-4b92-9977-3c2f3500a544.png" style="width:1.58em;height:1.17em;"/> <sub> </sub>乘以<img class="fm-editor-equation" src="img/67beef13-3471-40ef-869e-0c5009fa2a91.png" style="width:1.83em;height:1.17em;"/>，<img class="fm-editor-equation" src="img/d0b9f56d-d74f-40fe-9160-7c65b95b390b.png" style="width:0.67em;height:1.33em;"/>乘以1；所有这些乘积相加，然后传递到<em>符号</em>激活函数，该函数在感知器中的操作如下:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="img/25c4e4bc-29d9-4ee4-8ff0-0d8ada66ccb1.png" style="width:21.42em;height:3.08em;"/></p>
<p>激活标志的主要作用是将模型的任何响应映射成一个二进制输出:<sub> <img class="fm-editor-equation" src="img/c2b8debd-da54-4f91-82aa-f673eacf1584.png" style="width:3.50em;height:1.00em;"/> </sub>。</p>
<p class="mce-root">现在我们来谈谈一般意义上的张量。</p>
<p class="mce-root"/>
<h2 id="uuid-c03577ac-f6d5-4781-a8a7-893c5d4b0b8a">张量运算</h2>
<p>在Python中，感知器的实现需要一些简单的张量(向量)运算，这些运算可以通过标准的NumPy功能来执行。首先我们可以假设给我们的数据<sub> <img class="fm-editor-equation" src="img/0d85ebf4-d57f-4fd1-88ee-e155345e4c6f.png" style="width:6.83em;height:1.33em;"/> </sub>是一个向量的形式，包含多个向量<img class="fm-editor-equation" src="img/59afade2-2430-4eba-b0c8-28c69956ec56.png" style="width:0.75em;height:0.83em;"/>(一个矩阵)，表示为<sub> <img class="fm-editor-equation" src="img/585f7ba2-8841-4a53-8431-2d1be8d31af2.png" style="width:5.25em;height:1.25em;"/> </sub>，多个单独的目标表示为一个向量<sub> <img class="fm-editor-equation" src="img/b7e3a027-88db-4c9b-ad29-639468ea172f.png" style="width:5.17em;height:1.33em;"/> </sub>。但是，请注意，为了更容易实现感知器，有必要将<img class="fm-editor-equation" src="img/3b0fa764-dbef-4a77-a092-47b93447c9ed.png" style="width:0.50em;height:1.00em;"/>包含在<img class="fm-editor-equation" src="img/7d70cc37-fb85-4d34-99b5-37b13204746e.png" style="width:0.83em;height:0.67em;"/>中，如图5.1 中的<em>所示，这样，如果我们将<img class="fm-editor-equation" src="img/5c464c60-6998-4884-9f1d-38fbca822376.png" style="width:0.83em;height:0.92em;"/>修改为<img src="img/5fc1e66a-27b1-4b71-a1f9-e19233e0ffd6.png" style="width:10.08em;height:1.25em;"/>，将<img class="fm-editor-equation" src="img/71f4dc57-8177-4f50-b003-612501ff1d00.png" style="width:1.08em;height:0.83em;"/>修改为<sub> <img class="fm-editor-equation" src="img/c6fb2cba-f4dc-4519-8728-4d7bfb626352.png" style="width:10.08em;height:1.17em;"/> </sub>，则<sub> <img class="fm-editor-equation" src="img/55992dcb-f61a-4702-9339-fb13e815fdec.png" style="width:17.83em;height:1.17em;"/> </sub>中的乘积和加法可以简化。这样，对于输入<sub> <img class="fm-editor-equation" src="img/e8377d7f-1b36-43b3-882d-751ea830c783.png" style="width:9.58em;height:1.17em;"/> </sub>的感知器响应可以简化如下:</em></p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="img/e56c3f72-ce08-4071-9aac-2104e2d42362.png" style="width:17.42em;height:3.08em;"/></p>
<p>注意<strong> <img class="fm-editor-equation" src="img/6d483edb-3362-4650-a2ef-4ad86675a0ae.png" style="width:0.50em;height:1.00em;"/> </strong>现在隐含在<img class="fm-editor-equation" src="img/e75323b6-725a-475c-9fe5-29dd9a0a45dc.png" style="width:1.08em;height:0.83em;"/>中。</p>
<p>说我们要有训练数据<kbd>X</kbd>，需要为感知器准备；我们可以使用一个简单的线性可分数据集来实现这一点，该数据集可以通过scikit-learn的dataset方法<kbd>make_classification</kbd>生成，如下所示:</p>
<pre>from sklearn.datasets import <strong>make_classification</strong><br/><br/>X, y = <strong>make_classification</strong>(n_samples=100, n_features=2, n_classes=2,<br/>                           n_informative=2, n_redundant=0, n_repeated=0,<br/>                           n_clusters_per_class=1, class_sep=1.5, <br/>                           random_state=5)</pre>
<p>在这里，我们使用<kbd>make_classification</kbd>构造函数为两个类(<kbd>n_classes</kbd>)产生100个数据点(<kbd>n_samples</kbd>)，并且有足够的间隔(<kbd>class_sep</kbd>)来产生线性可分的数据。但是数据集在集合<sub> <img class="fm-editor-equation" src="img/5fbae48a-781e-4d49-8899-6cefba685f19.png" style="width:2.25em;height:1.08em;"/> </sub>中的<kbd>y</kbd>产生了二进制值，我们需要将其转换为集合<sub> <img class="fm-editor-equation" src="img/db866a8e-aee2-419a-9953-74d93d142c8d.png" style="width:3.25em;height:0.92em;"/> </sub>中的值。这可以通过简单地执行以下操作，用负目标替换零目标来容易地实现:</p>
<pre>y[y==0] = -1</pre>
<p>生成的数据集如图<em>图5.2 </em>所示:</p>
<div><img src="img/7fe6eb0e-e3ec-42a1-a4f3-7e9aa99ab418.png" style="width:29.17em;height:20.50em;"/></div>
<p>图5.2–感知器测试的二维数据样本</p>
<p>接下来，我们可以通过将长度为<kbd>N=100</kbd>的一个向量添加到<kbd>X</kbd>来将数字1添加到每个输入向量，如下所示:</p>
<pre>import numpy as np<br/>X = np.append(np.ones((N,1)), X, 1)    </pre>
<p><kbd>X</kbd>中的新数据现在包含一个1的向量。这将允许更容易地计算所有<img src="img/f58f01a4-07d4-4577-aabe-6f1b9e92ce78.png" style="width:6.25em;height:1.00em;"/>的张量运算<sub> <img class="fm-editor-equation" src="img/f401a345-a15c-485c-8636-1fefe31ec8d7.png" style="width:2.50em;height:1.25em;"/> </sub>。将矩阵<img src="img/3964917c-2a69-4948-b032-f55e473de016.png" style="width:8.92em;height:1.50em;"/>简化为<img src="img/caa93965-f1c5-4721-8829-197f116857b1.png" style="width:8.58em;height:1.67em;"/>，这种常见的张量运算可以在一个单一步骤中完成。我们甚至可以将此操作和符号激活功能合并在一个步骤中，如下所示:</p>
<pre>np.sign(<strong>w.T</strong>.dot(<strong>X</strong>[n]))</pre>
<p>这相当于数学张量运算<sub> <img class="fm-editor-equation" src="img/3ac75eb8-972d-46c9-9647-b8cda8237618.png" style="width:4.83em;height:1.25em;"/> </sub>。记住这一点，让我们使用前面介绍的数据集和刚才描述的操作来更详细地回顾一下PLA。</p>
<p class="mce-root"/>
<h1 id="uuid-dc5d10ae-c185-4fae-801f-d51fa03c9b0e">感知器学习算法</h1>
<p class="mce-root"><strong>感知器学习算法</strong> ( <strong> PLA </strong>)如下:</p>
<p><strong>输入</strong>:二进制类数据集<sub> <img src="img/d22b2bbd-59f3-4f40-a4ab-6df6e95bf1ff.png" style="width:7.25em;height:1.42em;"/> </sub></p>
<ul>
<li>将<img style="font-size: 1em;color: black;width:1.08em;height:0.83em;" src="img/e75323b6-725a-475c-9fe5-29dd9a0a45dc.png"/>初始化为零，并迭代计数器<img style="font-size: 1em;color: black;width:2.75em;height:1.00em;" src="img/467437b4-17a7-42a3-af49-cd3cc8b8816e.png"/></li>
<li>虽然有任何错误分类的例子:</li>
<li style="padding-left: 30px">挑一个分类错误的例子，称之为<img src="img/01a87dc1-7f59-4c55-9c52-a1803ae95db7.png" style="width:1.08em;height:1.00em;"/>，其真正的标签是<img src="img/16eb7743-58d1-4037-ae7f-67757849756f.png" style="width:1.00em;height:1.33em;"/></li>
<li style="padding-left: 30px">更新<img src="img/e75323b6-725a-475c-9fe5-29dd9a0a45dc.png" style="width:1.17em;height:0.92em;"/>如下:<img src="img/cfa7a1f4-17cd-4f28-b07e-f17fbac38691.png" style="width:8.00em;height:1.17em;"/></li>
<li style="padding-left: 30px">增加迭代计数器，<sub> <img src="img/99f0af99-2238-4552-9903-2580096b8378.png" style="width:3.58em;height:0.83em;"/> </sub>，并重复</li>
</ul>
<p class="mce-root"><strong>返回</strong> : <img src="img/8ca08b66-f0e6-466a-bd50-d4d584c2cf06.png" style="width:1.25em;height:1.00em;"/></p>
<p class="mce-root">现在，让我们看看这是如何在Python中实现的。</p>
<h2 id="uuid-77b681c9-e5c7-413d-b02b-7cd41f6898b1">Python中的PLA</h2>
<p>下面是Python中的一个实现，我们将一部分一部分地讨论，其中一些已经讨论过了:</p>
<pre>N = 100 # number of samples to generate<br/>random.seed(a = 7) # add this to achieve for reproducibility<br/><br/>X, y = make_classification(n_samples=N, n_features=2, n_classes=2,<br/>                           n_informative=2, n_redundant=0, n_repeated=0,<br/>                           n_clusters_per_class=1, class_sep=1.2, <br/>                           random_state=5)<br/><br/>y[y==0] = -1<br/><br/>X_train = np.append(np.ones((N,1)), X, 1) # add a column of ones<br/><br/># initialize the weights to zeros<br/>w = np.zeros(X_train.shape[1])<br/>it = 0<br/><br/># Iterate until all points are correctly classified<br/>while classification_error(w, X_train, y) != 0:<br/>  it += 1<br/>  # Pick random misclassified point<br/>  x, s = choose_miscl_point(w, X_train, y)<br/>  # Update weights<br/>  w = w + s*x<br/><br/>print("Total iterations: ", it)</pre>
<p>本章的<em>张量运算</em>部分已经讨论了前几行。使用<kbd>w = np.zeros(X_train.shape[1])</kbd>将<img class="fm-editor-equation" src="img/e75323b6-725a-475c-9fe5-29dd9a0a45dc.png" style="width:1.08em;height:0.83em;"/>初始化为零。这个向量的大小取决于输入的维度。然后，<kbd>it</kbd>仅仅是一个迭代计数器，用于跟踪在PLA收敛之前执行的迭代次数。</p>
<p><kbd>classification_error()</kbd>方法是一个助手方法，它将参数<kbd>w</kbd>的当前向量、输入数据<kbd>X_train</kbd>和相应的目标数据<kbd>y</kbd>作为参数。该方法的目的是确定当前状态<img class="fm-editor-equation" src="img/e75323b6-725a-475c-9fe5-29dd9a0a45dc.png" style="width:1.08em;height:0.83em;"/>下误分类点的数量，如果有，返回错误总数。该方法可以定义如下:</p>
<pre>def classification_error(w, X, y):<br/>  err_cnt = 0<br/>  N = len(X)<br/>  for n in range(N):<br/>    s = np.sign(w.T.dot(X[n]))<br/>    if y[n] != s:<br/>      err_cnt += 1    # we could break here on large datasets<br/>  return err_cnt      # returns total number of errors</pre>
<p>这种方法可以简化如下:</p>
<pre>def classification_error(w, X, y):<br/>  s = np.sign(X.dot(w))<br/>  return sum(s != y)</pre>
<p>然而，虽然这对于小数据集来说是一个很好的优化，但是对于大数据集来说，可能没有必要计算所有的误差点。因此，可以使用第一种(也是更长的)方法，并根据预期的数据类型进行修改，如果我们知道我们将处理大型数据集，我们可以在第一次出现错误时中断该方法。</p>
<p class="mce-root"/>
<p>我们代码中的第二个助手方法是<kbd>choose_miscl_point()</kbd>。该方法的主要目的是随机选择一个错误分类的点，如果有的话。它将参数<kbd>w</kbd>的当前向量、输入数据<kbd>X_train</kbd>和相应的目标数据<kbd>y</kbd>作为自变量。它返回一个错误分类的点<kbd>x</kbd>，以及对应的目标符号应该是什么<kbd>s</kbd>。该方法可以如下实现:</p>
<pre>def choose_miscl_point(w, X, y):<br/>  mispts = []<br/>  for n in range(len(X)):<br/>    if np.sign(w.T.dot(X[n])) != y[n]:<br/>      mispts.append((X[n], y[n]))<br/>  return mispts[random.randrange(0,len(mispts))]</pre>
<p>类似地，这可以通过随机化索引列表、遍历它们并返回找到的第一个索引来优化速度，如下所示:</p>
<pre>def choose_miscl_point(w, X, y):<br/> for idx in random.permutation(len(X)):<br/>   if np.sign(w.T.dot(X[idx])) != y[idx]:<br/>     return X[idx], y[idx]</pre>
<p>然而，第一个实现对于绝对的初学者或者那些想要对错误分类的点进行一些额外分析的人来说可能是有用的，这可以在列表<kbd>mispts</kbd>中方便地获得。</p>
<p>不管实现如何，关键点是随机选择错误分类的点。</p>
<p>最后，使用当前参数、错误分类点和执行<kbd>w = w + s*x</kbd>的行上的相应目标进行更新。</p>
<p>如果您运行完整的程序，它应该输出如下内容:</p>
<pre>Total iterations: 14</pre>
<p>迭代的总次数可以根据数据的类型和误分类点选择的随机性质而变化。对于我们正在使用的特定数据集，决策边界可能如图<em>图5.3 </em>所示:</p>
<div><img src="img/61a2413b-086d-4458-bab7-0433588aa938.png" style="width:29.17em;height:20.50em;"/></div>
<p>图5.3–用计划发现的决策边界</p>
<p>迭代的次数也将取决于特征空间中数据点之间的间隔或间隙。差距越大，越容易找到解决办法，反之亦然。最坏的情况是数据是非线性可分的，我们接下来会谈到这一点。</p>
<h1 id="uuid-1eef7b49-70a7-446b-a7a3-45e38c840556">基于非线性可分数据的感知器</h1>
<p>正如我们之前讨论的，如果数据是可分的，感知器会在有限的时间内找到解决方案。然而，找到一个解需要多少次迭代取决于这些组在特征空间中彼此的接近程度。</p>
<div><strong>Convergence</strong> is when the learning algorithm finds a solution or reaches a steady state that is acceptable to the designer of the learning model.</div>
<p class="mce-root">以下段落将讨论不同类型数据的收敛性:线性可分数据和非线性可分数据。</p>
<p>线性可分数据的收敛性</p>
<h2 id="uuid-a059f10b-9545-41d7-a659-0ad0c03a0b15">对于我们在本章中学习的特定数据集，两组数据之间的分离是一个可以变化的参数(这通常是真实数据的一个问题)。参数为<kbd>class_sep</kbd>，可以取实数；例如:</h2>
<p>这使我们能够研究，如果我们改变分离参数，感知器算法平均需要多少次迭代才能收敛。实验可以设计如下:</p>
<pre>X, y = make_classification(..., <strong>class_sep</strong>=2.0, ...)</pre>
<p>我们将从大到小改变分离系数，记录收敛所需的迭代次数:2.0，1.9，..., 1.2, 1.1.</p>
<ul>
<li>我们将重复1000次，并记录平均迭代次数和相应的标准偏差。</li>
<li>请注意，我们决定将这个实验运行到1.1，因为1.0已经产生了一个非线性可分离的数据集。如果我们进行实验，我们可以将结果记录在一个表格中，结果如下所示:</li>
</ul>
<p><strong>跑</strong></p>
<table style="border-collapse: collapse;width: 100%" border="1">
<tbody>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p><strong> 2.0 </strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong> 1.9 </strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong> 1.8 </strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong> 1.7 </strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong> 1.6 </strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong> 1.5 </strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong> 1.4 </strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong> 1.3 </strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong> 1.2 </strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong> 1.1 </strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>一</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p>2</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>2</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>2</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>2</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>七</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>10</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>四</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>15</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>13</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>86</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>2</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p>5</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>一</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>2</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>2</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>四</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>8</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>6</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>26</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>62</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>169</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>3</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p>四</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>四</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>5</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>6</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>6</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>10</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>11</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>29</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>27</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>293</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>...</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p>...</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>...</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>...</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>...</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>...</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>...</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>...</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>...</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>...</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>...</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>998</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p>2</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>5</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>3</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>一</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>9</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>3</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>11</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>9</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>35</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>198</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>紧急服务电话</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p>2</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>2</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>四</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>七</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>6</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>8</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>2</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>四</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>14</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>135</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>1000</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p>2</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>一</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>2</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>2</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>2</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>8</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>13</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>25</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>27</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>36</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>平均。</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p><strong> 2.79 </strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong> 3.05 </strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong> 3.34 </strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong> 3.67 </strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong> 4.13 </strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong> 4.90 </strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong> 6.67 </strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong> 10.32 </strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong> 24.22 </strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong> 184.41 </strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong>标准。</strong></p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p><strong> 1.2 </strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong> 1.3 </strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong> 1.6 </strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong> 1.9 </strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong> 2.4 </strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong> 3.0 </strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong> 4.7 </strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong> 7.8 </strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong> 15.9 </strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong> 75.5 </strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong>75.5</strong></p>
</td>
</tr>
</tbody>
</table>
<p> </p>
<p class="mce-root"/>
<p class="mce-root">该表显示，当数据被很好地分离时，平均迭代次数相当稳定；然而，随着分离间隙的减小，迭代次数急剧增加。为了直观起见，表中的相同数据现在以对数标度显示在<em>图5.4 </em>中:</p>
<p class="mce-root">图5.4–随着数据组越来越接近，PLA迭代次数的增长</p>
<div><img src="img/6f74dcd4-9806-4fd8-b92b-035c1de18d94.png" style="width:26.50em;height:18.50em;"/></div>
<p>很明显，随着分离间隙的缩小，迭代次数会呈指数增长。<em>图5.5 </em>描绘了最大分离间隙2.0，表明PLA在四次迭代后找到了解决方案:</p>
<p>图5.5–感知器在4次迭代中找到了分离间隙为2.0的解决方案</p>
<div><img src="img/5364997e-8957-4e62-9ba5-d183b96c0dec.png" style="width:26.50em;height:18.67em;"/></div>
<p>Figure 5.5 – The perceptron found a solution in four iterations for a separation gap of 2.0</p>
<p class="mce-root">同样，<em>图5.6 </em>显示，对于最大的缺口，1.1，PLA需要183次迭代；仔细观察该图可以发现，后一种情况的解决方案很难找到，因为数据组彼此过于接近:</p>
<p>图5.6–感知器在183次迭代中找到了分离间隙为1.1的解决方案</p>
<div><img src="img/cd93c7a8-7a43-491c-b7d6-f61bcbc8b8ef.png" style="width:29.17em;height:20.50em;"/></div>
<p>如前所述，不可线性分离的数据可以产生1.0的间隙，并且PLA将在无限循环中运行，因为总会有数据点被错误分类，并且<kbd>classification_error()</kbd>方法永远不会返回零值。对于这些情况，我们可以修改PLA以允许在非线性可分数据上寻找解决方案，我们将在下一节中讨论。</p>
<p>非线性可分数据的收敛性</p>
<h2 id="uuid-6e6c20fb-1ed1-410d-9f23-27adf0dfb696">对原始PLA的修改相当简单，但足以在大多数情况下找到可接受的解决方案。我们需要添加到PLA中的两个主要内容如下:</h2>
<p class="mce-root">防止算法永远运行的机制</p>
<ul>
<li>一种存储已发现的最佳解决方案的机制</li>
<li>关于第一点，我们可以简单地指定算法可以停止的迭代次数。关于第二点，我们可以简单地保存一个解决方案，并将其与当前迭代中的解决方案进行比较。</li>
</ul>
<p class="mce-root">With respect to the first point, we can simply specify a number of iterations at which the algorithm can stop. With respect to the second point, we can simply keep a solution in storage, and compare it to the one in the current iteration.</p>
<p class="mce-root">此处显示了PLA的相关部分，新的更改用粗体标出，我们将对此进行详细讨论:</p>
<p>在这段代码中，<kbd>bestW</kbd>是一个记录目前为止最佳结果的字典，它被初始化为合理的值。首先请注意，循环现在被数字1，000所限制，这是您当前允许的最大迭代次数，您可以将其更改为您希望的最大迭代次数。对于每次迭代成本都很高的大型数据集或高维数据集，减少这个数量是合理的。</p>
<pre>X, y = make_classification(n_samples=N, n_features=2, n_classes=2,<br/> n_informative=2, n_redundant=0, n_repeated=0,<br/> n_clusters_per_class=1, class_sep=1.0, <br/> random_state=5)<br/><br/>y[y==0] = -1<br/><br/>X_train = np.append(np.ones((N,1)), X, 1) # add a column of ones<br/><br/># initialize the weights to zeros<br/>w = np.zeros(X_train.shape[1])<br/>it = 0<br/><strong>bestW = {}<br/>bestW['err'] = N + 1 # dictionary to keep best solution</strong><br/><strong>bestW['w'] = []</strong><br/><strong>bestW['it'] = it</strong><br/><br/># Iterate until all points are correctly classified<br/>#   or maximum iterations (i.e. 1000) are reached<br/><strong>while it &lt; 1000:</strong><br/><strong>  err = classification_error(w, X_train, y)</strong><br/><strong>  if err &lt; bestW['err']:   # enter to save a new w</strong><br/><strong>    bestW['err'] = err</strong><br/><strong>    bestW['it'] = it</strong><br/><strong>    bestW['w'] = list(w)</strong><br/><strong>  if err == 0:  # exit loop if there are no errors</strong><br/><strong>    break</strong><br/>  it += 1<br/>  # Pick random misclassified point<br/>  x, s = choose_miscl_point(w, X_train, y)<br/>  # Update weights<br/>  w += s*x<br/><br/><strong>print("Best found at iteration: ", bestW['it'])</strong><br/><strong>print("Number of misclassified points: ", bestW['err'])</strong></pre>
<p>接下来的变化是包含了条件语句<kbd>if err &lt; bestW['err']</kbd>，它决定了我们是否应该存储一组新的参数。每当由错误分类样本的总数确定的误差低于存储参数的误差时，就进行更新。为了完成，我们仍然必须检查没有错误，这表明数据是线性可分的，已经找到了解决方案，循环需要终止。</p>
<p class="mce-root">最后几个<kbd>print</kbd>语句将简单地告知记录最佳解决方案时获得的迭代和误差。输出可能如下所示:</p>
<p>该输出是通过在数据集上运行更新的PLA产生的，间隔为1.0，如图<em>图5.7 </em>所示:</p>
<p>图5.7-更新的PLA在95次迭代后找到了一个只有一个错误分类点的解决方案</p>
<pre>Best found at iteration: 95<br/>Number of misclassified points: 1</pre>
<p>从图中可以看出，有一个来自阳性类别的样本被错误地分类。知道在这个例子中总共有100个数据点，我们可以确定精度是99/100。</p>
<div><img src="img/2e1b5cd4-6720-4c62-bd35-ec5bdf23c214.png" style="width:29.17em;height:20.17em;"/></div>
<p>这种类型的算法存储了迄今为止的<em>最佳解</em>，通常被称为<strong>袖珍算法</strong> (Muselli，M. 1997)。并且学习算法的提前终止的想法受到了众所周知的数值优化方法的启发。</p>
<p>From the figure, it can be seen that there is one sample from the positive class that is incorrectly classified. Knowing that in this example there is a total of 100 data points, we can determine that the accuracy is 99/100.</p>
<p>一个普遍的限制是，感知器只能产生基于二维直线或多维线性超平面的解决方案。然而，这种限制可以通过将几个感知器放在一起并放在多个层中来容易地解决，以产生用于可分离和不可分离问题的高度复杂的非线性解决方案。这将是下一章的主题。</p>
<p class="mce-root">摘要</p>
<p>本章概述了经典的感知器模型。我们讨论了线性和非线性可分离数据集的理论模型及其Python实现。在这一点上，你应该感到自信，你对感知机有足够的了解，你可以自己实现它。您应该能够在神经元的环境中识别感知器模型。此外，您现在应该能够在感知器中实现口袋算法和提前终止策略，或者任何其他一般的学习算法。</p>
<h1 id="uuid-ac714f90-15bd-4fd8-80f6-1e6fe32aeb48">由于感知器是为深度神经网络铺平道路的最重要的元素，在我们在这里讲述完之后，下一步是前往第六章<a href="a6dd89cc-54bd-454d-8bea-7dd4518e85b0.xhtml"/>，<em>训练多层神经元。</em>在该章中，您将接触到使用多层感知器算法进行深度学习的挑战，例如用于误差最小化的梯度下降技术，以及实现泛化的超参数优化。但是在你去那里之前，请试着用下面的问题来测验你自己。</h1>
<p>问题和答案</p>
<p><strong>数据的可分性与PLA的迭代次数有什么关系？</strong></p>
<h1 id="uuid-ebd70e9d-ce6b-4079-91d8-f944eb4fcf4f">随着数据组彼此接近，迭代次数会呈指数增长。</h1>
<ol>
<li><strong>解放军会一直收敛吗？</strong></li>
</ol>
<p style="padding-left: 60px">并不总是这样，只是对于线性可分的数据。</p>
<ol start="2">
<li><strong>PLA能收敛于非线性可分数据吗？</strong></li>
</ol>
<p style="padding-left: 60px">不会。但是，您可以通过修改pocket算法找到一个可接受的解决方案。</p>
<ol start="3">
<li><strong>为什么感知器很重要？</strong></li>
</ol>
<p style="padding-left: 60px">因为这是最基本的学习策略之一，有助于理解学习的可能性。如果没有感知器，科学界可能需要更长时间才能意识到基于计算机的自动学习算法的潜力。</p>
<ol start="4">
<li>参考</li>
</ol>
<p style="padding-left: 60px">罗森布拉特，F. (1958)。感知器:大脑中信息存储和组织的概率模型。<em>心理评论</em>，65卷6期，386页。</p>
<h1 id="uuid-4db8c86d-3a1c-4d65-90dc-ff4b29e5519f">穆塞利博士(1997年)。pocket算法的收敛性。<em> IEEE神经网络汇刊</em>，8(3)，623-629。</h1>
<ul>
<li>Rosenblatt, F. (1958). The perceptron: a probabilistic model for information storage and organization in the brain. <em>Psychological review</em>, 65(6), 386.</li>
<li>Muselli, M. (1997). On convergence properties of the pocket algorithm. <em>IEEE Transactions on Neural Networks</em>, 8(3), 623-629.</li>
</ul>


            

            
        
    


</body></html>