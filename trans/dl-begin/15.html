<html><head/><body>


    
        <title>Convolutional Neural Networks</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    Convolutional Neural Networks
                
            
            
                
<p class="mce-root">本章介绍了卷积神经网络，从卷积运算开始，向前移动到卷积运算的集合层，目的是学习对数据集进行操作的过滤器。然后引入池化策略来展示这种变化如何能够改进模型的训练和性能。本章最后展示了如何可视化学习到的过滤器。</p>
<p>到本章结束时，你将熟悉卷积神经网络背后的动机，并将知道卷积运算在一维和二维中是如何工作的。当你读完这一章，你将知道如何在层中实现卷积，以便通过梯度下降学习过滤器。最后，您将有机会使用您之前学习的许多工具，包括dropout和batch normalization，但是现在您将知道如何使用pooling作为一种替代方法来减少问题的维度并创建信息抽象级别。</p>
<p>本章组织如下:</p>
<ul>
<li>卷积神经网络简介</li>
<li><em> n </em>中的卷积-尺寸</li>
<li>卷积层</li>
<li>集中策略</li>
<li>过滤器的可视化</li>
</ul>
<h1 id="uuid-ea3fed3c-f130-4a77-9cce-5501df0070f9">卷积神经网络简介</h1>
<p>之前，在《T2》第11章、<em>深度和广度神经网络</em>中，我们使用了一个对通用网络来说非常具有挑战性的数据集。然而，<strong>卷积神经网络</strong>(<strong>CNN</strong>)将被证明是更有效的，正如你将看到的。CNN从80年代后期就已经存在了(LeCun，y .等人(1989))。他们改变了计算机视觉和音频处理的世界(李，Y. D .等人(2016))。如果你的智能手机有某种基于人工智能的物体识别功能，它很可能使用某种CNN架构；例如:</p>
<ul>
<li>图像中物体的识别</li>
<li>数字指纹的识别</li>
<li>语音命令的识别</li>
</ul>
<p>CNN很有趣，因为它们解决了计算机视觉中一些最具挑战性的问题，包括在一个名为ImageNet的图像识别问题上击败人类(Krizhevsky，a .，et al. (2012) <em> ) </em>。如果你能想到最复杂的物体识别任务，CNN应该是你实验的首选:它们永远不会让人失望！</p>
<p>CNN成功的关键在于它们独特的编码空间关系的能力。如果我们对比两个不同的数据集，一个关于学生学校记录，包括当前和过去的成绩、出勤、在线活动等，另一个关于猫和狗的图像，如果我们的目标是对学生或猫和狗进行分类，数据是不同的。其中一个是没有空间关系的学生特征。</p>
<p>举个例子，如果等级是第一特征，考勤不一定要挨着，所以两者的位置可以互换，分类表现应该不受影响吧？然而，对于猫和狗的图像，眼睛的特征(像素)必须靠近鼻子或耳朵；当您更改空间特征并观察两只眼睛中间的一只耳朵(奇怪)时，分类器的性能应该会受到影响，因为通常没有猫或狗的耳朵在两只眼睛之间。这是CNN擅长编码的空间关系类型。你也可以想到音频或语音处理。你知道，在某些单词中，有些音必须跟在其他音之后。如果数据集允许空间关系，CNN有潜力表现良好。</p>
<h1 id="uuid-2b9d4f72-5755-499a-be07-74dc15597084">n维卷积</h1>
<p class="mce-root">CNN的名字来源于它们的招牌运算:<strong>卷积</strong>。该运算是信号处理领域中非常常见的数学运算。让我们继续讨论卷积运算。</p>
<h2 id="uuid-f8ae0f2b-e5e9-4c2e-b104-e6787dd01cee">一维</h2>
<p>让我们从一维的离散时间卷积函数开始。假设我们有输入数据<img class="fm-editor-equation" src="img/42ad14d7-4801-41de-8544-14d064f7699d.png" style="width:3.33em;height:1.00em;"/>和一些权重<img class="fm-editor-equation" src="img/e36d108f-f5b2-4205-bfb8-f85808f76dcd.png" style="width:3.83em;height:1.00em;"/>，我们可以定义两者之间的离散时间卷积运算如下:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="img/e5873a07-cbe5-4f7c-ba38-1088178ab472.png" style="width:22.67em;height:3.83em;"/>。</p>
<p>在该等式中，卷积运算由符号<strong> * </strong>表示。不用把事情弄得太复杂，我们可以说<img class="fm-editor-equation" src="img/8f014c06-f6a8-402e-9f00-0e79616523e0.png" style="width:1.08em;height:0.83em;"/>反过来，<img class="fm-editor-equation" src="img/a2dbdcc5-92fb-4538-b80d-949e4edb4024.png" style="width:3.00em;height:1.17em;"/>，再移位，<img class="fm-editor-equation" src="img/3cfe2848-09e8-4fb0-9069-e4c711729d34.png" style="width:3.42em;height:1.00em;"/>。产生的向量是<img class="fm-editor-equation" src="img/781cc345-61b4-4a24-936a-8aeba31cd79c.png" style="width:5.08em;height:1.00em;"/>，当应用过滤器<img class="fm-editor-equation" src="img/4dea30d8-7115-4bcc-8634-64707a585ecd.png" style="width:1.17em;height:0.92em;"/>时，它可以被解释为输入的<em>过滤的</em>版本。</p>
<p>如果我们如下定义两个向量，<img class="fm-editor-equation" src="img/667d186e-3d70-423b-a31b-05a9832f428a.png" style="width:4.50em;height:1.08em;"/>和<img class="fm-editor-equation" src="img/04ea475a-0074-4c06-b144-ed21e87c8651.png" style="width:6.08em;height:1.08em;"/>，那么卷积运算产生<img class="fm-editor-equation" src="img/45c8af6e-5ee5-494c-8f9c-033d812a2478.png" style="width:9.42em;height:1.33em;"/>。</p>
<p class="mce-root"><em>图12.1 </em>显示了通过反转和移位滤波器并乘以输入数据来获得该结果的每个步骤:</p>
<p>图12.1 -涉及两个向量的卷积运算示例</p>
<div><img src="img/54f721f1-4fd2-497f-ae21-6733dc208006.png" style="width:15.83em;height:24.50em;"/></div>
<p>在NumPy中，我们可以通过使用如下的<kbd>convolve()</kbd>方法来实现这一点:</p>
<p>这将输出以下内容:</p>
<pre>import numpy as np<br/>h = np.convolve([2, 3, 2], [-1, 2, -1])<br/>print(h)</pre>
<p>现在，如果你想一想，最“完整”的信息是当过滤器与输入数据完全重叠时，那是为了<img class="fm-editor-equation" src="img/1c903d14-4aa7-4725-8ac5-12b0e536298d.png" style="width:2.92em;height:1.00em;"/>。在Python中，您可以通过使用如下的<kbd>'valid'</kbd>参数来获得:</p>
<pre>[-2, 1, 2, 1, -2]</pre>
<p>这简单地给出了以下内容:</p>
<pre>import numpy as np<br/>h = np.convolve([2, 3, 2], [-1, 2, -1], 'valid')<br/>print(h)</pre>
<p>同样，这只是为了最大化与<em>相关的</em>信息，因为卷积运算在矢量的边缘周围<em>更不确定</em>，也就是说，在矢量不完全重叠的开始和结束处。此外，为了方便起见，我们可以通过使用<kbd>'same'</kbd>参数获得一个与输入大小相同的输出向量，如下所示:</p>
<pre>2</pre>
<p>这会打印以下内容:</p>
<pre>import numpy as np<br/>h = np.convolve([2, 3, 2], [-1, 2, -1], 'same')<br/>print(h)</pre>
<p>以下是使用卷积的三种方式的一些实际原因:</p>
<pre>[1 2 1]</pre>
<p>当您需要所有的<em>良好的</em>信息，而没有任何由滤波器部分重叠引起的噪声时，请使用<kbd>'valid'</kbd>。</p>
<ul>
<li>当你想让计算变得更容易时，使用<kbd>'same'</kbd>。这将使它变得容易，因为你将在输入和输出中有相同的维度。</li>
<li>否则，不要使用任何东西来获得卷积运算的完整解析解。</li>
<li>随着专门从事快速乘法和加法运算的微处理器的激增，以及快速傅立叶变换算法的发展，卷积变得非常流行。FFT利用了离散时域中的卷积等价于傅立叶域中的乘法的数学特性，反之亦然。</li>
</ul>
<p>现在，让我们进入下一个维度。</p>
<p>二维</p>
<h2 id="uuid-d718049a-5acc-429c-a66c-b937396e4c8b">二维卷积与一维卷积非常相似。然而，我们将有一个矩阵，而不是一个向量，这就是为什么图像在这里是直接适用的。</h2>
<p>A two-dimensional convolution is very similar to the one-dimensional convolution. However, rather than having a vector, we will have a matrix, and that's why images are directly applicable here. </p>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p class="mceNonEditable">假设我们有两个矩阵:一个表示一些输入数据，另一个是过滤器，如下所示:</p>
<p><img class="fm-editor-equation" src="img/747990d4-5b12-4a09-a25d-26fb3ac8571f.png" style="width:18.17em;height:3.83em;"/>。</p>
<p class="CDPAlignCenter CDPAlign">我们可以通过反转(在两个维度上)和移位(也在两个维度上)滤波器来计算二维离散卷积。方程式如下:</p>
<p><img class="fm-editor-equation" src="img/6e04a69a-26bc-4b1a-b54b-76a0d621c719.png" style="width:42.00em;height:3.92em;"/></p>
<p class="CDPAlignCenter CDPAlign">这和一维版本很像。下图说明了前两个步骤和最后一个步骤，以节省空间并避免重复:</p>
<p>图12.2 -二维离散卷积示例</p>
<div><img src="img/e1f06f2e-de3f-47be-8e56-57bf9822d95d.png" style="width:22.42em;height:25.33em;"/></div>
<p>在Python中，我们可以使用SciPy的<kbd>convolve2d</kbd>方法计算二维卷积，如下所示:</p>
<p>这将输出以下内容:</p>
<pre>import numpy as np<br/>from scipy.signal import convolve2d<br/>x = np.array([[2,2,2],[2,3,2],[2,2,2]])<br/>w = np.array([[-1,-1,-1],[-1,8,-1],[-1,-1,-1]])<br/>h = convolve2d(x,w)<br/>print(h)</pre>
<p>此处显示的结果是完整的分析结果。然而，与一维实现类似，如果您只想要完全重叠的结果，您可以调用一个<kbd>'valid'</kbd>结果，或者如果您想要与输入大小相同的结果，您可以调用如下的<kbd>'same'</kbd>替代:</p>
<pre>[[-2 -4 -6 -4 -2]<br/> [-4  9  5  9 -4]<br/> [-6  5  8  5 -6]<br/> [-4  9  5  9 -4]<br/> [-2 -4 -6 -4 -2]]</pre>
<p>这将产生以下结果:</p>
<pre>import numpy as np<br/>from scipy.signal import convolve2d<br/>x = np.array([[2,2,2],[2,3,2],[2,2,2]])<br/>w = np.array([[-1,-1,-1],[-1,8,-1],[-1,-1,-1]])<br/>h = convolve2d(x,w,mode='valid')<br/>print(h)<br/>h = convolve2d(x,w,mode='same')<br/>print(h)</pre>
<p>现在，让我们继续讨论n维卷积。</p>
<pre>[[8]]<br/><br/>[[9 5 9]<br/> [5 8 5]<br/> [9 5 9]]</pre>
<p>Now, let's move on to n-dimensional convolutions.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable">n维</p>
<h2 id="uuid-3934766c-2c9d-4c7a-ade6-29784a6384b5">一旦你理解了一维和二维的卷积，你就理解了它背后的基本概念。但是，您可能仍然需要在更大的维度上执行卷积，例如，在多光谱数据集中。为此，我们可以简单地准备任意维数的NumPy数组，然后使用SciPy的<kbd>convolve()</kbd>功能。考虑下面的例子:</h2>
<p>这里，向量<img class="fm-editor-equation" src="img/a239161e-8542-499f-bb20-3ad3a3fd22ca.png" style="width:5.58em;height:1.08em;"/>是三维数组，可以成功卷积，产生以下输出:</p>
<pre>import numpy as np<br/>from scipy.signal import convolve<br/>x = np.array([[[1,1],[1,1]],[[2,2],[2,2]]])<br/>w = np.array([[[1,-1],[1,-1]],[[1,-1],[1,-1]]])<br/>h = convolve(x,w)<br/>print(h)</pre>
<p>关于n维卷积，唯一困难的部分可能是将它们可视化，或者在你的头脑中想象它们。我们人类可以很容易地理解一维、二维和三维，但更高维度的空间很难解释。但是请记住，如果你理解卷积在一维和二维中的工作原理，你可以相信数学和算法在任何维度中都是有效的。</p>
<pre>[[[ 1 0 -1]<br/>  [ 2 0 -2]<br/>  [ 1 0 -1]]<br/><br/> [[ 3 0 -3]<br/>  [ 6 0 -6]<br/>  [ 3 0 -3]]<br/><br/> [[ 2 0 -2]<br/>  [ 4 0 -4]<br/>  [ 2 0 -2]]]</pre>
<p>接下来，让我们看看如何<em>通过定义Keras层并将其添加到模型中来学习</em>这样的卷积滤波器。</p>
<p>卷积层</p>
<h1 id="uuid-3ac18e84-2222-4188-9832-bdf607f889bb">卷积具有许多在深度学习领域非常有趣的性质:</h1>
<p>它可以成功地编码和解码数据的空间属性。</p>
<ul>
<li>可以用最新的动态相对快速的计算出来。</li>
<li>它可以用来解决几个计算机视觉问题。</li>
<li>它可以与其他类型的层结合使用，以获得最佳性能。</li>
<li>Keras为TensorFlow提供了包装器函数，这些函数涉及最流行的维度，即一维、二维和三维:<kbd>Conv1D</kbd>、<kbd>Conv2D</kbd>和<kbd>Conv3D</kbd>。在这一章中，我们将继续关注二维卷积，但请确保如果你已经理解了这个概念，你可以很容易地继续使用其他的。</li>
</ul>
<p>Conv2D</p>
<h2 id="uuid-7f254de3-c1c1-42b6-8133-ae3f30e3b03e">二维卷积法有如下签名:<kbd>tensorflow.keras.layers.Conv2D</kbd>。卷积层中最常用的参数如下:</h2>
<p><kbd>filters</kbd>指在该特定层中要学习的滤波器数量，并影响该层输出的维度。</p>
<ul>
<li><kbd>kernel_size</kbd>指过滤器的尺寸；例如，在<em>图12.2 </em>的情况下，它将是size (3，3)。</li>
<li>对我们来说是新的。步幅定义为滤波器滑过输入时的步长。到目前为止，我们展示的所有示例都假设我们遵循卷积的原始定义，并采取单位步长。然而，在卷积层中，您可以采取更大的步长，这将导致更小的输出，但也会丢失信息。</li>
<li><kbd>padding='valid'</kbd>指处理卷积结果边缘信息的方式。请注意，这里的选项只有<kbd>'valid'</kbd>或<kbd>'same'</kbd>，无法获得完整的分析结果。意思和我们之前在本章看到的一样。</li>
<li><kbd>activation=None</kbd>如果需要，提供在层中包含激活功能的选项；比如<kbd>activation='relu'</kbd>。</li>
<li>为了举例说明这一点，考虑如下图所示的卷积层，其中第一层是卷积层(在2D ),具有64个大小为9x9的滤波器，跨距为2，2(即每个方向两个)。我们将在下图中继续解释模型的其余部分:</li>
</ul>
<p>图12.3-CIFAR 10的卷积神经网络架构</p>
<div><img src="img/0025a4ec-aba0-4f83-9a6c-ce8f45623b47.png"/></div>
<p>图中的第一个卷积层可以定义如下:</p>
<p>这实际上将创建一个具有给定规格的卷积层。打印语句将有效地产生以下内容:</p>
<pre>import tensorflow as tf<br/>from tensorflow.keras.layers import <strong>Conv2D</strong><br/>input_shape = (1, 32, 32, 3)<br/>x = tf.random.normal(input_shape)<br/>l = <strong>Conv2D(64, (9,9), strides=(2,2), activation='relu',</strong> <br/>           input_shape=input_shape)(l)<br/>print(l.shape)</pre>
<p>如果您计算一下，64个滤波器中的每一个都会产生23x23 <kbd>'valid'</kbd>输出，但由于使用了(2，2)步距，因此应该会获得11.5x11.5的输出。然而，由于我们不能有分数，TensorFlow将四舍五入到12x12。因此，我们最终以前面的形状作为输出。</p>
<pre>(1, 12, 12, 64)</pre>
<p>层+激活组合</p>
<h2 id="uuid-de11df58-c426-42bb-b79c-df9b15d5dc49">如前所述，<kbd>Conv2D</kbd>类能够包含您选择的激活函数。这是非常值得赞赏的，因为它将为所有想要学习高效编码的人节省一些代码行。然而，我们必须小心不要忘记在某处记录所使用的激活类型。</h2>
<p><em>图12.3 </em>显示了独立模块中的激活。这是一个很好的想法来跟踪什么激活被使用。卷积层最常见的激活函数是ReLU，或者ReLU系列的任何激活函数，例如泄漏ReLU和eLU。下一个<em>新的</em>元素是一个池层。我们来谈谈这个。</p>
<p>集中策略</p>
<h1 id="uuid-351dc64d-6af2-437d-9097-0b63a532e957">您通常会发现伴随卷积层的池。池化是一种通过降低问题的维度来减少计算量的思想。在Keras，我们有一些可用的池策略，但最重要和最受欢迎的是以下两个:</h1>
<p>平均池2D</p>
<ul>
<li>MaxPooling2D</li>
<li>这些也存在于其他维度，如1D。但是，为了理解池，我们可以简单地看一下下图中的例子:</li>
</ul>
<p>图12.4-2D的最大池示例</p>
<div><img src="img/d6c68d88-284b-460f-abb6-48b69dfe190c.png" style="width:20.08em;height:14.17em;"/></div>
<p>在图中，您可以观察到max pooling如何看待单个2x2方块一次移动两个空格，从而导致2x2结果。汇集的全部目的是<strong>找到有问题的数据</strong>的一个更小的摘要。当谈到神经网络时，我们通常会看到最受<em>激发</em>的神经元，因此将最大值视为较大部分数据的良好代表是有意义的。但是，记住你也可以看数据的平均值(<kbd>AveragePooling2D</kbd>)，从各种意义上来说也是好的。</p>
<p>支持最大池的时间性能略有差异，但这种差异非常小。</p>
<p>在Keras中，我们可以非常容易地实现池。例如，在2D的max pooling案例中，我们可以简单地执行以下操作:</p>
<p>In Keras, we can implement pooling very easily. In the case of max pooling in 2D, for example, we can simply do the following:</p>
<pre>import tensorflow as tf<br/>from tensorflow.keras.layers import <strong>MaxPooling2D</strong><br/>x = tf.constant([[-2, -4, -6, -4],<br/>                 [-4, 9, 5, 9],<br/>                 [-6, 5, 8, 5],<br/>                 [-4, 9, 5, 9]])<br/>x = tf.reshape(x, [1, 4, 4, 1])<br/><strong>y = MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='valid')</strong><br/>print(tf.reshape(y(x), [2, 2]))</pre>
<p class="mce-root"/>
<p class="mce-root">这产生了与图12.4 中<em>相同的输出:</em></p>
<p>我们也可以对平均池进行同样的操作，如下所示:</p>
<pre>tf.Tensor(<br/>[[9 9]<br/> [9 9]], shape=(2, 2), dtype=int32)</pre>
<p>这将产生以下输出:</p>
<pre>import tensorflow as tf<br/>from tensorflow.keras.layers import <strong>AveragePooling2D</strong><br/>x = tf.constant([[-2., -4., -6., -4],<br/>                 [-4., 9., 5., 9.],<br/>                 [-6., 5., 8., 5.],<br/>                 [-4., 9., 5., 9.]])<br/>x = tf.reshape(x, [1, 4, 4, 1])<br/><strong>y = AveragePooling2D(pool_size=(2, 2), strides=(2, 2), padding='valid')</strong><br/>print(tf.reshape(y(x), [2, 2]))</pre>
<p>在汇总数据方面，这两种池策略都非常好。你选择任何一个都是安全的。</p>
<pre>tf.Tensor(<br/>[[-0.25 1. ]<br/> [ 1. 6.75]], shape=(2, 2), dtype=float32)</pre>
<p>现在是大揭秘。接下来，我们将把所有这些放入CNN。</p>
<p>CIFAR-10的卷积神经网络</p>
<h1 id="uuid-351d2255-c218-4bb2-b8ad-9e1152672fb4">在查看了各个部分之后，我们已经可以实际实现一个功能完整的CNN了:理解卷积运算，理解池化，以及理解如何实现卷积层和池化。现在，我们将实现图12.3 所示的CNN架构。</h1>
<p>履行</p>
<h2 id="uuid-56038b34-d126-4de4-8613-e447a2d024ca">我们将逐步实施图12.3 中的网络，并将其分解为子部分。</h2>
<p>加载数据</p>
<h3 id="uuid-88bd59fa-1323-4038-a8ab-af82bdebf891">让我们按如下方式加载CIFAR-10数据集:</h3>
<p>这将有效地加载数据集并打印其形状，如下所示:</p>
<pre>from tensorflow.keras.datasets import <strong>cifar10</strong><br/>from tensorflow.keras.utils import to_categorical<br/>import numpy as np<br/><br/># The data, split between train and test sets:<br/>(x_train, y_train), (x_test, y_test) = <strong>cifar10</strong>.load_data()<br/>x_train = x_train.astype('float32') / 255.<br/>x_test = x_test.astype('float32') / 255.<br/><br/>y_train = to_categorical(y_train, 10)<br/>y_test = to_categorical(y_test, 10)<br/>print('x_train shape:', x_train.shape)<br/>print('x_test shape:', x_test.shape)</pre>
<p>这非常简单，但是我们可以更进一步，通过加载和绘制<kbd>x_train</kbd>集合中每个类的第一个图像来验证数据是否被正确加载，如下所示:</p>
<pre>x_train shape: (50000, 32, 32, 3)<br/>x_test shape: (10000, 32, 32, 3)</pre>
<p>这将产生如下屏幕截图所示的输出:</p>
<pre>import matplotlib.pyplot as plt<br/>import numpy as np<br/><br/>(_, _), (_, labels) = cifar10.load_data()<br/>idx = [3, 6, 25, 46, 58, 85, 93, 99, 108, 133]<br/><br/>clsmap = {0: 'airplane',<br/>          1: 'automobile', <br/>          2: 'bird', <br/>          3: 'cat', <br/>          4: 'deer',<br/>          5: 'dog',<br/>          6: 'frog',<br/>          7: 'horse',<br/>          8: 'ship',<br/>          9: 'truck'}<br/><br/>plt.figure(figsize=(10,4))<br/>for i, (img, y) in enumerate(zip(x_test[idx].reshape(10, 32, 32, 3), labels[idx])):<br/>  plt.subplot(2, 5, i+1)<br/>  plt.imshow(img, cmap='gray')<br/>  plt.xticks([])<br/>  plt.yticks([])<br/>  plt.title(str(y[0]) + ": " + clsmap[y[0]])<br/>plt.show()</pre>
<p>图12.5-CIFAR-10样本</p>
<div><img src="img/3031d060-a19c-492f-91ca-df7a029dffb0.png" style="width:44.92em;height:19.33em;"/></div>
<p>接下来，我们将实现网络的各个层。</p>
<p>编译模型</p>
<h3 id="uuid-e62cc70c-681f-4614-9fbf-66462e3af260">再次回忆一下图12.3 中的模型，以及我们如何实现它。您将要看到的一切都是我们在本章和前几章中已经看过的:</h3>
<p>我们继续添加更多的卷积层，如下所示:</p>
<pre># Importing the Keras libraries and packages<br/>from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten<br/>from tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization<br/>from tensorflow.keras.models import Model<br/>from tensorflow.keras.optimizers import RMSprop<br/><br/># dimensionality of input and latent encoded representations<br/>inpt_dim = (32, 32, 3)<br/><br/>inpt_img = Input(shape=inpt_dim)<br/><br/># Convolutional layer<br/>cl1 = Conv2D(64, (9, 9), strides=(2, 2), input_shape = inpt_dim, <br/>             activation = 'relu')(inpt_img)<br/><br/># Pooling and BatchNorm<br/>pl2 = MaxPooling2D(pool_size = (2, 2))(cl1)<br/>bnl3 = BatchNormalization()(pl2)</pre>
<p>然后，我们可以编译模型并打印如下摘要:</p>
<pre># Add a second convolutional layer<br/>cl4 = Conv2D(128, (3, 3), strides=(1, 1), activation = 'relu')(bnl3)<br/>pl5 = MaxPooling2D(pool_size = (2, 2))(cl4)<br/>bnl6 = BatchNormalization()(pl5)<br/><br/># Flattening for compatibility<br/>fl7 = Flatten()(bnl6)<br/><br/># Dense layers + Dropout<br/>dol8 = Dropout(0.5)(fl7)<br/>dl9 = Dense(units = 256, activation = 'relu')(dol8)<br/>dol10 = Dropout(0.2)(dl9)<br/>dl11 = Dense(units = 64, activation = 'relu')(dol10)<br/>dol12 = Dropout(0.1)(dl11)<br/>output = Dense(units = 10, activation = 'sigmoid')(dol12)<br/><br/>classifier = Model(inpt_img, output)</pre>
<p>这将输出如下所示的网络总结:</p>
<pre># Compiling the CNN with RMSprop optimizer<br/>opt = RMSprop(learning_rate=0.001)<br/><br/>classifier.compile(optimizer = opt, loss = 'binary_crossentropy', <br/>                   metrics = ['accuracy'])<br/><br/>print(classifier.summary())</pre>
<p>在这一点上，有一点对你来说一定非常明显，那就是这个网络的参数数量。如果你回忆一下上一章，你会惊讶地发现这个网络有将近25万个参数，而宽或深的网络有几百万个参数。此外，你很快就会看到，这个相对较小的网络，虽然仍然<em>过度参数化</em>，将会比前一章中有更多参数的网络表现得更好。</p>
<pre>Model: "model"<br/>_________________________________________________________________<br/>Layer (type)                 Output Shape          Param # <br/>=================================================================<br/>input_1 (InputLayer)         [(None, 32, 32, 3)]   0 <br/>_________________________________________________________________<br/>conv2d (Conv2D)              (None, 12, 12, 64)    15616 <br/>_________________________________________________________________<br/>max_pooling2d_4 (MaxPooling2 (None, 6, 6, 64)      0 <br/>_________________________________________________________________<br/>batch_normalization (BatchNo (None, 6, 6, 64)      256 <br/>_________________________________________________________________<br/>.<br/>.<br/>.<br/>_________________________________________________________________<br/>dropout_2 (Dropout)          (None, 64)            0 <br/>_________________________________________________________________<br/>dense_2 (Dense)              (None, 10)            650 <br/>=================================================================<br/>Total params: <strong>238,666</strong><br/>Trainable params: 238,282<br/>Non-trainable params: 384</pre>
<p>接下来，我们来训练网络。</p>
<p>训练CNN</p>
<h3 id="uuid-8f716761-1800-43a5-bbe9-ace5af7a29ea">我们可以使用我们在<a href="03e9a734-fb56-485d-ae90-66fb98ecd4d1.xhtml">第11章</a>、<em>深度和广度神经网络</em>中学习的<em>回调</em>来训练CNN，如果没有进展，则提前停止网络，如果达到<em>平稳期</em>，则降低学习速率以集中梯度下降算法的努力。</h3>
<p>我们将对其进行如下培训:</p>
<p>We will train it as follows:</p>
<pre># Fitting the CNN to the images<br/>from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping<br/><br/>reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, <br/>                              min_delta=1e-4, mode='min', verbose=1)<br/><br/>stop_alg = EarlyStopping(monitor='val_loss', patience=35, <br/>                         restore_best_weights=True, verbose=1)<br/><br/>hist = classifier.fit(x_train, y_train, batch_size=100, epochs=1000, <br/>                   callbacks=[stop_alg, reduce_lr], shuffle=True, <br/>                   validation_data=(x_test, y_test))<br/><br/>classifier.save_weights("cnn.hdf5")</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable">这样做的结果会因计算机而异。例如，它可能需要更少或更多的历元，或者如果小批量(随机选择)包含几个边缘情况，梯度可能采取不同的方向。但是，在大多数情况下，您应该会得到与此类似的结果:</p>
<p>此时，当训练结束后，你可以得到一个83.15%准确率的估计。小心，这不是一个<strong>平衡的</strong>精度。为此，我们将在下一节中查看<strong>平衡错误率</strong> ( <strong> BER </strong>)指标。但在此之前，我们可以看看训练曲线，看看损失是如何最小化的。</p>
<pre>Epoch 1/1000<br/>500/500 [==============================] - 3s 5ms/step - loss: 0.2733 - accuracy: 0.3613 - val_loss: 0.2494 - val_accuracy: 0.4078 - lr: 0.0010<br/>Epoch 2/1000<br/>500/500 [==============================] - 2s 5ms/step - loss: 0.2263 - accuracy: 0.4814 - val_loss: 0.2703 - val_accuracy: 0.4037 - lr: 0.0010<br/>.<br/>.<br/>.<br/>Epoch 151/1000<br/>492/500 [============================&gt;.] - ETA: 0s - loss: 0.0866 - accuracy: 0.8278<br/>Epoch 00151: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.<br/>500/500 [==============================] - 2s 4ms/step - loss: 0.0866 - accuracy: 0.8275 - val_loss: 0.1153 - val_accuracy: 0.7714 - lr: 7.8125e-06<br/>Epoch 152/1000<br/>500/500 [==============================] - 2s 4ms/step - loss: 0.0864 - accuracy: 0.8285 - val_loss: 0.1154 - val_accuracy: 0.7707 - lr: 3.9063e-06<br/>Epoch 153/1000<br/>500/500 [==============================] - 2s 4ms/step - loss: 0.0861 - accuracy: 0.8305 - val_loss: 0.1153 - val_accuracy: 0.7709 - lr: 3.9063e-06<br/>Epoch 154/1000<br/>500/500 [==============================] - 2s 4ms/step - loss: 0.0860 - accuracy: 0.8306 - val_loss: 0.1153 - val_accuracy: 0.7709 - lr: 3.9063e-06<br/>Epoch 155/1000<br/>500/500 [==============================] - 2s 4ms/step - loss: 0.0866 - accuracy: 0.8295 - val_loss: 0.1153 - val_accuracy: 0.7715 - lr: 3.9063e-06<br/>Epoch 156/1000<br/>496/500 [============================&gt;.] - ETA: 0s - loss: 0.0857 - accuracy: 0.8315Restoring model weights from the end of the best epoch.<br/>500/500 [==============================] - 2s 4ms/step - loss: 0.0857 - accuracy: 0.8315 - val_loss: 0.1153 - val_accuracy: 0.7713 - lr: 3.9063e-06<br/>Epoch 00156: early stopping</pre>
<p>At this point, when the training is finished, you can get an estimate of the accuracy of 83.15%. Be careful, this is not a <strong>balanced</strong> accuracy. For that, we will take a look at the <strong>Balanced Error Rate</strong> (<strong>BER</strong>) metric in the next section. But before we do that, we can look at the training curve to see how the loss was minimized.</p>
<p class="mce-root">下面的代码将产生我们想要的结果:</p>
<p>这给出了<em>图12.6 </em>中所示的曲线图:</p>
<pre>import matplotlib.pyplot as plt<br/><br/>fig = plt.figure(figsize=(10,6))<br/>plt.plot(hist.history['loss'], color='#785ef0')<br/>plt.plot(hist.history['val_loss'], color='#dc267f')<br/>plt.title('Model Loss Progress')<br/>plt.ylabel('Brinary Cross-Entropy Loss')<br/>plt.xlabel('Epoch')<br/>plt.legend(['Training Set', 'Test Set'], loc='upper right')<br/>plt.show()</pre>
<p>This gives the plot shown in <em>Figure 12.6</em>:</p>
<div><img src="img/cbf15643-e4bd-451a-86c6-1d71011cb5c4.png" style="width:39.50em;height:24.50em;"/></div>
<p class="mce-root">图12.6-CIFAR-10上CNN的损耗最小化</p>
<p>从该图中，您可以了解学习曲线的起伏，尤其是在训练集曲线上可见的起伏，这是由于通过回调函数<kbd>ReduceLROnPlateau</kbd>降低了学习速率。由于<kbd>EarlyStopping</kbd>回调，在测试集上损失不再改善后，训练停止。</p>
<p>From this diagram, you can appreciate the bumps that the learning curve has, particularly visible on the training set curve, which are due to the reduction in the learning rate through the callback function, <kbd>ReduceLROnPlateau</kbd>. The training stops after the loss no longer improves on the test set, thanks to the <kbd>EarlyStopping</kbd> callback. </p>
<p class="mce-root"/>
<p class="mce-root">结果</p>
<h2 id="uuid-a714e0cc-30fc-42d5-b213-db25ea7235ff">现在，让我们看看客观的数字结果:</h2>
<p>这将给出以下数值结果，我们可以将这些结果与上一章的结果进行比较:</p>
<pre>from sklearn.metrics import classification_report<br/>from sklearn.metrics import confusion_matrix<br/>from sklearn.metrics import balanced_accuracy_score<br/>import matplotlib.pyplot as plt<br/>import numpy as np<br/><br/>(_, _), (_, labels) = cifar10.load_data()<br/><br/>y_ = labels<br/>y_hat = classifier.predict(x_test)<br/>y_pred = np.argmax(y_hat, axis=1)<br/><br/>print(classification_report(np.argmax(y_test, axis=1), <br/>                            np.argmax(y_hat, axis=1), <br/>                            labels=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]))<br/>cm = confusion_matrix(np.argmax(y_test, axis=1), <br/>                      np.argmax(y_hat, axis=1), <br/>                      labels=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9])<br/>print(cm)<br/>ber = 1- balanced_accuracy_score(np.argmax(y_test, axis=1), <br/>                                 np.argmax(y_hat, axis=1))<br/>print('BER', ber)</pre>
<p>特定类别的准确率可高达87%，而最低准确率为66%。这比上一章的前几款要好得多。BER为0.2288，这都可以解释为77.12%的平衡精度。这与训练期间测试集中报告的准确度相匹配，这表明模型被正确训练。为了便于比较，下图显示了混淆矩阵的直观表示:</p>
<pre>  precision  recall  f1-score  support<br/><br/>0      0.80    0.82      0.81     1000<br/>1      0.89    0.86      0.87     1000<br/>2      0.73    0.66      0.69     1000<br/>3      0.57    0.63      0.60     1000<br/>4      0.74    0.74      0.74     1000<br/>5      0.67    0.66      0.66     1000<br/>6      0.84    0.82      0.83     1000<br/>7      0.82    0.81      0.81     1000<br/>8      0.86    0.88      0.87     1000<br/>9      0.81    0.85      0.83     1000<br/><br/>               accuracy  0.77     10000<br/><br/>[[821  12  36  18  12   8   4   4  51  34]<br/> [ 17 860   3   7   2   6   8   1  22  74]<br/> [ 61   2 656  67  72  53  43  24  11  11]<br/> [ 11   7  47 631  55 148  38  36  10  17]<br/> [ 21   2  48  63 736  28  31  54  12   5]<br/> [ 12   3  35 179  39 658  16  41   4  13]<br/> [  2   4  32  67  34  20 820   8   8   5]<br/> [ 12   3  18  41  42  52   5 809   3  15]<br/> [ 43  22  12  12   2   5   3   0 875  26]<br/> [ 29  51  10  19   2   3   5   9  26 846]]<br/><br/>BER 0.2288</pre>
<p><img src="img/db92150e-3920-4e15-b975-40e7e6f287f7.png" style="width:29.08em;height:25.42em;"/></p>
<p>图12.7 -通过CIFAR-10训练的CNN的混淆矩阵</p>
<p>从视觉混淆矩阵中可以更清楚地看出，类3和类5比其他类更容易混淆。3类和5类分别对应猫和狗。</p>
<p>It might be a bit clearer from the visual confusion matrix that classes 3 and 5 can be confused between themselves more than other classes. Classes 3 and 5 correspond to cats and dogs, respectively.</p>
<p class="mce-root"/>
<p class="mce-root">就是这样。正如你所看到的，这已经是一个很好的结果了，但是你可以自己进行更多的实验。您可以编辑和添加更多的卷积层到您的模型，使它变得更好。如果你好奇的话，还有其他更大的CNN也非常成功。这里有两个最著名的例子:</p>
<p>VGG-19:它包含12个卷积层和3个密集层(Simonyan，k .等人(2014))。</p>
<ul>
<li>ResNet:这包含110个卷积层和1个密集层(He，k .，et al. (2016))。这种特殊的配置在CIFAR-10上可以实现低至6.61% ( 0.16%)的错误率。</li>
<li>接下来让我们讨论如何可视化学习到的过滤器。</li>
</ul>
<p>过滤器的可视化</p>
<h2 id="uuid-557bd542-2709-4a09-8a81-52d27b6af210">这一章的最后一部分处理学习过的过滤器的可视化。如果你想研究网络在学习什么，这可能对你有用。这可能有助于网络的可解释性。然而，请注意，网络越深，理解它就越复杂。</h2>
<p>以下代码将帮助您可视化网络第一个卷积层的滤波器:</p>
<p>The following code will help you visualize the filters of the first convolutional layer of the network:</p>
<pre>from sklearn.preprocessing import MinMaxScaler<br/><br/>cnnl1 = classifier.layers[1].name   # get the name of the first conv layer<br/>W = classifier.get_layer(name=cnnl1).get_weights()[0]   #get the filters<br/>wshape = W.shape  #save the original shape<br/><br/># this part will scale to [0, 1] for visualization purposes<br/>scaler = MinMaxScaler()<br/>scaler.fit(W.reshape(-1,1))<br/>W = scaler.transform(W.reshape(-1,1))<br/>W = W.reshape(wshape)<br/><br/># since there are 64 filters, we will display them 8x8<br/>fig, axs = plt.subplots(8,8, figsize=(24,24))<br/>fig.subplots_adjust(hspace = .25, wspace=.001)<br/>axs = axs.ravel()<br/>for i in range(W.shape[-1]):<br/>  # we reshape to a 3D (RGB) image shape and display<br/>  h = np.reshape(W[:,:,:,i], (9,9,3))<br/>  axs[i].imshow(h)<br/>  axs[i].set_title('Filter ' + str(i))</pre>
<p class="mce-root">该代码在很大程度上依赖于了解您想要可视化的图层、想要可视化的滤镜数量以及滤镜本身的大小。在这种情况下，我们希望将第一个卷积层可视化。它有64个过滤器(显示在8×8的网格中)，每个过滤器是9x9x3，因为输入是彩色图像。<em>图12.8 </em>显示了前面代码的结果图:</p>
<p>图12.8 -在第一个卷积层中学习的过滤器</p>
<div><img src="img/f37fe0c8-877d-4ae1-8d94-b468a47f6295.png" style="width:46.33em;height:47.00em;"/></div>
<p>如果您是图像处理专家，您可能会认出其中一些模式，因为它们类似于Gabor滤波器(Jain，A. K .等人(1991))。这些过滤器中的一些被设计成寻找边缘、纹理或特定形状。文献表明，在卷积网络中，较深的层通常编码高度复杂的信息，而第一层用于检测边缘等特征。</p>
<p>请随意继续，并通过进行必要的修改来尝试显示另一个层。</p>
<p>摘要</p>
<h1 id="uuid-4c7dcb24-fcba-499d-bf73-6fab4e48d6e5">这中间的一章展示了如何创建CNN。你学习了卷积运算，这是它们背后的基本概念。您还了解了如何创建卷积层和聚合池策略。您设计了一个网络来学习过滤器，以基于CIFAR-10识别对象，并学习了如何显示学习到的过滤器。</h1>
<p>在这一点上，你应该有信心解释根植于计算机视觉和信号处理的卷积神经网络背后的动机。使用NumPy、SciPy和Keras/TensorFlow对一维和二维卷积运算进行编码，您应该会感觉很舒服。此外，你应该有信心在层中实现卷积运算，并通过梯度下降技术学习过滤器。如果要求您展示网络已经学习了什么，您应该准备好实现一个简单的可视化方法来显示学习到的过滤器。</p>
<p>CNN擅长编码高度相关的空间信息，如图像、音频或文本。然而，有一种有趣的网络是用来对本质上是连续的信息进行编码的。<a href="a6e892c5-e890-4c0a-ad92-c5442328a64a.xhtml">第十三章</a>、<em>递归神经网络</em>，将呈现递归网络最基本的概念，从而引出长短期记忆模型。我们将探索序列模型的多种变体及其在图像分类和自然语言处理中的应用。</p>
<p>CNNs are great at encoding highly correlated spatial information, such as images, audio, or text. However, there is an interesting type of network that is meant to encode information that is sequential in nature. <a href="a6e892c5-e890-4c0a-ad92-c5442328a64a.xhtml">Chapter 13</a>, <em>Recurrent Neural Networks</em>, will present the most fundamental concepts of recurrent networks, leading to long short-term memory models. We will explore multiple variants of sequential models with applications in image classification and natural language processing.</p>
<p class="mce-root"/>
<p class="mce-root">问题和答案</p>
<h1 id="uuid-d0e9aa9d-bec6-4548-b8cd-b76e2527e6f1"><strong>本章讨论的什么数据汇总策略可以降低卷积模型的维数？</strong></h1>
<ol>
<li>共用。</li>
</ol>
<p style="padding-left: 60px"><strong>增加更多的卷积层数会让网络变得更好吗？</strong></p>
<ol start="2">
<li>不总是。已经表明，更多的层对网络有积极的影响，但是在某些情况下没有增益。你应该通过实验来确定层数、过滤器尺寸和汇集。</li>
</ol>
<p style="padding-left: 60px"><strong>CNN还有哪些应用？</strong></p>
<ol start="3">
<li>音频处理和分类；图像去噪；图像超分辨率；文本摘要和其他文本处理和分类任务；数据的加密。</li>
</ol>
<p style="padding-left: 60px">参考</p>
<h1 id="uuid-1a08f56c-8d54-4b50-a37f-627f094cd410">LeCun，y .，Boser，b .，Denker，J. S .，Henderson，d .，Howard，R. E .，Hubbard，w .，Jackel，L. D. (1989)。<em>应用于手写邮政编码识别的反向传播</em>。<em>神经计算</em>，1(4)，541-551。</h1>
<ul>
<li>李耀德，郝志斌，雷，h(2016)。<em>卷积神经网络综述</em>。<em>计算机应用杂志</em>，36(9)，2508-2515。</li>
<li>Krizhevsky，a .，Sutskever，I .，和Hinton，G. E. (2012年)。<em>深度卷积神经网络的Imagenet分类</em>。在<em>神经信息处理系统的进展</em>(第1097-1105页)。</li>
<li>Simonyan，k .和Zisserman，A. (2014年)。<em>用于大规模图像识别的极深度卷积网络</em>。arXiv预印本arXiv:1409.1556。</li>
<li>贺，张，谢，任，孙(2016)。<em>用于图像识别的深度残差学习</em>。IEEE计算机视觉和模式识别会议论文集(第770-778页)。</li>
<li>贾恩和法罗克尼亚(1991年)。<em>使用Gabor滤波器的无监督纹理分割</em>。<em>模式识别</em>，24(12)，1167-1186。</li>
<li>Jain, A. K., and Farrokhnia, F. (1991). <em>Unsupervised texture segmentation using Gabor filters</em>. <em>Pattern recognition</em>, 24(12), 1167-1186.</li>
</ul>


            

            
        
    


</body></html>