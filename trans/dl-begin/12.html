<html><head/><body>


    
        <title>Restricted Boltzmann Machines</title>
        
        <meta charset="utf-8"/>
    

    
        

                            
                    Restricted Boltzmann Machines
                
            
            
                
<p>我们一起看到了无监督学习的力量，并希望说服自己，它可以应用于不同的问题。我们将用一种令人兴奋的方法来结束无监督学习的话题，这种方法被称为<strong>受限玻尔兹曼机器</strong> ( <strong> RBMs </strong>)。当我们不在乎拥有大量的层时，我们可以使用RBM从数据中学习，并找到满足能量函数的方法，该能量函数将产生在表示输入数据方面稳健的模型。</p>
<p class="mce-root">本章补充了<a href="6677b8b1-806c-4c39-8c1e-371e83501acf.xhtml">第8章</a>、<em>深度自动编码器</em>，介绍了RBM的后向-前向性质，同时将其与<strong>自动编码器</strong> ( <strong> AEs </strong>)的仅前向性质进行了对比。本章以MNIST为例，比较了RBMs和AEs在降维问题上的异同。学完本章后，您应该能够使用scikit来使用RBM——学习并实现一个使用伯努利RBM的解决方案。你将能够对RBM和AE的潜在空间进行视觉比较，也能直观地了解RBM和AE内部工作原理的重量。</p>
<p class="mce-root">本章组织如下:</p>
<ul>
<li class="mce-root">成果管理制简介</li>
<li class="mce-root">用RBMs学习数据表示</li>
<li class="mce-root">比较RBM和AEs</li>
</ul>
<h1 id="uuid-6c4a9c19-6691-4c46-943e-bc9913750e2a">成果管理制简介</h1>
<p>RBM是无监督模型，可用于需要丰富潜在表示的不同应用中。它们通常用于具有分类模型的管道中，目的是从数据中提取特征。它们是基于<strong>玻尔兹曼机器</strong> ( <strong> BMs </strong>)，我们接下来会讨论(Hinton，G. E .和Sejnowski，T. J. (1983))。</p>
<h2 id="uuid-f109c63c-5d67-40da-b8a2-3eeaf6875aa6">电池管理系统</h2>
<p>BM可以被认为是一个无向稠密图，如图<em>图10.1 </em>所示:</p>
<div><img src="img/1dd88078-d42e-4544-8491-77f715f7696f.png" style="width:23.17em;height:19.50em;"/></div>
<p>图10.1–一个BM模型</p>
<p>这个无向图有一些被建模为<strong>可见</strong>、<img class="fm-editor-equation" src="img/bc0fc9ef-6923-4dad-905f-d7c40116b3d7.png" style="width:3.92em;height:1.33em;"/>的神经单元，以及一组被<strong>隐藏</strong>、<img class="fm-editor-equation" src="img/ec3962a8-a124-46ce-b17f-ea74d72f1067.png" style="width:5.92em;height:1.33em;"/>的神经单元。当然，可能不止这些。但是这个模型的要点是所有的神经元都是互相连接的:它们都互相交流。这里不讨论该模型的训练，但本质上它是一个迭代过程，其中输入呈现在可见层中，每个神经元(一次一个)调整其与其他神经元的连接，以满足损失函数(通常基于能量函数)，并且该过程重复，直到学习过程被认为是令人满意的。</p>
<p>虽然RB模型非常有趣和强大，但它花了很长时间来训练！考虑到这是在20世纪80年代早期，在比这更大的图形上和使用更大的数据集执行计算可能会对训练时间产生重大影响。然而，在1983年，G. E. Hinton和他的合作者通过限制神经元之间的通讯提出了一个简化的BM模型，我们将在下面讨论。</p>
<p class="mce-root"/>
<h2 id="uuid-31d75d54-be06-4b58-bf99-ae57556479cd">RBMs</h2>
<p>传统BMs的<em>限制</em>在于神经元之间的通信；即可见神经元只能与隐藏神经元对话，隐藏神经元只能与可见神经元对话，如图<em>图10.2 </em>所示:</p>
<div><img src="img/40d2dc56-c956-418d-a481-ea4ff53f5d42.png" style="width:22.67em;height:13.67em;"/></div>
<p>图10.2-RBM模型。与图10.1中的BM模型相比</p>
<p><em>图10.2 </em>所示的图称为<strong>稠密二部图</strong>。也许你在想，它看起来很像我们迄今为止一直使用的典型的密集神经网络；但是，又不太一样。主要区别在于，我们使用的所有神经网络都只能从输入层(可见层)到隐藏层传递信息，而RBM可以双向传递信息！其余的元素都很熟悉:我们有需要学习的权重和偏差。</p>
<p>如果我们坚持图10.2所示的简单模型，我们可以用更简单的术语解释RBM背后的学习理论。</p>
<p>让我们将每一个神经单元解释为一个随机变量，其当前状态取决于其他神经单元的状态。</p>
<p>这种解释允许我们使用与<strong>马尔可夫链蒙特卡罗</strong> ( <strong> MCMC </strong> ) (Brooks，s .，et al. (2011))相关的采样技术；然而，我们不会在本书中深入讨论这些细节。</p>
<p class="mce-root"/>
<p>使用这种解释，我们可以如下定义模型的能量函数:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="img/7eafb87d-5b01-4b10-ab01-98aad9a12ae3.png" style="width:28.50em;height:4.17em;"/></p>
<p>其中<img class="fm-editor-equation" src="img/da9561fe-3f4a-4973-9c06-1c311d126aae.png" style="width:3.25em;height:1.25em;"/>分别表示可见神经单元和隐藏神经单元上的偏差。结果，我们也可以将神经和隐藏单元的联合概率密度函数表示如下:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="img/f43952f8-d909-4104-8a8e-c173c9c19d4b.png" style="width:9.00em;height:2.75em;"/></p>
<p>有一个简单的边际分布:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="img/46d00194-e21e-4dd7-849b-273079cd4c45.png" style="width:18.58em;height:3.83em;"/></p>
<p>条件和边际中的分母称为归一化因子，它的作用只是确保概率值加起来等于1，可以定义如下:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="img/c442f2fe-1dfa-4915-8db1-90a8e5b4da7e.png" style="width:8.92em;height:3.17em;"/></p>
<p>这些公式允许我们快速找到用于训练的MCMC技术；最值得注意的是，你会在文献中发现涉及吉布斯抽样的对比发散是最常见的方法(Tieleman，T. (2008))。</p>
<p>只有少数已实施的成果管理制可供学习者随时开始使用；其中之一是在scikit-learn中提供的伯努利RBM，我们接下来将讨论它。</p>
<p class="mce-root"/>
<h2 id="uuid-27a9cb20-3de6-4e00-8017-3d2d9308a30b">伯努利RBMs</h2>
<p class="mce-root">虽然广义RBM模型不会对其使用的数据做出任何假设，但伯努利RBM模型会假设输入数据表示可解释为概率值的范围[0，1]内的值。在理想情况下，值在集合{0，1}中，这与伯努利试验密切相关。如果您感兴趣，还有其他方法假设输入遵循高斯分布。你可以通过阅读Yamashita，t .等人(2014)的文章了解更多。</p>
<p class="mce-root">只有少数数据集可用于这种类型的RBM；MNIST就是一个例子，它可以被解释为二进制输入，当没有数字轨迹时，数据为0，而当有数字信息时，数据为1。在scikit-learn中，<kbd>BernoulliRBM</kbd>模型可以在神经网络集合中找到:<kbd>sklearn.neural_network</kbd>。<br/> <br/>在类似伯努利的输入分布的假设下，这个RBM模型<em>使用一种叫做<strong>持续对比散度</strong> ( <strong> PCD </strong> ) (Tieleman，t .和Hinton，G. (2009))的方法近似地</em>优化对数似然。事实证明，PCD比当时任何其他算法都快得多，并引发了讨论和兴奋，但与密集网络相比，<strong>反向传播</strong>的普及很快掩盖了这种兴奋。</p>
<p>在下一节中，我们将在MNIST上实现伯努利RBM，目的是学习数据集的表示。</p>
<p class="mce-root"/>
<h1 id="uuid-9d392fdb-adbd-4cfa-849f-170706a8de42">用RBMs学习数据表示</h1>
<p>现在你已经知道了RBMs背后的基本思想，我们将使用<kbd>BernoulliRBM</kbd>模型以一种无监督的方式学习数据表示。和以前一样，我们将使用MNIST数据集来进行比较。</p>
<p>对于一些人来说，<strong>学习表示法</strong>的任务可以被认为是<strong>特征工程</strong>。后者对这个术语有一个可解释的成分，而前者并不一定要求我们给习得的表征规定意义。</p>
<p>在scikit-learn中，我们可以通过调用以下指令来创建RBM的实例:</p>
<pre>from sklearn.neural_network import BernoulliRBM<br/>rbm = BernoulliRBM()</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p>RBM的构造函数中的默认参数如下:</p>
<ul>
<li><kbd>n_components=256</kbd>，是隐藏单元的数量<img class="fm-editor-equation" src="img/73826389-c0a2-4f77-972f-ed916b23f0a9.png" style="width:0.92em;height:1.00em;"/>，而可见单元的数量<img class="fm-editor-equation" src="img/89773bbc-964d-486b-b725-57c2ef2aa47e.png" style="width:1.08em;height:1.00em;"/>，是从输入的维度中推断出来的。</li>
<li><kbd>learning_rate=0.1</kbd>控制学习算法相对于更新的强度，建议使用集合{ <em> 1，0.1，0.01，0.001 </em> }中的值进行探索。</li>
<li><kbd>batch_size=10</kbd>控制在批量学习算法中使用多少样本。</li>
<li><kbd>n_iter=10</kbd>控制在我们停止学习算法之前运行的迭代次数。算法的本质允许它按照我们想要的那样继续下去；然而，该算法通常在几次迭代中找到好的解决方案。</li>
</ul>
<p>我们将只更改组件的默认数量，使其为100。由于MNIST数据集中的原始维数是784(因为它由28 x 28的图像组成)，因此拥有100维似乎不是一个坏主意。</p>
<p>要用加载到<kbd>x_train</kbd>中的100个MNIST训练数据来训练RBM，我们可以做如下操作:</p>
<pre>from sklearn.neural_network import BernoulliRBM<br/>from tensorflow.keras.datasets import mnist<br/>import numpy as np<br/><br/>(x_train, y_train), (x_test, y_test) = mnist.load_data()<br/><br/>image_size = x_train.shape[1]<br/>original_dim = image_size * image_size<br/>x_train = np.reshape(x_train, [-1, original_dim])<br/>x_test = np.reshape(x_test, [-1, original_dim])<br/>x_train = x_train.astype('float32') / 255<br/>x_test = x_test.astype('float32') / 255<br/><br/>rbm = BernoulliRBM(verbose=True)<br/><br/>rbm.n_components = 100<br/>rbm.fit(x_train)</pre>
<p>培训期间的输出可能如下所示:</p>
<pre>[BernoulliRBM] Iteration 1, pseudo-likelihood = -104.67, time = 12.84s<br/>[BernoulliRBM] Iteration 2, pseudo-likelihood = -102.20, time = 13.70s<br/>[BernoulliRBM] Iteration 3, pseudo-likelihood = -97.95, time = 13.99s<br/>[BernoulliRBM] Iteration 4, pseudo-likelihood = -99.79, time = 13.86s<br/>[BernoulliRBM] Iteration 5, pseudo-likelihood = -96.06, time = 14.03s<br/>[BernoulliRBM] Iteration 6, pseudo-likelihood = -97.08, time = 14.06s<br/>[BernoulliRBM] Iteration 7, pseudo-likelihood = -95.78, time = 14.02s<br/>[BernoulliRBM] Iteration 8, pseudo-likelihood = -99.94, time = 13.92s<br/>[BernoulliRBM] Iteration 9, pseudo-likelihood = -93.65, time = 14.10s<br/>[BernoulliRBM] Iteration 10, pseudo-likelihood = -96.97, time = 14.02s</pre>
<p>我们可以通过调用MNIST测试数据<kbd>x_test</kbd>上的<kbd>transform()</kbd>方法来研究学习到的表示，如下所示:</p>
<pre>r = rbm.transform(x_test)</pre>
<p>在这种情况下，有784个输入维度，但是<kbd>r</kbd>变量将有100个维度。为了可视化RBM诱发的潜在空间中的测试集，我们可以像之前一样使用UMAPs，这将产生如图<em>图10.3 </em>所示的二维图:</p>
<div><img src="img/17976fd6-6978-4fe1-aa16-31bfe3ae1cce.png" style="width:28.17em;height:24.92em;"/></div>
<p>图10.3-RBM对MNIST试验数据的学习表示的UMAP表示</p>
<p class="mce-root"/>
<p>使用UMAP从RBM特征空间生成该图的完整代码如下:</p>
<pre>import matplotlib.pyplot as plt<br/>import umap<br/><br/>y_ = list(map(int, y_test))<br/><strong>X_ = rbm.transform(x_test)</strong><br/><br/>X_ = umap.UMAP().fit_transform(X_)<br/><br/>plt.figure(figsize=(10,8))<br/>plt.title('UMAP of 100 RBM Learned Components on MNIST')<br/>plt.scatter(X_[:,0], X_[:,1], s=5.0, c=y_, alpha=0.75, cmap='tab10')<br/>plt.xlabel('$z_1$')<br/>plt.ylabel('$z_2$')<br/>plt.colorbar()</pre>
<p>将<em>图10.3 </em>与前几章所示的图示进行比较。从图中我们可以看出，存在明显的类分离和聚类，同时类之间存在轻微的重叠。例如，数字3和8之间有一些重叠，这是可以预料的，因为这些数字看起来很像。该图还显示，由于图10.3<em>中的数据来自模型看不到的数据，因此RBM概括得非常好。</em></p>
<p>我们可以进一步检查RBM学习到的重量(或<em>组件</em>);也就是说，我们可以检索与可见层相关联的权重，如下所示:</p>
<pre>v = rbm.components_</pre>
<p>在这种情况下，<kbd>v</kbd>变量将是一个784 x 100的矩阵，用于描述学习到的权重。我们可以可视化每一个神经元，并重建与这些神经元相关联的权重，这将看起来像<em>图10.4 </em>中的组件:</p>
<div><img src="img/9ac6b554-5cef-4031-b2af-73bc85f5a4b9.png" style="width:29.75em;height:28.33em;"/></div>
<p>图10.4-RBM的已知重量</p>
<p>对<em>图10.4 </em>的仔细研究告诉我们，有一些权重关注对角线特征，或圆形特征，或通常非常具体到特定数字和边缘的特征。例如，底部一行的特征似乎与数字2和6相关联。</p>
<p>图10.4 中的<em>所示的权重可用于将输入空间转换为更丰富的表示形式，这些表示形式稍后可用于支持该任务的管道中的分类。</em></p>
<p>为了满足我们学习的好奇心，我们还可以通过使用<kbd>gibbs()</kbd>方法对网络进行采样来研究RBM及其各州。这意味着，我们可以想象当我们向可见层提供输入时会发生什么，然后从隐藏层得到什么样的响应，然后再次使用它作为输入，并重复查看模型的刺激如何变化。例如，运行以下代码:</p>
<pre>import matplotlib.pyplot as plt<br/>plt.figure()<br/>cnt = 1<br/>for i in range(10):    #we look into the first ten digits of test set<br/>  <strong>x</strong> = x_test[i]<br/>  for j in range(10):  #we project and reuse as input ten times<br/>    plt.subplot(10, 10, cnt)<br/>    plt.imshow(x.reshape((28, 28)), cmap='gray')<br/>    <strong>x = rbm.gibbs(x)   </strong>#here use current as input and use as input again<br/>    cnt += 1<br/>plt.show()</pre>
<p>这将有效地产生如图<em>图5 </em>所示的图:</p>
<div><img src="img/8144dac0-0e70-4382-9d00-a6452b5336c7.png" style="width:33.42em;height:31.92em;"/></div>
<p>图10.5-MNIST RBM的Gibbs抽样</p>
<p><em>图10.5 </em>第一列显示输入，其余10列是连续的采样调用。显然，当输入在RBM内来回传播时，它会遭受一些轻微的变形。取第五行，对应数字4；我们可以看到输入是如何变形的，直到它看起来像一个数字2。除非在第一次采样调用时观察到强烈的变形，否则该信息对学习的特征没有直接影响。</p>
<p>在下一节中，我们将使用AE与RBM进行比较。</p>
<p class="mce-root"/>
<h1 id="uuid-86de33b9-e633-4279-ad44-15bcb170ac2c">比较RBM和AEs</h1>
<p>既然我们已经看到了RBM的表现，就应该和AEs做一个比较。为了使这种比较公平，我们可以提出一个AE可以具有的最接近RBM的配置；也就是说，我们将拥有相同数量的隐藏单元(编码器层中的神经元)和相同数量的可见层(解码器层)中的神经元，如图<em>图10.6 </em>所示:</p>
<div/>
<div><img src="img/d8fefc75-8a3e-40ae-8e91-2ab36d6fea70.png" style="width:27.25em;height:23.83em;"/></div>
<p>图10.6–与RBM相当的AE配置</p>
<p>我们可以使用<a href="480521d9-845c-4c0a-b82b-be5f15da0171.xhtml">第7章</a>、<em>自动编码器</em>中介绍的工具对我们的AE进行建模和训练，如下所示:</p>
<pre>from tensorflow.keras.layers import Input, Dense<br/>from tensorflow.keras.models import Model<br/><br/>inpt_dim = 28*28    # 784 dimensions<br/>ltnt_dim = 100      # 100 components<br/><br/>inpt_vec = Input(shape=(<strong>inpt_dim</strong>,))<br/><strong>encoder</strong> = Dense(<strong>ltnt_dim</strong>, activation='sigmoid') (inpt_vec)<br/>latent_ncdr = Model(inpt_vec, encoder)<br/>decoder = Dense(<strong>inpt_dim</strong>, activation='sigmoid') (encoder)<br/><strong>autoencoder</strong> = Model(inpt_vec, decoder)<br/><br/>autoencoder.compile(loss='binary_crossentropy', optimizer='adam')<br/>autoencoder.fit(x_train, x_train, epochs=200, batch_size=1000)</pre>
<p>这里没有什么新的东西，除了我们只使用两个足够大的密集层来提供良好的表示。<em>图10.7 </em>描绘了测试集上学习表征的UMAP可视化:</p>
<div><img src="img/01853091-de4e-420d-a897-43d3f4b7e6b3.png" style="width:38.92em;height:34.42em;"/></div>
<p>图10.7–使用UMAP显像的AE诱发表现</p>
<p>上图是使用以下代码生成的:</p>
<pre>import matplotlib.pyplot as plt<br/>import umap<br/><br/>y_ = list(map(int, y_test))<br/>X_ = latent_ncdr.predict(x_test)<br/><br/>X_ = umap.UMAP().fit_transform(X_)<br/><br/>plt.figure(figsize=(10,8))<br/>plt.title('UMAP of 100 AE Learned Components on MNIST')<br/>plt.scatter(X_[:,0], X_[:,1], s=5.0, c=y_, alpha=0.75, cmap='tab10')<br/>plt.xlabel('$z_1$')<br/>plt.ylabel('$z_2$')<br/>plt.colorbar()</pre>
<p>从<em>图10.7 </em>中，您可以看到数据被很好地聚类；虽然聚类比<em>图10.3 </em>中的聚类靠得更近，但聚类内的分离似乎更好。类似于RBM，我们可以将学习到的权重可视化。</p>
<p><kbd>tensorflow.keras</kbd>中的每个<kbd>Model</kbd>对象都有一个名为<kbd>get_weights()</kbd>的方法，可以检索每一层的所有权重列表。让我们运行这个:</p>
<pre>latent_ncdr.<strong>get_weights</strong>()<strong>[0]</strong></pre>
<p class="mce-root"/>
<p>它使我们能够访问第一层的权重，并允许我们以与RBM权重相同的方式可视化它们。<em>图10.8 </em>显示了学习到的权重:</p>
<div><img src="img/af57fcd9-06c3-4286-959e-699e42a5904a.png" style="width:37.42em;height:35.67em;"/></div>
<p>图10.8–AE权重</p>
<p>图10.8 中<em>所示的重量与图10.4 </em>中<em>RBM的重量相比，没有明显的数字特征。这些特征似乎面向非常独特的区域中的纹理和边缘。这非常有趣，因为它表明，根本不同的模型将产生根本不同的潜在空间。</em></p>
<p>如果RBM和AEs都产生有趣的潜在空间，想象一下如果我们在深度学习项目中使用它们，我们会取得什么样的成就！试试看！</p>
<p class="mce-root"/>
<p>最后，为了证明AE实现了建模的高质量重建，我们可以查看<em>图10.9 </em>:</p>
<div><img src="img/37f7f0ac-e0ba-498e-80ac-6064f79c94a2.png" style="width:46.67em;height:10.08em;"/></div>
<p>图10.9–AE输入(顶行)和重建(底行)</p>
<p> </p>
<p>使用100个组件的重建看起来质量很高，如图<em>图10.9 </em>所示。然而，这对于RBM来说是不可能的，因为正如我们在本章中解释的那样，它们的目的不一定是重构数据。</p>
<h1 id="uuid-31f39b65-1079-4e6c-92d3-7c5ff853dc55">摘要</h1>
<p>这一中级章节向您展示了RBM工作原理及其应用背后的基本理论。我们特别关注伯努利RBM，它对可能遵循类似伯努利分布的输入数据进行操作，以实现快速学习和高效计算。我们使用MNIST数据集来展示RBM的学习表示是多么有趣，我们还可视化了学习的权重。我们通过比较RBM和一个非常简单的AE得出结论，并表明两者都学习高质量的潜在空间，同时是根本不同的模型。</p>
<p>此时，您应该能够实现自己的RBM模型，可视化其学习到的组件，并通过投影(转换)输入数据和查看隐藏层投影来查看学习到的潜在空间。您应该对在大型数据集(如MNIST)上使用RBM充满信心，甚至可以与AE进行比较。</p>
<p>下一章是关于监督深度学习的一组新章节的开始。<a href="03e9a734-fb56-485d-ae90-66fb98ecd4d1.xhtml">第11章</a>，<em>深度和广度神经网络</em>，将让我们开始一系列围绕监督深度学习的令人兴奋的新主题。本章将解释在监督环境下深度神经网络和广度神经网络的性能差异和复杂性。它将根据神经元之间的连接引入密集网络和稀疏网络的概念。你不能错过它！</p>
<p class="mce-root"/>
<h1 id="uuid-71cf4b97-b1d1-4ea5-8343-00449dd4feb9">问题和答案</h1>
<ol>
<li class="mce-root"><strong>为什么我们不能使用RBM执行数据重建？</strong></li>
</ol>
<p style="padding-left: 60px">成果管理制从根本上不同于工程设计。RBM旨在优化能量函数，而AE旨在优化数据重建函数。因此，我们不能用RBM进行重建。然而，这种根本的区别允许新的潜在空间是有趣和强大的。</p>
<ol start="2">
<li>我们能给RBM增加更多的层吗？</li>
</ol>
<p style="padding-left: 60px">不，在目前的模型中没有。神经元堆叠层的概念更符合深度AEs的概念。</p>
<ol start="3">
<li><strong>那么RBMs有什么酷的呢？</strong></li>
</ol>
<p style="padding-left: 60px">它们很简单。他们速度很快。它们提供了丰富的潜在空间。在这一点上他们没有对手。最接近的竞争对手是AEs。</p>
<h1 id="uuid-ce91dc15-81fa-4dcc-abc1-6282379a6da2">参考</h1>
<ul>
<li>辛顿和塞伊诺夫斯基(1983年6月)。最优感知推理。在<em> IEEE计算机视觉和模式识别会议</em>的会议记录中(第448卷)。IEEE纽约。</li>
<li>布鲁克斯，盖尔曼，琼斯和孟。).(2011).<em>马尔可夫链蒙特卡罗手册</em>。CRC出版社。</li>
<li>t .蒂勒曼(2008年7月)。使用似然梯度的近似值训练受限玻尔兹曼机。《第25届<em>机器学习国际会议论文集</em>(第1064-1071页)。</li>
<li>Yamashita、m . Tanaka、e . Yoshida、y . yama uchi和h . Fujiyoshii(2014年8月)。对于受限玻尔兹曼机是伯努利还是高斯。2014年第22届<em>国际模式识别会议</em>(第1520-1525页)。IEEE。</li>
<li>t .蒂勒曼和g .辛顿(2009年6月)。使用快速权重改善持续的对比差异。《第26届年度<em>机器学习国际会议论文集</em>(第1033-1040页)。</li>
</ul>


            

            
        
    


</body></html>