<html><head/><body><html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Image Classification and Recognition</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:a4c88040-7c66-4c85-9d59-26726397fe76" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">图像分类和识别</h1>

                

            

            

                

<p>在前几章中，我们研究了为分类和回归问题开发深度神经网络模型的过程。在这两种情况下，我们处理的是结构化数据，模型是监督学习类型的，目标变量是可用的。图像或图片属于非结构化数据范畴。在本章中，我们将借助一个易于理解的示例，使用Keras包来说明深度学习神经网络在图像分类和识别中的应用。我们将从一个小样本开始，来说明开发一个图像分类模型的步骤。我们将把这个模型应用到涉及图像或图片标记的监督学习环境中。</p>

<p>Keras包含几个用于图像分类的内置数据集，如CIFAR10、CIFAR100、MNIST和时尚MNIST。CIFAR10包含50，000个32 x 32颜色训练图像和10，000个测试图像，具有10个标签类别。另一方面，CIFAR100包含50，000个32 x 32颜色训练图像和10，000个测试图像，标签类别多达100个。MNIST数据集有60，000幅28 x 28灰度图像用于训练，10，000幅图像用于测试10个不同的数字。时尚-MNIST数据集有60，000幅28 x 28灰度图像用于训练，10，000幅图像用于测试10个时尚类别。这些数据集的格式已经可以直接用于开发深度神经网络模型，只需最少的数据准备相关步骤。然而，为了更好地处理图像数据，我们将从将原始图像从我们的计算机读入RStudio开始，并回顾使图像数据为构建分类模型做好准备所需的所有步骤。</p>

<p>所涉及的步骤包括探索图像数据、调整图像大小和形状、一键编码、开发顺序模型、编译模型、拟合模型、评估模型、进行预测以及使用混淆矩阵进行模型性能评估。</p>

<p>更具体地说，在本章中，我们将讨论以下主题:</p>

<ul>

<li>处理图像数据</li>

<li>数据准备</li>

<li>创建和拟合模型</li>

<li>模型评估和预测</li>

<li>性能优化技巧和最佳实践</li>

</ul>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Handling image data</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:a4c88040-7c66-4c85-9d59-26726397fe76" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">处理图像数据</h1>

                

            

            

                

<p>在本节中，我们将把图像数据读入R，并进一步研究它，以了解图像数据的各种特征。读取和显示图像的代码如下:</p>

<pre># Libraries<br/>library(keras)<br/>library(EBImage)<br/><br/># Reading and plotting images<br/>setwd("~/Desktop/image18")<br/>temp = list.files(pattern="*.jpg")<br/>mypic &lt;- list()<br/>for (i in 1:length(temp)) {mypic[[i]] &lt;- readImage(temp[i])}<br/>par(mfrow = c(3,6))<br/>for (i in 1:length(temp)) plot(mypic[[i]])<br/>par(mfrow = c(1,1))</pre>

<p>从前面的代码中可以看出，我们将利用<kbd>keras</kbd>和<kbd>EBImage</kbd>库。<kbd>EBImage</kbd>库对于处理和探索图像数据很有用。我们将从读取存储在我电脑的<kbd>image18</kbd>文件夹中的18个JPEG图像文件开始。这些图片各包含6张从互联网上下载的自行车、汽车和飞机的图片。使用<kbd>readImage</kbd>功能读取这些图像文件，并存储在<kbd>mypic</kbd>中。</p>

<p>以下截图显示了所有18张图片:</p>

<p class="CDPAlignCenter CDPAlign"><img src="img/44e3c243-de5b-4199-b4ae-dd2183181391.png" style="width:36.17em;height:21.00em;"/></p>

<p>从前面的截图中，我们可以看到自行车、汽车和飞机的六幅图像。你可能已经注意到，并非所有的图片都是同样的大小。例如，第五辆和第六辆自行车的尺寸明显不同。类似地，第四架和第五架飞机的大小也明显不同。让我们使用下面的代码更仔细地看看第五辆自行车的数据:</p>

<pre># Exploring 5th image data<br/>print(mypic[[5]])<br/><br/>OUTPUT<br/><strong>Image </strong><br/><strong>  colorMode    : Color </strong><br/><strong>  storage.mode : double </strong><br/><strong>  dim          : 299 169 3 </strong><br/><strong>  frames.total : 3 </strong><br/><strong>  frames.render: 1 </strong><br/><br/><strong>imageData(object)[1:5,1:6,1]</strong><br/><strong>     [,1] [,2] [,3] [,4] [,5] [,6]</strong><br/><strong>[1,]    1    1    1    1    1    1</strong><br/><strong>[2,]    1    1    1    1    1    1</strong><br/><strong>[3,]    1    1    1    1    1    1</strong><br/><strong>[4,]    1    1    1    1    1    1</strong><br/><strong>[5,]    1    1    1    1    1    1</strong><br/><br/>hist(mypic[[5]])</pre>

<p>使用<kbd>print</kbd>函数，我们可以看到一辆自行车的图像(非结构化数据)是如何被转换成数字(结构化数据)的。第五辆自行车的尺寸是299 x 169 x 3，通过将这三个数相乘得到总共151，593个数据点或像素。第一个数字299表示以像素为单位的图像宽度，第二个数字169表示以像素为单位的图像高度。请注意，彩色图像由代表红色、蓝色和绿色的三个通道组成。从数据中提取的小表显示了<em> x </em>维度中的前五行数据，以及<em> y </em>维度中的前六行数据，并且<em> z </em>维度的值为1。虽然表体中的所有值都是<kbd>1</kbd>，但是它们应该在<kbd>0</kbd>和<kbd>1</kbd>之间变化。</p>

<p>彩色图像有红色、绿色和蓝色通道。灰度图像只有一个通道。</p>

<p>第五辆自行车的这些数据点用于创建直方图，如下面的屏幕截图所示:</p>

<p class="CDPAlignCenter CDPAlign"><img src="img/e4f078ef-6f58-4453-80e3-65b411e56765.png" style="width:38.58em;height:23.92em;"/></p>

<p>前面的直方图显示了第五幅图像数据的强度值的分布。可以看出，对于该图像，大多数数据点具有高强度值。</p>

<p>现在让我们看看下面基于第16张图像(飞机图像)的数据直方图，以作比较:</p>

<p class="CDPAlignCenter CDPAlign"><img src="img/00dbaf85-4c53-4c46-a5d5-fcfb93c55782.png" style="width:42.00em;height:25.00em;"/></p>

<p>从前面的直方图中，我们可以看到该图像对于红色、绿色和蓝色具有不同的强度值。一般来说，强度值介于0和1之间。接近零的数据点代表图像中较暗的颜色，接近一的数据点代表图像中较亮的颜色。</p>

<p>让我们用下面的代码来看看与第16张图片有关的数据，一架飞机:</p>

<pre># Exploring 16th image data<br/>print(mypic[[16]])<br/><br/>OUTPUT<br/><br/>Image <br/> colorMode : Color <br/> storage.mode : double <br/> dim : 318 159 3 <br/> frames.total : 3 <br/> frames.render: 1 <br/><br/>imageData(object)[1:5,1:6,1]<br/> [,1] [,2] [,3] [,4] [,5] [,6]<br/>[1,] 0.2549020 0.2549020 0.2549020 0.2549020 0.2549020 0.2549020<br/>[2,] 0.2549020 0.2549020 0.2549020 0.2549020 0.2549020 0.2549020<br/>[3,] 0.2549020 0.2549020 0.2549020 0.2549020 0.2549020 0.2549020<br/>[4,] 0.2588235 0.2588235 0.2588235 0.2588235 0.2588235 0.2588235<br/>[5,] 0.2588235 0.2588235 0.2588235 0.2588235 0.2588235 0.2588235</pre>

<p>从前面代码提供的输出中，我们可以看到两幅图像具有不同的尺寸。第16幅图像的尺寸为318 x 159 x 3，总共有151，686个数据点或像素。</p>

<p>为了准备这些数据来开发图像分类模型，我们将从调整所有图像的尺寸到相同的尺寸开始。</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Data preparation</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:a4c88040-7c66-4c85-9d59-26726397fe76" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">数据准备</h1>

                

            

            

                

<p>在这一节中，我们将回顾为开发图像分类模型准备图像数据的步骤。这些步骤将涉及调整图像大小以获得所有图像的相同大小，随后是整形、数据分区和响应变量的一键编码。</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Resizing and reshaping</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:a4c88040-7c66-4c85-9d59-26726397fe76" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">调整大小和形状</h1>

                

            

            

                

<p>为了准备用于开发分类模型的数据，我们首先使用以下代码将所有18幅图像的尺寸调整为相同的大小:</p>

<pre># Resizing<br/>for (i in 1:length(temp)) {mypic[[i]] &lt;- resize(mypic[[i]], 28, 28)}</pre>

<p>从前面的代码中可以看出，所有图像的大小现在都调整为28 x 28 x 3。让我们再次绘制所有的图像，看看使用下面的代码调整大小的影响:</p>

<pre># Plot images<br/>par(mfrow = c(3,6))<br/>for (i in 1:length(temp)) plot(mypic[[i]])<br/>par(mfrow = c(1,1)</pre>

<p>当我们缩小图片的尺寸时，会导致像素数量减少，进而导致图片质量下降，如下图所示:</p>

<p class="CDPAlignCenter CDPAlign"><img src="img/0e274d67-8078-4686-97f5-f43f485b07b4.png" style="width:28.00em;height:17.25em;"/></p>

<p class="mce-root">接下来，我们将使用以下代码将28 x 28 x 3的维度重新调整为28 x 28 x 3的单一维度(或2，352个向量):</p>

<pre># Reshape<br/>for (i in 1:length(temp)) {mypic[[i]] &lt;- array_reshape(mypic[[i]], c(28, 28,3))}<br/>str(mypic)<br/><br/>OUTPUT<br/><br/>List of 18<br/> $ : num [1:28, 1:28, 1:3] 1 1 1 1 1 1 1 1 1 1 ...<br/> $ : num [1:28, 1:28, 1:3] 1 1 1 1 1 ...<br/> $ : num [1:28, 1:28, 1:3] 1 1 1 1 1 1 1 1 1 1 ...<br/> $ : num [1:28, 1:28, 1:3] 1 1 1 1 1 1 1 1 1 1 ...<br/> $ : num [1:28, 1:28, 1:3] 1 1 1 1 1 1 1 1 1 1 ...<br/> $ : num [1:28, 1:28, 1:3] 1 1 1 1 1 1 1 1 1 1 ...<br/> $ : num [1:28, 1:28, 1:3] 0.953 0.953 0.953 0.953 0.953 ...<br/> $ : num [1:28, 1:28, 1:3] 1 1 1 1 1 1 1 1 1 1 ...<br/> $ : num [1:28, 1:28, 1:3] 1 1 1 1 1 1 1 1 1 1 ...<br/> $ : num [1:28, 1:28, 1:3] 1 1 1 1 1 1 1 1 1 1 ...<br/> $ : num [1:28, 1:28, 1:3] 1 1 1 1 1 1 1 1 1 1 ...<br/> $ : num [1:28, 1:28, 1:3] 1 1 1 1 1 1 1 1 1 1 ...<br/> $ : num [1:28, 1:28, 1:3] 1 1 1 1 1 1 1 1 1 1 ...<br/> $ : num [1:28, 1:28, 1:3] 1 1 1 1 1 1 1 1 1 1 ...<br/> $ : num [1:28, 1:28, 1:3] 1 1 1 1 0.328 ...<br/> $ : num [1:28, 1:28, 1:3] 0.26 0.294 0.312 0.309 0.289 ...<br/> $ : num [1:28, 1:28, 1:3] 0.49 0.49 0.49 0.502 0.502 ...<br/> $ : num [1:28, 1:28, 1:3] 1 1 1 1 1 1 1 1 1 1 ..</pre>

<p>通过使用<kbd>str(mypic)</kbd>观察前面数据的结构，我们可以看到列表中有18个不同的条目，对应于我们开始时的18幅图像。</p>

<p>接下来，我们将创建培训、验证和测试数据。</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Training, validation, and test data</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:a4c88040-7c66-4c85-9d59-26726397fe76" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">培训、验证和测试数据</h1>

                

            

            

                

<p>我们将分别使用自行车、汽车和飞机的前三幅图像进行训练，每种类型的第四幅图像用于验证，每种类型的其余两幅图像用于测试。因此，训练数据将有九个图像，验证数据将有三个图像，测试数据将有六个图像。下面是实现这一点的代码:</p>

<pre># Training Data<br/>a &lt;- c(1:3, 7:9, 13:15)<br/>trainx &lt;- NULL<br/>for (i in a) {trainx &lt;- rbind(trainx, mypic[[i]]) }<br/>str(trainx)<br/><br/>OUTPUT<br/><br/>num [1:9, 1:2352] 1 1 1 1 0.953 ...<br/><br/># Validation data<br/>b &lt;- c(4, 10, 16)<br/>validx &lt;- NULL<br/>for (i in b) {validx &lt;- rbind(validx, mypic[[i]]) }<br/>str(validx)<br/><br/>OUTPUT<br/><br/>num [1:3, 1:2352] 1 1 0.26 1 1 ...<br/><br/># Test Data<br/>c &lt;- c(5:6, 11:12, 17:18)<br/>testx &lt;- NULL<br/>for (i in c) {testx &lt;- rbind(testx, mypic[[i]])}<br/>str(testx)<br/><br/>OUTPUT<br/><br/>num [1:6, 1:2352] 1 1 1 1 0.49 ...</pre>

<p>正如您在前面的代码中看到的，我们将使用<kbd>rbind</kbd>函数在创建训练、验证和<kbd>test</kbd>数据时组合每个图像的数据行。在组合来自九个图像的数据行之后，<kbd>trainx</kbd>的结构指示有9行和2352列。类似地，对于验证数据，我们有3行2，352列，对于测试数据，我们有6行2，352列。</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>One-hot encoding</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:a4c88040-7c66-4c85-9d59-26726397fe76" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">一键编码</h1>

                

            

            

                

<p>对于响应变量的一次性编码，我们使用以下代码:</p>

<pre># Labels<br/>trainy &lt;- c(0,0,0,1,1,1,2,2,2)<br/>validy &lt;- c(0,1,2)<br/>testy &lt;- c(0,0,1,1,2,2)<br/><br/># One-hot encoding<br/>trainLabels &lt;- to_categorical(trainy)<br/>validLabels &lt;- to_categorical(validy)<br/>testLabels &lt;- to_categorical(testy)<br/>trainLabels<br/><br/>OUTPUT<br/><strong>      [,1] [,2] [,3]

 [1,]    1    0    0

 [2,]    1    0    0

 [3,]    1    0    0

 [4,]    0    1    0

 [5,]    0    1    0

 [6,]    0    1    0

 [7,]    0    0    1

 [8,]    0    0    1

 [9,]    0    0    1</strong></pre>

<p>从前面的代码中，我们可以看到以下内容:</p>

<ul>

<li>我们在<kbd>trainy</kbd>、<kbd>validy</kbd>和<kbd>testy</kbd>中存储了每个图像的目标值，其中<kbd>0</kbd>、<kbd>1</kbd>和<kbd>2</kbd>分别表示自行车、汽车和飞机图像。</li>

<li>我们通过使用<kbd>to_categorical</kbd>函数对<kbd>trainy</kbd>、<kbd>validy</kbd>和<kbd>testy</kbd>进行一键编码。这里的一键编码有助于将因子变量转换成零和一的组合。</li>

</ul>

<p>现在，我们有了可以用于开发深度神经网络分类模型的数据格式，这就是我们将在下一节中做的事情。</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Creating and fitting the model</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:a4c88040-7c66-4c85-9d59-26726397fe76" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">创建和拟合模型</h1>

                

            

            

                

<p>在这一部分中，我们将开发一个图像分类模型来对自行车、汽车和飞机的图像进行分类。我们将首先指定一个模型架构，然后我们将编译该模型，然后使用训练和验证数据来拟合该模型。</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Developing the model architecture</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:a4c88040-7c66-4c85-9d59-26726397fe76" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">开发模型架构</h1>

                

            

            

                

<p>在开发模型架构时，我们首先创建一个顺序模型，然后添加不同的层。以下是代码:</p>

<pre># Model architecture<br/>model &lt;- keras_model_sequential() <br/>model %&gt;% <br/>  layer_dense(units = 256, activation = 'relu', input_shape = c(2352)) %&gt;% <br/>  layer_dense(units = 128, activation = 'relu') %&gt;% <br/>  layer_dense(units = 3, activation = 'softmax')<br/>summary(model)<br/><br/>OUTPUT<br/><strong>______________________________________________________________________</strong><br/><strong>Layer (type)                   Output Shape              Param #     </strong><br/><strong>======================================================================</strong><br/><strong>dense_1 (Dense)                (None, 256)               602368      </strong><br/><strong>______________________________________________________________________</strong><br/><strong>dense_2 (Dense)                (None, 128)               32896       </strong><br/><strong>_____________________________________________________________________</strong><br/><strong>dense_3 (Dense)                (None, 3)                  387         </strong><br/><strong>======================================================================</strong><br/><strong>Total params: 635,651</strong><br/><strong>Trainable params: 635,651</strong><br/><strong>Non-trainable params: 0</strong><br/><strong>_______________________________________________________________________</strong></pre>

<p>从前面的代码可以看出，输入层有<kbd>2352</kbd>个单元(28 x 28 x 3)。对于初始模型，我们使用两个隐藏层，分别为256和128个单元。对于两个隐藏层，我们将使用<kbd>relu</kbd>激活功能。对于输出层，我们将使用3个单位，因为目标变量有3个类，分别代表自行车、汽车和飞机。该模型的参数总数为635，651。</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Compiling the model</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:a4c88040-7c66-4c85-9d59-26726397fe76" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">编译模型</h1>

                

            

            

                

<p>开发完模型架构后，我们可以使用以下代码编译模型:</p>

<pre># Compile model<br/>model %&gt;% compile(loss = 'categorical_crossentropy',<br/>  optimizer = 'adam',<br/>  metrics = 'accuracy')</pre>

<p>我们通过使用<kbd>categorical_crossentropy</kbd>来编译损失模型，因为我们正在进行多类分类。我们已经分别为优化器和指标指定了<kbd>adam</kbd>和<kbd>accuracy</kbd>。</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Fitting the model</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:a4c88040-7c66-4c85-9d59-26726397fe76" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">拟合模型</h1>

                

            

            

                

<p>现在我们准备训练模型。以下是这方面的代码:</p>

<pre># Fit model<br/>model_one &lt;- model %&gt;% fit(trainx, <br/>                         trainLabels, <br/>                         epochs = 30, <br/>                         batch_size = 32, <br/>                         validation_data =  list(validx, validLabels))<br/>plot(model_one)</pre>

<p>从前面的代码中，我们可以看到以下事实:</p>

<ul>

<li>我们可以使用存储在<kbd>trainx</kbd>中的<kbd>independent</kbd>变量和存储在<kbd>trainLabels</kbd>中的<kbd>target</kbd>变量来拟合模型。为了防止过度配合，我们将使用<kbd>validation_data</kbd>。</li>

</ul>

<p>注意，在前面的章节中，我们通过指定某个百分比来使用<kbd>validation_split</kbd>，比如20%；然而，如果我们以20%的比率使用<kbd>validation_split</kbd>，它将使用最后20%的训练数据(所有飞机图像)进行验证。</p>

<ul>

<li>这将产生这样的情况，其中训练数据没有来自飞机图像的样本，并且分类模型将仅基于自行车和汽车图像。</li>

</ul>

<ul>

<li>因此，得到的图像分类模型会有偏差，并且仅在自行车和汽车图像上表现良好。因此，在这种情况下，我们没有使用<kbd>validation_split </kbd>函数，而是使用了<kbd>validation_data</kbd>，我们已经确保在训练和验证数据中都有每种类型的样本。</li>

</ul>

<p>下图显示了分别针对训练和验证数据的30个时期的损失和准确性:</p>

<p class="CDPAlignCenter CDPAlign"><img src="img/90692e5d-7fe2-4fd2-b639-2de1f207dc26.png" style="width:36.33em;height:22.92em;"/></p>

<p>从前面的图中，我们可以得出以下结论:</p>

<ul>

<li>从处理精度的图形部分，我们可以看到，从第18个历元开始，训练数据的精度值达到最高值1。</li>

<li>另一方面，基于验证数据的准确性主要在三分之二左右，即66.7%。因为我们有来自用于验证的三个图像的数据，如果来自验证数据的所有三个图像都被正确分类，则报告的准确度将是1。在这种情况下，三分之二的图像被正确分类，这导致66.7%的准确率。</li>

<li>从处理损失的图表部分，我们可以看到，对于训练数据，损失值在8个时期后从大约3显著下降到小于1。他们从那时起继续减少；然而，损失值的下降速度减慢。</li>

</ul>

<ul>

<li>基于验证数据，可以看到大致相似的模式。</li>

<li>此外，由于损失在其计算中使用概率值，我们观察到与准确性相关图相比，损失相关图的趋势更清晰。</li>

</ul>

<p>接下来，让我们更详细地评估模型的图像分类性能，以了解其行为。</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Model evaluation and prediction</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:a4c88040-7c66-4c85-9d59-26726397fe76" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">模型评估和预测</h1>

                

            

            

                

<p>在本节中，我们将进行模型评估，并在预测的帮助下为训练和测试数据创建混淆矩阵。让我们从使用训练数据评估模型的图像分类性能开始。</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Loss, accuracy, and confusion matrices for training data</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:a4c88040-7c66-4c85-9d59-26726397fe76" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">训练数据的损失、准确性和混淆矩阵</h1>

                

            

            

                

<p>我们现在将获得训练数据的损失和准确性值，然后使用以下代码创建混淆矩阵:</p>

<pre># Model evaluation<br/>model %&gt;% evaluate(trainx, trainLabels)<br/><br/>OUTPUT<br/><strong>12/12 [==============================] - 0s 87us/step</strong><br/>$loss<br/>[1] 0.055556579307<br/><br/>$acc<br/>[1] 1<br/><br/># Confusion matrix<br/>pred &lt;- model %&gt;%   predict_classes(trainx)<br/>table(Predicted=pred, Actual=trainy)<br/><br/>OUTPUT<br/><strong>         Actual</strong><br/><strong>Predicted 0 1 2</strong><br/><strong>        0 3 0 0</strong><br/><strong>        1 0 3 0</strong><br/><strong>        2 0 0 3</strong></pre>

<p>从前面的输出可以看出，损耗和精度值分别是<kbd>0.056</kbd>和<kbd>1</kbd>。基于训练数据的混淆矩阵指示所有九个图像都被正确地分类为三个类别，因此得到的准确度是1。</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Prediction probabilities for training data</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:a4c88040-7c66-4c85-9d59-26726397fe76" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">训练数据的预测概率</h1>

                

            

            

                

<p>我们现在可以查看该模型提供的训练数据中所有九幅图像的三个类别的概率。以下是代码:</p>

<pre># Prediction probabilities<br/>prob &lt;- model %&gt;%   predict_proba(trainx) <br/>cbind(prob, Predicted_class = pred, Actual = trainy)<br/><br/>OUTPUT                                                <br/>                                                    <strong>Predicted_class Actual

 [1,] 0.9431666135788 0.007227868307 0.049605518579          0        0

 [2,] 0.8056846261024 0.005127847660 0.189187481999          0        0

 [3,] 0.9556384682655 0.001881886506 0.042479615659          0        0

 [4,] 0.0018005876336 0.988727569580 0.009471773170          1        1

 [5,] 0.0002136278927 0.998095452785 0.001690962003          1        1

 [6,] 0.0008950306219 0.994426369667 0.004678600468          1        1

 [7,] 0.0367377623916 0.010597365908 0.952664911747          2        2

 [8,] 0.0568452328444 0.011656147428 0.931498587132          2        2

 [9,] 0.0295505002141 0.011442330666 0.959007143974          2        2</strong></pre>

<p>在前面的输出中，前三列显示图像是自行车、汽车或飞机的概率，这三个概率的总和是1。我们可以从输出中观察到以下情况:</p>

<ul>

<li>对于自行车、汽车和飞机，训练数据中第一幅图像的概率分别为<kbd>0.943</kbd>、<kbd>0.007</kbd>和<kbd>0.049</kbd>。由于概率最高的是第一类，所以基于模型的预测类是<kbd>0</kbd>(针对自行车)，这也是图像的实际类。</li>

<li>虽然所有9幅图像都被正确分类，但正确分类的概率从<kbd>0.806</kbd>(图像2)到<kbd>0.998</kbd>(图像5)不等。</li>

<li>对于汽车图像(第4行到第6行)，正确分类的概率范围从<kbd>0.989</kbd>到<kbd>0.998</kbd>，并且对于所有三幅图像都是一致高的。因此，这种分类模型在对汽车图像进行分类时表现最佳。</li>

<li>对于自行车图像(第1行到第3行)，正确分类的概率在<kbd>0.806</kbd>到<kbd>0.956</kbd>之间，这表明对自行车图像进行正确分类有一定的难度。</li>

</ul>

<ul>

<li>对于代表自行车图像的第二个样本，第二高的概率是作为飞机图像的<kbd>0.189</kbd>。显然，这个模型在判断这张图片是自行车还是飞机时有点困惑。</li>

<li>对于飞机图像(第7行到第9行)，正确分类的概率范围从<kbd>0.931</kbd>到<kbd>0.959</kbd>，这对于所有三幅图像也是一致高的。</li>

</ul>

<p>通过查看预测概率，我们可以更深入地了解模型的分类性能，这不能仅通过查看准确性值来获得。然而，虽然训练数据的良好性能是必要的，但这不足以达到可靠的图像分类模型。当分类模型遇到过度拟合问题时，我们很难根据模型没有看到的测试数据的训练数据来复制好的结果。因此，对一个好的分类模型的真正测试是当它对测试数据表现良好时。现在让我们回顾一下测试数据模型的图像分类性能。</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Loss, accuracy, and confusion matrices for test data</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:a4c88040-7c66-4c85-9d59-26726397fe76" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">测试数据的损失、准确性和混淆矩阵</h1>

                

            

            

                

<p>我们现在可以获得测试数据的损失和准确性值，然后使用以下代码创建混淆矩阵:</p>

<pre># Loss and accuracy<br/>model %&gt;% evaluate(testx, testLabels)<br/><br/>OUTPUT<br/><strong>6/6 [==============================] - 0s 194us/step</strong><br/><strong>$loss</strong><br/><strong>[1] 0.5517520905</strong><br/><br/>$acc<br/>[1] 0.8333333<br/><br/># Confusion matrix<br/>pred &lt;- model %&gt;%   predict_classes(testx)<br/>table(Predicted=pred, Actual=testy)<br/><br/>OUTPUT<br/><strong>         Actual</strong><br/><strong>Predicted 0 1 2</strong><br/><strong>        0 2 0 0</strong><br/><strong>        1 0 1 0</strong><br/><strong>        2 0 1 2</strong></pre>

<p>从前面的输出可以看出，测试数据中图像的损失值和精度值分别是<kbd>0.552</kbd>和<kbd>0.833</kbd>。这些结果略逊于我们看到的训练数据的数字；然而，当基于看不见的数据评估模型时，性能会有所下降。混淆矩阵指示一个错误分类的图像，其中汽车的图像被误认为是飞机的图像。因此，对于六个正确分类中的五个，基于测试数据的模型准确度是83.3%。现在，让我们通过调查基于测试数据中的图像的概率值来更深入地了解模型的预测性能。</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Prediction probabilities for test data</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:a4c88040-7c66-4c85-9d59-26726397fe76" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">测试数据的预测概率</h1>

                

            

            

                

<p>我们现在可以查看测试数据中所有六个图像的三个类别的概率。以下是代码:</p>

<pre class="mce-root"># Prediction probabilities<br/>prob &lt;- model %&gt;%   predict_proba(testx) <br/>cbind(prob, Predicted_class = pred, Actual = testy)<br/><br/>OUTPUT                                         <br/><br/><strong>                                                Predicted_class Actual

[1,] 0.587377548218 0.02450981364 0.38811263442           0      0

[2,] 0.532718658447 0.04708640277 0.42019486427           0      0

[3,] 0.115497209132 0.18486714363 0.69963568449           2      1

[4,] 0.001700860681 0.98481327295 0.01348586939           1      1

[5,] 0.230999588966 0.03030913882 0.73869132996           2      2

[6,] 0.112148292363 0.02054920420 0.86730253696           2      2</strong></pre>

<p>通过观察这些预测的概率，我们可以得出以下结论:</p>

<ul>

<li>自行车图像预测正确，如前两个示例所示。然而，预测概率在<kbd>0.587</kbd>和<kbd>0.533</kbd>相对较低。</li>

<li>汽车图像(第3行和第4行)的结果是混合的，第四个样本以高概率<kbd>0.985</kbd>正确预测，但是第三个汽车图像以大约<kbd>0.7</kbd>的概率被错误分类为飞机。</li>

<li>飞机图像由第五和第六个样本表示。这两幅图像的预测概率分别为<kbd>0.739</kbd>和<kbd>0.867</kbd>。</li>

<li>尽管六幅图像中有五幅被正确分类，但与模型在训练数据上的表现相比，许多预测概率相对较低。</li>

</ul>

<p>因此，总的来说，我们可以说，该模型的性能肯定还有进一步改进的余地。在下一节中，我们将探讨如何提高模型的性能。</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Performance optimization tips and best practices</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:a4c88040-7c66-4c85-9d59-26726397fe76" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">性能优化技巧和最佳实践</h1>

                

            

            

                

<p>在这一节中，我们将探索一个更深层次的网络来提高图像分类模型的性能。我们将查看结果进行比较。</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Deeper networks</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:a4c88040-7c66-4c85-9d59-26726397fe76" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">更深的网络</h1>

                

            

            

                

<p>本节中用于试验更深层次网络的代码如下:</p>

<pre># Model architecture<br/>model &lt;- keras_model_sequential() <br/>model %&gt;% <br/>  layer_dense(units = 512, activation = 'relu', input_shape = c(2352)) %&gt;% <br/>  layer_dropout(rate = 0.1) %&gt;%<br/>  layer_dense(units = 256, activation = 'relu') %&gt;%<br/>  layer_dropout(rate = 0.1) %&gt;%<br/>  layer_dense(units = 3, activation = 'softmax')<br/>summary(model)<br/><br/>OUTPUT<br/><strong>_______________________________________________________________________</strong><br/><strong>Layer (type)                    Output Shape             Param #        </strong><br/><strong>=======================================================================</strong><br/><strong>dense_1 (Dense)                  (None, 512)               1204736  </strong><br/><strong>_______________________________________________________________________</strong><br/><strong>dropout_1 (Dropout)              (None, 512)               0              </strong><br/><strong>_______________________________________________________________________</strong><br/><strong>dense_2 (Dense)                  (None, 256)              131328         </strong><br/><strong>_______________________________________________________________________</strong><br/><strong>dropout_2 (Dropout)              (None, 256)               0              </strong><br/><strong>_______________________________________________________________________</strong><br/><strong>dense_3 (Dense)                  (None, 3)                 771            </strong><br/><strong>=======================================================================</strong><br/><strong>Total params: 1,336,835</strong><br/><strong>Trainable params: 1,336,835</strong><br/><strong>Non-trainable params: 0</strong><br/><strong>_______________________________________________________________________</strong><br/><br/># Compile model<br/>model %&gt;% compile(loss = 'categorical_crossentropy',<br/>  optimizer = 'adam',<br/>  metrics = 'accuracy')<br/><br/># Fit model<br/>model_two &lt;- model %&gt;% fit(trainx, <br/> trainLabels, <br/> epochs = 30, <br/> batch_size = 32, <br/> validation_data = list(validx, validLabels))<br/>plot(model_two)</pre>

<p>从前面的代码中，我们可以看到以下内容:</p>

<ul>

<li>我们将第一和第二隐藏层中的单元数量分别增加到<kbd>512</kbd>和<kbd>256</kbd>。</li>

<li>我们还在每个隐藏层后添加了10%的脱落率的脱落层。</li>

<li>这一变化的参数总数现在已经上升到<kbd>1336835</kbd>。</li>

<li>这一次，我们也将运行模型50个纪元。我们不对模型进行任何其他更改。</li>

</ul>

<p>下图提供了50个时期的训练和验证数据的准确度和损失值:</p>

<p class="CDPAlignCenter CDPAlign"><img src="img/b84682d8-3b5d-46f7-9e11-b4a021d69b5c.png" style="width:37.00em;height:23.83em;"/></p>

<p>从前面的图表中，我们可以看到以下内容:</p>

<ul>

<li>与早期模型相比，在精确度和损耗值方面观察到了一些重大变化。</li>

<li>50个时期后，训练和验证数据的准确度都是100%。</li>

<li>此外，损失和准确性的训练和验证相关曲线的接近程度表明该图像分类模型不太可能遭受过拟合问题。</li>

</ul>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Results</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:a4c88040-7c66-4c85-9d59-26726397fe76" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">结果</h1>

                

            

            

                

<p>为了进一步探索模型的图像分类性能的任何变化，这些变化从图形摘要中可能不明显，让我们看一些数字摘要:</p>

<ol>

<li>我们将首先查看基于训练数据的结果，并将使用以下代码:</li>

</ol>

<pre style="padding-left: 60px"># Loos and accuracy<br/>model %&gt;% evaluate(trainx, trainLabels)<br/>OUTPUT<br/>12/12 [==============================] - 0s 198us/step<br/>$loss<br/>[1] 0.03438224643<br/><br/>$acc<br/>[1] 1<br/><br/># Confusion matrix<br/>pred &lt;- model %&gt;%   predict_classes(trainx)<br/>table(Predicted=pred, Actual=trainy)<br/><br/>OUTPUT<br/><strong>         Actual</strong><br/><strong>Predicted 0 1 2</strong><br/><strong>        0 3 0 0</strong><br/><strong>        1 0 3 0</strong><br/><strong>        2 0 0 3</strong></pre>

<p style="padding-left: 60px">从前面的输出中，我们可以看到损耗值现在已经减少到<kbd>0.034</kbd>，精度保持在<kbd>1.0</kbd>。对于训练数据，我们获得了与之前相同的混淆矩阵结果，因为所有九幅图像都被模型正确分类，这给出了100%的准确度水平。</p>

<ol start="2">

<li>为了更深入地了解模型的分类性能，我们使用了以下代码和输出:</li>

</ol>

<pre style="padding-left: 30px"># Prediction probabilities<br/>prob &lt;- model %&gt;%   predict_proba(trainx) <br/>cbind(prob, Predicted_class = pred, Actual = trainy)<br/><br/>OUTPUT        <br/><strong>                                                   Predicted_class Actual

 [1,] 0.97638195753098 0.0071088117547 0.01650915294886     0         0

 [2,] 0.89875286817551 0.0019298568368 0.09931717067957     0         0

 [3,] 0.98671281337738 0.0004396488657 0.01284754090011     0         0

 [4,] 0.00058794603683 0.9992876648903 0.00012432398216     1         1

 [5,] 0.00005639552546 0.9999316930771 0.00001191849515     1         1

 [6,] 0.00020669832884 0.9997472167015 0.00004611289114     1         1

 [7,] 0.03771930187941 0.0022936603054 0.95998704433441     2         2

 [8,] 0.08463590592146 0.0022607713472 0.91310334205627     2         2

 [9,] 0.03016609139740 0.0019471622072 0.96788680553436     2         2</strong></pre>

<p style="padding-left: 60px">根据我们作为训练数据的输出而获得的前述预测概率，我们可以得出以下观察结果:</p>

<ul>

<li>正确的分类现在比早期的模型有更高的概率值。</li>

<li>基于第二行的最低正确分类概率是<kbd>0.899</kbd>。</li>

<li>因此，与以前的模型相比，这个模型在正确分类图像时似乎更有把握。</li>

</ul>

<ol start="3">

<li>现在让我们看看测试数据中是否也看到了这种改进。我们将使用以下代码和输出:</li>

</ol>

<pre style="padding-left: 60px"># Loss and accuracy<br/>model %&gt;% evaluate(testx, testLabels)<br/><br/>OUTPUT<br/><br/><strong>6/6 [==============================] - 0s 345us/step</strong><br/><strong>$loss</strong><br/><strong>[1] 0.40148338683</strong><br/><br/>$acc<br/>[1] 0.8333333<br/><br/># Confusion matrix<br/>pred &lt;- model %&gt;%   predict_classes(testx)<br/>table(Predicted=pred, Actual=testy)<br/><br/>OUTPUT<br/><strong>         Actual</strong><br/><strong>Predicted 0 1 2</strong><br/><strong>        0 2 0 0</strong><br/><strong>        1 0 1 0</strong><br/><strong>        2 0 1 2</strong></pre>

<p style="padding-left: 60px">如前面的输出所示，测试数据丢失和精度值分别为<kbd>0.401</kbd>和<kbd>0.833</kbd>。我们确实看到损失值有所改善；然而，精度值再次与先前相同。查看混淆矩阵，我们可以看到，这一次，一辆汽车的图像被错误地归类为飞机。因此，我们看不出基于混淆矩阵的任何重大差异。</p>

<ol start="4">

<li>接下来，让我们使用以下代码及其输出来查看预测概率:</li>

</ol>

<pre style="padding-left: 60px"># Prediction probabilities<br/>prob &lt;- model %&gt;%   predict_proba(testx) <br/>cbind(prob, Predicted_class = pred, Actual = testy)<br/><br/>OUTPUT<br/><strong>                                                   Predicted_class Actual

[1,] 0.7411330938339 0.015922509134 0.242944419384           0      0

[2,] 0.7733710408211 0.021422179416 0.205206796527           0      0

[3,] 0.3322730064392 0.237866103649 0.429860889912           2      1

[4,] 0.0005808877177 0.999227762222 0.000191345287           1      1

[5,] 0.2163420319557 0.009395645000 0.774262309074           2      2

[6,] 0.1447975188494 0.002772571286 0.852429926395           2      2</strong></pre>

<p>使用测试数据的预测概率，我们可以得出以下两个观察结果:</p>

<ul>

<li>我们看到了与我们根据训练数据的结果观察到的模式一致的相似模式。该模型以比早期模型(<kbd>0.53</kbd>到<kbd>0.98</kbd>)更高的概率(<kbd>0.74</kbd>到<kbd>0.99</kbd>)正确地分类测试数据中的图像。</li>

</ul>

<ul>

<li>对于测试数据中的第四个样本，模型似乎混淆了自行车和飞机的图像，而实际上，这个图像是一辆汽车。</li>

</ul>

<p>因此，总的来说，我们观察到，通过开发更深层次的神经网络，我们能够提高模型的性能。从精度计算来看，性能的提高并不明显；然而，预测概率的计算允许我们开发更好的洞察力和比较模型性能。</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Summary</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:a4c88040-7c66-4c85-9d59-26726397fe76" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">摘要</h1>

                

            

            

                

<p>在这一章中，我们探讨了图像数据和深度神经网络图像分类模型。我们使用了来自自行车、汽车和飞机的18幅图像的数据，并进行了适当的数据处理，以使数据准备好用于Keras库。我们将图像数据划分为训练、验证和测试数据，随后使用训练数据开发了一个深度神经模型，并通过查看训练和测试数据的损失、准确性、混淆矩阵和概率值来评估其性能。我们还对模型进行了修改，以提高其分类性能。此外，我们观察到，当混淆矩阵提供相同水平的性能时，预测概率可能能够帮助提取两个模型之间的细微差异。</p>

<p>在下一章中，我们将回顾使用<strong>卷积神经网络</strong>(<strong>CNN</strong>)开发深度神经网络图像分类模型的步骤，这在图像分类应用中变得非常流行。细胞神经网络被认为是图像分类问题的黄金标准，对于大规模的图像分类应用非常有效。</p>





            



            

        

    </body>



</html></body></html>