<html><head/><body><html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Image Classification for Small Data Using Transfer Learning</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:a4c88040-7c66-4c85-9d59-26726397fe76" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">基于迁移学习的小数据图像分类</h1>

                

            

            

                

<p class="mce-root">在前面的章节中，我们开发了深度学习网络，并探索了与图像数据相关的各种应用示例。与我们将在本章讨论的内容相比，一个主要的区别是，在前面的章节中，我们从头开始开发模型。</p>

<p class="mce-root">迁移学习可以被定义为一种方法，在这种方法中，我们重用经过训练的深度网络所学的知识来解决一个新的但相关的问题。例如，我们可能能够重复使用一个深度学习网络来分类数千种不同的时尚物品，以开发一个深度网络来分类三种不同类型的服装。这种方法类似于我们在现实生活中观察到的情况，老师将多年来获得的知识或学习传授给学生，教练将学习或经验传授给新球员。另一个例子是，学习骑自行车被转化为学习骑摩托车，这反过来对学习如何驾驶汽车变得有用。</p>

<p class="mce-root">在本章中，我们将在开发图像分类模型时利用预训练的深度网络。预训练模型允许我们将我们从更大的数据集中学到的有用特征转移到我们有兴趣用有点类似但新的相对较小的数据集开发的模型中。预训练模型的使用不仅允许我们克服由于数据集小而导致的问题，而且有助于减少开发模型的时间和成本。</p>

<p>为了说明预训练图像分类模型的使用，在本章中，我们将讨论以下主题:</p>

<ul>

<li>使用预训练模型来识别图像</li>

<li>使用CIFAR10数据集</li>

<li>基于CNN的图像分类</li>

<li>使用预训练的RESNET50模型对图像进行分类</li>

<li>模型评估和预测</li>

<li>性能优化技巧和最佳实践</li>

</ul>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Using a pretrained model to identify an image</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:a4c88040-7c66-4c85-9d59-26726397fe76" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">使用预训练模型来识别图像</h1>

                

            

            

                

<p>在我们继续之前，让我们加载本节需要的三个包:</p>

<pre># Libraries used<br/>library(keras)<br/>library(EBImage)<br/>library(tensorflow)</pre>

<p>Keras和TensorFlow库将用于开发预训练图像分类模型，而EBImage库将用于处理和可视化图像数据。</p>

<p class="mce-root">在Keras中，以下预训练图像分类模型可用:</p>

<ul>

<li>例外</li>

<li>VGG16</li>

<li>VGG19</li>

<li>ResNet50</li>

<li>InceptionV3</li>

<li>InceptionResNetV2</li>

<li>MobileNet</li>

<li>MobileNetV2</li>

<li>DenseNet</li>

<li>纳斯网</li>

</ul>

<p>这些预训练的模型在来自ImageNet(<a href="http://www.image-net.org/">http://www.image-net.org/</a>)的图像上进行训练。ImageNet是一个巨大的图像数据库，包含几百万张图像。</p>

<p>我们将从使用被称为<kbd>resnet50</kbd>的预训练模型来识别图像开始。下面是我们可以用来利用这个预训练模型的代码:</p>

<pre># Pretrained model<br/>pretrained &lt;- application_resnet50(weights = "imagenet")<br/>summary(pretrained)</pre>

<p>这里，我们将<kbd>weights</kbd>指定为<kbd>"imagenet"</kbd>。这使得我们可以重复使用RESNET50网络的预训练权重。RESNET50是深度为50层的深度残差网络，包括卷积神经网络层。请注意，如果我们只想使用没有预训练权重的模型架构，并且希望从头开始训练，那么我们可以将<kbd>weights</kbd>指定为<kbd>null</kbd>。通过使用<kbd>summary</kbd>，我们可以获得RESNET50网络的架构。但是，为了节省空间，我们不提供摘要的任何输出。该网络中的参数总数为25，636，712。RESNET50网络在使用来自ImageNet的100多万幅图像方面接受了培训，并有能力将图像分为1，000个不同的类别。</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Reading an image</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:a4c88040-7c66-4c85-9d59-26726397fe76" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">读取图像</h1>

                

            

            

                

<p>我们先来读一个RStudio里的一只狗的图像。以下代码加载一个图像文件，然后获取相应的输出:</p>

<p>使用RESNET50网络时，允许的最大目标大小是224 x 224，允许的最小目标大小是32 x 32。</p>

<pre># Read image data<br/>setwd("~/Desktop")<br/>img &lt;- image_load("dog.jpg", target_size = c(224,224))<br/>x &lt;- image_to_array(img)<br/>str(x)<br/>OUTPUT<br/>num [1:224, 1:224, 1:3] 70 69 68 73 88 79 18 22 21 20 ...<br/><br/># Image plot<br/>plot(as.raster(x, max = 255))  <br/><br/># Summary and histogram<br/>summary(x)<br/>OUTPUT<br/> Min. 1st Qu. Median Mean 3rd Qu. Max. <br/> 0.0 89.0 150.0 137.7 190.0 255.0 <br/>hist(x)</pre>

<p>在前面的代码中，我们可以观察到以下情况:</p>

<ul>

<li>使用Keras的<kbd>image_load()</kbd>功能，从224 x 224大小的电脑桌面上加载一张诺里奇梗狗的图片。</li>

<li>请注意，原始图像的大小可能不是224 x 224。然而，在加载图像的时候指定这个维度允许我们容易地调整原始图像的大小，以便它具有新的维度。</li>

<li>使用<kbd>image_to_array()</kbd>函数将该图像转换成一个数字数组。该阵列的结构显示了224×224×3的尺寸。</li>

<li>该数组的摘要显示它包含0到255之间的数字。</li>

</ul>

<p class="mce-root">以下是诺威奇梗犬的224 x 224彩色图片。这可以使用plot命令获得:</p>

<p class="CDPAlignCenter CDPAlign"><img src="img/c7f94f76-0669-4b3b-94fd-923433580bf8.png" style="width:25.75em;height:25.58em;"/></p>

<p>上图是一只诺威奇梗犬坐着向前看的照片。我们将利用这张图片，检查RESNET50模型是否能准确预测图片中狗的类型。</p>

<p>由数组中的值生成的直方图如下:</p>

<p class="CDPAlignCenter CDPAlign"><img src="img/6a64050d-273a-438d-b893-acc6f8172087.png" style="width:25.42em;height:24.00em;"/></p>

<p>阵列中前面的直方图值显示强度值的范围从0到255，大多数值集中在200左右。接下来，我们将预处理图像数据。该直方图可用于比较图像数据的结果变化。</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Preprocessing the input</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:a4c88040-7c66-4c85-9d59-26726397fe76" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">预处理输入</h1>

                

            

            

                

<p>现在，我们可以对输入进行预处理，使其可以用于预训练的RESNET50模型。预处理数据的代码如下:</p>

<pre># Preprocessing of input data<br/>x &lt;- array_reshape(x, c(1, dim(x))) <br/>x &lt;- imagenet_preprocess_input(x)<br/>hist(x)</pre>

<p>在前面的代码中，我们可以观察到以下情况:</p>

<ul>

<li>在应用<kbd>array_reshape()</kbd>函数后，数组的尺寸将变为1 x 224 x 224 x 3。</li>

<li>我们使用了<kbd>imagnet_preprocess_input()</kbd>函数，使用预先训练的模型以所需的格式准备数据。</li>

</ul>

<p>预处理后直方图形式的数据图如下:</p>

<p class="CDPAlignCenter CDPAlign"><img src="img/e135198d-2aef-4e2d-8f41-9b507ea26fdf.png" style="width:28.50em;height:26.33em;"/></p>

<p>预处理后的直方图显示了位置的移动。现在大部分数值都集中在50到100之间。但是，直方图的整体模式没有重大变化。</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Top five categories</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:a4c88040-7c66-4c85-9d59-26726397fe76" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">五大类别</h1>

                

            

            

                

<p>现在，我们可以通过提供预处理的图像数据作为输入，使用预训练的模型来进行预测。实现这一点的代码如下:</p>

<pre># Predictions for top 5 categories<br/>preds &lt;- pretrained %&gt;% predict(x)<br/>imagenet_decode_predictions(preds, top = 5)[[1]]<br/>Output<br/>  class_name  class_description       score<br/>1  n02094258    Norwich_terrier 0.769952953<br/>2  n02094114    Norfolk_terrier 0.126662806<br/>3  n02096294 Australian_terrier 0.046003290<br/>4  n02096177              cairn 0.040896162<br/>5  n02093991      Irish_terrier 0.005021056</pre>

<p>在前面的代码中，我们可以观察到以下情况:</p>

<ul>

<li>预测是使用<kbd>predict</kbd>函数进行的，包含1000个不同类别的概率，其中概率最高的五个类别是使用<kbd>imagenet_decode_predictions()</kbd>函数获得的。</li>

<li>大约0.7699的最高分正确地识别出该图片是诺威奇梗狗。</li>

<li>第二高的分数是诺福克梗狗，它看起来很像诺里奇梗狗。</li>

<li>预测还表明，照片可能是另一种类型的梗狗；然而，这些概率相对较小或可以忽略不计。</li>

</ul>

<p>在下一节中，我们将着眼于更大的图像数据集而不是单个图像，并使用预训练网络来开发图像分类模型。</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Working with the CIFAR10 dataset</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:a4c88040-7c66-4c85-9d59-26726397fe76" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">使用CIFAR10数据集</h1>

                

            

            

                

<p>为了说明预训练模型与新数据的使用，我们将使用CIFAR10数据集。CIFAR代表<em>加拿大高级研究所</em>，10指的是数据中包含的10类图像。CIFAR10数据集是Keras库的一部分，获取它的代码如下:</p>

<pre># CIFAR10 data<br/>data &lt;- dataset_cifar10()<br/>str(data)<br/>OUTPUT<br/>List of 2<br/> $ train:List of 2<br/>  ..$ x: int [1:50000, 1:32, 1:32, 1:3] 59 154 255 28 170 159 164 28 134 125 ...<br/>  ..$ y: int [1:50000, 1] 6 9 9 4 1 1 2 7 8 3 ...<br/> $ test :List of 2<br/>  ..$ x: int [1:10000, 1:32, 1:32, 1:3] 158 235 158 155 65 179 160 83 23 217 ...<br/>  ..$ y: num [1:10000, 1] 3 8 8 0 6 6 1 6 3 1 ...</pre>

<p>在前面的代码中，我们可以观察到以下情况:</p>

<ul>

<li>我们可以使用<kbd>dataset_cifar10()</kbd>函数读取数据集。</li>

<li>数据的结构显示，有50，000个带标签的训练图像可用。</li>

<li>它还包含10，000张带标签的测试图像。</li>

</ul>

<p>接下来，我们将使用以下代码从CIFAR10中提取训练和测试数据:</p>

<pre># Partitioning the data into train and test<br/>trainx &lt;- data$train$x       <br/>testx &lt;- data$test$x<br/>trainy &lt;- to_categorical(data$train$y, num_classes = 10)<br/>testy &lt;- to_categorical(data$test$y, num_classes = 10)<br/><br/>table(data$train$y)<br/>OUTPUT<br/>   0    1    2    3    4    5    6    7    8    9 <br/>5000 5000 5000 5000 5000 5000 5000 5000 5000 5000 <br/><br/>table(data$test$y)<br/>OUTPUT<br/>   0    1    2    3    4    5    6    7    8    9 <br/>1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 <br/><br/></pre>

<p>从前面的代码中，我们可以观察到以下内容:</p>

<ul>

<li>我们在<kbd>trainx</kbd>中保存训练图像数据，在<kbd>testx</kbd>中保存测试图像数据。</li>

<li>我们还使用<kbd>to_categorical()</kbd>函数对训练和测试数据标签进行一次性编码，并将结果分别保存在<kbd>trainy</kbd>和<kbd>testy</kbd>中。</li>

<li>训练数据的表格表明图像被分为10个不同的类别，每个类别正好包含5，000幅图像。</li>

<li>类似地，测试数据包含10个类别中每一个类别的1000幅图像。</li>

</ul>

<p>例如，可以使用以下代码获得训练数据中前64个图像的标签:</p>

<pre># Category Labels<br/>data$train$y[1:64,]<br/> [1] 6 9 9 4 1 1 2 7 8 3 4 7 7 2 9 9 9 3 2 6 4 3 6 6 2 6 3 5 4 0 0 9 1<br/>[34] 3 4 0 3 7 3 3 5 2 2 7 1 1 1 2 2 0 9 5 7 9 2 2 5 2 4 3 1 1 8 2</pre>

<p>正如我们所看到的，每张图片都用0到9之间的数字进行标记。下表描述了10种不同类别的图像:</p>

<table border="1" style="border-collapse: collapse;width: 827px;height: 456px">

<thead>

<tr>

<th class="CDPAlignCenter CDPAlign" style="width: 263.576px">标签</th>

<th class="CDPAlignCenter CDPAlign" style="width: 555.799px">描述</th>

</tr>

</thead>

<tbody>

<tr>

<td class="CDPAlignCenter CDPAlign" style="width: 263.576px">0</td>

<td class="CDPAlignCenter CDPAlign" style="width: 555.799px">飞机</td>

</tr>

<tr>

<td class="CDPAlignCenter CDPAlign" style="width: 263.576px">一</td>

<td class="CDPAlignCenter CDPAlign" style="width: 555.799px">汽车</td>

</tr>

<tr>

<td class="CDPAlignCenter CDPAlign" style="width: 263.576px">2</td>

<td class="CDPAlignCenter CDPAlign" style="width: 555.799px">伯德</td>

</tr>

<tr>

<td class="CDPAlignCenter CDPAlign" style="width: 263.576px">3</td>

<td class="CDPAlignCenter CDPAlign" style="width: 555.799px">猫</td>

</tr>

<tr>

<td class="CDPAlignCenter CDPAlign" style="width: 263.576px">四</td>

<td class="CDPAlignCenter CDPAlign" style="width: 555.799px">鹿</td>

</tr>

<tr>

<td class="CDPAlignCenter CDPAlign" style="width: 263.576px">5</td>

<td class="CDPAlignCenter CDPAlign" style="width: 555.799px">狗</td>

</tr>

<tr>

<td class="CDPAlignCenter CDPAlign" style="width: 263.576px">6</td>

<td class="CDPAlignCenter CDPAlign" style="width: 555.799px">青蛙</td>

</tr>

<tr>

<td class="CDPAlignCenter CDPAlign" style="width: 263.576px">七</td>

<td class="CDPAlignCenter CDPAlign" style="width: 555.799px">马</td>

</tr>

<tr>

<td class="CDPAlignCenter CDPAlign" style="width: 263.576px">8</td>

<td class="CDPAlignCenter CDPAlign" style="width: 555.799px">船</td>

</tr>

<tr>

<td class="CDPAlignCenter CDPAlign" style="width: 263.576px">9</td>

<td class="CDPAlignCenter CDPAlign" style="width: 555.799px">卡车</td>

</tr>

</tbody>

</table>

<p>请注意，这10个类别之间没有重叠。例如，汽车类别指轿车和SUV，而卡车类别仅指大型卡车。</p>

<p>样本图像</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Sample images</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:a4c88040-7c66-4c85-9d59-26726397fe76" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">CIFAR10的训练数据中的前64个图像可以使用以下代码绘制。这样，我们可以对数据集中包含的图像类型有所了解:</h1>

                

            

            

                

<p>CIFAR10的图像都是32 x 32的彩色图像。下图显示了8 x 8网格中的64幅图像:</p>

<pre># Plot of first 64 pictures<br/>par(mfrow = c(8,8), mar = rep(0, 4))<br/>for (i in 1:64) plot(as.raster(trainx[i,,,], max = 255))<br/>par(mfrow = c(1,1))</pre>

<p><img src="img/1db8a05c-8153-45ea-8868-1d191620f6b8.png" style="width:43.50em;height:40.83em;"/></p>

<p class="CDPAlignCenter CDPAlign">从前面的图像中，我们可以看到这些图像具有各种背景，并且分辨率较低。此外，有时这些图像并不完全可见，这使得图像分类成为一项具有挑战性的任务。</p>

<p>预处理和预测</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Preprocessing and prediction</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:a4c88040-7c66-4c85-9d59-26726397fe76" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">我们可以使用预训练的RESNET50模型来识别训练数据中的第二个图像。请注意，由于定型数据中的第二个图像大小为32 x 32，而RESNET50是在大小为224 x 224的图像上定型的，因此我们需要在应用之前使用的代码之前调整图像的大小。以下代码用于识别图像:</h1>

                

            

            

                

<p>从前面的代码中，我们可以观察到得分为0.9988的顶级类别是针对移动货车的。其他四个类别的分数相对来说可以忽略不计。</p>

<pre># Pre-processing and prediction<br/>x &lt;- resize(trainx[2,,,], w = 224, h = 224)<br/>x &lt;- array_reshape(x, c(1, dim(x)))<br/>x &lt;- imagenet_preprocess_input(x)<br/>preds &lt;- pretrained %&gt;% predict(x)<br/>imagenet_decode_predictions(preds, top = 5)[[1]]<br/>OUTPUT<br/>  class_name class_description        score<br/>1  n03796401        moving_van 9.988740e-01<br/>2  n04467665     trailer_truck 7.548324e-04<br/>3  n03895866     passenger_car 2.044246e-04<br/>4  n04612504              yawl 2.441246e-05<br/>5  n04483307          trimaran 1.862814e-05</pre>

<p>基于CNN的图像分类</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Image classification with CNN</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:a4c88040-7c66-4c85-9d59-26726397fe76" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">在本节中，我们将使用CIFAR10数据集的一个子集来开发一个基于卷积神经网络的图像分类模型，并评估其分类性能。</h1>

                

            

            

                

<p>数据准备</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Data preparation</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:a4c88040-7c66-4c85-9d59-26726397fe76" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">我们将通过仅使用来自CIFAR10的训练和测试数据中的前2，000个图像来保持较小的数据大小。这将允许图像分类模型在普通计算机或笔记本电脑上运行。我们还将调整训练和测试图像的大小，从32 x 32维调整到224 x 224维，以便能够比较训练前模型的分类性能。下面的代码包含了我们在本章前面讨论过的必要的预处理:</h1>

                

            

            

                

<p>在前面的代码中，在将尺寸从32 x 32调整到224 x 224时，我们使用了双线性插值，这是EBImage包的一部分。双线性插值将线性插值扩展到两个变量，在这种情况下是图像的高度和宽度。从下图所示的卡车前后图像中可以观察到双线性插值的效果:</p>

<pre># Selecting first 2000 images<br/>trainx &lt;- data$train$x[1:2000,,,] <br/>testx &lt;- data$test$x[1:2000,,,] <br/><br/># One-hot encoding<br/>trainy &lt;- to_categorical(data$train$y[1:2000,], num_classes = 10)<br/>testy &lt;- to_categorical(data$test$y[1:2000,] , num_classes = 10)<br/><br/># Resizing train images to 224x224<br/>x &lt;- array(rep(0, 2000 * 224 * 224 * 3), dim = c(2000, 224, 224, 3))<br/>for (i in 1:2000) { x[i,,,] &lt;- resize(trainx[i,,,], 224, 224) }<br/><br/># Plot of before/after resized image<br/>par(mfrow = c(1,2), mar = rep(0, 4))  <br/>plot(as.raster(trainx[2,,,], max = 255))<br/>plot(as.raster(x[2,,,], max = 255))<br/>par(mfrow = c(1,1))<br/><br/>trainx &lt;- imagenet_preprocess_input(x)<br/><br/># Resizing test images to 224x224<br/>x &lt;- array(rep(0, 2000 * 224 * 224 * 3), dim = c(2000, 224, 224, 3))<br/>for (i in 1:2000) { x[i,,,] &lt;- resize(testx[i,,,], 224, 224) }<br/>testx &lt;- imagenet_preprocess_input(x)</pre>

<p><img src="img/b87b0400-79fc-43ed-b4d3-bfc2b466229a.png" style="width:36.92em;height:19.00em;"/></p>

<p class="CDPAlignCenter CDPAlign">这里，我们可以看到，与原始图像(第一幅图像)相比，后一幅图像(第二幅图像)看起来更平滑，因为它包含更多的像素。</p>

<p>CNN模型</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>CNN model</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:a4c88040-7c66-4c85-9d59-26726397fe76" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">我们将从使用不太深的卷积神经网络来开发图像分类模型开始。为此，我们将使用以下代码:</h1>

                

            

            

                

<p class="mce-root">从前面的代码中，我们可以观察到以下内容:</p>

<pre># Model architecture<br/>model &lt;- keras_model_sequential()<br/>model %&gt;% <br/>  layer_conv_2d(filters = 32, kernel_size = c(3,3), activation = 'relu', <br/>                input_shape = c(224,224,3)) %&gt;%   <br/>  layer_conv_2d(filters = 32, kernel_size = c(3,3), activation = 'relu') %&gt;%  <br/>  layer_max_pooling_2d(pool_size = c(2,2)) %&gt;% <br/>  layer_dropout(rate = 0.25) %&gt;%   <br/>  layer_flatten() %&gt;% <br/>  layer_dense(units = 256, activation = 'relu') %&gt;%  <br/>  layer_dropout(rate = 0.25) %&gt;% <br/>  layer_dense(units = 10, activation = 'softmax') <br/>summary(model)<br/><strong>_________________________________________________________________________</strong><br/><strong>Layer (type)                    Output Shape                Param #       </strong><br/><strong>=========================================================================</strong><br/><strong>conv2d_6 (Conv2D)               (None, 222, 222, 32)         896           </strong><br/><strong>_________________________________________________________________________</strong><br/><strong>conv2d_7 (Conv2D)               (None, 220, 220, 32)         9248          </strong><br/><strong>_________________________________________________________________________</strong><br/><strong>max_pooling2d_22 (MaxPooling2D) (None, 110, 110, 32)          0             </strong><br/><strong>_________________________________________________________________________</strong><br/><strong>dropout_6 (Dropout)             (None, 110, 110, 32)          0             </strong><br/><strong>_________________________________________________________________________</strong><br/><strong>flatten_18 (Flatten)            (None, 387200)                0             </strong><br/><strong>__________________________________________________________________________</strong><br/><strong>dense_35 (Dense)                (None, 256)                99123456      </strong><br/><strong>__________________________________________________________________________</strong><br/><strong>dropout_7 (Dropout)             (None, 256)                   0             </strong><br/><strong>__________________________________________________________________________</strong><br/><strong>dense_36 (Dense)                (None, 10)                   2570          </strong><br/><strong>==========================================================================</strong><br/><strong>Total params: 99,136,170</strong><br/><strong>Trainable params: 99,136,170</strong><br/><strong>Non-trainable params: 0</strong><br/><strong>_______________________________________________________________________________________</strong>____<br/><br/># Compile<br/>model %&gt;% compile(loss = 'categorical_crossentropy',<br/> optimizer = 'rmsprop', <br/> metrics = 'accuracy')<br/><br/># Fit<br/>model_one &lt;- model %&gt;% fit(trainx, <br/>                         trainy, <br/>                         epochs = 10, <br/>                         batch_size = 10, <br/>                         validation_split = 0.2)</pre>

<p>该网络的参数总数为99，136，170。</p>

<ul>

<li>在编译模型时，我们使用<kbd>categorical_crossentropy</kbd>作为损失函数，因为响应有10个类别。</li>

<li>对于优化器，我们指定了<kbd>rmsprop</kbd>，这是一种基于梯度的优化方法，是一种流行的选择，可以提供相当好的性能。</li>

<li>我们用10个时期和10个批量来训练模型。</li>

<li>在训练数据的2，000幅图像中，20%(或400幅图像)用于评估验证误差，剩余的80%(或1，600幅图像)用于训练。</li>

<li><kbd>model_one</kbd>训练模型后的精度和损失值的曲线图如下:</li>

</ul>

<p><img src="img/a382f094-6b43-4c2c-9725-6168925657f7.png" style="width:38.92em;height:30.42em;"/></p>

<p class="CDPAlignCenter CDPAlign">从前面的图中，可以得出以下观察结果:</p>

<p>准确度和损失值的图显示，在大约4个时期后，训练和验证数据的损失和准确度值或多或少保持恒定。</p>

<ul>

<li>尽管训练数据的准确度达到接近100%的高值，但是基于验证数据中的图像，似乎对准确度没有影响。</li>

<li>此外，训练和验证数据的准确性之间的差距似乎很大，这表明存在过度拟合。在评估该模型的性能时，我们预计该模型在图像分类方面的准确性较低。</li>

<li>请注意，使用CNN开发一个像样的图像分类模型需要大量的图像进行训练，因此需要更多的时间和资源。在本章的后面，我们将学习如何使用预训练网络来帮助我们克服这个问题。不过，现在让我们继续评估图像分类模型的性能。</li>

</ul>

<p>模型性能</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Model performance</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:a4c88040-7c66-4c85-9d59-26726397fe76" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">为了评估模型的性能，我们将对训练和测试数据的损失、准确性和混淆矩阵进行计算。</h1>

                

            

            

                

<p>使用训练数据的性能评估</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Performance assessment with training data</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:a4c88040-7c66-4c85-9d59-26726397fe76" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">基于训练数据获得损失、准确度和混淆矩阵的代码如下:</h1>

                

            

            

                

<p>在这里，我们可以看到训练数据的损失和准确度值分别是3.335和0.846。混淆矩阵显示了基于训练数据的不错的结果。然而，对于某些类型的图像，错误分类率很高。例如，来自类别7(马)的12幅图像被错误分类为类别9(卡车)。同样，分别属于类别6(青蛙)和类别8(船)的11幅图像也被误划为类别9(卡车)。</p>

<pre># Loss and accuracy<br/>model %&gt;% evaluate(trainx, trainy)<br/>$loss<br/>[1] 3.335224<br/>$acc<br/>[1] 0.8455<br/><br/># Confusion matrix<br/>pred &lt;- model %&gt;%   predict_classes(trainx)<br/>table(Predicted=pred, Actual=data$train$y[1:2000,])<br/>         Actual<br/>Predicted   0   1   2   3   4   5   6   7   8   9<br/>        0 182   2   8   2   9   4   1   2  10   5<br/>        1   1 176   3   5   6   5   2   3   4   7<br/>        2   1   0 167   4   3   4   3   2   0   1<br/>        3   0   0   0 157   2   1   1   2   1   0<br/>        4   2   1   5   6 167   4   2   1   0   0<br/>        5   2   0   4   4   3 149   3   4   4   3<br/>        6   1   1   3   6   5   2 173   5   0   0<br/>        7   3   2   4   2   4   3   9 166   0   1<br/>        8  10   1   7   1   6   4   2   2 173   5<br/>        9   0   8   2   8   9   7  11  12  11 181</pre>

<p>使用测试数据进行性能评估</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Performance assessment with test data</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:a4c88040-7c66-4c85-9d59-26726397fe76" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">基于测试数据获得损失、准确度和混淆矩阵的代码如下:</h1>

                

            

            

                

<p>根据前面的输出，可以得出以下结论:</p>

<pre># Loss and accuracy<br/>model %&gt;% evaluate(testx, testy)<br/>$loss<br/>[1] 16.4562<br/>$acc<br/>[1] 0.2325<br/><br/># Confusion matrix<br/>pred &lt;- model %&gt;% predict_classes(testx)<br/>table(Predicted = pred, Actual = data$test$y[1:2000,])<br/>         Actual<br/>Predicted  0  1  2  3  4  5  6  7  8  9<br/>        0 82 24 29 17 16 10 17 19 67 19<br/>        1 16 65 20 26 18 21 26 26 33 53<br/>        2 10  0 26 20 20 18 14  5  1  2<br/>        3  6  5  8 21 12 22  9 12  9  3<br/>        4  4  8 22 11 22 16 25  9  6  4<br/>        5  5  7 12 29 17 29  9 19  4  9<br/>        6  6  6 20 17 23 15 51 25  6 13<br/>        7  3 10 10 15 21 16 11 37  3  5<br/>        8 34 22 20 12 22  2  7  7 61 24<br/>        9 30 51 28 31 27 36 47 34 27 71</pre>

<p>测试数据的损失值和准确度值分别为16.456和0.232。</p>

<ul>

<li>由于过拟合问题，这些结果不如我们对训练数据所观察到的那样令人印象深刻。</li>

<li>虽然我们可以尝试和开发更深的网络，以努力改善图像分类结果，或者尝试和增加训练数据，以提供更多的样本来学习，但在这里，我们将利用预训练的网络来获得更好的结果。</li>

</ul>

<p>使用预训练的RESNET50模型对图像进行分类</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Classifying images using the pretrained RESNET50 model</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:a4c88040-7c66-4c85-9d59-26726397fe76" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">在本节中，我们将利用预训练的RESNET50模型来开发一个图像分类模型。我们将使用与上一节中相同的训练和测试数据，以便更容易地比较分类性能。</h1>

                

            

            

                

<p>模型架构</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Model architecture</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:a4c88040-7c66-4c85-9d59-26726397fe76" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">我们将上传RESNET50模型，但不包括顶层。这将有助于我们定制预训练模型，以用于CIFAR10数据。由于RESNET50模型是在超过100万幅图像的帮助下训练的，因此它可以捕获有用的特征和图像表示，这些图像可以与新的但相似且较小的数据一起重复使用。预训练模型的这种可重用性不仅有助于减少从头开发图像分类模型的时间和成本，而且在训练数据相对较少时尤其有用。</h1>

                

            

            

                

<p>用于开发模型的代码如下:</p>

<p>上传RESNET50模型时，基于彩色图像的数据的输入尺寸被指定为224 x 224 x 3。虽然更小的尺寸也可以，但图像尺寸不能小于32 x 32 x 3。CIFAR10数据集中的图像尺寸为32 x 32 x 3，但我们将它们的尺寸调整为224 x 224 x 3，因为这样可以提高图像分类的准确性。</p>

<pre># RESNET50 network without the top layer<br/>pretrained &lt;- application_resnet50(weights = "imagenet",<br/>                                   include_top = FALSE,<br/>                                   input_shape = c(224, 224, 3))<br/><br/>model &lt;- keras_model_sequential() %&gt;% <br/>         pretrained %&gt;% <br/>         layer_flatten() %&gt;% <br/>         layer_dense(units = 256, activation = "relu") %&gt;% <br/>         layer_dense(units = 10, activation = "softmax")<br/>summary(model)<br/><strong>_______________________________________________________________________</strong><br/><strong>Layer (type)                   Output Shape              Param #     </strong><br/><strong>=======================================================================</strong><br/><strong>resnet50 (Model)               (None, 7, 7, 2048)        23587712    </strong><br/><strong>________________________________________________________________________</strong><br/><strong>flatten_6 (Flatten)            (None, 100352)               0           </strong><br/><strong>________________________________________________________________________</strong><br/><strong>dense_12 (Dense)               (None, 256)               25690368    </strong><br/><strong>________________________________________________________________________</strong><br/><strong>dense_13 (Dense)               (None, 10)                  2570        </strong><br/><strong>========================================================================</strong><br/><strong>Total params: 49,280,650</strong><br/><strong>Trainable params: 49,227,530</strong><br/><strong>Non-trainable params: 53,120</strong><br/><strong>_________________________________________________________________________</strong></pre>

<p>从前面的总结中，我们可以观察到以下情况:</p>

<p>RESNET50网络的输出尺寸为7 x 7 x 2，048。</p>

<ul>

<li>我们使用展平层将输出形状更改为具有7 x 7 x 2，048 = 100，352个元素的单列。</li>

<li>增加了256个单位的密集层和<kbd>relu</kbd>激活功能。</li>

<li>这个密集层导致(100，353 x 256) + 256 = 25，690，368个参数。</li>

<li>最后的密集层具有10个单元，用于具有10个类别和一个<kbd>softmax</kbd>激活功能的图像。该网络共有49，280，650个参数。</li>

<li>在网络的所有参数中，49，227，530个是可训练参数。</li>

<li>尽管我们可以用所有这些参数来训练网络，但这是不可取的。训练和更新与RESNET50网络相关的参数将导致我们失去从超过100万幅图像中学习到的特征所带来的好处。我们只使用来自2000幅图像的数据进行训练，并有10个不同的类别。因此，对于每个类别，我们只有大约200张图片。因此，冻结RESNET50网络中的权重非常重要，这将使我们获得使用预训练网络的好处。</li>

</ul>

<p>冻结预训练网络权重</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Freezing pretrained network weights</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:a4c88040-7c66-4c85-9d59-26726397fe76" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">冻结RESNET50网络的权重，然后编译模型的代码如下:</h1>

                

            

            

                

<p>在前面的代码中，我们可以观察到以下情况:</p>

<pre># Freeze weights of resnet50 network<br/>freeze_weights(pretrained)<br/><br/># Compile<br/>model %&gt;% compile(loss = 'categorical_crossentropy',<br/> optimizer = 'rmsprop', <br/> metrics = 'accuracy')<br/><br/><strong>summary(model)</strong><br/><strong>______________________________________________________</strong><br/><strong>Layer (type) Output Shape Param # </strong><br/><strong>======================================================</strong><br/><strong>resnet50 (Model) (None, 7, 7, 2048) 23587712 </strong><br/><strong>______________________________________________________</strong><br/><strong>flatten_6 (Flatten) (None, 100352) 0 </strong><br/><strong>______________________________________________________</strong><br/><strong>dense_12 (Dense) (None, 256) 25690368 </strong><br/><strong>______________________________________________________</strong><br/><strong>dense_13 (Dense) (None, 10) 2570 </strong><br/><strong>======================================================</strong><br/><strong>Total params: 49,280,650</strong><br/><strong>Trainable params: 25,692,938</strong><br/><strong>Non-trainable params: 23,587,712</strong><br/><strong>______________________________________________________</strong></pre>

<p>为了冻结RESNET50网络中的权重，我们使用了<kbd>freeze_weights()</kbd>功能。</p>

<ul>

<li>注意，在冻结预训练的网络权重之后，需要编译模型。</li>

<li>在冻结RESNET50网络的权重之后，我们观察到可训练参数的数量从49，227，530下降到较低值25，692，938。</li>

<li>这些参数属于我们添加的两个密集层，将帮助我们自定义来自RESNET50网络的结果，以便我们可以将它们应用于来自我们正在使用的CIFAR10数据的映像。</li>

<li>拟合模型</li>

</ul>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Fitting the model</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:a4c88040-7c66-4c85-9d59-26726397fe76" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">拟合模型的代码如下:</h1>

                

            

            

                

<p>从前面的代码中，我们可以观察到以下内容:</p>

<pre># Fit model<br/>model_two &lt;- model %&gt;% fit(trainx, <br/>                        trainy, <br/>                        epochs = 10, <br/>                        batch_size = 10, <br/>                        validation_split = 0.2)</pre>

<p class="mce-root">我们用10个历元和10的批量大小来训练网络。</p>

<ul>

<li class="mce-root">我们指定20%(或400张图像)用于评估验证损失和验证准确性，剩余的80%(或1600张图像)用于训练。</li>

<li class="mce-root">训练模型后的准确度和损失值的曲线如下:</li>

</ul>

<p class="mce-root"><img src="img/5deac824-d6ec-4195-917a-6a696101b72d.png" style="width:43.42em;height:32.17em;"/></p>

<p class="CDPAlignCenter CDPAlign">从损耗和精度值的曲线图中，我们可以得出以下观察结果:</p>

<p>与之前的图相比有一个重要的区别，在之前的图中没有使用预训练模型。该图向我们显示，与之前的图相比，该模型在第二个时期达到了超过60%的准确度，而之前的图保持在25%以下。因此，我们可以看到，预训练模型的使用对图像分类有直接影响。</p>

<ul>

<li>与训练数据相比，基于验证数据的改进是缓慢的。</li>

<li>尽管基于验证数据的准确度值显示出逐渐提高，但验证数据的损失值显示出更多的可变性。</li>

<li>在下一节中，我们将评估该模型并评估其预测性能。</li>

</ul>

<p>模型评估和预测</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Model evaluation and prediction</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:a4c88040-7c66-4c85-9d59-26726397fe76" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">现在，我们将针对训练和测试数据评估该模型的性能。将执行与损失、准确性和混淆矩阵相关的计算，以便我们可以评估模型图像的分类性能。我们还将获得10个类别中每一个类别的准确性。</h1>

                

            

            

                

<p>训练数据的损失、准确性和混淆矩阵</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Loss, accuracy, and confusion matrix with the training data</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:a4c88040-7c66-4c85-9d59-26726397fe76" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">获取训练数据的损失、准确性和混淆矩阵的代码如下:</h1>

                

            

            

                

<p>从前面的输出中，我们可以观察到以下情况:</p>

<pre># Loss and accuracy<br/>model %&gt;% evaluate(trainx, trainy)<br/>$loss<br/>[1] 1.954347<br/>$acc<br/>[1] 0.8785<br/><br/># Confusion matrix<br/>pred &lt;- model %&gt;%   predict_classes(trainx)<br/>table(Predicted=pred, Actual=data$train$y[1:2000,])<br/>         Actual<br/>Predicted   0   1   2   3   4   5   6   7   8   9<br/>        0 182   0   5   2   3   0   2   0  10   1<br/>        1   1 156   1   1   1   0   2   0   4   0<br/>        2   2   0 172   3   4   0   4   0   1   0<br/>        3   0   0   1 133   2  12   2   1   0   0<br/>        4   1   0   8   4 188   3   4   2   0   0<br/>        5   1   0   4  22   3 162   1   3   0   0<br/>        6   0   0   3   9   3   0 192   1   1   0<br/>        7   3   0   5  10  10   5   0 188   0   0<br/>        8   5   0   3   3   0   1   0   1 182   0<br/>        9   7  35   1   8   0   0   0   3   5 202<br/><br/># Accuracy for each category<br/>100*diag(tab)/colSums(tab)<br/>       0        1        2        3        4 <br/>90.09901 81.67539 84.72906 68.20513 87.85047 <br/>       5        6        7        8        9 <br/>88.52459 92.75362 94.47236 89.65517 99.50739 </pre>

<p>基于训练数据的损失和准确度分别是1.954和0.879。</p>

<ul>

<li>这两个数字都比基于先前模型的相应结果有所改进。</li>

<li>混淆矩阵显示了不错的图像分类性能。</li>

<li>类别9(卡车)的图像分类性能最好，只有一幅图像被错误分类为类别0(飞机)，准确率为99.5%。</li>

<li>该模型对于类别-3(猫)是最混乱的，类别-3大部分被分类为类别-5(狗)或类别-7(马)，并且对于该类别仅提供68.2%的准确度。</li>

<li>在错误分类中，最高的情况(35幅图像)是当类别1(汽车)被错误分类为类别9(卡车)时。</li>

<li>接下来，我们将使用测试数据评估模型的性能。</li>

</ul>

<p>测试数据的损失、准确性和混淆矩阵</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Loss, accuracy, and confusion matrix with the test data</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:a4c88040-7c66-4c85-9d59-26726397fe76" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">获取测试数据的损失、准确性和混淆矩阵的代码如下:</h1>

                

            

            

                

<p>从前面的输出中，我们可以观察到以下情况:</p>

<pre># Loss and accuracy<br/>model %&gt;% evaluate(testx, testy)<br/>$loss<br/>[1] 4.437256<br/>$acc<br/>[1] 0.768<br/><br/># Confusion matrix<br/>pred &lt;- model %&gt;% predict_classes(testx)<br/>table(Predicted = pred, Actual = data$test$y[1:2000,])<br/>         Actual<br/>Predicted   0   1   2   3   4   5   6   7   8   9<br/>        0 158   1  12   0   5   1   6   2  15   0<br/>        1   3 142   0   2   0   2   3   1   9   2<br/>        2   2   0 139   8   6   3   6   0   0   0<br/>        3   0   0   3  86   5  13   6   1   0   0<br/>        4   4   0  14   6 138   5  10   4   1   0<br/>        5   0   0  15  47   6 148   2  12   0   0<br/>        6   0   0   4  12   9   3 178   0   0   0<br/>        7   2   0   4  23  27   9   3 169   0   0<br/>        8  13   1   1   5   1   0   0   0 179   2<br/>        9  14  54   3  10   1   1   2   4  13 199<br/><br/># Accuracy for each category<br/>100*diag(tab)/colSums(tab)<br/>       0        1        2        3        4 <br/>80.61224 71.71717 71.28205 43.21608 69.69697 <br/>       5        6        7        8        9 <br/>80.00000 82.40741 87.56477 82.48848 98.02956 </pre>

<p>基于测试数据的损耗和准确度分别为4.437和0.768。</p>

<ul>

<li>尽管这种基于测试数据的性能不如基于训练数据的结果，但与第一个模型的结果相比，这是一个显著的改进。</li>

<li>混淆矩阵提供了对模型性能的进一步了解。性能最好的是类别9(卡车)，有199个正确的分类，准确率为98%。</li>

</ul>

<ul>

<li>对于测试数据，该模型似乎对分类错误最多的类别3 (cat)最困惑。这一类的准确率可以低至43.2%。</li>

<li>单个类别(54张图像)的最高错误分类是类别1(汽车)，该类别被错误分类为类别9(卡车)。</li>

<li>76.8%的准确率，可以说这个图像分类性能还过得去。预训练模型的使用使我们能够将对涉及超过100万张图像的数据进行训练的模型的学习转移到包含来自CIFAR10数据集的2，000张图像的新数据。与完全从零开始构建图像分类模型相比，这是一个巨大的优势，因为从零开始构建图像分类模型需要更多的时间和计算成本。既然我们已经从模型中获得了不错的性能，我们可以探索如何进一步改进它。</li>

</ul>

<p>性能优化技巧和最佳实践</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Performance optimization tips and best practices</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:a4c88040-7c66-4c85-9d59-26726397fe76" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">为了探索进一步的图像分类改进，在本节中，我们将尝试三个实验。在第一个实验中，我们将主要在编译模型时使用<kbd>adam</kbd>优化器。在第二个实验中，我们将通过改变密集层中的单元数量、脱落层中的脱落百分比以及拟合模型时的批次大小来进行超参数调整。最后，在第三个实验中，我们将使用另一个名为VGG16的预训练网络。</h1>

                

            

            

                

<p>试用adam优化器</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Experimenting with the adam optimizer</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:a4c88040-7c66-4c85-9d59-26726397fe76" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">在第一个实验中，我们将在编译模型时使用<kbd>adam</kbd>优化器。在训练模型时，我们还会将纪元的数量增加到20。</h1>

                

            

            

                

<p>训练模型后的准确度和损失值的曲线如下:</p>

<p><img src="img/f0987325-56b5-4aa2-ad2a-d7ea843fb6f4.png" style="width:45.17em;height:40.50em;"/></p>

<p class="CDPAlignCenter CDPAlign">该模型的前面的损失和准确度图显示，与训练数据相关的值在大约六个时期后是平坦的。对于验证数据，损失值显示出逐渐增加，而准确度值在第三个时期之后是平坦的。</p>

<p>获取测试数据的损失、准确性和混淆矩阵的代码如下:</p>

<p>从前面的输出中，我们可以观察到以下情况:</p>

<pre># Loss and accuracy<br/>model %&gt;% evaluate(testx, testy)<br/>$loss<br/>[1] 4.005393<br/>$acc<br/>[1] 0.7715<br/><br/># Confusion matrix<br/>pred &lt;- model %&gt;% predict_classes(testx)<br/>table(Predicted = pred, Actual = data$test$y[1:2000,])<br/>         Actual<br/>Predicted   0   1   2   3   4   5   6   7   8   9<br/>        0 136   0  20   4   2   0   1   5   2   4<br/>        1   3 177   1   1   0   0   0   0   2  26<br/>        2   7   0 124   2   3   1   3   0   1   0<br/>        3   2   0   4  80   7   6   7   2   1   0<br/>        4   3   1  18   9 151   4   8   9   0   0<br/>        5   2   0   3  58   3 152   4   5   0   3<br/>        6   3   2   8  22   8   8 190   0   6   2<br/>        7   1   0  14  18  22  14   2 172   0   0<br/>        8  36  11   3   5   2   0   1   0 205  12<br/>        9   3   7   0   0   0   0   0   0   0 156<br/><br/># Accuracy for each category<br/>100*diag(tab)/colSums(tab)<br/>       0        1        2        3        4 <br/>69.38776 89.39394 63.58974 40.20101 76.26263 <br/>       5        6        7        8        9 <br/>82.16216 87.96296 89.11917 94.47005 76.84729 </pre>

<p>测试数据的损失和准确度分别为4.005和0.772。</p>

<ul>

<li>这些结果略好于<kbd>model_two</kbd>的结果。</li>

<li>与之前的模型相比，混淆矩阵显示了稍微不同的图像分类模式。</li>

</ul>

<ul>

<li>类别8(船只)获得了最好的分类结果，在217个图像分类中有205个是正确的(准确率为94.5%)。</li>

<li>分类性能最低的是类别3 (cat)，199个预测中有80个正确(40.2%的准确率)。</li>

<li>最糟糕的错误分类是来自类别3(猫)的58幅图像被错误分类为类别5(狗)。</li>

<li>接下来，我们将进行超参数调整实验。</li>

</ul>

<p>超参数调谐</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Hyperparameter tuning</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:a4c88040-7c66-4c85-9d59-26726397fe76" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">在本实验中，我们将改变密集层中的单位、丢失率和批量大小，以获得有助于提高分类性能的值。这也说明了通过实验获得合适参数值的有效方式。我们将从使用以下代码创建一个<kbd>TransferLearning.R</kbd>文件开始:</h1>

                

            

            

                

<p>在前面的代码中，在阅读了预训练的模型之后，我们为想要试验的参数声明了三个标志。现在，我们可以在模型架构(密集单元和辍学率)和拟合模型的代码(批量大小)中使用这些标志。我们将时期的数量减少到了五个，对于优化器，在编译模型时，我们保留了<kbd>adam</kbd>。我们将把这个R文件保存在我们电脑的桌面上，我们称之为<kbd>TransferLearning.R</kbd>。</p>

<pre># Model with RESNET50<br/>pretrained &lt;- application_resnet50(weights = 'imagenet',<br/>                                   include_top = FALSE,<br/>                                   input_shape = c(224, 224, 3))<br/># Flags for hyperparameter tuning<br/>FLAGS &lt;- flags(flag_integer("dense_units", 256),<br/>               flag_numeric("dropout", 0.1),<br/>               flag_integer("batch_size", 10))<br/><br/># Model architecture<br/>model &lt;- keras_model_sequential() %&gt;% <br/>         pretrained %&gt;% <br/>         layer_flatten() %&gt;% <br/>         layer_dense(units = FLAGS$dense_units, activation = 'relu') %&gt;% <br/>         layer_dropout(rate = FLAGS$dropout) %&gt;%<br/>         layer_dense(units = 10, activation = 'softmax')<br/>freeze_weights(pretrained)<br/><br/># Compile<br/>model %&gt;% compile(loss = "categorical_crossentropy",<br/>                  optimizer = 'adam',<br/>                  metrics = 'accuracy')<br/><br/># Fit model<br/>history &lt;- model %&gt;% fit(trainx,<br/>                         trainy,<br/>                         epochs = 5,<br/>                         batch_size = FLAGS$batch_size,<br/>                         validation_split = 0.2)</pre>

<p>运行这个实验的代码如下:</p>

<p>在前面的代码中，我们可以看到工作目录被设置在<kbd>TransferLearning.R</kbd>文件的位置。注意，这个实验的输出也将保存在这个目录中。为了运行超参数调优实验，我们将使用<kbd>tfruns</kbd>库。对于密集层中的单元数量，我们将尝试256和512作为值。对于辍学率，我们将实验0.1和0.3。最后，对于批量，我们将尝试10和30。有三个参数，每个参数在两个值下进行试验，试验运行的总数将是2 <sup> 3 </sup> = 8。</p>

<pre># Set working directory<br/>setwd('~/Desktop')<br/><br/># Hyperparameter tuning<br/>library(tfruns)<br/>runs &lt;- tuning_run("TransferLearning.R", <br/>                   flags = list(dense_units = c(256, 512),<br/>                                dropout = c(0.1,0.3),<br/>                                batch_size = c(10, 30)))</pre>

<p>从该实验中获得的结果摘录如下:</p>

<p>前面的输出显示了基于所有八次实验运行的验证数据的损失和准确度值。为了便于参考，它还包括参数值。我们可以从前面的输出中观察到以下情况:</p>

<pre># Results<br/>runs[,c(6:10)]<br/>Data frame: 8 x 5 <br/><br/>  metric_val_loss metric_val_acc flag_dense_units flag_dropout flag_batch_size<br/>1          1.1935         0.7525              512          0.3              30<br/>2          0.9521         0.7725              256          0.3              30<br/>3          1.1260         0.8200              512          0.1              30<br/>4          1.3276         0.7950              256          0.1              30<br/>5          1.1435         0.7700              512          0.3              10<br/>6          1.3096         0.7275              256          0.3              10<br/>7          1.3458         0.7850              512          0.1              10<br/>8          1.0248         0.7950              256          0.1              10</pre>

<p>当密集单元的数量为512，退出率为0.1，批次大小为30时，获得最高精度值(第3行)。</p>

<ul>

<li>另一方面，当密集单元的数量是256、漏失率是0.3并且批量大小是10时，获得最低准确度值(行6)。</li>

<li>使用实验第3行的测试数据获得损失、准确性和混淆矩阵的代码如下:</li>

</ul>

<p>根据上述结果，我们可以得出以下结论:</p>

<pre># Loss and accuracy<br/>model %&gt;% evaluate(testx, testy)<br/>$loss<br/>[1] 1.095251<br/>$acc<br/>[1] 0.7975<br/><br/># Confusion matrix<br/>pred &lt;- model %&gt;% predict_classes(testx)<br/>(tab &lt;- table(Predicted = pred, Actual = data$test$y[1:2000,]))<br/>         Actual<br/>Predicted   0   1   2   3   4   5   6   7   8   9<br/>        0 167   5  20   4   5   4   4  15  10   8<br/>        1   1 176   0   3   0   0   1   1   2  15<br/>        2   3   0 139   9   2   4   8   1   1   0<br/>        3   0   0   3  92   6   6   5   0   0   0<br/>        4   4   0  20  16 177  12  17  23   0   1<br/>        5   0   0   7  50   1 149   1   9   0   0<br/>        6   1   0   2  11   2   5 177   1   0   1<br/>        7   0   0   0   5   3   4   1 143   0   0<br/>        8  16   3   3   5   2   0   1   0 203   6<br/>        9   4  14   1   4   0   1   1   0   1 172<br/><br/># Accuracy for each category<br/>100*diag(tab)/colSums(tab)<br/>       0        1        2        3        4        5        6        7 <br/>85.20408 88.88889 71.28205 46.23116 89.39394 80.54054 81.94444 74.09326 <br/>       8        9 <br/>93.54839 84.72906 </pre>

<p>测试数据的损失值和准确度值都比我们目前获得的结果要好。</p>

<ul>

<li>类别8(船只)获得了最好的分类结果，在217个图像分类中有203个是正确的(准确率为93.5%)。</li>

<li>分类性能最低的是类别3 (cat)，199个预测中有92个是正确的(准确率为46.2%)。</li>

<li>最糟糕的错误分类是来自类别3(猫)的50幅图像被错误分类为类别5(狗)。</li>

<li>在下一个实验中，我们将使用另一个预训练的网络:VGG16。</li>

</ul>

<p>将VGG16作为预训练网络进行试验</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Experimenting with VGG16 as a pretrained network</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:a4c88040-7c66-4c85-9d59-26726397fe76" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">在本实验中，我们将使用名为VGG16的预训练网络。VGG16是一个16层深的卷积神经网络，可以将图像分类成数千种类别。该网络还使用来自ImageNet数据库的超过一百万幅图像进行训练。模型架构和编译的代码如下所示:</h1>

                

            

            

                

<p>从前面的总结中，我们可以观察到以下情况:</p>

<pre># Pretrained model<br/>pretrained &lt;- application_vgg16(weights = 'imagenet', <br/>                           include_top = FALSE,<br/>                           input_shape = c(224, 224, 3))<br/><br/># Model architecture<br/>model &lt;- keras_model_sequential() %&gt;% <br/>  pretrained %&gt;% <br/>  layer_flatten() %&gt;% <br/>  layer_dense(units = 256, activation = "relu") %&gt;% <br/>  layer_dense(units = 10, activation = "softmax")<br/>summary(model)<br/><br/>freeze_weights(pretrained)<br/>summary(model)<br/><strong>_________________________________________________________________________</strong><br/><strong>Layer (type)                    Output Shape            Param #      </strong><br/><strong>=========================================================================</strong><br/><strong>vgg16 (Model)                  (None, 7, 7, 512)        14714688     </strong><br/><strong>__________________________________________________________________________</strong><br/><strong>flatten (Flatten)              (None, 25088)              0            </strong><br/><strong>__________________________________________________________________________</strong><br/><strong>dense (Dense)                  (None, 256)              6422784      </strong><br/><strong>__________________________________________________________________________</strong><br/><strong>dense_1 (Dense)                (None, 10)                2570         </strong><br/><strong>==========================================================================</strong><br/><strong>Total params: 21,140,042</strong><br/><strong>Trainable params: 6,425,354</strong><br/><strong>Non-trainable params: 14,714,688</strong><br/><strong>___________________________________________________________________________</strong><br/><br/># Compile model<br/>model %&gt;% compile(loss = 'categorical_crossentropy',<br/>                  optimizer = 'adam',    <br/>                  metrics = 'accuracy')<br/><br/># Fit model<br/>model_four &lt;- model %&gt;% fit(trainx, <br/>                         trainy, <br/>                         epochs = 10, <br/>                         batch_size = 10, <br/>                         validation_split = 0.2)</pre>

<p>该模型有21，140，042个参数，冻结VGG16的权重后，可训练参数总数减少到6，425，354个。</p>

<ul>

<li>在编译模型时，我们保留了对<kbd>adam</kbd>优化器的使用。</li>

<li>此外，我们运行10个时期来训练模型。所有其他设置都与我们在以前的型号中使用的设置相同。</li>

<li>训练模型后的准确度和损失值的曲线图如下:</li>

</ul>

<p><img src="img/57485ef8-cf6f-45da-be86-19fd0123cf60.png" style="width:44.25em;height:39.75em;"/></p>

<p class="CDPAlignCenter CDPAlign">前面的训练和验证数据的损失和准确度图表明，在大约四个时期之后，模型性能保持平稳。这与之前的模型形成对比，在之前的模型中，验证数据的损失值显示逐渐增加。</p>

<p>获取测试数据的损失、准确性和混淆矩阵的代码如下:</p>

<p>从前面的输出中，我们可以观察到以下情况:</p>

<pre># Loss and accuracy<br/>model %&gt;% evaluate(testx, testy)<br/>$loss<br/> [1] 1.673867<br/>$acc<br/> [1] 0.7565<br/><br/># Confusion matrix<br/>pred &lt;- model %&gt;% predict_classes(testx)<br/>(tab &lt;- table(Predicted = pred, Actual = data$test$y[1:2000,]))<br/>         Actual<br/>Predicted   0   1   2   3   4   5   6   7   8   9<br/>        0 137   2  12   0   6   0   0   1  11   6<br/>        1   9 172   1   0   0   0   0   1   9  21<br/>        2   7   0 123  11  11   3   3   5   3   0<br/>        3   3   0  11 130  10  35   7   7   0   0<br/>        4   7   0  13   5 118   7  10   5   1   0<br/>        5   1   0  11  27   3 125   2   7   0   0<br/>        6   2   5  20  18  21   8 192   3   4   1<br/>        7   6   0   4   6  25   7   2 163   2   1<br/>        8  18   6   0   2   4   0   0   1 182   3<br/>        9   6  13   0   0   0   0   0   0   5 171<br/><br/># Accuracy for each category<br/>100*diag(tab)/colSums(tab)<br/>       0        1        2        3        4 <br/>69.89796 86.86869 63.07692 65.32663 59.59596 <br/>       5        6        7        8        9 <br/>67.56757 88.88889 84.45596 83.87097 84.23645 </pre>

<p>测试数据的损失和准确度分别为1.674和0.757。</p>

<ul>

<li>困惑矩阵提供了进一步的见解。在对类别6(青蛙)进行分类时，该模型具有88.9%的最佳分类准确率。</li>

<li>另一方面，当对类别4(鹿)图像进行分类时，精度仅为大约59.6%。</li>

<li>在本节中，我们尝试了三种情况:</li>

</ul>

<p>使用<kbd>adam</kbd>优化器稍微改善了结果，并提供了大约77.2%的测试数据准确性。</p>

<ul>

<li>在第二个实验中，超参数调整提供了最佳结果，密集单元数为512，退出率为0.1，批量为30。这种参数组合帮助我们获得了大约79.8%的测试数据准确性。</li>

<li>第三个实验中，我们使用VGG16预训练网络，也提供了不错的结果。然而，它提供的测试数据准确度略低于75.7%。</li>

<li>处理较小数据集的另一种方法是使用数据扩充。在这种方法中，修改现有的图像(通过翻转、旋转、移动等)来创建新的样本。由于图像数据集中的图像并不总是居中，因此这种人工创建的新样本有助于我们了解有用的特征，从而提高图像分类性能。</li>

</ul>

<p>摘要</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Summary</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:a4c88040-7c66-4c85-9d59-26726397fe76" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">在这一章中，我们举例说明了使用预训练的深度神经网络来开发图像分类模型。这种预先训练的网络使用超过100万张图像进行训练，捕捉可重复使用的特征，这些特征可应用于类似但新的数据。当使用相对较小的数据集开发图像分类模型时，这一方面变得很有价值。此外，它们节省了计算资源和时间。我们首先利用RESNET50预训练网络来识别诺里奇梗狗的图像。随后，我们利用来自CIFAR10数据集的2，000幅图像来说明将预训练网络应用于相对较小的数据集的有效性。我们从零开始构建的初始卷积神经网络模型遭受过拟合，并且没有产生有用的结果。</h1>

                

            

            

                

<p>接下来，我们使用预训练的RESNET50网络，并通过在预训练网络之上添加两个密集层来定制它以满足我们的需求。我们获得了不错的结果，测试数据准确率约为76.8%。虽然预训练模型可以提供更快的结果，需要更少的历元，但我们需要借助一些实验来探索我们可以对模型性能进行的改进。为了探索更好的结果，我们用<kbd>adam</kbd>优化器进行了实验，它产生了大约77.2%的测试数据准确性。我们还进行了超参数调整，在密集层中的单元数量为512，脱落层中的脱落率为0.1，以及拟合模型时的批量大小为30方面产生了最佳水平。使用这种组合的图像分类精度产生了大约79.8%的测试数据精度。最后，我们对经过预训练的VGG16网络进行了实验，获得了大约75.6%的测试数据准确率。这些实验说明了我们可以如何探索和改进模型性能。</p>

<p>在下一章，我们将探索另一类有趣且流行的深度网络，称为<strong>生成对抗网络</strong> ( <strong> GANs </strong>)。我们将利用GANs来创造新的形象。</p>

<p>In the next chapter, we will explore another interesting and popular class of deep networks, called <strong>generative adversarial networks</strong> (<strong>GANs</strong>). We will make use of GANs to create new images.</p>





            



            

        

    </body>



</html></body></html>