<html><head/><body><html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Applying Autoencoder Neural Networks Using Keras</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:a4c88040-7c66-4c85-9d59-26726397fe76" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">使用Keras应用自动编码器神经网络</h1>

                

            

            

                

<p>自动编码器网络属于无监督学习类别的方法，其中标记的目标值不可用。然而，由于自动编码器通常使用的目标是某种形式的输入数据，它们也可以被称为自我监督学习方法。在本章中，我们将学习如何使用Keras应用自动编码器神经网络。我们将讨论自动编码器的三个应用:降维、图像去噪和图像校正。本章中的例子将使用时尚物品的图像、数字的图像和包含人物的图片。</p>

<p>更具体地说，在本章中，我们将讨论以下主题:</p>

<ul>

<li>自动编码器的类型</li>

<li>降维自动编码器</li>

<li>降噪自动编码器</li>

<li>图像校正自动编码器</li>

</ul>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Types of autoencoders</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:a4c88040-7c66-4c85-9d59-26726397fe76" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">自动编码器的类型</h1>

                

            

            

                

<p>自动编码器神经网络由两个主要部分组成:</p>

<ul>

<li>第一部分称为编码器，它降低了输入数据的维数。一般来说，这是一个图像。当来自输入图像的数据通过导致较低维度的网络传递时，网络被迫仅提取输入数据的最重要特征。</li>

<li>自动编码器的第二部分称为解码器，它试图从编码器的输出中重构原始数据。通过指定该网络应该尝试匹配什么输出来训练自动编码器网络。</li>

</ul>

<p>让我们考虑一些使用图像数据的例子。如果指定的输出与作为输入给出的图像相同，那么在训练之后，自动编码器网络被期望提供具有较低分辨率的图像，该图像保留了输入图像的关键特征，但是丢失了作为原始输入图像的一部分的一些更精细的细节。这种类型的自动编码器可用于降维应用。由于自动编码器基于能够捕捉数据中非线性的神经网络，因此与仅使用线性函数的方法相比，它们具有更好的性能。下图显示了自动编码器网络的编码器和解码器部分:</p>

<p class="CDPAlignCenter CDPAlign"><img src="img/0b47cb56-2078-4095-8607-d50b35bbfe3c.png"/></p>

<p class="mce-root">如果我们训练自动编码器，使得输入图像包含一些噪声或不清晰，并且输出与没有任何噪声的图像相同，那么我们可以创建去噪自动编码器。类似地，如果我们用这样的输入/输出图像训练自动编码器，其中我们有戴眼镜和不戴眼镜的图像，或者有胡子和没有胡子的图像，等等，我们可以创建有助于图像校正/修改的网络。</p>

<p class="mce-root">接下来，我们将看看如何使用自动编码器的三个独立示例:使用降维、图像去噪和图像校正。我们将从使用自动编码器进行降维开始。</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Dimension reduction autoencoders</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:a4c88040-7c66-4c85-9d59-26726397fe76" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">降维自动编码器</h1>

                

            

            

                

<p>在本节中，我们将使用时尚MNIST数据，指定自动编码器模型架构，编译模型，拟合模型，然后重建图像。注意，时尚MNIST是Keras库的一部分。</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>MNIST fashion data</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:a4c88040-7c66-4c85-9d59-26726397fe76" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">MNIST时尚数据</h1>

                

            

            

                

<p>我们将继续使用Keras和EBImage库。读取时尚MNIST数据的代码如下:</p>

<pre class="r"># Libraries<br/>library(keras)

library(EBImage)<br/><br/># Fashion-MNIST data<br/>mnist &lt;- dataset_fashion_mnist() <br/>str(mnist)<br/>List of 2<br/> $ train:List of 2<br/>  ..$ x: int [1:60000, 1:28, 1:28] 0 0 0 0 0 0 0 0 0 0 ...<br/>  ..$ y: int [1:60000(1d)] 9 0 0 3 0 2 7 2 5 5 ...<br/> $ test :List of 2<br/>  ..$ x: int [1:10000, 1:28, 1:28] 0 0 0 0 0 0 0 0 0 0 ...<br/>  ..$ y: int [1:10000(1d)] 9 2 1 1 6 1 4 6 5 7 ...</pre>

<p>这里，训练数据具有60，000幅图像，测试数据具有10，000幅时尚物品的图像。由于我们将在本例中使用无监督学习方法，因此我们将不使用可用于训练和测试数据的标签。</p>

<p>我们将训练图像数据存储在<kbd>trainx</kbd>中，将测试图像数据存储在<kbd>testx</kbd>中，如以下代码所示:</p>

<pre class="r"># Train and test data<br/>trainx &lt;- mnist$train$x

testx &lt;- mnist$test$x<br/><br/># Plot of 64 images

par(mfrow = c(8,8), mar = rep(0, 4))

for (i in 1:64) plot(as.raster(trainx[i,,], max = 255))</pre>

<p>在下图中可以看到时尚单品的前64个图像:</p>

<p class="CDPAlignCenter CDPAlign"><img src="img/7a0f5048-5008-4987-95a7-7adc47b687b6.png"/></p>

<p>接下来，我们将把图像数据整形为合适的格式，如下面的代码所示:</p>

<pre class="r"># Reshape images<br/>trainx &lt;- array_reshape(trainx, c(nrow(trainx), 28, 28, 1))

testx &lt;- array_reshape(testx, c(nrow(testx), 28, 28, 1))

trainx &lt;- trainx / 255

testx &lt;- testx / 255</pre>

<p>这里，我们还将<kbd>trainx</kbd>和<kbd>testx</kbd>除以255，将0-255之间的值的范围改为0-1之间的范围。</p>

<p class="mce-root"/>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Encoder model</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:a4c88040-7c66-4c85-9d59-26726397fe76" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">编码器型号</h1>

                

            

            

                

<p>为了指定编码器模型架构，我们将使用以下代码:</p>

<pre class="r"># Encoder<br/>input_layer &lt;- <br/>         layer_input(shape = c(28,28,1)) <br/>encoder &lt;-  input_layer %&gt;% <br/>         layer_conv_2d(filters = 8, <br/>                       kernel_size = c(3,3), <br/>                       activation = 'relu', <br/>                       padding = 'same') %&gt;%   <br/>         layer_max_pooling_2d(pool_size = c(2,2),<br/>                              padding = 'same') %&gt;% <br/>         layer_conv_2d(filters = 4, <br/>                       kernel_size = c(3,3), <br/>                       activation = 'relu', <br/>                       padding = 'same') %&gt;%  <br/>         layer_max_pooling_2d(pool_size = c(2,2), <br/>                              padding = 'same')  

summary(encoder)<br/>Output<br/>Tensor("max_pooling2d_10/MaxPool:0", shape=(?, 7, 7, 4), dtype=float32)</pre>

<p>这里，对于编码器的输入，我们指定输入层的大小为28 x 28 x 1。使用两个卷积层，一个具有8个滤波器，另一个具有4个滤波器。这两层的激活功能都使用<strong>整流线性单元</strong> ( <strong> relus </strong>)。卷积层包括<kbd>padding = 'same'</kbd>，在输出时保留输入的高度和宽度。例如，在第一个卷积层之后，输出的高度和宽度为28 x 28。每个卷积层之后是池层。在第一个池层之后，高度和宽度变为14 x 14，在第二个池层之后，高度和宽度变为7 x 7。在这个例子中，编码器网络的输出是7×7×4。</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Decoder model</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:a4c88040-7c66-4c85-9d59-26726397fe76" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">解码器模型</h1>

                

            

            

                

<p>为了指定解码器模型架构，我们将使用以下代码:</p>

<pre class="r"># Decoder<br/>decoder &lt;- encoder %&gt;% <br/>         layer_conv_2d(filters = 4, <br/>                       kernel_size = c(3,3), <br/>                       activation = 'relu',<br/>                       padding = 'same') %&gt;%   <br/>         layer_upsampling_2d(c(2,2)) %&gt;% <br/>         layer_conv_2d(filters = 8, <br/>                       kernel_size = c(3,3), <br/>                       activation = 'relu',<br/>                       padding = 'same') %&gt;%  <br/>         layer_upsampling_2d(c(2,2)) %&gt;% <br/>         layer_conv_2d(filters = 1, <br/>                       kernel_size = c(3,3), <br/>                       activation = 'sigmoid',<br/>                       padding = 'same')

summary(decoder)<br/>Output<br/>Tensor("conv2d_25/Sigmoid:0", shape=(?, 28, 28, 1), dtype=float32)</pre>

<p>这里，编码器模型已经成为解码器模型的输入。对于解码器网络，我们使用类似的结构，第一卷积层具有4个滤波器，第二卷积层具有8个滤波器。此外，我们现在使用上采样层，而不是池层。第一个上采样层将高度和宽度更改为14×14，第二个上采样层将其恢复为28×28的原始高度和宽度。在最后一层，我们利用sigmoid激活函数，确保输出值保持在0和1之间。</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Autoencoder model</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:a4c88040-7c66-4c85-9d59-26726397fe76" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">自动编码器模型</h1>

                

            

            

                

<p>自动编码器模型和显示输出形状和每个层的参数数量的模型概要如下:</p>

<pre class="r"># Autoencoder<br/>ae_model &lt;- keras_model(inputs = input_layer, outputs = decoder)

summary(ae_model)<br/><strong>__________________________________________________________________________

Layer (type)                      Output Shape               Param #       

==========================================================================

input_5 (InputLayer)              (None, 28, 28, 1)            0             

__________________________________________________________________________

conv2d_21 (Conv2D)                (None, 28, 28, 8)            80            

__________________________________________________________________________

max_pooling2d_9 (MaxPooling2D)    (None, 14, 14, 8)             0             

__________________________________________________________________________

conv2d_22 (Conv2D)                (None, 14, 14, 4)            292           

__________________________________________________________________________

max_pooling2d_10 (MaxPooling2D)   (None, 7, 7, 4)               0             

__________________________________________________________________________

conv2d_23 (Conv2D)                (None, 7, 7, 4)              148           

___________________________________________________________________________

up_sampling2d_9 (UpSampling2D)    (None, 14, 14, 4)             0             

___________________________________________________________________________

conv2d_24 (Conv2D)                (None, 14, 14, 8)            296           

___________________________________________________________________________

up_sampling2d_10 (UpSampling2D)   (None, 28, 28, 8)             0             

___________________________________________________________________________

conv2d_25 (Conv2D)                (None, 28, 28, 1)             73           

===========================================================================

Total params: 889

Trainable params: 889

Non-trainable params: 0

____________________________________________________________________________________</strong></pre>

<p>这里，除了输入层之外，自动编码器模型具有五个卷积层、两个最大池层和两个上采样层。这里，这个自动编码器模型中的参数总数是889。</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Compiling and fitting the model</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:a4c88040-7c66-4c85-9d59-26726397fe76" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">编译和拟合模型</h1>

                

            

            

                

<p>接下来，我们将使用以下代码编译并拟合模型:</p>

<pre class="r"># Compile model<br/>ae_model %&gt;% compile( loss='mean_squared_error',

         optimizer='adam')<br/><br/># Fit model<br/>model_one &lt;- ae_model %&gt;% fit(trainx, 

                         trainx, 

                         epochs = 20, 

                         shuffle=TRUE,

                         batch_size = 32, 

                         validation_data = list(testx,testx))</pre>

<p>这里，我们使用均方误差作为损失函数来编译模型，并将<kbd>adam</kbd>指定为优化器。为了训练模型，我们将使用<kbd>trainx </kbd>作为输入和输出。我们将使用<kbd>textx</kbd>进行验证。我们用32的批量大小来拟合模型，并使用20个时期。</p>

<p>以下输出显示了训练和验证数据的损失值图:</p>

<p class="CDPAlignCenter CDPAlign"><img src="img/76f4d73c-4630-49ef-b704-94b3edda787c.png"/></p>

<p>前面的图显示了良好的收敛性，没有显示任何过度拟合的迹象。</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Reconstructed images</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:a4c88040-7c66-4c85-9d59-26726397fe76" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">重建图像</h1>

                

            

            

                

<p>为了获得重建的图像，我们使用<kbd>predict_on_batch</kbd>来预测使用自动编码器模型的输出。我们使用以下代码来实现这一点:</p>

<pre class="r"># Reconstruct and plot images - train data<br/>rc &lt;-   ae_model %&gt;%    keras::predict_on_batch(x = trainx)

par(mfrow = c(2,5), mar = rep(0, 4))

for (i in 1:5) plot(as.raster(trainx[i,,,]))

for (i in 1:5) plot(as.raster(rc[i,,,]))</pre>

<p>来自训练数据的前五幅时尚图像(第一行)和相应的重建图像(第二行)如下:</p>

<p class="CDPAlignCenter CDPAlign"><img src="img/c8c31f06-520c-4258-a0b7-9bdb92d08ee3.png"/></p>

<p>这里，正如所预期的，可以看到重建的图像捕获了训练图像的关键特征。然而，它忽略了某些更细微的细节。例如，在原始训练图像中更清晰可见的标志在重建图像中变得模糊。</p>

<p>我们还可以使用来自测试数据的图像来查看原始图像和重建图像的绘图。为此，我们可以使用下面的代码:</p>

<pre class="r"># Reconstruct and plot images - train data<br/>rc &lt;- ae_model %&gt;% keras::predict_on_batch(x = testx) <br/>par(mfrow = c(2,5), mar = rep(0, 4)) <br/>for (i in 1:5) plot(as.raster(testx[i,,,])) <br/>for (i in 1:5) plot(as.raster(rc[i,,,]))</pre>

<p>下图显示了原始图像(第一行)和使用测试数据重建的图像(第二行):</p>

<p class="CDPAlignCenter CDPAlign"><img src="img/a2fc374d-bcf1-4380-b27f-c35a2ded9a8c.png"/></p>

<p>这里，重建图像的行为与它们之前对训练数据所做的一样。</p>

<p>在这个例子中，我们使用MNIST时尚数据来构建自动编码器网络，该网络通过保留主要特征并移除涉及更精细细节的特征来帮助降低图像的维度。接下来，我们将看看自动编码器模型的另一种变体，它有助于消除图像中的噪声。</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Denoising autoencoders</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:a4c88040-7c66-4c85-9d59-26726397fe76" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">降噪自动编码器</h1>

                

            

            

                

<p>在输入图像包含不需要的噪声的情况下，可以训练自动编码器网络来消除这种噪声。这是通过提供带有噪声的图像作为输入并提供相同图像的干净版本作为输出来实现的。训练自动编码器网络，使得自动编码器的输出尽可能接近输出图像。</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>MNIST data</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:a4c88040-7c66-4c85-9d59-26726397fe76" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">MNIST数据</h1>

                

            

            

                

<p>我们将利用Keras包中可用的MNIST数据来说明创建去噪自动编码器网络所涉及的步骤。可以使用以下代码读取MNIST数据:</p>

<pre class="r"># MNIST data<br/>mnist &lt;- dataset_mnist()

str(mnist)<br/>List of 2

 $ train:List of 2

  ..$ x: int [1:60000, 1:28, 1:28] 0 0 0 0 0 0 0 0 0 0 ...

  ..$ y: int [1:60000(1d)] 5 0 4 1 9 2 1 3 1 4 ...

 $ test :List of 2

  ..$ x: int [1:10000, 1:28, 1:28] 0 0 0 0 0 0 0 0 0 0 ...

  ..$ y: int [1:10000(1d)] 7 2 1 0 4 1 4 9 5 9 ...</pre>

<p>MNIST数据的结构表明它包含训练和测试数据，以及各自的标签。训练数据有60，000个从0到9的数字图像。类似地，测试数据有10，000个从0到9的数字图像。尽管每个图像都有一个标识图像的相应标签，但在本例中，标签数据不是必需的，因此我们将忽略此信息。</p>

<p>我们将在<kbd>trainx</kbd>中存储训练图像，在<kbd>testx</kbd>中存储测试图像。为此，我们将使用以下代码:</p>

<pre class="r"># Train and test data<br/>trainx &lt;- mnist$train$x

testx &lt;- mnist$test$x<br/><br/># Plot<br/>par(mfrow = c(8,8), mar = rep(0, 4))

for (i in 1:64) plot(as.raster(trainx[i,,], max = 255))</pre>

<p>下图显示了一个由8行8列64幅图像组成的图，这些图像基于MNIST的0到9之间的数字图像:</p>

<p class="CDPAlignCenter CDPAlign"><img src="img/a795fbe1-fb3c-48ff-8367-9667b7b0a8fb.png" style="width:23.67em;height:14.58em;"/></p>

<p>前面的图显示了各种书写风格的手写数字。我们将按照要求的格式对图像数据进行整形，并添加随机噪声。</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Data preparation</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:a4c88040-7c66-4c85-9d59-26726397fe76" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">数据准备</h1>

                

            

            

                

<p>接下来，我们将使用以下代码以所需格式重塑图像:</p>

<pre class="r"># Reshape<br/>trainx &lt;- array_reshape(trainx, c(nrow(trainx),28,28,1))

testx &lt;- array_reshape(testx, c(nrow(testx),28,28,1))

trainx &lt;- trainx / 255

testx &lt;- testx / 255</pre>

<p>在这里，我们重新调整了训练数据，使其大小为60，000 x 28 x 28 x 1，重新调整了测试数据，使其大小为10，000 x 28 x 28 x 1。我们还将介于0和255之间的像素值除以255，以获得介于0和1之间的新范围。</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Adding noise</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:a4c88040-7c66-4c85-9d59-26726397fe76" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">添加噪声</h1>

                

            

            

                

<p>为了向训练图像添加噪声，我们需要使用以下代码使用均匀分布来获得0和1之间的60，000 × 28 × 28个随机数:</p>

<pre class="r"># Random numbers from uniform distribution<br/>n &lt;- runif(60000*28*28,0,1)

n &lt;- array_reshape(n, c(60000,28,28,1))<br/><br/># Plot

par(mfrow = c(8,8), mar = rep(0, 4))

for (i in 1:64) plot(as.raster(n[i,,,]))</pre>

<p>在这里，我们正在对使用均匀分布生成的随机数进行整形，以匹配我们为训练图像提供的矩阵的维度。结果以图像的形式绘制，显示包含噪声的结果图像。</p>

<p>下图显示了包含噪点的图像:</p>

<p class="CDPAlignCenter CDPAlign"><img src="img/c4569d41-9dba-4c13-a702-617696522fcc.png" style="width:24.17em;height:14.83em;"/></p>

<p>描绘噪声的图像被添加到存储在<kbd>trainx</kbd>的图像中。我们需要将它除以2，以保持得到的<kbd>trainn</kbd>值在0和1之间。我们可以使用下面的代码来做到这一点:</p>

<pre class="r"># Adding noise to handwritten images - train data<br/>trainn &lt;- (trainx + n)/2

par(mfrow = c(8,8), mar = rep(0, 4))

for (i in 1:64) plot(as.raster(trainn[i,,,]))</pre>

<p>下图显示了前64幅训练图像及其噪波:</p>

<p class="CDPAlignCenter CDPAlign"><img src="img/385dc121-daff-4c4f-82b2-230468ef3b50.png" style="width:24.17em;height:14.92em;"/></p>

<p>尽管原始手写数字中添加了噪声，但这些数字仍然是可读的。使用去噪自动编码器的主要目的是训练一个保留手写数字并从图像中去除噪声的网络。</p>

<p>我们将使用以下代码对测试数据重复相同的步骤:</p>

<pre class="r"># Adding noise to handwritten images - test data<br/>n1 &lt;- runif(10000*28*28,0,1) <br/>n1 &lt;- array_reshape(n1, c(10000,28,28,1)) <br/>testn &lt;- (testx +n1)/2</pre>

<p>在这里，我们给测试图像添加了噪声，并将它们存储在<kbd>testn</kbd>中。现在，我们可以指定编码器架构。</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Encoder model</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:a4c88040-7c66-4c85-9d59-26726397fe76" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">编码器型号</h1>

                

            

            

                

<p>用于编码器网络的代码如下:</p>

<pre class="r"># Encoder<br/>input_layer &lt;- <br/>         layer_input(shape = c(28,28,1)) <br/>encoder &lt;-  input_layer %&gt;% <br/>         layer_conv_2d(filters = 32, <br/>                       kernel_size = c(3,3), <br/>                       activation = 'relu', <br/>                       padding = 'same') %&gt;%   <br/>         layer_max_pooling_2d(pool_size = c(2,2),<br/>                              padding = 'same') %&gt;% <br/>         layer_conv_2d(filters = 32, <br/>                       kernel_size = c(3,3), <br/>                       activation = 'relu', <br/>                       padding = 'same') %&gt;%  <br/>         layer_max_pooling_2d(pool_size = c(2,2), <br/>                              padding = 'same') 

summary(encoder)<br/>OutputTensor("max_pooling2d_6/MaxPool:0", shape=(?, 7, 7, 32), dtype=float32)</pre>

<p>这里，输入层的大小被指定为28 x 28 x 1。我们使用两个卷积层，每个卷积层有32个滤波器，一个整流器线性单元作为激活函数。每个卷积层之后是池层。在第一个池层之后，高度和宽度变为14 x 14，在第二个池层之后，高度和宽度变为7 x 7。在这个例子中，编码器网络的输出具有7×7×32个维度。</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Decoder model</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:a4c88040-7c66-4c85-9d59-26726397fe76" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">解码器模型</h1>

                

            

            

                

<p>对于解码器网络，我们保持相同的结构，只是我们使用上采样层，而不是池层。我们可以使用下面的代码来做到这一点:</p>

<pre class="r"># Decoder<br/>decoder &lt;- encoder %&gt;% <br/>         layer_conv_2d(filters = 32, <br/>                       kernel_size = c(3,3), <br/>                       activation = 'relu',<br/>                       padding = 'same') %&gt;%   <br/>         layer_upsampling_2d(c(2,2)) %&gt;% <br/>         layer_conv_2d(filters = 32, <br/>                       kernel_size = c(3,3), <br/>                       activation = 'relu',<br/>                       padding = 'same') %&gt;%  <br/>         layer_upsampling_2d(c(2,2)) %&gt;% <br/>         layer_conv_2d(filters = 1, <br/>                       kernel_size = c(3,3), <br/>                       activation = 'sigmoid',<br/>                       padding = 'same')

summary(decoder)<br/>Output<br/>Tensor("conv2d_15/Sigmoid:0", shape=(?, 28, 28, 1), dtype=float32)</pre>

<p>在前面的代码中，第一个上采样层将高度和宽度更改为14 x 14，第二个上采样层将其恢复为28 x 28的原始高度和宽度。在最后一层，我们使用sigmoid激活函数，确保输出值保持在0和1之间。</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Autoencoder model</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:a4c88040-7c66-4c85-9d59-26726397fe76" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">自动编码器模型</h1>

                

            

            

                

<p>现在，我们可以指定自动编码器网络。自动编码器的型号和概要如下:</p>

<pre class="r"># Autoencoder<br/>ae_model &lt;- keras_model(inputs = input_layer, outputs = decoder)

summary(ae_model)<br/><br/><strong>______________________________________________________________________

Layer (type)                    Output Shape             Param #       

======================================================================

input_3 (InputLayer)           (None, 28, 28, 1)           0             

______________________________________________________________________

conv2d_11 (Conv2D)             (None, 28, 28, 32)         320           

______________________________________________________________________

max_pooling2d_5 (MaxPooling2D) (None, 14, 14, 32)          0             

_______________________________________________________________________

conv2d_12 (Conv2D)             (None, 14, 14, 32)         9248          

_______________________________________________________________________

max_pooling2d_6 (MaxPooling2D) (None, 7, 7, 32)            0             

_______________________________________________________________________

conv2d_13 (Conv2D)             (None, 7, 7, 32)           9248          

_______________________________________________________________________

up_sampling2d_5 (UpSampling2D) (None, 14, 14, 32)          0             

_______________________________________________________________________

conv2d_14 (Conv2D)             (None, 14, 14, 32)         9248          

_______________________________________________________________________

up_sampling2d_6 (UpSampling2D) (None, 28, 28, 32)          0             

________________________________________________________________________

conv2d_15 (Conv2D)             (None, 28, 28, 1)           289           

========================================================================

Total params: 28,353

Trainable params: 28,353

Non-trainable params: 0

________________________________________________________________________</strong></pre>

<p>从前面对自动编码器网络的总结中，我们可以看到总共有28，353个参数。接下来，我们将使用以下代码编译这个模型:</p>

<pre class="r"># Compile model<br/>ae_model %&gt;% compile( loss='binary_crossentropy', optimizer='adam')</pre>

<p>对于去噪自动编码器，<kbd>bianary_crossentropy </kbd>损失函数的性能优于其他选项。</p>

<p>当编译自动编码器模型时，我们将使用<kbd>binary_crossentropy</kbd>作为损失函数，因为输入值在0和1之间。对于优化器，我们将使用<kbd>adam</kbd>。编译完模型后，我们就可以开始拟合了。</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Fitting the model</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:a4c88040-7c66-4c85-9d59-26726397fe76" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">拟合模型</h1>

                

            

            

                

<p>为了训练模型，我们使用存储在<kbd>trainn</kbd>中的带有噪声的图像作为输入，使用存储在<kbd>trainx</kbd>中的不带噪声的图像作为输出。用于拟合模型的代码如下:</p>

<pre># Fit model<br/>model_two &lt;- ae_model %&gt;% fit(trainn, <br/>                         trainx, <br/>                         epochs = 100, <br/>                         shuffle = TRUE,<br/>                         batch_size = 128,  <br/>                        validation_data = list(testn,testx))</pre>

<p>这里，我们还使用<kbd>testn</kbd>和<kbd>testx</kbd>来监控验证错误。我们将运行100个epochs，批量大小为128。网络训练完成后，我们使用以下代码获得训练和测试数据的损失值:</p>

<pre># Loss for train data<br/>ae_model %&gt;% evaluate(trainn, trainx)<br/>      loss <br/>0.07431865<br/><br/># Loss for test data<br/>ae_model %&gt;% evaluate(testn, testx)<br/>      loss <br/>0.07391542</pre>

<p>训练和测试数据的损失分别是0.0743和0.0739。这两个数字的接近表明不存在过拟合问题。</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Image reconstruction</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:a4c88040-7c66-4c85-9d59-26726397fe76" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">图像重建</h1>

                

            

            

                

<p>拟合模型后，我们可以使用以下代码重建图像:</p>

<pre class="r"># Reconstructing images - train data<br/>rc &lt;- ae_model %&gt;%   keras::predict_on_batch(x = trainn)<br/><br/># Plot

par(mfrow = c(8,8), mar = rep(0, 4))

for (i in 1:64) plot(as.raster(rc[i,,,]))</pre>

<p>在前面的代码中，我们已经使用<kbd>ae_model</kbd>通过提供包含在<kbd>trainn</kbd>中的噪声的图像来重建图像。如下图所示，我们绘制了前64幅重建图像，以观察噪声图像是否变得更加清晰:</p>

<p class="CDPAlignCenter CDPAlign"><img src="img/639dcb99-aa19-45ac-b4c9-0c984f5ae639.png" style="width:25.33em;height:15.58em;"/></p>

<p>从前面的图中，我们可以观察到自动编码器网络已经成功地消除了噪声。我们还可以在<kbd>ae_model</kbd>的帮助下使用以下代码重建测试数据的图像:</p>

<pre class="r"># Reconstructing images - test data<br/>rc &lt;- ae_model %&gt;% keras::predict_on_batch(x = testn) <br/>par(mfrow = c(8,8), mar = rep(0, 4)) <br/>for (i in 1:64) plot(as.raster(rc[i,,,]))</pre>

<p>测试数据中前64个手写数字的结果图像如下:</p>

<p class="CDPAlignCenter CDPAlign"><img src="img/cdb8d104-dd40-4bbf-90f1-f12906fd341a.png" style="width:25.42em;height:15.67em;"/></p>

<p>在这里，我们可以观察到去噪自动编码器在从0到9位数的图像中去除噪声方面做得不错。为了更仔细地观察模型的性能，我们可以绘制测试数据中的第一幅图像、带有噪声的相应图像以及去除噪声后的重建图像，如下所示:</p>

<p class="CDPAlignCenter CDPAlign"><img src="img/8631120a-b2aa-4b7e-bf32-bc466e01fb34.png" style="width:26.67em;height:8.67em;"/></p>

<p>在前面的截图中，第一个图像是原始图像，而第二个图像是添加噪声后获得的图像。向自动编码器提供第二幅图像作为输入，并且使从模型(第三幅图像)获得的结果与第一幅图像匹配。这里，我们可以看到去噪自动编码器网络有助于消除噪声。请注意，第三个图像无法保留我们在第一个图像中看到的原始图像的一些更精细的细节。例如，在原始图像中，与第三个图像相比，第七个图像在开始时看起来稍微厚一些，并且朝向下部。然而，它确实成功地从包含带有噪声的数字7的图像中提取了7的整体模式。</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Image correction</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:a4c88040-7c66-4c85-9d59-26726397fe76" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">图像校正</h1>

                

            

            

                

<p>在第三个应用程序中，我们将检查一个示例，在该示例中，我们将开发一个自动编码器模型来消除各种图片上某些人为创建的标记。我们将使用25张图片，图片上有一条黑线。读取图像文件并执行相关处理的代码如下:</p>

<pre># Reading images and image processing<br/>setwd("~/Desktop/peoplex")<br/>temp = list.files(pattern="*.jpeg")<br/>mypic &lt;- list()<br/>for (i in 1:length(temp)) {mypic[[i]] &lt;- readImage(temp[i])}<br/>for (i in 1:length(temp)) {mypic[[i]] &lt;- resize(mypic[[i]], 128, 128)}<br/>for (i in 1:length(temp)) {dim(mypic[[i]]) &lt;- c(128, 128,3)}</pre>

<p>在前面的代码中，我们从<kbd>peoplex</kbd>文件夹中读取带有<kbd>.jpeg</kbd>扩展名的图像，并调整这些图像的大小，使它们的高度和宽度为128 x 128。我们还将尺寸更新为128 x 128 x 3，因为所有图像都是彩色图像。</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Images that need correction</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:a4c88040-7c66-4c85-9d59-26726397fe76" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">需要校正的图像</h1>

                

            

            

                

<p>我们将使用以下代码组合25幅图像，然后绘制它们:</p>

<pre class="r"># Combine and plot images<br/>trainx &lt;- combine(mypic)<br/>str(trainx)<br/>Formal class 'Image' [package "EBImage"] with 2 slots

  ..@ .Data    : num [1:128, 1:128, 1:3, 1:16] 0.04435 0 0.00357 0.05779 0.05815 ...

  ..@ colormode: int 2<br/>trainx &lt;- aperm(trainx, c(4,1,2,3)<br/>par(mfrow = c(4,4), mar = rep(0, 4))

for (i in 1:16) plot(as.raster(trainx[i,,,]))</pre>

<p>在这里，我们在将所有25幅图像组合成<kbd>trainx</kbd>后保存它们的数据。查看<kbd>tranix</kbd>的结构，我们可以看到，合并图像数据后，尺寸现在变成了128 x 128 x 3 x 16。为了将其转换为所需的16 x 128 x 128 x 3格式，我们使用了<kbd>aperm</kbd>功能。然后，我们绘制所有25幅图像。请注意，如果图像是通过旋转绘制的，则可以在任何计算机上非常容易地将它们调整到正确的方向。以下是25张图片，所有图片上都有一条黑线:</p>

<p class="CDPAlignCenter CDPAlign"><img src="img/89a138f3-655a-4d24-8728-7160f9b4e5d4.png" style="width:28.33em;height:28.33em;"/></p>

<p>该应用程序中的自动编码器模型将使用这些带有黑线的图像作为输入，并将被训练以去除黑线。</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Clean images</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:a4c88040-7c66-4c85-9d59-26726397fe76" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">干净的图像</h1>

                

            

            

                

<p>我们还将读取同样的25幅没有黑线的图像，并将它们保存在<kbd>trainy</kbd>中，如以下代码所示:</p>

<pre># Read image files without black line<br/>setwd("~/Desktop/people")<br/>temp = list.files(pattern="*.jpg")<br/>mypic &lt;- list()<br/>for (i in 1:length(temp)) {mypic[[i]] &lt;- readImage(temp[i])}<br/>for (i in 1:length(temp)) {mypic[[i]] &lt;- resize(mypic[[i]], 128, 128)}<br/>for (i in 1:length(temp)) {dim(mypic[[i]]) &lt;- c(128, 128,3)}<br/>trainy &lt;- combine(mypic)<br/>trainy &lt;- aperm(trainy, c(4,1,2,3))<br/>par(mfrow = c(4,4), mar = rep(0, 4))<br/>for (i in 1:16) plot(as.raster(trainy[i,,,]))<br/>par(mfrow = c(1,1))</pre>

<p>这里，在调整大小和改变尺寸之后，我们正在合并图像，就像我们之前做的那样。我们还需要对尺寸进行一些调整，以获得所需的格式。接下来，我们将绘制所有25个干净的图像，如下所示:</p>

<p class="CDPAlignCenter CDPAlign"><img src="img/50fd289c-9b2f-4f3c-92d4-3571b323aa59.png" style="width:24.17em;height:24.17em;"/></p>

<p>在训练自动编码器网络时，我们将使用这些干净的图像作为输出。接下来，我们将指定编码器模型架构。</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Encoder model</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:a4c88040-7c66-4c85-9d59-26726397fe76" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">编码器型号</h1>

                

            

            

                

<p>对于编码器模型，我们将使用具有512、512和256个滤波器的三个卷积层，如以下代码所示:</p>

<pre># Encoder network<br/>input_layer &lt;- layer_input(shape = c(128,128,3)) <br/>encoder &lt;-  input_layer %&gt;% <br/>         layer_conv_2d(filters = 512, kernel_size = c(3,3), activation = 'relu', padding = 'same') %&gt;%   <br/>         layer_max_pooling_2d(pool_size = c(2,2),padding = 'same') %&gt;% <br/>         layer_conv_2d(filters = 512, kernel_size = c(3,3), activation = 'relu', padding = 'same') %&gt;%   <br/>         layer_max_pooling_2d(pool_size = c(2,2),padding = 'same') %&gt;% <br/>         layer_conv_2d(filters = 256, kernel_size = c(3,3), activation = 'relu', padding = 'same') %&gt;%  <br/>         layer_max_pooling_2d(pool_size = c(2,2), padding = 'same')  <br/>summary(encoder)<br/>Output<br/>Tensor("max_pooling2d_22/MaxPool:0", shape=(?, 16, 16, 256), dtype=float32)</pre>

<p>这里，编码器网络的大小为16 x 16 x 256。我们将保持其他类似于我们在前两个例子中使用的编码器模型的特性。现在，我们将指定自动编码器网络的解码器架构。</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Decoder model</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:a4c88040-7c66-4c85-9d59-26726397fe76" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">解码器模型</h1>

                

            

            

                

<p>对于解码器模型，前三个卷积层具有256、512和512个滤波器，如以下代码所示:</p>

<pre># Decoder network<br/>decoder &lt;- encoder %&gt;% <br/>         layer_conv_2d(filters = 256, kernel_size = c(3,3), activation = 'relu',padding = 'same') %&gt;%   <br/>         layer_upsampling_2d(c(2,2)) %&gt;% <br/>         layer_conv_2d(filters = 512, kernel_size = c(3,3), activation = 'relu',padding = 'same') %&gt;%   <br/>         layer_upsampling_2d(c(2,2)) %&gt;% <br/>         layer_conv_2d(filters = 512, kernel_size = c(3,3), activation = 'relu',padding = 'same') %&gt;%  <br/>         layer_upsampling_2d(c(2,2)) %&gt;% <br/>         layer_conv_2d(filters = 3, kernel_size = c(3,3), activation = 'sigmoid',padding = 'same')<br/>summary(decoder)<br/>Output<br/>Tensor("conv2d_46/Sigmoid:0", shape=(?, 128, 128, 3), dtype=float32)</pre>

<p>这里，我们使用了上采样层。在最后一个卷积层中，我们使用了一个sigmoid激活函数。在最后一个卷积层，我们使用了三个过滤器，因为我们正在利用彩色图像。最后，解码器模型的输出具有128×128×3个维度。</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Compiling and fitting the model</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:a4c88040-7c66-4c85-9d59-26726397fe76" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">编译和拟合模型</h1>

                

            

            

                

<p>现在，我们可以使用以下代码编译和拟合模型:</p>

<pre># Compile and fit model<br/>ae_model &lt;- keras_model(inputs = input_layer, outputs = decoder)<br/>ae_model %&gt;% compile( loss='mse',<br/>         optimizer='adam')<br/>model_three &lt;- ae_model %&gt;% fit(trainx, <br/>                         trainy, <br/>                         epochs = 100, <br/>                         batch_size = 128, <br/>                         validation_split = 0.2)<br/>plot(model_three)</pre>

<p>在前面的代码中，我们使用均方误差作为损失函数来编译自动编码器模型，并将<kbd>adam</kbd>指定为优化器。我们使用<kbd>trainx</kbd>，其中包含有黑线穿过的图像，作为模型的输入，使用<kbd>trainy</kbd>，其中包含干净的图像，作为模型试图匹配的输出。我们将历元的数量指定为100，并使用128的批量大小。使用0.2或20%的验证分割，我们将使用25个图像中的20个用于训练，使用25个图像中的5个用于计算验证误差。<br/></p>

<p>下图显示了<kbd>model_three</kbd>的训练和验证图像的100个时期的均方误差:</p>

<p class="CDPAlignCenter CDPAlign"><img src="img/82f0b487-14ef-4626-bac6-cde615926fd0.png"/></p>

<p>均方误差图表明，随着模型训练的进行，基于训练和验证数据的模型性能有所提高。我们还可以看到，在大约80到100个历元之间，模型的性能变得近似平坦。除此之外，增加历元数不太可能进一步提高模型性能。</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Reconstructing images from training data</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:a4c88040-7c66-4c85-9d59-26726397fe76" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">从训练数据重建图像</h1>

                

            

            

                

<p>现在，我们可以使用已经获得的模型从训练数据中重建图像。为此，我们可以使用以下代码:</p>

<pre># Reconstructing images - training<br/>rc &lt;- ae_model %&gt;%  keras::predict_on_batch(x = trainx)<br/>par(mfrow = c(5,5), mar = rep(0, 4))<br/>for (i in 1:25) plot(as.raster(rc[i,,,]))</pre>

<p>在前面的代码中，我们使用<kbd>predict_on_batch</kbd>来重建输入<kbd>trainx</kbd>后的图像，其中包含带有黑线的图像。这里可以看到所有25幅重建图像:</p>

<p class="CDPAlignCenter CDPAlign"><img class="details-image" src="img/05fe2408-d189-4377-a1be-5debedec0717.png" style="width:29.75em;height:29.75em;"/></p>

<p>从前面的图中可以看出，自动编码器模型已经学会了从输入图像中去除黑线。照片有些模糊，因为自动编码器模型试图只输出图像的主要特征，而忽略了某些细节。</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Reconstructing images from new data</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:a4c88040-7c66-4c85-9d59-26726397fe76" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">从新数据重建图像</h1>

                

            

            

                

<p>为了用新的和看不见的数据测试autoencoder模型，我们将使用25张有黑线的新图像。为此，我们将使用以下代码:</p>

<pre># 25 new images<br/>setwd("~/Desktop/newx")<br/>temp = list.files(pattern="*.jpg")<br/>mypic &lt;- list()<br/>for (i in 1:length(temp)) {mypic[[i]] &lt;- readImage(temp[i])}<br/>for (i in 1:length(temp)) {mypic[[i]] &lt;- resize(mypic[[i]], 128, 128)}<br/>for (i in 1:length(temp)) {dim(mypic[[i]]) &lt;- c(128, 128,3)}<br/>newx &lt;- combine(mypic)<br/>newx &lt;- aperm(newx, c(4,1,2,3))<br/>par(mfrow = c(4,4), mar = rep(0, 4))<br/>for (i in 1:16) plot(as.raster(newx[i,,,]))</pre>

<p>如前面的代码所示，我们读取新的图像数据，然后像前面一样格式化所有图像。下面的图中显示了所有25张有黑线穿过的新图片:</p>

<p class="CDPAlignCenter CDPAlign"><img src="img/498d8de0-0595-43d8-b0df-f46cd3507b1f.png" style="width:30.58em;height:30.58em;"/></p>

<p>这里，所有25张图片都有一条黑线穿过。我们将使用来自这些新图像的数据，并使用我们开发的自动编码器模型来重建图像，以消除黑线。用于重建和绘制图像的代码如下:</p>

<pre># Reconstructing images - new images<br/>rc &lt;- ae_model %&gt;% keras::predict_on_batch(x = newx)<br/>par(mfrow = c(5,5), mar = rep(0, 4))<br/>for (i in 1:25) plot(as.raster(rc[i,,,]))</pre>

<p>下面的屏幕截图显示了使用自动编码器模型后基于25个新图像重建的图像，这些新图像上有一条黑线穿过:</p>

<p class="CDPAlignCenter CDPAlign"><img src="img/4986e67b-57f0-4576-a6fe-3322e07f6c0b.png" style="width:32.17em;height:32.17em;"/></p>

<p>前面的屏幕截图再次显示了autoencoder模型成功地从所有图像中删除了黑线。然而，正如我们前面观察到的，图像质量很低。这个例子提供了有希望的结果。如果获得的结果也有更高质量的图像输出，那么我们可以在几种不同的情况下使用它。例如，我们可以将戴眼镜的图像重建为不戴眼镜的图像，反之亦然，或者我们可以将一个人不笑的图像重建为一个人微笑的图像。这种方法有几种变体，它们有可能具有重要的商业价值。</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Summary</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:a4c88040-7c66-4c85-9d59-26726397fe76" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">摘要</h1>

                

            

            

                

<p>在本章中，我们讨论了自动编码器网络的三个应用示例。第一种类型的自动编码器涉及降维应用。这里，我们使用了自动编码器网络架构，它只允许我们了解输入图像的关键特征。使用包含数字图像的MNIST数据来说明第二种类型的自动编码器。我们人为地在数字图像中添加噪声，并训练网络，使其学会从输入图像中去除噪声。第三种类型的自动编码器网络涉及图像校正应用。该应用中的自动编码器网络被训练为从输入图像中去除黑线。</p>

<p>在下一章中，我们将介绍另一类深度网络，称为<strong>迁移学习</strong>，并使用它们进行图像分类。</p>





            



            

        

    </body>



</html></body></html>