<html><head/><body>
<html xmlns:epub="http://www.idpf.org/2007/ops">
  <head>
    <title>NLP - Vector Representation</title>
    <meta content="urn:uuid:3fd593c7-d9c4-4d18-84e8-0b6efc257c5b" name="Adept.expected.resource"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body class="calibre">
        

                            
                    <h1 class="header-title">NLP向量表示法</h1>
                
            
            
                
<p class="calibre2"><strong class="calibre13">自然语言处理</strong> ( <strong class="calibre13"> NLP </strong>)是机器学习中最重要的技术之一。理解复杂的语言是<strong class="calibre13">人工智能</strong> ( <strong class="calibre13"> AI </strong>)至关重要的一部分。自然语言处理的应用几乎无处不在，因为我们主要通过语言交流，也主要用语言存储人类的知识。这包括网络搜索、广告、电子邮件、客户服务、机器翻译等等。一些热门研究领域包括语言建模(语音识别、机器翻译)、词义学习和消歧、知识库推理、声学建模、词性标注、命名实体识别、情感分析、聊天机器人、问答等。这些任务中的每一个都需要对任务或应用的深刻理解，以及有效和快速的机器学习模型。</p>
<p class="calibre2">类似于计算机视觉，从文本中提取特征是非常重要的。最近，深度学习方法在文本数据的表示学习方面取得了重大进展。本章将描述NLP中的单词嵌入。我们将讨论三种最先进的嵌入模型:Word2Vec、Glove和FastText。我们将展示一个如何在TensorFlow中训练Word2Vec的例子，以及如何做可视化。我们还将讨论Word2Vec、Glove和FastText之间的区别，以及如何在文本分类等应用中使用它们。</p>


            

            
        
    </body>

</html>


<html xmlns:epub="http://www.idpf.org/2007/ops">
  <head>
    <title>Traditional NLP</title>
    <meta content="urn:uuid:3fd593c7-d9c4-4d18-84e8-0b6efc257c5b" name="Adept.expected.resource"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body class="calibre">
        

                            
                    <h1 class="header-title">传统自然语言处理</h1>
                
            
            
                
<p class="calibre2">从基于文本的信息中提取有用的信息绝非易事。对于一个基本的应用，比如文档分类，常用的特征提取方式叫做<strong class="calibre13">单词包</strong> ( <strong class="calibre13"> BoW </strong>)，其中每个单词出现的频率作为训练分类器的特征。我们将在下一节中简要讨论BoW，以及tf-idf方法，该方法旨在反映一个单词对集合或语料库中的文档有多重要。</p>


            

            
        
    </body>

</html>


<html xmlns:epub="http://www.idpf.org/2007/ops">
  <head>
    <title>Bag of words</title>
    <meta content="urn:uuid:3fd593c7-d9c4-4d18-84e8-0b6efc257c5b" name="Adept.expected.resource"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body class="calibre">
        

                            
                    <h1 class="header-title">一袋单词</h1>
                
            
            
                
<p class="calibre2">BoW主要用于对文档进行分类。它也用于计算机视觉。这个想法是将文档表示为一个包或一组单词，而不考虑语法和单词序列的顺序。</p>
<p class="calibre2">在对文本进行预处理之后，通常称为<strong class="calibre13">语料库</strong>，生成一组词汇表，并在此基础上构建每个文档的BoW表示。</p>
<p class="calibre2">以下面两个文本示例为例:</p>
<pre class="calibre21">“The quick brown fox jumps over the lazy dog”<br class="title-page-name"/>“never jump over the lazy dog quickly”</pre>
<p class="calibre2">然后，语料库(文本样本)形成一个字典，将关键字作为单词，第二列作为单词<kbd class="calibre12">ID</kbd>:</p>
<pre class="calibre21">{<br class="title-page-name"/>    'brown': 0,<br class="title-page-name"/>    'dog': 1,<br class="title-page-name"/>    'fox': 2,<br class="title-page-name"/>    'jump': 3,<br class="title-page-name"/>    'jumps': 4,<br class="title-page-name"/>    'lazy': 5,<br class="title-page-name"/>    'never': 6,<br class="title-page-name"/>    'over': 7,<br class="title-page-name"/>    'quick': 8,<br class="title-page-name"/>    'quickly': 9,<br class="title-page-name"/>    'the': 10,<br class="title-page-name"/> }</pre>
<p class="calibre2">词汇表的大小(V=10)是语料库中唯一单词的数量。句子将被表示为长度为10的向量，其中每个条目对应于词汇表中的一个单词。此条目的值由相应单词在文档或句子中出现的次数决定。</p>
<p class="calibre2">在这种情况下，这两个句子将被编码为10个元素的向量，如下所示:</p>
<pre class="calibre21">Sentence 1: [1,1,1,0,1,1,0,1,1,0,2]<br class="title-page-name"/>Sentence 2: [0,1,0,1,0,1,1,1,0,1,1]</pre>
<p class="calibre2">向量的每个元素表示每个单词在语料库(文本句子)中的出现次数。因此，在第一句话中，对于<kbd class="calibre12">brown</kbd>(在向量中的位置0)有<kbd class="calibre12">1</kbd>计数，<kbd class="calibre12">dog</kbd>有<kbd class="calibre12">1</kbd>，<kbd class="calibre12">1</kbd>有<kbd class="calibre12">fox</kbd>，以此类推。类似地，对于第二个句子，没有出现<kbd class="calibre12">brown</kbd>，所以我们得到位置0的<kbd class="calibre12">0</kbd>、<kbd class="calibre12">dog</kbd>(在数组向量的位置1)的<kbd class="calibre12">1</kbd>计数、<kbd class="calibre12">fox</kbd>的<kbd class="calibre12">0</kbd>计数，等等。</p>


            

            
        
    </body>

</html>


<html xmlns:epub="http://www.idpf.org/2007/ops">
  <head>
    <title>Weighting the terms tf-idf</title>
    <meta content="urn:uuid:3fd593c7-d9c4-4d18-84e8-0b6efc257c5b" name="Adept.expected.resource"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body class="calibre">
        

                            
                    <h1 class="header-title">加权术语tf-idf</h1>
                
            
            
                
<p class="calibre2">在大多数语言中，一些单词往往比其他单词出现得更频繁，但可能不包含关于判断两个文档相似性的太多区别信息。例如,<em class="calibre20">是</em>、<em class="calibre20">是</em>和<em class="calibre20">是</em>这样的单词，它们在英语中都很常见。如果我们像上一节课那样只考虑它们的原始频率，我们可能不能有效地区分不同类别的文档，或者检索与核心内容匹配的相似文档。</p>
<p class="calibre2">解决这个问题的一种方法叫做<strong class="calibre13">术语频率和逆文档频率</strong> ( <strong class="calibre13"> tf-idf </strong>)。如其名，它考虑了两个术语:<strong class="calibre13">词频</strong> ( <strong class="calibre13"> tf </strong>)和<strong class="calibre13">逆文档频</strong> ( <strong class="calibre13"> idf </strong>)。</p>
<p class="calibre2">使用tf，<em class="calibre20"> tf(t，d) </em>，最简单的选择是使用文档中某个术语的原始计数；即术语<em class="calibre20"> t </em>在文档<em class="calibre20"> d </em>中出现的次数。然而，为了防止偏向较长的文档，通常的方法是将原始频率除以文档中任何术语的最大频率:</p>
<p class="calibre39"><img class="fm-editor-equation80" src="img/6dd4066f-e84f-401e-a707-8395d28f5d8e.png"/></p>
<p class="calibre2">在这个等式中，<em xmlns:epub="http://www.idpf.org/2007/ops" class="calibre20"> f <sub class="calibre34"> t，d</sub>T22】是该术语在文档中出现的次数(原始计数)。</em></p>
<p class="calibre2">idf衡量单词提供了多少信息；即该术语在所有文档中是常见的还是罕见的。确定该术语的一种常用方法是取包含该术语的文档比例的倒数的对数:</p>
<p class="calibre39"><img class="fm-editor-equation81" src="img/904e5a9b-24e6-4e1c-9e3d-2238dda3bdc8.png"/></p>
<p class="calibre2">将两个值相乘，tf-idf计算如下:</p>
<p class="calibre39"><img class="fm-editor-equation82" src="img/96403019-0fa1-48e4-a7a6-18dcb9b3a711.png"/></p>
<p class="calibre2">tf-idf常用于信息检索和文本挖掘。</p>


            

            
        
    </body>

</html>


<html xmlns:epub="http://www.idpf.org/2007/ops">
  <head>
    <title>Deep learning NLP</title>
    <meta content="urn:uuid:3fd593c7-d9c4-4d18-84e8-0b6efc257c5b" name="Adept.expected.resource"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body class="calibre">
        

                            
                    <h1 class="header-title">深度学习NLP</h1>
                
            
            
                
<p class="calibre2">深度学习在学习自然语言的多层次表示方面带来了多重好处。在这一节中，我们将讨论使用深度学习和分布式表示进行自然语言处理的动机、单词嵌入和几种执行单词嵌入的方法，以及应用。</p>


            

            
        
    </body>

</html>


<html xmlns:epub="http://www.idpf.org/2007/ops">
  <head>
    <title>Motivation and distributed representation</title>
    <meta content="urn:uuid:3fd593c7-d9c4-4d18-84e8-0b6efc257c5b" name="Adept.expected.resource"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body class="calibre">
        

                            
                    <h1 class="header-title">动机和分布式表征</h1>
                
            
            
                
<p class="calibre2">与许多其他情况一样，数据的表示，即信息如何编码并显示给机器学习算法，通常是所有学习或人工智能管道中最重要和最基本的部分。表示的有效性和可扩展性在很大程度上决定了下游机器学习模型和应用的性能。</p>
<p class="calibre2">如前一节所述，传统的NLP通常使用一键编码来表示固定词汇表中的单词，并使用BoW来表示文档。这种方法将每个单词都视为一个原子符号，例如，房子、道路、树。独热编码将生成类似于[0000000000010000]的表示。表示的长度就是词汇量的大小。使用这种表示法，人们通常会得到巨大的稀疏向量。例如，在典型的语音应用中，词汇量可以从20，000到500，000。但是它有一个很明显的问题，就是忽略了任意一对词之间的关系，比如:motel[0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0。此外，编码实际上是任意的，例如在一个设置中，<em class="calibre20">猫</em>可以表示为<em class="calibre20"> Id321 </em>，而<em class="calibre20">狗</em>表示为<em class="calibre20"> Id453 </em>，这意味着长稀疏向量的第453个条目是1。这种表示没有向系统提供关于各个符号之间可能存在的相互作用或相似性的有用信息。</p>
<p class="calibre2">这使得模型的学习变得困难，因为当模型处理关于<em class="calibre20">狗</em>的数据时，它将不能利用它已经了解的关于<em class="calibre20">猫</em>的很多信息。因此，离散id将单词的实际语义与其表示分离开来。虽然可以在文档级别计算一些统计信息，但是原子级别的信息非常有限。这就是分布式向量表示，特别是深度学习的用处。</p>
<p class="calibre2">深度学习算法试图学习越来越复杂/抽象的多层次表示。</p>
<p class="calibre2">使用深度学习解决NLP问题有多种好处:</p>
<ul class="calibre7">
<li class="calibre8">通常直接来自数据或问题，改进手工制作的特性的不完整性和过度规范。手工制作功能通常非常耗时，并且可能需要针对每个任务或特定领域的问题反复执行。从一个领域学到的特征通常很少表现出对其他领域或领域的推广能力。相反，深度学习从数据和跨多个级别的表示中学习信息，其中较低的级别对应于更通用的信息，这些信息可以直接或在微调后被其他领域利用。</li>
</ul>
<ul class="calibre7">
<li class="calibre22">学习不相互排斥的特征可能比类最近邻或类聚类模型更有效。原子符号表示不捕捉单词之间的任何语义相互关系。由于单词被独立处理，自然语言处理系统非常脆弱。在有限向量空间中捕捉这些相似性的分布表示为下面的NLP系统提供了进行更复杂的推理和知识推导的机会。</li>
</ul>
<ul class="calibre7">
<li class="calibre22">学习可以在无人监督的情况下进行。鉴于目前的数据规模，非常需要无监督学习。在许多实际情况下，获取标签通常是不现实的。</li>
</ul>
<ul class="calibre7">
<li class="calibre22">深度学习学习多层次的表征。这是深度学习最重要的优势之一，为此，学习到的信息是通过组合逐层构建的。较低级别的表示通常可以跨任务共享。</li>
</ul>
<ul class="calibre7">
<li class="calibre22">自然处理人类语言的递归性。人类的句子是由具有一定结构的单词和短语组成的。深度学习，尤其是递归神经模型，能够更好地捕捉序列信息。</li>
</ul>


            

            
        
    </body>

</html>


<html xmlns:epub="http://www.idpf.org/2007/ops">
  <head>
    <title>Word embeddings</title>
    <meta content="urn:uuid:3fd593c7-d9c4-4d18-84e8-0b6efc257c5b" name="Adept.expected.resource"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body class="calibre">
        

                            
                    <h1 class="header-title">单词嵌入</h1>
                
            
            
                
<p class="calibre2">基于分布相似性的表示法的基本思想是，一个词可以通过它的邻居来表示。正如J. R. Firth 1957年11月所说:</p>
<p>从一个人交的朋友，你就可以知道这个人说的话</p>
<p class="calibre2">这可能是现代统计NLP最成功的想法之一。邻居的定义可以变化，以考虑局部或更大的上下文，从而获得更多的语法或语义表示。</p>


            

            
        
    </body>

</html>


<html xmlns:epub="http://www.idpf.org/2007/ops">
  <head>
    <title>Idea of word embeddings</title>
    <meta content="urn:uuid:3fd593c7-d9c4-4d18-84e8-0b6efc257c5b" name="Adept.expected.resource"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body class="calibre">
        

                            
                    <h1 class="header-title">单词嵌入的概念</h1>
                
            
            
                
<p class="calibre2">首先，一个词被表示为一个密集向量。单词嵌入可以被认为是从单词到一个<em class="calibre20"> n </em>维空间的映射函数；也就是<img class="fm-editor-equation83" src="img/54851ac7-b77a-4f52-ada8-aa34566e956f.png"/>，其中<em class="calibre20"> W </em>是将某种语言中的单词映射到高维向量(例如200到500维的向量)的参数化函数。你也可以把<em class="calibre20"> W </em>看成一个查找表，大小为<em class="calibre20"> V X N </em>，其中<em class="calibre20"> V </em>是词汇量的大小，<em class="calibre20"> N </em>是维度的大小，每行对应一个单词。例如，我们可能会发现:</p>
<pre class="calibre21">W("dog")=(0.1, -0.5, 0.8, ...)<br class="title-page-name"/>W(‘‘mat")=(0.0, 0.6, -0.1, ...)</pre>
<p class="calibre2">这里，<kbd class="calibre12">W</kbd>经常被初始化为每个单词都有随机向量，然后我们让网络学习并更新<kbd class="calibre12">W</kbd>以便执行一些任务。</p>
<p class="calibre2">例如，我们可以训练网络，让它预测一个<em class="calibre20">n</em>-gram(n<em class="calibre20">n</em>字的序列)是否有效。假设我们得到一个单词序列，如<em class="calibre20">一只狗对陌生人吠叫</em>，并把它作为一个带有肯定标签的输入(意味着有效)。然后，我们用随机单词替换这个句子中的一些单词，并将其转移到<em class="calibre20">猫对陌生人吠叫</em>，标签是否定的，因为这几乎肯定意味着这个5-gram是无意义的:</p>
<div><img class="alignnone66" src="img/9b51f7fc-d3cb-4714-84aa-15189bc63ff4.png"/></div>
<p class="calibre2">如上图所示，我们通过查找矩阵<strong class="calibre13"> W </strong>输入<em class="calibre20">n</em>gram来训练模型，并获得代表每个单词的向量。然后，向量通过输出神经元进行组合，并将结果与目标值进行比较。一个完美的预测将导致以下结果:</p>
<pre class="calibre21">R(W("a"), W(‘‘dog"), W(‘barks"), W(‘‘at”), W("strangers"))=1<br class="title-page-name"/>R(W("a"), W(‘‘cat"),  W(‘barks"), W(‘‘at”), W("strangers"))=0</pre>
<p class="calibre2">目标值和预测值之间的差异/误差将用于更新<kbd class="calibre12">W</kbd>和<kbd class="calibre12">R</kbd> <em class="calibre20"> </em>(聚合函数，例如sum)。</p>
<p class="calibre2">学习过的单词嵌入有一些有趣的性质。</p>
<p class="calibre2">首先，单词表示在高维空间中的位置由它们的含义确定，使得具有相近含义的单词聚集在一起:</p>
<div><img class="alignnone67" src="img/c71f19f6-e302-4aaa-82bc-9f9950fd9bf7.png"/></div>
<p>从模型中学习到的嵌入之间的线性关系</p>
<p class="calibre2">第二，也是更有趣的，单词向量有线性关系。词与词之间的关系可以认为是一对词所形成的方向和距离。例如，从字<strong class="calibre13">王</strong>的位置开始，在<strong class="calibre13">男</strong>和<strong class="calibre13">女</strong>之间移动相同的距离和方向，就会得到字<strong class="calibre13">后</strong>，即:</p>
<p class="calibre39"><em class="calibre20">【国王】-【男人】+【女人】~ =【女王】</em></p>
<p class="calibre2">研究人员发现，如果使用大量数据进行训练，得到的向量可以反映非常微妙的语义关系，例如一个城市和它所属的国家。例如，法国对于巴黎就像德国对于柏林一样。</p>
<p class="calibre2">另一个例子是，找到一个与“最大”和“大”意思相同的单词。人们可以简单地计算出<em class="calibre20">向量X =向量(最大)——向量(大)+向量(小)</em>。还可以捕捉许多其他种类的语义关系，例如对立和比较。在米科洛夫的出版物中可以找到一些很好的例子，<em class="calibre20">向量空间中单词表示的有效估计</em>(【https://arxiv.org/pdf/1301.3781.pdf】T5)，如下图所示:</p>
<div><img src="img/4061d116-e5a7-4e59-bb59-d71edba340db.png" class="calibre46"/></div>
<p>Mikolov和他们的合著者在<em class="calibre26">矢量空间中单词表示的有效估计</em>中的论文中的语义句法单词关系测试集中的五种语义和九种句法问题的示例</p>


            

            
        
    </body>

</html>


<html xmlns:epub="http://www.idpf.org/2007/ops">
  <head>
    <title>Advantages of distributed representation</title>
    <meta content="urn:uuid:3fd593c7-d9c4-4d18-84e8-0b6efc257c5b" name="Adept.expected.resource"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body class="calibre">
        

                            
                    <h1 class="header-title">分布式表示的优势</h1>
                
            
            
                
<p class="calibre2">使用分布式单词向量来解决NLP问题有很多优点。随着微妙的语义关系被捕获，在改进许多现有的自然语言处理应用方面有很大的潜力，例如机器翻译、信息检索和问答系统。一些明显的优势是:</p>
<ul class="calibre7">
<li class="calibre8">捕获本地同现统计</li>
<li class="calibre8">产生最先进的线性语义关系</li>
<li class="calibre8">有效利用统计数据</li>
<li class="calibre8">可以在(相对)少量数据和大量数据上进行训练</li>
<li class="calibre8">快速，只有非零计数才重要</li>
<li class="calibre8">对下游任务很重要的小(100-300)维向量的良好性能</li>
</ul>


            

            
        
    </body>

</html>


<html xmlns:epub="http://www.idpf.org/2007/ops">
  <head>
    <title>Problems of distributed representation</title>
    <meta content="urn:uuid:3fd593c7-d9c4-4d18-84e8-0b6efc257c5b" name="Adept.expected.resource"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body class="calibre">
        

                            
                    <h1 class="header-title">分布式表示的问题</h1>
                
            
            
                
<p class="calibre2">请记住，没有一种方法可以解决所有问题，同样，分布式表示也不是灵丹妙药。为了正确使用它，我们需要了解它的一些已知问题:</p>
<ul class="calibre7">
<li class="calibre22"><strong class="calibre1">相似性和相关度不一样</strong>:虽然在一些出版物中给出了很好的评估结果，但并不能保证其实际应用的成功。一个原因是，当前的标准评估通常基于相关度，而不是人类创造的一组单词。有可能来自模型的表示与人类评估很好地相关，但是在给定特定任务的情况下并没有提高性能。这可能是由于大多数评估数据集没有区分单词相似性和相关度。例如，<em class="calibre26">男</em>和<em class="calibre26">男</em>相似，而<em class="calibre26">电脑</em>和<em class="calibre26"> </em> <em class="calibre26">键盘</em>相关但不相似。</li>
</ul>
<ul class="calibre7">
<li class="calibre22"><strong class="calibre1">单词歧义</strong>:当单词有多个意思时，就会出现这个问题。比如<em class="calibre26">银行</em>这个词除了有<em class="calibre26">一个金融机构</em>的意思外，还有<em class="calibre26">坡地</em>的意思。这样，在不考虑单词歧义的情况下，将单词表示为一个向量是有限制的。已经提出了一些方法来学习每个单词的多种表示。例如，Trask和他们的合作者提出了一种方法，基于监督歧义消除为每个单词的多个嵌入建模(<a href="https://arxiv.org/abs/1511.06388" target="_blank" class="calibre11">https://arxiv.org/abs/1511.06388</a>)。当任务需要时，可以参考这些方法。</li>
</ul>


            

            
        
    </body>

</html>


<html xmlns:epub="http://www.idpf.org/2007/ops">
  <head>
    <title>Commonly used pre-trained word embeddings</title>
    <meta content="urn:uuid:3fd593c7-d9c4-4d18-84e8-0b6efc257c5b" name="Adept.expected.resource"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body class="calibre">
        

                            
                    <h1 class="header-title">常用的预训练单词嵌入</h1>
                
            
            
                
<p class="calibre2">下表列出了一些常用的预训练单词嵌入:</p>
<table class="calibre27">
<tbody class="calibre28">
<tr class="calibre29">
<td class="calibre30">
<p class="calibre2"><strong class="calibre13">名称</strong></p>
</td>
<td class="calibre30">
<p class="calibre2"><strong class="calibre13">年份</strong></p>
</td>
<td class="calibre30">
<p class="calibre2"><strong class="calibre13">网址</strong></p>
</td>
<td class="calibre30">
<p class="calibre2"><strong class="calibre13">评论</strong></p>
</td>
</tr>
<tr class="calibre29">
<td class="calibre30">
<p class="calibre2">Word2Vec</p>
</td>
<td class="calibre30">
<p class="calibre2">2013</p>
</td>
<td class="calibre30">
<p class="calibre2"><a href="https://code.google.com/archive/p/word2vec/" target="_blank" class="calibre11">https://code.google.com/archive/p/word2vec/</a></p>
</td>
<td class="calibre30">
<p class="calibre2">在<a href="https://github.com/Kyubyong/wordvectors" class="calibre11">https://github.com/Kyubyong/wordvectors.</a>可获得多语种预训练矢量</p>
</td>
</tr>
<tr class="calibre29">
<td class="calibre30">
<p class="calibre2">手套</p>
</td>
<td class="calibre30">
<p class="calibre2">2014</p>
</td>
<td class="calibre30">
<p class="calibre2"><a href="http://nlp.stanford.edu/projects/glove/" class="calibre11">http://nlp.stanford.edu/projects/glove/</a></p>
</td>
<td class="calibre30">
<p class="calibre2">斯坦福开发的，号称比Word2Vec好。GloVe本质上是一个基于计数的模型，它结合了全局矩阵分解和局部上下文窗口。</p>
</td>
</tr>
<tr class="calibre29">
<td class="calibre30">
<p class="calibre2">快速文本</p>
</td>
<td class="calibre30">
<p class="calibre2">2016</p>
</td>
<td class="calibre30">
<p class="calibre2">https://github.com/icoxfog417/fastTextJapaneseTutorial<a xmlns:epub="http://www.idpf.org/2007/ops" href="https://github.com/icoxfog417/fastTextJapaneseTutorial" class="calibre11"/></p>
</td>
<td class="calibre30">
<p class="calibre2">在FastText中，原子单位是<em class="calibre20"> n </em>个gram字符，一个词向量由<em class="calibre20"> n </em>个gram字符的集合表示。学的挺快的。</p>
</td>
</tr>
<tr class="calibre29">
<td class="calibre30">
<p class="calibre2">LexVec</p>
</td>
<td class="calibre30">
<p class="calibre2">2016</p>
</td>
<td class="calibre30">
<p class="calibre2"><a href="https://github.com/alexandres/lexvec" class="calibre11">https://github.com/alexandres/lexvec</a></p>
</td>
<td class="calibre30">
<p class="calibre2">LexVec使用<strong xmlns:epub="http://www.idpf.org/2007/ops" class="calibre13">窗口采样和</strong> ( <strong xmlns:epub="http://www.idpf.org/2007/ops" class="calibre13"> WSNS </strong>)负采样对<strong xmlns:epub="http://www.idpf.org/2007/ops" class="calibre13">正逐点互信息</strong> ( <strong xmlns:epub="http://www.idpf.org/2007/ops" class="calibre13"> PPMI </strong>)矩阵进行因式分解。Salle等人在其使用位置上下文和外部记忆增强LexVec分布式单词表示模型的工作<em xmlns:epub="http://www.idpf.org/2007/ops" class="calibre20">中提出，在单词相似性和语义类比任务中，LexVec匹配并经常优于竞争模型。</em></p>
</td>
</tr>
<tr class="calibre29">
<td class="calibre30">
<p class="calibre2">元嵌入</p>
</td>
<td class="calibre30">
<p class="calibre2">2016</p>
</td>
<td class="calibre30">
<p class="calibre2"><a href="http://cistern.cis.lmu.de/meta-emb/" class="calibre11">http://cistern.cis.lmu.de/meta-emb/</a></p>
</td>
<td class="calibre30">
<p class="calibre2">尹等，<em class="calibre20">学习词元嵌入</em>，2016。它结合不同的公共嵌入集来生成更好的向量(元嵌入)。</p>
</td>
</tr>
</tbody>
</table>
<p class="calibre2"> </p>
<p class="calibre2">下面几节主要讲三个比较流行的:Word2Vec、GloVe、FastText。特别是，我们将更深入地研究Word2Vec的核心思想、两个不同的模型、训练过程，以及如何利用开源的预训练Word2Vec表示。</p>


            

            
        
    </body>

</html>


<html xmlns:epub="http://www.idpf.org/2007/ops">
  <head>
    <title>Word2Vec</title>
    <meta content="urn:uuid:3fd593c7-d9c4-4d18-84e8-0b6efc257c5b" name="Adept.expected.resource"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body class="calibre">
        

                            
                    <h1 class="header-title">Word2Vec</h1>
                
            
            
                
<p class="calibre2">Word2Vec是一组有效的预测模型，用于从原始文本中学习单词嵌入。它将单词映射成向量。在映射的向量空间中，共享共同上下文的单词彼此靠近。在本节中，我们将详细讨论Word2Vec及其两个具体模型。我们还将描述如何使用TensorFlow训练一个Word2Vec。</p>


            

            
        
    </body>

</html>


<html xmlns:epub="http://www.idpf.org/2007/ops">
  <head>
    <title>Basic idea of Word2Vec</title>
    <meta content="urn:uuid:3fd593c7-d9c4-4d18-84e8-0b6efc257c5b" name="Adept.expected.resource"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body class="calibre">
        

                            
                    <h1 class="header-title">Word2Vec的基本思想</h1>
                
            
            
                
<p class="calibre2">Word2Vec型号只有三层；输入图层、投影图层和输出图层。它有两种模型，即<strong class="calibre13">连续单词包</strong> ( <strong class="calibre13"> CBOW </strong>)模型和跳格模型。它们非常相似，但不同之处在于输入层和输出层的构造方式。跳格模型将每个目标单词(例如，<em class="calibre20"> mat </em>)作为输入，并预测上下文/周围的单词作为输出(<em class="calibre20">猫坐在</em> <em class="calibre20"> the </em>)。另一方面，CBOW从源上下文单词开始(<em class="calibre20">猫坐在</em>上)，使用中间层进行聚合和转换，并预测目标单词(<em class="calibre20"> mat </em>)。下图说明了不同之处:</p>
<div><img class="alignnone68" src="img/1f54b458-5e00-4222-a3cf-778d02a33ca2.png"/></div>
<p class="calibre2">以CBOW模型为例。训练集中的每个单词被表示为一个独热编码向量。<em xmlns:epub="http://www.idpf.org/2007/ops" class="calibre20"> x <sub class="calibre34"> 1 </sub>，x <sub class="calibre34"> 2 </sub>，...，x<sub class="calibre34">c</sub>T7】是上下文单词的一键编码。目标词也用一键编码<em xmlns:epub="http://www.idpf.org/2007/ops" class="calibre20"> y </em>表示。隐藏层有<em xmlns:epub="http://www.idpf.org/2007/ops" class="calibre20"> N </em>个节点。矩阵<em xmlns:epub="http://www.idpf.org/2007/ops" class="calibre20">W<sub class="calibre34">V X N</sub>T15】表示输入层和隐藏层之间的权重矩阵(连接)，其行表示对应于词汇表中单词的权重。这个权重矩阵是我们感兴趣学习的，因为它包含了我们词汇表中所有单词的矢量编码(作为它的行)。<em xmlns:epub="http://www.idpf.org/2007/ops" class="calibre20">W′<sub class="calibre34">N X V</sub></em>是连接隐藏层与输出层的输出矩阵(连接)，也称为上下文字矩阵。这是与每个输出字向量相关联的矩阵。在Skip-Gram模型中，输入是目标单词表示向量<em xmlns:epub="http://www.idpf.org/2007/ops" class="calibre20"> x </em>，输出是长度为<em xmlns:epub="http://www.idpf.org/2007/ops" class="calibre20"> V </em>的向量，每个词条对应词汇表中的一个单词。对于同一个目标词<em xmlns:epub="http://www.idpf.org/2007/ops" class="calibre20"> x </em>，生成多个对(<em xmlns:epub="http://www.idpf.org/2007/ops" class="calibre20"> x </em>、<em xmlns:epub="http://www.idpf.org/2007/ops" class="calibre20"> y <sub class="calibre34"> 1 </sub> </em>)、(<em xmlns:epub="http://www.idpf.org/2007/ops" class="calibre20"> x </em>、<em xmlns:epub="http://www.idpf.org/2007/ops" class="calibre20"> y <sub class="calibre34"> 2 </sub> </em>)、(<em xmlns:epub="http://www.idpf.org/2007/ops" class="calibre20"> x </em>、<em xmlns:epub="http://www.idpf.org/2007/ops" class="calibre20"> y <sub class="calibre34"> c </sub> </em>)进行训练。目标是，通过网络的变换，给定一个输入的独热编码字<em xmlns:epub="http://www.idpf.org/2007/ops" class="calibre20"> x </em>，预测(一个<em xmlns:epub="http://www.idpf.org/2007/ops" class="calibre20"> 1 X V </em>长度向量)应该在对应于上下文字的独热编码向量的条目处具有更高的数字。</em></em></p>


            

            
        
    </body>

</html>


<html xmlns:epub="http://www.idpf.org/2007/ops">
  <head>
    <title>The word windows</title>
    <meta content="urn:uuid:3fd593c7-d9c4-4d18-84e8-0b6efc257c5b" name="Adept.expected.resource"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body class="calibre">
        

                            
                    <h1 class="header-title">视窗这个词</h1>
                
            
            
                
<p class="calibre2">请记住，从单词嵌入的概念这一节，我们知道单词可以通过它的上下文来表示，或者更具体地说，它的so单词。因此，我们可以使用一个窗口来确定我们希望与中心目标单词一起学习的周围单词(前后)的数量，如下图所示:</p>
<div><img class="alignnone69" src="img/4dfdab04-f89a-4800-9260-527092c9d9b2.png"/></div>
<p class="calibre2">在这种情况下，窗口大小为<strong class="calibre13"> 2 </strong>。为了学习目标单词<strong class="calibre13"> sits </strong>，将包括距离目标单词最多两个单词的邻近单词，以生成训练对。然后我们沿着结构化文本滑动窗口。</p>


            

            
        
    </body>

</html>


<html xmlns:epub="http://www.idpf.org/2007/ops">
  <head>
    <title>Generating training data</title>
    <meta content="urn:uuid:3fd593c7-d9c4-4d18-84e8-0b6efc257c5b" name="Adept.expected.resource"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body class="calibre">
        

                            
                    <h1 class="header-title">生成训练数据</h1>
                
            
            
                
<p class="calibre2">在Skip-Gram模型中，我们如下生成用于训练的词对:</p>
<div><img class="alignnone70" src="img/4acfc8ee-0866-4887-8063-f67edfe9891c.png"/></div>
<p>生成正训练对的示例，其中窗口覆盖了文档的开头</p>
<p class="calibre2">可以如下生成正训练对:</p>
<div><img class="alignnone71" src="img/df149cac-ee1f-450c-8833-e42bc086d4dc.png"/></div>
<p>生成正训练对的示例</p>
<p class="calibre2">从这些图示中，人们可以容易地看到，网络将从每个配对(目标、上下文)出现的次数中学习统计数据。例如，模特可能会看到更多的<em class="calibre20">约克</em>、<em class="calibre20">新</em>或<em class="calibre20">新</em>、<em class="calibre20">约克</em>而不是<em class="calibre20">旧</em>、<em class="calibre20">约克</em>的样品。因此，在测试阶段，如果你给出单词<em class="calibre20"> York </em>作为输入，它会输出一个大得多的概率为<em class="calibre20"> New </em>。</p>
<p class="calibre2">从生成训练数据的方式中，可以注意到被训练的神经网络不知道关于输出上下文单词相对于目标单词的偏移的任何事情。以上图为例，模型不会考虑到在句子中，<strong class="calibre13"> quick </strong>比<strong class="calibre13"> brown </strong>更远，<strong class="calibre13"> quick </strong>在目标词<strong class="calibre13"> fox </strong>之前，而<strong class="calibre13"> jump </strong>在之后。已经学习的信息是所有这些上下文单词都在目标单词附近，而不管它们的顺序或位置。因此，给定输入目标单词的上下文单词的预测概率仅表示它出现在附近的概率。</p>


            

            
        
    </body>

</html>


<html xmlns:epub="http://www.idpf.org/2007/ops">
  <head>
    <title>Negative sampling</title>
    <meta content="urn:uuid:3fd593c7-d9c4-4d18-84e8-0b6efc257c5b" name="Adept.expected.resource"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body class="calibre">
        

                            
                    <h1 class="header-title">负采样</h1>
                
            
            
                
<p class="calibre2">从损失函数中，我们可以看到，计算softmax层可能非常昂贵。交叉熵成本函数要求网络产生概率，这意味着需要对每个神经元的输出分数进行归一化，以生成每个类别的实际概率(例如，Skip-Gram模型中词汇表中的一个单词)。归一化需要用上下文单词矩阵中的每个单词计算隐藏层输出。为了处理这个问题，Word2Vec使用了一种叫做<strong class="calibre13">负采样</strong> ( <strong class="calibre13"> NEG </strong>)的技术，类似于<strong class="calibre13">噪声对比估计</strong> ( <strong class="calibre13"> NCE </strong>)。</p>
<p class="calibre2">NCE的核心思想是将一个多项式分类问题，例如基于上下文预测一个单词的情况，其中每个单词可以被认为是一个类，转化为一个<em class="calibre20">好对</em>对<em class="calibre20">坏对</em>的二元分类问题。</p>
<p class="calibre2">在训练期间，向网络输入<em xmlns:epub="http://www.idpf.org/2007/ops" class="calibre20">好词对、</em>和一些随机生成的<em xmlns:epub="http://www.idpf.org/2007/ops" class="calibre20">坏词对</em>，好词对是与上下文窗口中的另一个词配对的目标词，坏词对由目标词和从词汇表中随机选择的词构成。</p>
<p class="calibre2">因此，网络被迫区分好的和坏的，这最终导致基于上下文的表示的学习。</p>
<p class="calibre2">本质上，NEG保留了softmax层，但是具有修改的损失函数。目标是迫使单词的嵌入类似于单词在上下文中的嵌入，而不同于远离上下文的单词的嵌入。</p>
<p class="calibre2">采样过程根据一些特别设计的分布，例如单文法分布，选择一对窗口外上下文对，其可以被形式化为:</p>
<p class="calibre40"><img class="fm-editor-equation84" src="img/43515673-3bfd-401b-9d3b-16f7e8d12ae9.png"/></p>
<p class="calibre2">在Word2Vec的某些实现中，频率被提升到的幂，这是一个经验值:</p>
<p class="calibre40"><img class="fm-editor-equation85" src="img/15ac72be-ff4a-4cf5-aa0d-e03524f61d82.png"/></p>
<p class="calibre2">在这个等式中，<img class="fm-editor-equation86" src="img/2e10c02c-5d3e-4da3-946a-6782c9706514.png"/>是词频。通过这种分布进行选择实质上有利于频繁出现的单词被更频繁地抽取。改变采样策略会对学习结果产生重大影响。</p>
<p class="calibre2">下图说明了将目标单词与上下文之外的单词配对的方法，这些单词是从词汇表中随机选择的:</p>
<div><img class="alignnone72" src="img/0a76ffab-04a6-4817-ad48-88cf1590bb9f.png"/></div>


            

            
        
    </body>

</html>


<html xmlns:epub="http://www.idpf.org/2007/ops">
  <head>
    <title>Hierarchical softmax</title>
    <meta content="urn:uuid:3fd593c7-d9c4-4d18-84e8-0b6efc257c5b" name="Adept.expected.resource"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body class="calibre">
        

                            
                    <h1 class="header-title">分级softmax</h1>
                
            
            
                
<p class="calibre2">计算softmax是昂贵的，因为对于每个目标单词，我们必须计算分母来获得归一化的概率。然而，分母是隐藏层输出向量<em class="calibre20"> h </em>和词汇表中每个单词的输出嵌入<em class="calibre20">W</em><em class="calibre20">V</em>之间的内积之和。</p>
<p class="calibre2">为了解决这个问题，已经提出了许多不同的方法。有些是基于softmax的方法，如分层softmax、差分softmax和CNN softmax等，而其他是基于采样的方法。读者可以参考<a href="http://ruder.io/word-embeddings-softmax/index.html#cnnsoftmax" target="_blank" class="calibre11">http://ruder . io/word-embeddings-softmax/index . html # cnnsoftmax</a>更深入的了解近似soft max函数。</p>
<p class="calibre2">基于Softmax的方法是保持softmax层不变但修改其架构以提高其效率的方法(例如，分层softmax)。</p>
<p class="calibre2">然而，基于采样的方法将完全去除softmax层，而是优化新设计的损失函数来逼近softmax。例如，近似计算成本较低的分母，如NEG。一个很好的解释可以在Yoav Goldberg和Omer Levy的论文中找到，<em class="calibre20">word 2 vec Explained:derivating miko lov等人的负采样单词嵌入法</em>，2014(<a href="https://arxiv.org/abs/1402.3722" target="_blank" class="calibre11">https://arxiv.org/abs/1402.3722</a>)。</p>
<p class="calibre2">对于分层softmax，主要思想是基于词频构建霍夫曼树，其中每个词都是这棵树的一片叶子。然后，对特定单词的softmax值的计算被转化为计算树中代表该单词的从根到叶的节点的概率的乘积。在每个子树分裂点，我们计算去右分支或左分支的概率。左右分支的概率之和等于1；这保证了叶节点的所有概率之和等于1。有了平衡树，这可以将计算复杂度从<img class="fm-editor-equation87" src="img/cc058e5b-bead-4530-a133-928450859086.png"/>降低到<img class="fm-editor-equation88" src="img/c0e45309-ffa1-4166-9cde-24686620c50b.png"/>，其中<em class="calibre20"> V </em>是词汇量。</p>


            

            
        
    </body>

</html>


<html xmlns:epub="http://www.idpf.org/2007/ops">
  <head>
    <title>Other hyperparameters</title>
    <meta content="urn:uuid:3fd593c7-d9c4-4d18-84e8-0b6efc257c5b" name="Adept.expected.resource"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body class="calibre">
        

                            
                    <h1 class="header-title">其他超参数</h1>
                
            
            
                
<p class="calibre2">与传统的基于计数的方法相比，除了新算法的新颖性，如Skip-Gram模型(使用负采样)、CBOW(使用分层softmax)、NCE和GloVe，还有许多新的超参数或预处理步骤可以进行调整以提高性能。比如二次采样、去除生僻字、使用动态上下文窗口、使用上下文分布平滑、添加上下文向量等等。如果使用得当，它们中的每一个都将极大地帮助提高性能，尤其是在实际环境中。</p>


            

            
        
    </body>

</html>


<html xmlns:epub="http://www.idpf.org/2007/ops">
  <head>
    <title>Skip-Gram model</title>
    <meta content="urn:uuid:3fd593c7-d9c4-4d18-84e8-0b6efc257c5b" name="Adept.expected.resource"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body class="calibre">
        

                            
                    <h1 class="header-title">跳格模型</h1>
                
            
            
                
<p class="calibre2">我们现在关注Word2Vec中一个重要的模型架构，即跳格模型。如本节开头所述，跳格模型在给定输入目标单词的情况下预测上下文单词。我们要的嵌入这个词，其实就是输入层和隐藏层之间的权重矩阵。接下来，我们详细解释跳格模型。</p>


            

            
        
    </body>

</html>


<html xmlns:epub="http://www.idpf.org/2007/ops">
  <head>
    <title>The input layer</title>
    <meta content="urn:uuid:3fd593c7-d9c4-4d18-84e8-0b6efc257c5b" name="Adept.expected.resource"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body class="calibre">
        

                            
                    <h1 class="header-title">输入层</h1>
                
            
            
                
<p class="calibre2">嗯，我们不能把一个单词作为文本串直接输入神经网络。相反，我们需要数学上的东西。假设我们有10，000个独特单词的词汇量；通过使用one-hot编码，我们可以将每个单词表示为长度为10，000的向量，其中一个条目在对应于单词本身的位置为1，在所有其他位置为零。</p>
<p class="calibre2">跳跃语法模型的输入是单个单词，其长度等于词汇表的大小<em class="calibre20"> V </em>，输出由生成的词对决定。</p>


            

            
        
    </body>

</html>


<html xmlns:epub="http://www.idpf.org/2007/ops">
  <head>
    <title>The hidden layer</title>
    <meta content="urn:uuid:3fd593c7-d9c4-4d18-84e8-0b6efc257c5b" name="Adept.expected.resource"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body class="calibre">
        

                            
                    <h1 class="header-title">隐藏层</h1>
                
            
            
                
<p class="calibre2">在这种情况下，隐藏层没有任何激活功能。输入层和隐藏层之间的连接可以被认为是一个权重矩阵，<em xmlns:epub="http://www.idpf.org/2007/ops" class="calibre20">W<sub class="calibre34">V X N</sub>T5，其中<em xmlns:epub="http://www.idpf.org/2007/ops" class="calibre20"> N </em>是隐藏层中神经元的数量。<em xmlns:epub="http://www.idpf.org/2007/ops" class="calibre20"> W <sub class="calibre34"> V X N </sub>数字<em xmlns:epub="http://www.idpf.org/2007/ops" class="calibre20"> N </em>将是嵌入向量长度。还有另外一个辅助矩阵，<em xmlns:epub="http://www.idpf.org/2007/ops" class="calibre20">W′<sub class="calibre34">N X V</sub></em>，连接隐层和输出层，最小化单词<em xmlns:epub="http://www.idpf.org/2007/ops" class="calibre20"> W </em>与<em xmlns:epub="http://www.idpf.org/2007/ops" class="calibre20">幻觉</em>上下文单词(窗外单词)的相似度。</em></em></p>


            

            
        
    </body>

</html>


<html xmlns:epub="http://www.idpf.org/2007/ops">
  <head>
    <title>The output layer</title>
    <meta content="urn:uuid:3fd593c7-d9c4-4d18-84e8-0b6efc257c5b" name="Adept.expected.resource"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body class="calibre">
        

                            
                    <h1 class="header-title">输出层</h1>
                
            
            
                
<p class="calibre2">输出层是softmax回归分类器。它接受一个任意实值分数的向量(<em class="calibre20"> z </em>)，并将其压缩为一个介于零和一之间的值的向量，其总和为1:</p>
<p class="calibre40"><img class="fm-editor-equation89" src="img/bfb6486a-a843-499e-9543-c4b3e40634b0.png"/></p>
<p class="calibre2">隐藏层的输出(输入目标单词的单词向量，<em class="calibre20"> w </em>)乘以辅助矩阵<em class="calibre20">W’<sub class="calibre34">N X V</sub></em>，其中它的每一列代表一个单词(假设一个单词，<em class="calibre20"> c </em>)。点积产生一个值，在归一化之后，该值表示在给定输入目标单词<em class="calibre20"> w </em>的情况下，具有上下文单词<em class="calibre20"> c </em>的概率。</p>
<p class="calibre2">下面是计算单词<em class="calibre20"> car </em>的输出神经元的输出并应用softmax函数的示例:</p>
<div><img class="alignnone73" src="img/980a9961-411c-495b-b7de-2222ab39bd24.png"/></div>


            

            
        
    </body>

</html>


<html xmlns:epub="http://www.idpf.org/2007/ops">
  <head>
    <title>The loss function</title>
    <meta content="urn:uuid:3fd593c7-d9c4-4d18-84e8-0b6efc257c5b" name="Adept.expected.resource"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body class="calibre">
        

                            
                    <h1 class="header-title">损失函数</h1>
                
            
            
                
<p class="calibre2">损失函数是softmax输出的负对数:</p>
<p class="calibre39"><img class="fm-editor-equation90" src="img/213b008a-6e5b-4c99-8540-f5501a460eef.png"/></p>
<p class="calibre2">请记住，数据集的总损失是全部训练示例的平均值<em class="calibre20"> L <sub class="calibre34"> i </sub> </em>以及正则化项<em class="calibre20"> R(W) </em>。</p>
<p class="calibre2">从<strong class="calibre13">信息论</strong>的角度来看，这本质上是一个交叉熵损失函数。</p>
<p class="calibre2">我们可以这样理解。</p>
<p class="calibre2">交叉熵定义为:</p>
<p class="calibre39"><img class="fm-editor-equation91" src="img/32eacd92-3d25-4a55-a4f5-e7d3905f6d93.png"/></p>
<p class="calibre2">以实际分布<img class="fm-editor-equation92" src="img/f0b1d988-1b0c-448f-b4a2-f7b73b3ffc91.png"/>为δ函数，其熵项为<img class="fm-editor-equation93" src="img/638509d3-3c27-4a06-bfc6-a94218ae6376.png"/>。</p>
<p class="calibre2">因此:</p>
<p class="calibre47"><img class="fm-editor-equation94" src="img/055c8661-9f15-4678-86b3-a286604ee70f.png"/></p>
<p class="calibre2">在这个等式中，<img class="fm-editor-equation51" src="img/238fc591-e0e4-4a1b-ad3c-7fb192204bc2.png"/>是实际分布，<img class="fm-editor-equation95" src="img/acb5e083-2dc6-49f1-be65-92983a7dab5c.png"/>是分布的估计。</p>
<p class="calibre2">在我们的例子中，<img class="fm-editor-equation96" src="img/885c17ef-85d0-4d46-aba4-a1788714bbfa.png"/>本质上是softmax输出，即<img class="fm-editor-equation97" src="img/dbbb1706-391b-490b-8b26-4b0ab89c7f39.png"/>，而<img class="fm-editor-equation98" src="img/5114a1e6-4edd-4626-8ec6-55a1fac26746.png"/>中只有第<em class="calibre20"> j </em>个条目是<em class="calibre20"> 1 </em>。前面的等式可以简化为:</p>
<p class="calibre47"><img class="fm-editor-equation99" src="img/240d8354-8b15-462e-8dbb-e0423111fabb.png"/></p>
<p class="calibre2">所以，这里最小化损失函数等价于最小化交叉熵值。</p>
<p class="calibre2">从<strong class="calibre13">概率解释</strong>的角度来看，<img class="fm-editor-equation100" src="img/2c655f0f-4f2f-4264-b215-5728d04094d5.png"/>可以解释为分配给正确标签的(归一化)概率。我们实际上是在最小化正确类的负对数似然，也就是执行<strong class="calibre13">最大似然估计</strong> ( <strong class="calibre13"> MLE </strong>)。</p>


            

            
        
    </body>

</html>


<html xmlns:epub="http://www.idpf.org/2007/ops">
  <head>
    <title>Continuous Bag-of-Words model</title>
    <meta content="urn:uuid:3fd593c7-d9c4-4d18-84e8-0b6efc257c5b" name="Adept.expected.resource"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body class="calibre">
        

                            
                    <h1 class="header-title">连续词袋模型</h1>
                
            
            
                
<p class="calibre2">对于<strong class="calibre13">连续词袋</strong> ( <strong class="calibre13"> CBOW </strong>)模型来说，这个想法甚至更直接，因为这个模型使用周围的上下文来预测目标词。输入基本上仍然是具有固定窗口大小的周围环境，<em class="calibre20">C</em>；不同之处在于，我们首先聚合它们(添加它们的独热编码)，然后输入到神经网络。然后，这些单词将通过中间层进行处理，输出的是中间的目标单词。</p>


            

            
        
    </body>

</html>


<html xmlns:epub="http://www.idpf.org/2007/ops">
  <head>
    <title>Training a Word2Vec using TensorFlow</title>
    <meta content="urn:uuid:3fd593c7-d9c4-4d18-84e8-0b6efc257c5b" name="Adept.expected.resource"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body class="calibre">
        

                            
                    <h1 class="header-title">使用TensorFlow训练Word2Vec</h1>
                
            
            
                
<p class="calibre2">在本节中，我们将逐步解释如何使用TensorFlow构建和训练Skip-Gram模型。详细教程和源代码请参考<a href="https://www.tensorflow.org/tutorials/word2vec" target="_blank" class="calibre11">https://www.tensorflow.org/tutorials/word2vec</a>:</p>
<ol class="calibre15">
<li class="calibre8">我们可以从http://mattmahoney.net/dc/text8.zip下载数据集。</li>
<li class="calibre8">我们以单词列表的形式读入文件内容。</li>
<li class="calibre8">我们建立了张量流图。我们为输入单词和上下文单词创建占位符，它们表示为词汇表的整数索引:</li>
</ol>
<pre class="prettyprint1">train_inputs = tf.placeholder(tf.int32, shape=[batch_size]) train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])</pre>
<p class="calibre36">注意我们是分批训练的，所以<kbd class="calibre12">batch_size</kbd>指的是批次的大小。我们还创建了一个常量来保存验证集索引，其中<kbd class="calibre12">valid_examples</kbd>是用于验证的词汇表的整数索引数组:</p>
<pre class="prettyprint1">valid_dataset = tf.constant(valid_examples, dtype=tf.int32)</pre>
<p class="calibre36">我们通过计算验证集中的单词嵌入和词汇表之间的相似性来执行验证，并在词汇表中找到与验证集中的单词最相似的单词。</p>
<ol start="4" class="calibre15">
<li class="calibre8">我们设置嵌入矩阵变量:</li>
</ol>
<pre class="prettyprint1">embeddings = tf.Variable(<br class="title-page-name"/>    tf.random_uniform([vocabulary_size, embedding_size],<br class="title-page-name"/>                      -1.0, 1.0))<br class="title-page-name"/>embed = tf.nn.embedding_lookup(embeddings, train_inputs)</pre>
<ol start="5" class="calibre15">
<li class="calibre8">我们创建连接隐藏层和输出softmax层的权重和偏差。权重变量是一个大小为<kbd class="calibre12">vocabulary_size</kbd> × <kbd class="calibre12">embedding_size</kbd>的矩阵，其中<kbd class="calibre12">vocabulary_size</kbd>是输出层的大小，<kbd class="calibre12">embedding_size</kbd>是隐藏层的大小。<kbd class="calibre12">biases</kbd>变量的大小就是输出层的大小:</li>
</ol>
<pre class="prettyprint1">weights = tf.Variable(<br class="title-page-name"/>    tf.truncated_normal([vocabulary_size, embedding_size],<br class="title-page-name"/>                        stddev=1.0 / math.sqrt(embedding_size)))<br class="title-page-name"/>biases = tf.Variable(tf.zeros([vocabulary_size]))<br class="title-page-name"/>hidden_out = tf.matmul(embed, tf.transpose(weights)) + biases</pre>
<p class="calibre2">现在，我们将softmax应用于<kbd class="calibre12">hidden_out</kbd>，并使用交叉熵损失来优化权重、偏差和嵌入。在下面的代码中，我们还指定了一个学习速率为<kbd class="calibre12">1.0</kbd>的梯度下降优化器:</p>
<pre class="prettyprint1">train_one_hot = tf.one_hot(train_context, vocabulary_size)<br class="title-page-name"/>cross_entropy = tf.reduce_mean(<br class="title-page-name"/>    tf.nn.softmax_cross_entropy_with_logits(logits=hidden_out,<br class="title-page-name"/>                                            labels=train_one_hot))<br class="title-page-name"/>optimizer =<br class="title-page-name"/>    tf.train.GradientDescentOptimizer(1.0).minimize(cross_entropy)</pre>
<p class="calibre2">为了提高效率，我们可以将损失函数从<kbd class="calibre12">cross_entropy</kbd>损失改为NCE损失。NCE损失最初是在Michael Gutmann及其合著者的论文<em class="calibre20">Noise-contrast estimation:A new estimation principle for non normalized statistical models</em>中提出的:</p>
<pre class="prettyprint1">nce_loss = tf.reduce_mean(<br class="title-page-name"/>    tf.nn.nce_loss(weights=weights,<br class="title-page-name"/>                   biases=biases,<br class="title-page-name"/>                   labels=train_context,<br class="title-page-name"/>                   inputs=embed,<br class="title-page-name"/>                   num_sampled=num_sampled,<br class="title-page-name"/>                   num_classes=vocabulary_size))<br class="title-page-name"/>optimizer =<br class="title-page-name"/>    tf.train.GradientDescentOptimizer(1.0).minimize(nce_loss)</pre>
<p class="calibre2">对于验证，我们计算验证集中的单词嵌入和词汇表中的单词嵌入之间的余弦相似度。稍后，我们将打印词汇表中与验证单词嵌入最接近的前K个单词。嵌入<em class="calibre20"> A </em>和<em class="calibre20"> B </em>之间的余弦相似度定义为:</p>
<p class="calibre39"><img class="fm-editor-equation101" src="img/dd186789-9667-4184-bd1b-d7651fd253c2.png"/></p>
<p class="calibre2">这转化为以下代码:</p>
<pre class="prettyprint1">norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))<br class="title-page-name"/>normalized_embeddings = embeddings / norm<br class="title-page-name"/>valid_embeddings = tf.nn.embedding_lookup(<br class="title-page-name"/>    normalized_embeddings, valid_dataset)<br class="title-page-name"/>similarity = tf.matmul(<br class="title-page-name"/>    valid_embeddings, normalized_embeddings, transpose_b=True)</pre>
<ol start="6" class="calibre15">
<li class="calibre8">现在我们准备运行张量流图:</li>
</ol>
<pre class="prettyprint1">with tf.Session(graph=graph) as session:<br class="title-page-name"/>  # We must initialize all variables before we use them.<br class="title-page-name"/>  init.run()<br class="title-page-name"/>  print('Initialized')<br class="title-page-name"/>  average_loss = 0<br class="title-page-name"/>  for step in range(num_steps):<br class="title-page-name"/>    # This is your generate_batch function that generates input<br class="title-page-name"/>    # words and context words (labels) in a batch from data.<br class="title-page-name"/>    batch_inputs, batch_context = generate_batch(data,<br class="title-page-name"/>        batch_size, num_skips, skip_window)<br class="title-page-name"/>    feed_dict = {train_inputs: batch_inputs,<br class="title-page-name"/>                 train_context: batch_context}<br class="title-page-name"/>    # We perform one update step by evaluating the optimizer op<br class="title-page-name"/>    # and include it in the list of returned values for<br class="title-page-name"/>    # session.run()<br class="title-page-name"/>    _, loss_val = session.run(<br class="title-page-name"/>        [optimizer, cross_entropy], feed_dict=feed_dict)<br class="title-page-name"/>    average_loss += loss_val<br class="title-page-name"/>    if step % 2000 == 0:<br class="title-page-name"/>      if step &gt; 0:<br class="title-page-name"/>        average_loss /= 2000<br class="title-page-name"/>      # The average loss is an estimate of the loss over<br class="title-page-name"/>      # the last 2000 batches.<br class="title-page-name"/>      print('Average loss at step ', step, ': ', average_loss)<br class="title-page-name"/>      average_loss = 0<br class="title-page-name"/>   final_embeddings = normalized_embeddings.eval()</pre>
<ol start="7" class="calibre15">
<li class="calibre8">此外，我们希望打印出与我们的验证单词最相似的单词——我们通过调用我们前面定义的相似性操作来实现这一点，并对结果进行排序。请注意，这是一个开销很大的操作，因此我们每10，000步才执行一次:</li>
</ol>
<pre class="prettyprint1">if step % 10000 == 0:<br class="title-page-name"/>  sim = similarity.eval()<br class="title-page-name"/>  for i in range(valid_size):<br class="title-page-name"/>    # reverse_dictionary - maps codes(integers) to words(strings)<br class="title-page-name"/>    valid_word = reverse_dictionary[valid_examples[i]]<br class="title-page-name"/>    top_k = 8  # number of nearest neighbors<br class="title-page-name"/>    nearest = (-sim[i, :]).argsort()[1:top_k + 1]<br class="title-page-name"/>    log_str = 'Nearest to %s:' % valid_word<br class="title-page-name"/>    for k in range(top_k):<br class="title-page-name"/>      close_word = reverse_dictionary[nearest[k]]<br class="title-page-name"/>      log_str = '%s %s,' % (log_str, close_word)<br class="title-page-name"/>      print(log_str)</pre>
<p class="calibre2">有趣的是，在第一次迭代中，离<em class="calibre20">四个</em>最近的前八个单词是<kbd class="calibre12">lanthanides</kbd>、<kbd class="calibre12">dunant</kbd>、<kbd class="calibre12">jag</kbd>、<kbd class="calibre12">wheelbase</kbd>、<kbd class="calibre12">torso</kbd>、<kbd class="calibre12">bayesian</kbd>、<kbd class="calibre12">hoping</kbd>和<kbd class="calibre12">serena</kbd>，但是在30，000步之后，离<em class="calibre20">四个</em>最近的前八个单词是<kbd class="calibre12">six</kbd>、<kbd class="calibre12">nine</kbd>、<kbd class="calibre12">zero</kbd>、<kbd class="calibre12">two</kbd>、<kbd class="calibre12">seven</kbd>、<kbd class="calibre12">eight</kbd>、<kbd class="calibre12">three</kbd>和<kbd class="calibre12">five</kbd>我们可以使用来自<em class="calibre20">使用t-SNE</em>(2008)<a href="http://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf" target="_blank" class="calibre11">http://www . jmlr . org/papers/volume 9/vandermaten 08 a/vandermaten 08 a . pdf</a>的t-SNE，使用以下代码来可视化嵌入:</p>
<pre class="calibre21">from sklearn.manifold import TSNE<br class="title-page-name"/>import matplotlib.pyplot as plt<br class="title-page-name"/>tsne = TSNE(perplexity=30, n_components=2,<br class="title-page-name"/>            init='pca', n_iter=5000, method='exact')<br class="title-page-name"/>plot_only = 500<br class="title-page-name"/>low_dim_embs = tsne.fit_transform(final_embeddings[:plot_only, :])<br class="title-page-name"/># reverse_dictionary - maps codes(integers) to words(strings)<br class="title-page-name"/>labels = [reverse_dictionary[i] for i in xrange(plot_only)]<br class="title-page-name"/>plt.figure(figsize=(18, 18)) # in inches<br class="title-page-name"/>for i, label in enumerate(labels):<br class="title-page-name"/>  x, y = low_dim_embs[i, :]<br class="title-page-name"/>  plt.scatter(x, y)<br class="title-page-name"/>  plt.annotate(label,<br class="title-page-name"/>               xy=(x, y),<br class="title-page-name"/>               xytext=(5, 2),<br class="title-page-name"/>               textcoords='offset points',<br class="title-page-name"/>               ha='right',<br class="title-page-name"/>               va='bottom')</pre>
<p class="calibre2">在下图中，我们将Word2Vec嵌入可视化，发现意思相近的单词彼此接近:</p>
<div><img class="alignnone74" src="img/0cb3aec3-2daa-451c-85f0-982b116481e9.png"/></div>
<p>使用t-SNE的Word2Vec嵌入的可视化</p>


            

            
        
    </body>

</html>


<html xmlns:epub="http://www.idpf.org/2007/ops">
  <head>
    <title>Using existing pre-trained Word2Vec embeddings</title>
    <meta content="urn:uuid:3fd593c7-d9c4-4d18-84e8-0b6efc257c5b" name="Adept.expected.resource"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body class="calibre">
        

                            
                    <h1 class="header-title">使用现有的预训练Word2Vec嵌入</h1>
                
            
            
                
<p class="calibre2">在本节中，我们将讨论以下主题:</p>
<ul class="calibre7">
<li class="calibre8">来自谷歌新闻的Word2Vec</li>
<li class="calibre8">使用预先训练的Word2Vec嵌入</li>
</ul>


            

            
        
    </body>

</html>


<html xmlns:epub="http://www.idpf.org/2007/ops">
  <head>
    <title>Word2Vec from Google News</title>
    <meta content="urn:uuid:3fd593c7-d9c4-4d18-84e8-0b6efc257c5b" name="Adept.expected.resource"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body class="calibre">
        

                            
                    <h1 class="header-title">来自谷歌新闻的Word2Vec</h1>
                
            
            
                
<p class="calibre2">Google在Google News数据集上训练的Word2Vec模型，特征维度为300。特性的数量被认为是一个超参数，你可以，也许应该，在你自己的应用程序中试验，看看哪个设置产生最好的结果。</p>
<p class="calibre2">在这个预训练的模型中，一些停用词如<em xmlns:epub="http://www.idpf.org/2007/ops" class="calibre20"> a </em>、<em xmlns:epub="http://www.idpf.org/2007/ops" class="calibre20">和</em>以及的<em xmlns:epub="http://www.idpf.org/2007/ops" class="calibre20">被排除，但其他如<em xmlns:epub="http://www.idpf.org/2007/ops" class="calibre20"> the </em>、<em xmlns:epub="http://www.idpf.org/2007/ops" class="calibre20"> also </em>以及<em xmlns:epub="http://www.idpf.org/2007/ops" class="calibre20"> should </em>被包括在内。一些拼错的单词也包括在内，例如，<em xmlns:epub="http://www.idpf.org/2007/ops" class="calibre20">拼错了</em>，而<em xmlns:epub="http://www.idpf.org/2007/ops" class="calibre20">拼错了</em>——后者才是正确的。</em></p>
<p class="calibre2">你可以找到开源工具，如<a href="https://github.com/chrisjmccormick/inspect_word2vec" target="_blank" class="calibre11">https://github.com/chrisjmccormick/inspect_word2vec</a>来检查预训练模型中的单词嵌入。</p>


            

            
        
    </body>

</html>


<html xmlns:epub="http://www.idpf.org/2007/ops">
  <head>
    <title>Using the pre-trained Word2Vec embeddings</title>
    <meta content="urn:uuid:3fd593c7-d9c4-4d18-84e8-0b6efc257c5b" name="Adept.expected.resource"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body class="calibre">
        

                            
                    <h1 class="header-title">使用预先训练的Word2Vec嵌入</h1>
                
            
            
                
<p class="calibre2">在本节中，我们将简要说明如何使用预训练向量。在阅读本节之前，从<a href="https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit" class="calibre11">https://drive . Google . com/file/d/0 b 7 xkcwpi 5 kdynlnuttlss 21 pqmm/edit</a>下载Word2Vec预训练向量，并加载模型:</p>
<pre class="calibre21">from gensim.models import KeyedVectors<br class="title-page-name"/># Load pre-trained model<br class="title-page-name"/>model = KeyedVectors.load_word2vec_format(<br class="title-page-name"/>    './GoogleNews-vectors-negative300.bin', binary=True)</pre>
<p class="calibre2">然后，我们找到与<kbd class="calibre12">woman</kbd>和<kbd class="calibre12">king</kbd>相似，但与<kbd class="calibre12">man</kbd>不同的前<kbd class="calibre12">5</kbd>个单词:</p>
<pre class="calibre21">model.wv.most_similar(<br class="title-page-name"/>    positive=['woman', 'king'], negative=['man'], topn=5)</pre>
<p class="calibre2">我们看到以下内容:</p>
<pre class="calibre21">[(u'queen', 0.7118192911148071),<br class="title-page-name"/> (u'monarch', 0.6189674139022827),<br class="title-page-name"/> (u'princess', 0.5902431607246399),<br class="title-page-name"/> (u'crown_prince', 0.5499460697174072),<br class="title-page-name"/> (u'prince', 0.5377321243286133)]</pre>
<p class="calibre2">这是有意义的，因为<kbd class="calibre12">queen</kbd>与<kbd class="calibre12">woman</kbd>和<kbd class="calibre12">king</kbd>共享相似的属性，但与<kbd class="calibre12">man</kbd>不共享属性。</p>


            

            
        
    </body>

</html>


<html xmlns:epub="http://www.idpf.org/2007/ops">
  <head>
    <title>Understanding GloVe</title>
    <meta content="urn:uuid:3fd593c7-d9c4-4d18-84e8-0b6efc257c5b" name="Adept.expected.resource"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body class="calibre">
        

                            
                    <h1 class="header-title">理解手套</h1>
                
            
            
                
<p class="calibre2">GloVe是一种无监督学习算法，用于获得单词的矢量表示(嵌入)。已经看到，利用相似的训练超参数，使用两种方法生成的嵌入往往在下游NLP任务中表现得非常相似。</p>
<p class="calibre2">不同的是，Word2Vec是一个预测模型，它通过最小化预测损失，即损失(目标词|上下文词；<em class="calibre20"> W </em>。在Word2Vec中，它被形式化为前馈神经网络，并使用优化方法(如SGD)来更新网络。</p>
<p class="calibre2">另一方面，GloVe本质上是一个基于计数的模型，其中首先构建一个共现矩阵。这个共现中的每个条目对应于我们看到目标单词(行)的频率，同时，我们看到上下文单词(列)。然后，分解该矩阵以产生低维(单词x特征)矩阵，其中每一行现在产生每个单词的向量表示。这就是降维的意义所在，因为目标是最小化重建损失，并找到低维表示来解释高维数据中的大部分差异。</p>
<p class="calibre2">尽管GloVe over Word2Vec有一些好处，因为它更容易并行化实现，从而可以在更多数据上训练模型。</p>
<p class="calibre2">网上有很多资源。对于使用TensorFlow的实现，可以查看<a href="https://github.com/GradySimon/tensorflow-glove/blob/master/Getting%20Started.ipynb" target="_blank" class="calibre11">https://github . com/grady Simon/tensor flow-glove/blob/master/Getting % 20 started . ipynb .</a></p>


            

            
        
    </body>

</html>


<html xmlns:epub="http://www.idpf.org/2007/ops">
  <head>
    <title>FastText</title>
    <meta content="urn:uuid:3fd593c7-d9c4-4d18-84e8-0b6efc257c5b" name="Adept.expected.resource"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body class="calibre">
        

                            
                    <h1 class="header-title">快速文本</h1>
                
            
            
                
<p class="calibre2">fast text(【https://fasttext.cc/】)是一个高效学习单词表示和句子分类的库。与Word2Vec相比，FastText嵌入的主要优势是在学习单词表示时考虑单词的内部结构，这对于形态学丰富的语言以及很少出现的单词非常有用。</p>
<p class="calibre2">Word2Vec和FastText的主要区别在于，对于Word2Vec，原子实体是每个单词，这是训练的最小单位。相反，在FastText中，最小单位是字符级的<em class="calibre20"> n </em> -grams，每个单词都被视为由字符<em class="calibre20"> n </em> -grams组成。例如，<em class="calibre20"> happy </em>的单词向量具有最小尺寸为3且最大尺寸为6的<em class="calibre20">n</em>-克，可以分解为:</p>
<p class="calibre2">&lt;哈、&lt;哈、&lt;哈普、&lt;哈普、哈普、哈普、哈普、哈普、哈普&gt;，app、appy、appy &gt;，ppy、ppy &gt;，py &gt;</p>
<p class="calibre2">正因为如此，FastText通常会为罕见的单词生成更好的单词嵌入，因为即使在单词级别，它们也是罕见的，并且组成的<em class="calibre20">n</em>gram字符仍然可以看到并与其他单词共享。而在Word2Vec中，一个罕见的单词通常有更少的邻居，因此可以学习的结果训练实例更少。还有，Word2Vec有固定的词汇大小，词汇是基于给定的训练数据通过预处理配置的。当面对一个不在词汇表中的新单词时，Word2Vec和GloVe都会无解。但由于FastText是在字符<em class="calibre20"> n </em> -gram层面上，只要来自生僻字的<em class="calibre20">n</em>-gram在训练语料库中出现过，FastText就能通过对字符<em class="calibre20"> n </em> -gram向量求和来构建单词层面的向量。</p>
<p class="calibre2">由于粒度更细，FastText也需要更长的时间和更多的内存来学习。因此，需要仔细选择控制<em class="calibre20"> n </em>克大小的超参数，例如<em class="calibre20"> n </em>克最大值和<em class="calibre20"> n </em>克最小值。使用预设的<em class="calibre20"> n </em> -gram min/max，可以调整最小字数，这决定了一个单词需要在语料库中出现的最小频率，才能包含在词汇表中。参数-桶数用于控制单词和字符<em class="calibre20">n</em>gram特征被散列到的桶数(为<em class="calibre20">n</em>gram选择一个向量是一个散列函数)。这有助于限制模型的内存使用。对于设置铲斗尺寸，如果<em class="calibre20"> n </em>克数不大，建议使用较小的铲斗尺寸。</p>
<p class="calibre2">读者可以在<a href="https://github.com/facebookresearch/fastText/blob/master/pretrained-vectors.md" target="_blank" class="calibre11">https://github . com/face book research/fast text/blob/master/pre trained-vectors . MD</a>找到294种语言的预训练词向量。</p>


            

            
        
    </body>

</html>


<html xmlns:epub="http://www.idpf.org/2007/ops">
  <head>
    <title>Applications</title>
    <meta content="urn:uuid:3fd593c7-d9c4-4d18-84e8-0b6efc257c5b" name="Adept.expected.resource"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body class="calibre">
        

                            
                    <h1 class="header-title">应用程序</h1>
                
            
            
                
<p class="calibre2">在本节中，我们将讨论一些示例用例以及NLP模型的微调。</p>


            

            
        
    </body>

</html>


<html xmlns:epub="http://www.idpf.org/2007/ops">
  <head>
    <title>Example use cases</title>
    <meta content="urn:uuid:3fd593c7-d9c4-4d18-84e8-0b6efc257c5b" name="Adept.expected.resource"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body class="calibre">
        

                            
                    <h1 class="header-title">示例使用案例</h1>
                
            
            
                
<p class="calibre2">通过预先训练的Word2Vec嵌入，NLP的下游应用可以很多，例如，文档分类或情感分类。一个例子叫做<strong class="calibre13"> Doc2Vec </strong>，它以最简单的形式获取文档中每个单词的Word2Vec向量，并通过取这些词的归一化总和或算术平均值来聚合它们。每个文档的结果向量用于文本分类。这种类型的应用可以被认为是所学单词嵌入的直接应用。</p>
<p class="calibre2">另一方面，我们可以将Word2Vec建模的思想应用于其他应用程序，例如，在电子商务环境中查找类似的物品。在每个会话窗口期间，在线用户可能会浏览多个项目。从这样的行为中，我们可以使用行为信息来寻找相似或相关的项目。在这种情况下，具有唯一ID的每个项目可以被认为是一个单词，同一会话中的项目可以被认为是上下文单词。我们可以进行类似的训练数据生成，并将生成的数据对传送到网络中。嵌入结果然后可以用于计算项目之间的相似性。</p>


            

            
        
    </body>

</html>


<html xmlns:epub="http://www.idpf.org/2007/ops">
  <head>
    <title>Fine-tuning</title>
    <meta content="urn:uuid:3fd593c7-d9c4-4d18-84e8-0b6efc257c5b" name="Adept.expected.resource"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body class="calibre">
        

                            
                    <h1 class="header-title">微调</h1>
                
            
            
                
<p class="calibre2">微调是指用来自另一个任务(例如无监督的训练任务)的参数初始化网络，然后根据手头的任务更新这些参数的技术。例如，NLP架构通常使用预先训练的单词嵌入，如Word2Vec，然后在训练期间或通过对特定任务(如情感分析)的持续训练来更新这些单词嵌入。</p>


            

            
        
    </body>

</html>


<html xmlns:epub="http://www.idpf.org/2007/ops">
  <head>
    <title>Summary</title>
    <meta content="urn:uuid:3fd593c7-d9c4-4d18-84e8-0b6efc257c5b" name="Adept.expected.resource"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body class="calibre">
        

                            
                    <h1 class="header-title">摘要</h1>
                
            
            
                
<p class="calibre2">在这一章中，我们介绍了使用神经网络学习分布式单词表示的基本思想和模型。我们特别深入到Word2Vec模型，并展示了如何训练模型，以及如何为下游NLP应用程序加载预先训练好的向量。在下一章，我们将讨论NLP中更高级的深度学习模型，如递归神经网络、长时短记忆模型，以及几个真实世界的应用。</p>


            

            
        
    </body>

</html>
</body></html>