<html><head/><body>

  
    <title>Getting Started with Neural Networks</title>
    <meta content="urn:uuid:3fd593c7-d9c4-4d18-84e8-0b6efc257c5b" name="Adept.expected.resource"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  


  
        

                            
                    <h1 class="header-title">神经网络入门</h1>
                
            
            
                
<p class="calibre2">在这一章中，我们将集中在神经网络的基础上，包括输入/输出层，隐藏层，以及网络如何通过正向和反向传播来学习。我们将从标准的多层感知器网络开始，讨论它们的构建模块，并说明它们如何一步一步地学习。我们还将介绍几个流行的标准模型，如<strong class="calibre13">卷积神经网络</strong> ( <strong class="calibre13"> CNN </strong>)、<strong class="calibre13">受限玻尔兹曼机</strong> ( <strong class="calibre13"> RBM </strong>)、<strong class="calibre13">递归神经网络</strong> ( <strong class="calibre13"> RNN </strong>)及其变种<strong class="calibre13">长短期记忆</strong> ( <strong class="calibre13"> LSTM </strong>)。我们将概述模型成功应用的关键要素，并解释一些重要概念，以帮助您更好地理解为什么这些网络在某些领域如此有效。除了理论上的介绍，我们还将展示使用TensorFlow的示例代码片段，说明如何构建层和激活函数，以及如何连接不同的层。最后，我们将使用TensorFlow演示一个端到端的MNIST分类示例。有了你从第2章<a href="7ea715d3-38a1-45ce-83af-0583b92b2efc.xhtml" target="_blank" class="calibre11">学到的设置，<em class="calibre20">让自己为深度学习做好准备</em>是时候我们跳到一些真实的例子中来动手了。</a></p>
<p class="calibre2">本章的大纲如下:</p>
<ul class="calibre7">
<li class="calibre22">多层感知器；<ul class="calibre23">
<li class="calibre8">输入层</li>
<li class="calibre8">输出层</li>
<li class="calibre8">隐藏层</li>
<li class="calibre8">激活功能</li>
</ul>
</li>
<li class="calibre22">网络如何学习</li>
<li class="calibre22">深度学习模型:<ul class="calibre23">
<li class="calibre22">卷积神经网络</li>
<li class="calibre22">受限玻尔兹曼机器</li>
<li class="calibre22">RNN/LSTM</li>
</ul>
</li>
<li class="calibre22">MNIST动手分类示例</li>
</ul>


            

            
        
    





  
    <title>Multilayer perceptrons</title>
    <meta content="urn:uuid:3fd593c7-d9c4-4d18-84e8-0b6efc257c5b" name="Adept.expected.resource"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  


  
        

                            
                    <h1 class="header-title">多层感知器</h1>
                
            
            
                
<p class="calibre2">多层感知器是最简单的网络之一。本质上，它被定义为具有一个输入层、一个输出层和几个隐藏层(不止一个)。每层有多个神经元，相邻层完全连接。每个神经元都可以被认为是这些巨大网络中的一个细胞。它决定了输入信号的流动和转换。来自先前层的信号通过连接的权重被向前推至下一层的神经元。对于每个人工神经元，它通过将信号乘以权重并加上偏差来计算所有输入的加权和。然后，加权和将通过一个被称为<strong class="calibre13">激活函数</strong>的函数，以决定它是否应该被触发，从而产生下一级的输出信号。</p>
<p class="calibre2">例如，下图描绘了一个完全连接的前馈神经网络。你可能注意到了，每一层上都有一个截取节点(<em class="calibre20">x<sub class="calibre34">0</sub>T3】和<em class="calibre20"> a <sub class="calibre34"> 0 </sub> </em>)。网络的非线性主要由激活函数的形状决定。</em></p>
<p class="calibre2">这种完全连接的前馈神经网络的架构本质上如下所示:</p>
<div><img class="alignnone20" src="img/9984e020-36f7-4b41-a260-ad968612b4cf.png"/></div>
<p>具有两个隐藏层的全连接前馈神经网络</p>


            

            
        
    





  
    <title>The input layer</title>
    <meta content="urn:uuid:3fd593c7-d9c4-4d18-84e8-0b6efc257c5b" name="Adept.expected.resource"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  


  
        

                            
                    <h1 class="header-title">输入层</h1>
                
            
            
                
<p class="calibre2">输入图层通常被定义为原始输入数据。对于文本数据，这可以是单词或字符。对于图像，这可以是来自不同颜色通道的原始像素值。此外，随着输入数据维度的变化，它会形成不同的结构，如一维向量或类似张量的结构。</p>


            

            
        
    





  
    <title>The output layer</title>
    <meta content="urn:uuid:3fd593c7-d9c4-4d18-84e8-0b6efc257c5b" name="Adept.expected.resource"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  


  
        

                            
                    <h1 class="header-title">输出层</h1>
                
            
            
                
<p class="calibre2">输出层基本上是网络的输出值，根据问题设置而形成。在无监督学习中，如编码或解码，输出可以与输入相同。对于分类问题，输出层可以有<em class="calibre20"> n </em>个神经元用于<em class="calibre20"> n </em>路分类，并利用softmax函数输出每个类别的概率。总的来说，输出层映射到你的目标空间，其中的感知器会根据你的问题设置相应地改变。</p>


            

            
        
    





  
    <title>Hidden layers</title>
    <meta content="urn:uuid:3fd593c7-d9c4-4d18-84e8-0b6efc257c5b" name="Adept.expected.resource"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  


  
        

                            
                    <h1 class="header-title">隐藏层</h1>
                
            
            
                
<p class="calibre2">隐藏层是输入层和输出层之间的层。隐藏层上的神经元可以采取各种形式，如最大池层、卷积层等，都执行不同的数学功能。如果您将整个网络视为数学变换的管道，则隐藏的图层将分别进行变换，然后组合在一起，以将输入数据映射到输出空间。在本章后面的章节中，当我们讨论卷积神经网络和RNN时，我们将介绍隐藏层的更多变化。</p>


            

            
        
    





  
    <title>Activation functions</title>
    <meta content="urn:uuid:3fd593c7-d9c4-4d18-84e8-0b6efc257c5b" name="Adept.expected.resource"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  


  
        

                            
                    <h1 class="header-title">激活功能</h1>
                
            
            
                
<p class="calibre2">每个人工神经元中的激活函数决定输入信号是否已经达到阈值，是否应该输出下一级的信号。因为梯度消失的问题，设置正确的激活函数是至关重要的，我们将在后面讨论。</p>
<p class="calibre2">激活函数的另一个重要特征是它应该是可微的。网络从输出层计算的误差中学习。需要可微分的激活函数来执行反向传播优化，同时在网络中反向传播，以计算相对于权重的误差(损失)的梯度，然后相应地优化权重，使用梯度下降或任何其他优化技术来减少误差。</p>
<p class="calibre2">下表列出了一些常见的激活功能。我们将深入探讨它们，讨论它们之间的差异，并解释如何选择正确的激活功能:</p>
<div><img class="alignnone21" src="img/96ec8e86-7c23-41d3-b8fd-f0566bbd89e6.png"/></div>
<p class="calibre2"> </p>


            

            
        
    





  
    <title>Sigmoid or logistic function</title>
    <meta content="urn:uuid:3fd593c7-d9c4-4d18-84e8-0b6efc257c5b" name="Adept.expected.resource"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  


  
        

                            
                    <h1 class="header-title">乙状结肠或逻辑函数</h1>
                
            
            
                
<p class="calibre2">sigmoid函数具有独特的<em class="calibre20"> S </em>形状，并且对于任何实输入值，它都是可微的实函数。其范围在0和1之间。它是以下形式的激活函数:</p>
<p class="calibre39"><img class="alignnone22" src="img/2a1da2e7-0333-4f7d-aea1-942a46ae9c93.png"/></p>
<p class="calibre2">在训练步骤的反向传播期间使用的它的一阶导数具有以下形式:</p>
<p class="calibre39"><img class="alignnone23" src="img/3b04ea36-9efe-41e7-b99a-30e238a19e99.png"/></p>
<div><p class="calibre2">实现如下:</p>
<pre class="calibre21">def sigmoid(x):<br class="title-page-name"/>    return tf.div(tf.constant(1.0),<br class="title-page-name"/>                  tf.add(tf.constant(1.0), tf.exp(tf.neg(x))))</pre>
<p class="calibre2"><kbd class="calibre12">sigmoid</kbd>函数的导数如下:</p>
<pre class="calibre21">def sigmoidprime(x):<br class="title-page-name"/>    return tf.multiply(sigmoid(x), tf.subtract(tf.constant(1.0), sigmoid(x)))</pre></div>
<p class="calibre2">然而，<kbd class="calibre12">sigmoid</kbd>函数会导致渐变消失问题或渐变饱和。众所周知，它的收敛速度很慢。所以在实际使用中，不建议使用一个<kbd class="calibre12">sigmoid</kbd>作为激活功能，ReLU已经比较流行了。</p>


            

            
        
    





  
    <title>Tanh or hyperbolic tangent function</title>
    <meta content="urn:uuid:3fd593c7-d9c4-4d18-84e8-0b6efc257c5b" name="Adept.expected.resource"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  


  
        

                            
                    <h1 class="header-title">双曲正切函数</h1>
                
            
            
                
<p class="calibre2">tanh的数学公式如下:</p>
<p class="calibre40"><img class="alignnone24" src="img/167b7972-8a94-4ff6-b6db-8974e39314e6.png"/></p>
<p class="calibre2">其输出以零为中心，范围为-1到1。因此，优化更容易，因此在实践中，它优于sigmoid激活函数。然而，它仍然受到消失梯度问题的困扰。</p>


            

            
        
    





  
    <title>ReLU</title>
    <meta content="urn:uuid:3fd593c7-d9c4-4d18-84e8-0b6efc257c5b" name="Adept.expected.resource"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  


  
        

                            
                    <h1 class="header-title">热卢</h1>
                
            
            
                
<p class="calibre2"><strong class="calibre13">整流线性单元(ReLU) </strong>近年来变得相当流行。它的数学公式如下:</p>
<p class="calibre39"><img class="alignnone25" src="img/5ac49f32-3efd-4f3a-b102-382bb90983a8.png"/></p>
<p class="calibre2">与sigmoid和tanh相比，它的计算更简单、更有效。事实证明，它将收敛提高了6倍(例如，Krizhevsky及其合著者在2012年<em class="calibre20"> ImageNet分类与深度卷积神经网络</em>的工作中提高了6倍)，这可能是因为它具有线性和非饱和形式。此外，与涉及昂贵的指数运算的tanh或sigmoid函数不同，ReLU可以通过简单地将激活阈值设为零来实现。因此，在过去的几年里，它变得非常流行。现在几乎所有的深度学习模型都使用ReLU。ReLU的另一个重要优点是它避免或纠正了消失梯度问题。</p>
<p class="calibre2">它的局限性在于它的直接输出不在概率空间中。它不能用于输出图层，只能用于隐藏图层。因此，对于分类问题，需要在最后一层使用softmax函数来计算类的概率。对于回归问题，应该简单地使用线性函数。ReLU的另一个问题是，它会导致死神经元问题。例如，如果大的梯度流过ReLU，它可能导致权重被更新，使得神经元将永远不会在任何其他未来数据点上活动。</p>
<p class="calibre2">为了解决这个问题，引入了另一个名为<strong class="calibre13"> Leaky ReLU </strong>的修改。为了解决神经元死亡的问题，它引入了一个小斜率来保持更新的活力。</p>


            

            
        
    





  
    <title>Leaky ReLU and maxout</title>
    <meta content="urn:uuid:3fd593c7-d9c4-4d18-84e8-0b6efc257c5b" name="Adept.expected.resource"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  


  
        

                            
                    <h1 class="header-title">泄漏ReLU和最大输出</h1>
                
            
            
                
<p class="calibre2">泄漏的ReLU在负侧将具有小的斜率α，例如0.01。斜率α也可以做成每个神经元的参数，比如在PReLU神经元中(P代表参数)。这种激活函数的问题是这种修改对各种问题的有效性不一致。</p>
<p class="calibre2">Maxout是解决ReLU中死神经元问题的又一次尝试。它以<img class="alignnone26" src="img/fde758cb-477c-41aa-b36c-bdd1a8d9c1f2.png"/>的形式出现。从这个形态我们可以看出，ReLU和漏ReLU都只是这个形态的特例，也就是对于ReLU来说，是<img class="alignnone27" src="img/26a8d1e3-4ea6-4b72-9a50-2bd02ed45e2c.png"/>。虽然它受益于线性和没有饱和，但它使每个单个神经元的参数数量增加了一倍。</p>


            

            
        
    





  
    <title>Softmax</title>
    <meta content="urn:uuid:3fd593c7-d9c4-4d18-84e8-0b6efc257c5b" name="Adept.expected.resource"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  


  
        

                            
                    <h1 class="header-title">Softmax</h1>
                
            
            
                
<p class="calibre2">当使用ReLU作为分类问题的激活函数时，在最后一层使用softmax函数。它有助于为每个类别生成概率，例如类似于(<img class="alignnone28" src="img/44b08c1a-ae23-4c2f-b99a-0f29c4d98856.png"/>)的分数:</p>
<p class="calibre39"><img class="alignnone29" src="img/c576240a-bb34-4024-9016-146b322f5d88.png"/></p>


            

            
        
    





  
    <title>Choosing the right activation function</title>
    <meta content="urn:uuid:3fd593c7-d9c4-4d18-84e8-0b6efc257c5b" name="Adept.expected.resource"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  


  
        

                            
                    <h1 class="header-title">选择正确的激活功能</h1>
                
            
            
                
<p class="calibre2">大多数情况下，我们总是应该首先考虑ReLU。但是请记住，ReLU应该只应用于隐藏层。如果你的模型患有死神经元，那么考虑调整你的学习速率，或者尝试Leaky ReLU或maxout。</p>
<p class="calibre2">不建议使用sigmoid或tanh，因为它们会遇到渐变消失的问题，而且收敛非常慢。以乙状结肠为例。它的导数处处大于0.25，使得反向传播期间的项更小。而对于ReLU，它的导数在零以上的每一点都是1，因此创建了一个更稳定的网络。</p>
<p class="calibre2">现在，您已经对神经网络中的关键组件有了基本的了解，让我们继续了解网络如何从数据中学习。</p>


            

            
        
    





  
    <title>How a network learns</title>
    <meta content="urn:uuid:3fd593c7-d9c4-4d18-84e8-0b6efc257c5b" name="Adept.expected.resource"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  


  
        

                            
                    <h1 class="header-title">网络如何学习</h1>
                
            
            
                
<p class="calibre2">假设我们有一个两层网络。我们用<img class="alignnone30" src="img/4ab0285d-2c84-4425-812c-81e31aa77982.png"/>来表示输入/输出，用状态来表示两层，也就是用偏置值来表示连接权重:<img class="alignnone31" src="img/48f590ae-be9e-44bf-a804-a1c3d3e8a35b.png"/>和<img class="alignnone32" src="img/75f7eadb-d451-4605-9a92-464d82ea8772.png"/>。我们还将使用σ作为激活函数。</p>


            

            
        
    





  
    <title>Weight initialization</title>
    <meta content="urn:uuid:3fd593c7-d9c4-4d18-84e8-0b6efc257c5b" name="Adept.expected.resource"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  


  
        

                            
                    <h1 class="header-title">重量初始化</h1>
                
            
            
                
<p class="calibre2">网络配置完成后，训练从初始化权重值开始。正确的权重初始化很重要，因为所有的训练都是调整系数，以便从数据中最好地捕获模式，从而成功地输出目标值的近似值。在大多数情况下，权重是随机初始化的。在一些微调设置中，使用预训练的模型初始化权重。</p>


            

            
        
    





  
    <title>Forward propagation</title>
    <meta content="urn:uuid:3fd593c7-d9c4-4d18-84e8-0b6efc257c5b" name="Adept.expected.resource"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  


  
        

                            
                    <h1 class="header-title">正向传播</h1>
                
            
            
                
<p class="calibre2">前向传播基本上是计算输入数据乘以网络的权重加上偏移，然后通过激活函数到达下一层:</p>
<p class="calibre40"><img class="alignnone33" src="img/644533fd-1afe-4fbc-b7ee-00d21f9caba2.png"/></p>
<p class="calibre2">使用TensorFlow的示例代码块可以编写如下:</p>
<pre class="calibre21"># dimension variables<br class="title-page-name"/>dim_in = 2 <br class="title-page-name"/>dim_middle = 5<br class="title-page-name"/>dim_out = 1<br class="title-page-name"/><br class="title-page-name"/># declare network variables<br class="title-page-name"/>a_0 = tf.placeholder(tf.float32, [None, dim_in])<br class="title-page-name"/>y = tf.placeholder(tf.float32, [None, dim_out])<br class="title-page-name"/><br class="title-page-name"/>w_1 = tf.Variable(tf.random_normal([dim_in, dim_middle]))<br class="title-page-name"/>b_1 = tf.Variable(tf.random_normal([dim_middle]))<br class="title-page-name"/>w_2 = tf.Variable(tf.random_normal([dim_middle, dim_out]))<br class="title-page-name"/>b_2 = tf.Variable(tf.random_normal([dim_out]))<br class="title-page-name"/><br class="title-page-name"/># build the network structure<br class="title-page-name"/>z_1 = tf.add(tf.matmul(a_0, w_1), b_1)<br class="title-page-name"/>a_1 = sigmoid(z_1)<br class="title-page-name"/>z_2 = tf.add(tf.matmul(a_1, w_2), b_2)<br class="title-page-name"/>a_2 = sigmoid(z_2)</pre>


            

            
        
    





  
    <title>Backpropagation</title>
    <meta content="urn:uuid:3fd593c7-d9c4-4d18-84e8-0b6efc257c5b" name="Adept.expected.resource"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  


  
        

                            
                    <h1 class="header-title">反向传播</h1>
                
            
            
                
<p class="calibre2">所有网络从误差中学习，然后基于给定的成本函数更新网络权重/参数以反映误差。梯度是表示网络权重与其误差之间关系的斜率。</p>


            

            
        
    





  
    <title>Calculating errors</title>
    <meta content="urn:uuid:3fd593c7-d9c4-4d18-84e8-0b6efc257c5b" name="Adept.expected.resource"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  


  
        

                            
                    <h1 class="header-title">计算误差</h1>
                
            
            
                
<p class="calibre2">反向传播的第一件事是计算目标值的正向传播误差。输入提供了<em class="calibre20"> y </em>作为对网络输出准确性的测试，因此我们计算以下向量:</p>
<p class="calibre40"><img class="alignnone34" src="img/db6bd8cd-84ff-431a-9578-0081fa88128e.png"/></p>
<p class="calibre2">这是用如下代码编写的:</p>
<pre class="calibre21"># define error, which is the difference between the activation function output from the last layer and the label<br class="title-page-name"/>error = tf.sub(a_2, y)</pre>


            

            
        
    





  
    <title>Backpropagation</title>
    <meta content="urn:uuid:3fd593c7-d9c4-4d18-84e8-0b6efc257c5b" name="Adept.expected.resource"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  


  
        

                            
                    <h1 class="header-title">反向传播</h1>
                
            
            
                
<p class="calibre2">对于误差，反向传播反向工作以更新误差梯度方向上的网络权重。首先，我们需要计算权重和偏差的增量。注意<img class="fm-editor-equation18" src="img/6208b68e-83e4-4ede-bf69-b55ae5b90283.png"/>用于更新<img class="alignnone35" src="img/4fc860e4-cf90-477e-bfe6-ac093d75b9c6.png"/>和<img class="alignnone36" src="img/cc40c73c-2292-49df-9bc7-6cad5d2a879b.png"/>,<img class="alignnone37" src="img/29f2967e-bb5b-4cb2-b625-ed8e6ed252bc.png"/>用于更新<img class="alignnone35" src="img/e45b61a0-7d48-4497-a4b5-7807d2063994.png"/>和<img class="alignnone38" src="img/224551e2-fe17-47ff-9de1-8bb94bb9d1e5.png"/>:</p>
<p class="calibre40"><img class="fm-editor-equation19" src="img/cf18837f-1d7a-4028-86e9-fca7c1c0af96.png"/></p>
<p class="calibre40"><img class="fm-editor-equation20" src="img/6b5d2926-985e-4eb7-9380-12f92f79d4cb.png"/></p>
<p class="calibre40"><img class="fm-editor-equation21" src="img/95541a23-f29e-4304-a274-a92297a3bc61.png"/></p>
<p class="calibre40"><img class="fm-editor-equation22" src="img/1e30c8c5-d175-4a60-868d-0a1179754c80.png"/></p>
<p class="calibre40"><img class="fm-editor-equation23" src="img/00572286-9b96-4ea3-bafd-062ad4ab5839.png"/></p>
<p class="calibre40"><img class="fm-editor-equation24" src="img/a49f3efa-97f8-4edc-8a0e-1f0596adecd7.png"/></p>
<p class="calibre2">这用张量流代码写成，如下所示:</p>
<pre class="calibre21">d_z_2 = tf.multiply(error, sigmoidprime(z_2))<br class="title-page-name"/>d_b_2 = d_z_2<br class="title-page-name"/>d_w_2 = tf.matmul(tf.transpose(a_1), d_z_2)<br class="title-page-name"/><br class="title-page-name"/>d_a_1 = tf.matmul(d_z_2, tf.transpose(w_2))<br class="title-page-name"/>d_z_1 = tf.multiply(d_a_1, sigmoidprime(z_1))<br class="title-page-name"/>d_b_1 = d_z_1<br class="title-page-name"/>d_w_1 = tf.matmul(tf.transpose(a_0), d_z_1)</pre>


            

            
        
    





  
    <title>Updating the network</title>
    <meta content="urn:uuid:3fd593c7-d9c4-4d18-84e8-0b6efc257c5b" name="Adept.expected.resource"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  


  
        

                            
                    <h1 class="header-title">更新网络</h1>
                
            
            
                
<p class="calibre2">现在已经计算出了增量，是时候更新网络的参数了。在大多数情况下，我们使用一种梯度下降。设<img class="fm-editor-equation25" src="img/6c113457-e73b-4612-9961-cb0f40d0835f.png"/>代表学习率，参数更新公式为:</p>
<p class="calibre39"><img class="fm-editor-equation26" src="img/76ab60e8-0668-4589-90d0-c79668444de3.png"/></p>
<p class="calibre39"><img class="fm-editor-equation27" src="img/6a181c2a-e31b-470d-8db8-60ccfeaae6cd.png"/></p>
<p class="calibre39"><img class="fm-editor-equation28" src="img/fdb49fc1-f75c-4374-b8be-c40a6bb775eb.png"/></p>
<p class="calibre39"><img class="fm-editor-equation29" src="img/e7d9b718-287b-45fa-9a41-2ac1eb5ef9f9.png"/></p>
<p class="calibre2">这用张量流代码写成，如下所示:</p>
<pre class="calibre21">eta = tf.constant(0.01)<br class="title-page-name"/>step = [<br class="title-page-name"/>    tf.assign(w_1,<br class="title-page-name"/>              tf.subtract(w_1, tf.multiply(eta, d_w_1))),<br class="title-page-name"/>    tf.assign(b_1,<br class="title-page-name"/>              tf.subtract(b_1, tf.multiply(eta,<br class="title-page-name"/>                               tf.reduce_mean(d_b_1, axis=[0])))),<br class="title-page-name"/>    tf.assign(w_2,<br class="title-page-name"/>              tf.subtract(w_2, tf.multiply(eta, d_w_2))),<br class="title-page-name"/>    tf.assign(b_2,<br class="title-page-name"/>              tf.subtract(b_2, tf.multiply(eta,<br class="title-page-name"/>                               tf.reduce_mean(d_b_2, axis=[0]))))<br class="title-page-name"/>]</pre>


            

            
        
    





  
    <title>Automatic differentiation</title>
    <meta content="urn:uuid:3fd593c7-d9c4-4d18-84e8-0b6efc257c5b" name="Adept.expected.resource"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  


  
        

                            
                    <h1 class="header-title">自动微分</h1>
                
            
            
                
<p class="calibre2">TensorFlow提供了一个非常方便的API，可以帮助我们直接导出增量并更新网络参数:</p>
<pre class="calibre21"># Define the cost as the square of the errors<br class="title-page-name"/>cost = tf.square(error)<br class="title-page-name"/><br class="title-page-name"/># The Gradient Descent Optimizer will do the heavy lifting<br class="title-page-name"/>learning_rate = 0.01<br class="title-page-name"/>optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)<br class="title-page-name"/><br class="title-page-name"/># Define the function we want to approximate<br class="title-page-name"/>def linear_fun(x):<br class="title-page-name"/>    y = x[:,0] * 2 + x[:,1] * 4 + 1<br class="title-page-name"/>    return y.reshape(y.shape[0],1)<br class="title-page-name"/><br class="title-page-name"/># Other variables during learning<br class="title-page-name"/>train_batch_size = 100<br class="title-page-name"/>test_batch_size = 50<br class="title-page-name"/><br class="title-page-name"/># Normal TensorFlow - initialize values, create a session and run the model<br class="title-page-name"/>sess = tf.Session() <br class="title-page-name"/>sess.run(tf.initialize_all_variables())<br class="title-page-name"/><br class="title-page-name"/>for i in range(1000):<br class="title-page-name"/>    x_value = np.random.rand(train_batch_size,2)<br class="title-page-name"/>    y_value = linear_fun(x_value)<br class="title-page-name"/>    sess.run(optimizer, feed_dict={a_0:x_value, y: y_value})<br class="title-page-name"/>    if i % 100 == 0:<br class="title-page-name"/>        test_x = np.random.rand(test_batch_size,2) <br class="title-page-name"/>        res_val = sess.run(res, feed_dict =<br class="title-page-name"/>            {a_0: test_x, y: linear_fun(test_x)})<br class="title-page-name"/>        print res_val</pre>
<p class="calibre2">除了这个基本设置之外，现在让我们来谈谈你在实践中可能遇到的几个重要概念。</p>


            

            
        
    





  
    <title>Vanishing and exploding gradients</title>
    <meta content="urn:uuid:3fd593c7-d9c4-4d18-84e8-0b6efc257c5b" name="Adept.expected.resource"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  


  
        

                            
                    <h1 class="header-title">消失和爆炸渐变</h1>
                
            
            
                
<p class="calibre2">这些都是很多深度神经网络中非常重要的问题。架构越深入，就越容易受到这些问题的困扰。正在发生的是，在反向传播阶段，权重与梯度值成比例地调整。所以我们可能有两种不同的情况:</p>
<ul class="calibre7">
<li class="calibre22">如果梯度太小，那么这被称为<em class="calibre26">消失梯度</em>问题。这使得学习过程非常缓慢，甚至完全停止更新。例如，使用sigmoid作为激活函数，其中它的导数总是小于0.25，在几层反向传播之后，较低的层将很难从误差中接收到任何有用的信号，因此网络没有被适当地更新。</li>
<li class="calibre22">如果梯度变得太大，那么它会导致学习发散，这被称为<em class="calibre26">爆炸梯度</em>。当激活函数不有界或者学习率过大时，经常会出现这种情况。</li>
</ul>


            

            
        
    





  
    <title>Optimization algorithms</title>
    <meta content="urn:uuid:3fd593c7-d9c4-4d18-84e8-0b6efc257c5b" name="Adept.expected.resource"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  


  
        

                            
                    <h1 class="header-title">优化算法</h1>
                
            
            
                
<p class="calibre2">优化是网络学习的关键。学习基本上是一个优化的过程。它指的是最小化误差、成本或找到最小误差点的过程。然后逐步调整网络系数。一个非常基本的优化方法是我们在上一节<em class="calibre20">梯度下降</em>中使用的方法。然而，有多种变体做着类似的工作，但增加了一点改进。TensorFlow提供了多个选项供您选择作为优化器，例如，<em class="calibre20"> GradientDescentOptimizer、AdagradOptimizer、MomentumOptimizer、AdamOptimizer、FtrlOptimizer </em>和<em class="calibre20"> RMSPropOptimizer </em>。有关API及其使用方法，请参见本页:</p>
<p class="calibre2"><a href="https://www.tensorflow.org/versions/master/api_docs/python/tf/train#optimizers" target="_blank" class="calibre11">https://www . tensor flow . org/versions/master/API _ docs/python/TF/train # optimizer</a>。</p>
<p class="calibre2">这些优化器对于大多数深度学习技术来说应该是足够的。如果你不确定使用哪一个，使用<em class="calibre20">梯度下降优化器</em>作为起点。</p>


            

            
        
    





  
    <title>Regularization</title>
    <meta content="urn:uuid:3fd593c7-d9c4-4d18-84e8-0b6efc257c5b" name="Adept.expected.resource"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  


  
        

                            
                    <h1 class="header-title">正规化</h1>
                
            
            
                
<p class="calibre2">像所有其他机器学习方法一样，过度拟合是需要一直控制的事情，尤其是考虑到网络有如此多的参数要学习。处理过拟合的方法之一叫做<strong class="calibre13">正则化</strong>。典型的正则化是通过对参数添加一些约束来完成的，例如L1或L2正则化，其防止网络的权重或系数变得太大。以L2正则化为例。这是通过用神经网络中所有权重的平方来增加成本函数来实现的。它所做的是严重惩罚峰值权重向量并扩散权重向量。</p>
<p class="calibre2">也就是说，我们通过更均匀地分散权重向量来鼓励网络使用其所有输入，而不是仅使用其一部分。过大的权重意味着网络过于依赖一些权重较大的输入，这可能会导致难以推广到新数据。在梯度下降阶段，L2正则化实质上导致每个权重衰减到零，这被称为<strong class="calibre13">权重衰减</strong>。</p>
<p class="calibre2">另一种常见的正则化类型是L1正则化，这通常会导致权重向量变得过于稀疏。通过将许多其他特征变为零，有助于了解哪些特征对预测更有用。这可能有助于网络更好地抵抗输入中的噪声，但是根据经验，L2正则化表现得更好。</p>
<p class="calibre2">Max-norm是对每个神经元的传入权重向量的大小施加绝对上限的另一种方式。也就是说，在梯度下降步骤中，我们用半径<em class="calibre20"> c </em> if <img class="fm-editor-equation30" src="img/83df633d-5a95-4c3b-838c-18e3704bcb46.png"/>将向量归一化。这被称为<strong class="calibre13">投影</strong> <strong class="calibre13">梯度下降</strong>。这有时会稳定网络的学习，因为即使学习速率太高，系数也不会变得太大(总是有界的)。</p>
<p class="calibre2">Dropout是一种非常不同的防止过度拟合的方法，通常与我们之前提到的技术一起使用。在训练过程中，只保持一定比例的神经元活动，而将其他神经元设置为零，就可以实现辍学。预设超参数<img class="fm-editor-equation31" src="img/315a131c-fbe7-4ab1-b1cb-4ed192fcdb3a.png"/>用于生成随机采样，其中神经元应设置为零(被删除)。<img class="fm-editor-equation32" src="img/544cd16e-1cad-4b23-aeac-cc7b52b68a57.png"/>在实践中经常使用。直观上，丢失使得网络的不同部分从不同的信息中学习，因为在每一批中只有网络的一部分在更新。总的来说，它通过提供一种指数地和有效地近似组合许多不同的神经网络结构的方式来防止过拟合。要了解更多细节，可以参考辛顿的辍学论文(<em class="calibre20">斯里瓦斯塔瓦等人，辍学:防止神经网络过度拟合的简单方法，2013 </em>)。</p>


            

            
        
    





  
    <title>Deep learning models</title>
    <meta content="urn:uuid:3fd593c7-d9c4-4d18-84e8-0b6efc257c5b" name="Adept.expected.resource"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  


  
        

                            
                    <h1 class="header-title">深度学习模型</h1>
                
            
            
                
<p class="calibre2">在本节中，我们将逐一深入研究三种流行的深度学习模型:CNN、<strong class="calibre13">受限玻尔兹曼机器</strong> ( <strong class="calibre13"> RBM </strong>)和<strong class="calibre13">递归神经网络</strong> ( <strong class="calibre13"> RNN </strong>)。</p>


            

            
        
    





  
    <title>Convolutional Neural Networks</title>
    <meta content="urn:uuid:3fd593c7-d9c4-4d18-84e8-0b6efc257c5b" name="Adept.expected.resource"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  


  
        

                            
                    <h1 class="header-title">卷积神经网络</h1>
                
            
            
                
<p class="calibre2">卷积神经网络是多层感知器的生物启发变体，并已被证明在图像识别和分类等领域非常有效。ConvNets已经成功应用于识别人脸、物体和交通标志，以及为机器人和自动驾驶汽车的视觉提供动力。CNN通过在相邻层的神经元之间实施局部连接模式来利用空间局部相关性。换句话说，层<em class="calibre20"> m </em>中隐藏单元的输入来自层<img class="fm-editor-equation33" src="img/20c1f702-9ce9-4d5c-8e3a-62f4157f16d7.png"/>中单元的子集，这些单元具有空间上连续的感受野。</p>
<p class="calibre2">LeNet是Yann LeCun在1988年提出的最早的卷积神经网络之一。它主要用于字符识别任务，如阅读邮政编码、数字等。2012年，Alex Krizhevsky和Hinton以惊人的改进赢得了ImageNet比赛，使用CNN将分类错误从26%降至15%，开启了深度学习的复兴时代。</p>
<p class="calibre2">CNN有几个基本的组成部分:</p>
<ul class="calibre7">
<li class="calibre22">卷积层(CONV)</li>
<li class="calibre22">激活层(非线性，例如ReLU)</li>
<li class="calibre22">混合或子采样层(混合)</li>
<li class="calibre22">全连接层(FC，使用softmax)</li>
</ul>
<p class="calibre2">ConvNet最常见的形式是在几对conv-雷鲁层上堆叠，每对层后面都有一个池层。这种模式重复进行，直到整个输入图像被聚集并在空间上转换成小块。然后，在最后一层，它过渡到一个完全连接的层，该层通常利用softmax输出概率，尤其是当它是一个多向分类问题时，如下图所示:</p>
<div><img class="alignnone39" src="img/211706f2-a389-4918-afa1-7e6167664c7a.png"/></div>
<p>一个典型的卷积神经网络，具有conv-雷鲁，池层重复，最后是全连接层</p>


            

            
        
    





  
    <title>Convolution</title>
    <meta content="urn:uuid:3fd593c7-d9c4-4d18-84e8-0b6efc257c5b" name="Adept.expected.resource"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  


  
        

                            
                    <h1 class="header-title">盘旋</h1>
                
            
            
                
<p class="calibre2">卷积涉及几个概念，如卷积、步长和填充。</p>
<p class="calibre2">对于二维图像，每个颜色通道都会发生卷积。假设您有一个权重矩阵和图像(显示为每个像素的值)，如下图所示。</p>
<p class="calibre2">权重矩阵(通常称为<strong class="calibre13">内核</strong>或<strong class="calibre13">滤波器</strong>)通过将内核放置在要卷积的图像上并在整个图像上移动来应用于图像。如果权重矩阵一次移动1个像素，则称之为1的<strong class="calibre13">步距</strong>。在每次放置时，来自原始图像的数字(像素值)乘以当前在其之前对齐的权重矩阵的数字。</p>
<p class="calibre2">所有这些乘积的总和除以内核的规格化器。将结果放入新图像中权重矩阵居中的位置。内核被转换到下一个像素位置，并且重复该过程，直到所有的图像像素都被处理。</p>
<p class="calibre2">正如我们从下图中看到的，步幅为2将导致以下结果:</p>
<div><img class="alignnone40" src="img/09acf784-bcff-489b-94c6-8efe0daaf6b9.png"/></div>
<div><img class="alignnone41" src="img/32fcd83f-351c-491e-889e-b0ef37e7e3a7.png"/></div>
<div><img class="alignnone42" src="img/852305d2-2f2a-4bdf-8342-6ae6c1183357.png"/></div>
<p>步幅= 2的卷积示例</p>
<p class="calibre2">所以步幅越大，图像收缩得越快。为了保持图像的原始大小，我们可以在图像的边界添加0(行和列)。这被称为<em class="calibre20">相同的填充</em>。步幅越大，衬垫就必须越大:</p>
<div><img class="alignnone43" src="img/64aca814-d75a-4b22-b9a3-8459a711d058.png"/></div>
<p>卷积的零填充</p>


            

            
        
    





  
    <title>Pooling/subsampling</title>
    <meta content="urn:uuid:3fd593c7-d9c4-4d18-84e8-0b6efc257c5b" name="Adept.expected.resource"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  


  
        

                            
                    <h1 class="header-title">合并/二次抽样</h1>
                
            
            
                
<p class="calibre2">汇集层逐渐减小表示的空间大小，以减少网络中的参数和计算的数量。对于彩色图像，池化是在每个颜色通道上独立完成的。通常应用的最常见的池层形式是<em class="calibre20">最大池</em>。还有其他类型的共用单位，如平均共用或L2-诺姆共用。你可能会发现一些早期的网络使用平均池。由于<em class="calibre20">最大池</em>通常在实践中表现更好，平均池最近已经失宠。应该注意的是，实践中常见的最大池只有两种变化形式:步幅= 2的3 x 3(也称为<strong class="calibre13">重叠池</strong>)，更常见的是步幅= 2的2 x 2池。具有较大感受野的池大小具有太大的破坏性。</p>
<p class="calibre2">下图说明了最大池化过程:</p>
<div><img class="alignnone44" src="img/695dba31-3bc5-4444-b84c-37174f3c4d26.png"/></div>
<p>最大池化</p>


            

            
        
    





  
    <title>Fully connected layer</title>
    <meta content="urn:uuid:3fd593c7-d9c4-4d18-84e8-0b6efc257c5b" name="Adept.expected.resource"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  


  
        

                            
                    <h1 class="header-title">全连接层</h1>
                
            
            
                
<p class="calibre2">全连接层中的神经元与前一层中的所有激活具有全连接，这与CONV层不同。在CONV层中，神经元仅连接到输入中的局部区域，并且CONV体积中的许多神经元共享参数。全连接层通常用在最后两层，用softmax函数代替其他激活函数来输出概率。</p>


            

            
        
    





  
    <title>Overall</title>
    <meta content="urn:uuid:3fd593c7-d9c4-4d18-84e8-0b6efc257c5b" name="Adept.expected.resource"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  


  
        

                            
                    <h1 class="header-title">全部的</h1>
                
            
            
                
<p class="calibre2">一轮训练包括向前传递和向后传递:</p>
<ul class="calibre7">
<li class="calibre22">对于每个输入图像，我们首先让它通过卷积层。卷积后的结果被送入激活函数(即CONV + ReLU)。</li>
<li class="calibre22">得到的激活图再通过最大池化函数即POOL进行聚合。池化将导致更小的补丁大小，并有助于减少特征的数量。</li>
<li class="calibre22">CONV (+ReLU)和池层在连接到完全连接的层之前会重复几次。这增加了网络的深度，从而增加了其模拟复杂数据的能力。此外，不同级别的过滤器在不同级别学习数据的分层表示。请参考<a href="493c55cb-3fea-436a-a44b-6e7231fe3dba.xhtml" target="_blank" class="calibre11">第一章</a>，<em class="calibre26">为什么要深度学习？</em>了解更多关于深度网络表征学习的细节。</li>
<li class="calibre22">输出层通常是完全连接的，但是使用softmax函数来帮助计算类似概率的输出。</li>
<li class="calibre22">然后将输出与地面实况进行比较，以生成误差值，然后使用这些误差值来计算成本。通常，损失函数被定义为均方误差，并用于优化阶段。</li>
<li class="calibre22">然后误差被反向传播以更新系数和偏差值。</li>
</ul>
<p class="calibre2">要深入了解计算机视觉的CovNet应用，请参考<a href="376d1d26-c4be-445b-8304-ab2b6b64f134.xhtml" target="_blank" class="calibre11">第四章</a>、<em class="calibre20">计算机视觉中的深度学习</em>了解详情。</p>


            

            
        
    





  
    <title>Restricted Boltzmann Machines</title>
    <meta content="urn:uuid:3fd593c7-d9c4-4d18-84e8-0b6efc257c5b" name="Adept.expected.resource"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  


  
        

                            
                    <h1 class="header-title">受限玻尔兹曼机器</h1>
                
            
            
                
<p class="calibre2">RBM是只有两层的神经网络:可见层和隐藏层。每个可见节点/神经元都连接到每个隐藏节点。限制意味着没有层内通信，也就是说，在可见-可见节点或隐藏-隐藏节点之间没有连接。它是人工智能领域中最早引入的模型之一，并已成功应用于许多领域，如降维、分类、特征学习和异常检测。</p>
<p class="calibre2">下图显示了它的基本结构:</p>
<div><img class="alignnone45" src="img/a9d33579-2caa-4225-992f-61e024446c80.png"/></div>
<p>RBM的基本结构，只有一个可见层和一个隐藏层</p>
<p class="calibre2">用数学形式表达RBM相对容易，因为只有几个参数:</p>
<ul class="calibre7">
<li class="calibre22">权重矩阵<img class="fm-editor-equation34" src="img/0635bcc7-098b-4763-a855-11161717af9c.png"/> ( <img class="fm-editor-equation35" src="img/7492fb34-6d25-4216-8943-59932d398439.png"/>)描述了可见节点和隐藏节点之间的连接强度。每一项<img class="fm-editor-equation36" src="img/8d1dc6da-467c-4c4b-9188-b9b13e053d26.png"/>都是可见节点<em class="calibre26"> i </em>和隐藏节点<em class="calibre26"> j </em>之间连接的权重。</li>
<li class="calibre22">可见层和隐藏层的两个偏置向量分别为<img class="fm-editor-equation37" src="img/11ae970e-3704-4830-8f9b-bc90e49face3.png"/> ( <img class="fm-editor-equation38" src="img/510b7fe3-284d-4fee-9089-b22cab33a9bd.png"/>)，元素<img class="fm-editor-equation39" src="img/d6678904-a584-4913-b67b-a6ac7fd98940.png"/>对应于第<em class="calibre26"> i </em>个可见节点的偏置值。类似地，向量<em class="calibre26"> b </em>对应于隐藏层的偏移值，每个元素<em class="calibre26">b<sub class="calibre34">j</sub>T17】对应于第<em class="calibre26"> j </em>个节点。</em></li>
</ul>
<p class="calibre2">与常见的神经网络相比，有一些显著的差异:</p>
<ul class="calibre7">
<li class="calibre22">RBM是一个生成的随机神经网络。参数被调整以学习该组输入的概率分布。</li>
<li class="calibre22">RBM是一个基于能源的模式。能量函数产生一个标量值，该值基本上对应于指示模型处于该配置的概率的配置。</li>
<li class="calibre22">它以二进制模式编码输出，而不是概率。</li>
<li class="calibre22">神经网络通常通过梯度下降来执行权重更新，但是RBM使用<strong class="calibre1">对比散度</strong> ( <strong class="calibre1"> CD </strong>)。我们将在下面的章节中更详细地讨论CD。</li>
</ul>


            

            
        
    





  
    <title>Energy function</title>
    <meta content="urn:uuid:3fd593c7-d9c4-4d18-84e8-0b6efc257c5b" name="Adept.expected.resource"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  


  
        

                            
                    <h1 class="header-title">能量函数</h1>
                
            
            
                
<p class="calibre2">RBM是一个基于能源的模式。能量函数产生一个标量值，指示模型处于该配置的概率。</p>
<p class="calibre2">根据Geoffrey Hinton的教程(<em class="calibre20"> Geoffrey Hinton，训练受限玻尔兹曼机器的实用指南，2010 </em>)，能量函数写如下:</p>
<div><img class="fm-editor-equation40" src="img/44a777eb-e723-4466-9a38-30a7980c5747.png"/></div>
<p class="calibre2">计算很简单。基本上，你在偏差和相应的单位(可见的或隐藏的)之间做点积来计算它们对能量函数的贡献。第三项是可见节点和隐藏节点之间连接的能量表示。</p>
<p class="calibre2">在模型学习阶段，该能量被最小化，也就是说，模型参数(<img class="fm-editor-equation41" src="img/2826683b-6495-424a-aa90-47daf0662bc6.png"/>和<img class="fm-editor-equation42" src="img/88d2812f-8d17-4a4a-a6f2-0603e58ab689.png"/>、<img class="fm-editor-equation43" src="img/a3c5a309-f6a0-475d-8bc8-34718431ce62.png"/>)朝着较低能量配置的方向被更新。</p>


            

            
        
    





  
    <title>Encoding and decoding</title>
    <meta content="urn:uuid:3fd593c7-d9c4-4d18-84e8-0b6efc257c5b" name="Adept.expected.resource"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  


  
        

                            
                    <h1 class="header-title">编码和解码</h1>
                
            
            
                
<p class="calibre2">RBM的训练可以被认为是两个过程，前向编码路径(构造)和后向解码(重建)。在无监督的设置中，我们喜欢训练网络来模拟输入数据的分布，向前和向后传递如下进行。</p>
<p class="calibre2">在前向传递中，数据的原始输入值(例如，图像的像素值)由可见节点表示。然后，它们与权重<img class="fm-editor-equation44" src="img/cde95b7e-3e89-443f-8943-b5d670e93736.png"/>相乘，并与隐藏的偏置值相加(注意，在正向传递中不使用可见的偏置值)。结果值通过激活函数传递，以获得最终输出。如果连接了以下层，此激活结果将用作输入以向前移动:</p>
<div><img class="alignnone46" src="img/03cafa31-24a0-4d8e-b830-8b9912375f85.png"/></div>
<p>RBM转发传球示例</p>
<p class="calibre2">在我们简单的RBM案例中，只有一个隐藏层和一个可见层，隐藏层的激活值成为反向传递的输入。它们乘以权重矩阵，通过权重的边，并向后填充到可见节点。在每个可见节点处，所有传入值相加，并添加到可见偏移值中(注意，反向传递中不使用隐藏偏移值):</p>
<div><img class="alignnone47" src="img/dd20282f-d81b-4340-bb7e-705e83365d84.png"/></div>
<p>RBM向后传球的例子</p>
<p class="calibre2">因为RBM的权重在开始时是随机化的，所以在最初几轮中，由重建值相对于实际数据值计算的重建误差可能很大。因此，通常需要几次迭代来最小化这种误差，直到达到误差最小值。向前和向后传递帮助模型学习数据输入<img class="fm-editor-equation45" src="img/0e03a12f-3a9d-4843-a441-5784d0357632.png"/>和激活结果(作为隐藏层的输出)<img class="fm-editor-equation46" src="img/0403f635-4de0-4549-b89b-486a93fb43a5.png"/>的联合概率分布。这就是为什么RBM被认为是一个生成学习算法。</p>
<p class="calibre2">现在的问题是如何更新网络参数。</p>
<p class="calibre2">首先，使用KL散度计算误差。要了解更多关于KL-divergence的信息，读者可以参考大卫·麦凯(David MacKay)(<a href="http://www.inference.org.uk/itprnn/book.pdf" class="calibre11">http://www.inference.org.uk/itprnn/book.pdf</a>，链接最后一次查看是在2018年1月)所著的《信息理论、推理和学习算法<em class="calibre20">》一书的<em class="calibre20">第34页</em>。基本上，它通过对两个分布的差进行积分来计算两个分布的差。最小化KL-散度意味着将学习的模型分布(以来自隐藏层的输出的激活值的形式)推向输入数据分布。在很多深度学习算法中，都会用到梯度下降，比如随机梯度下降。然而，RBM正在使用一种近似最大似然学习的方法，称为<strong class="calibre13">对比差异</strong>。</em></p>


            

            
        
    





  
    <title>Contrastive divergence (CD-k)</title>
    <meta content="urn:uuid:3fd593c7-d9c4-4d18-84e8-0b6efc257c5b" name="Adept.expected.resource"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  


  
        

                            
                    <h1 class="header-title">对比差异</h1>
                
            
            
                
<p class="calibre2">对比分歧可以被认为是一种<em class="calibre20">近似最大似然</em>学习算法。它计算正相位(第一次编码的能量)和负相位(最后一次编码的能量)之间的偏差/差异。这相当于最小化模型分布和(经验)数据分布之间的KL-散度。变量<em class="calibre20"> k </em>是你运行对比背离的次数。在实践中，<em class="calibre20"> k = 1 </em>似乎出奇地有效。</p>
<p class="calibre2">基本上，使用两个部分之间的差异来近似<em class="calibre20"><strong class="calibre13"/></em>梯度:正相位关联梯度和负相位关联梯度。正项和负项并不反映其项的符号，而是反映其对所学习的模型概率分布的影响。正相关梯度增加了训练数据的概率(通过减少相应的自由能)，而第二项减少了模型生成样本的概率。TensorFlow中的伪代码片段可以编写如下:</p>
<pre class="calibre21"># Define Gibbs-Sampling function<br class="title-page-name"/>def sample_prob(probs): <br class="title-page-name"/>    return tf.nn.relu(tf.sign(probs - tf.random_uniform(tf.shape(probs))))<br class="title-page-name"/><br class="title-page-name"/>hidden_probs_0 = sample_prob(tf.nn.sigmoid(tf.matmul(X, W) + hidden_bias))<br class="title-page-name"/>visible_probs = sample_prob(tf.nn.sigmoid( tf.matmul(hidden_0, tf.transpose(W)) + visible_bias))<br class="title-page-name"/>hidden_probs_1 = tf.nn.sigmoid(tf.matmul(visible_probs, W) + hidden_bias)<br class="title-page-name"/># positive associated gradients increases the probability of training data<br class="title-page-name"/>w_positive_grad = tf.matmul(tf.transpose(X), hidden_probs_0)<br class="title-page-name"/># decreases the probability of samples generated by the model. <br class="title-page-name"/>w_negative_grad = tf.matmul(tf.transpose(visible_probs), hidden_probs_1)<br class="title-page-name"/><br class="title-page-name"/>W = W + alpha * (w_positive_grad - w_negative_grad)<br class="title-page-name"/>vb = vb + alpha * tf.reduce_mean(X - visible_probs, 0)<br class="title-page-name"/>hb = hb + alpha * tf.reduce_mean(hidden_probs_0 - hidden_probs_1, 0)
X is the input data. For example, MNIST images have 784 pixels so the input <kbd class="calibre12">X</kbd> is a vector of 784 entries and accordingly, the visible layer has 784 nodes. Also note that in RBM the input data is encoded binary. For MNIST data, one can use one-hot encoding to transfer the input pixel value. In addition, <kbd class="calibre12">alpha</kbd> is the learning rate, <kbd class="calibre12">vb</kbd> is the bias of the visible layer, <kbd class="calibre12">hb</kbd> is the bias of the hidden layer, and <kbd class="calibre12">W</kbd> is the weight matrix. The sampling function <kbd class="calibre12">sample_prob</kbd> is the Gibbs-Sampling function and it decides which node to turn on.</pre>


            

            
        
    





  
    <title>Stacked/continuous RBM</title>
    <meta content="urn:uuid:3fd593c7-d9c4-4d18-84e8-0b6efc257c5b" name="Adept.expected.resource"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  


  
        

                            
                    <h1 class="header-title">堆叠/连续RBM</h1>
                
            
            
                
<p class="calibre2">深度信念网络(DBN)只是几个相互叠加的RBM。前一个RBM的输出成为后一个RBM的输入。2006年，Hinton在他的论文中提出了一种快速的贪婪算法:<em class="calibre20">深度信念网络的快速学习算法</em>，它可以一次一层地学习深度的有向信念网络。DBN学习输入的层次表示，目的是重建数据，因此它非常有用，特别是在无人监督的情况下。</p>
<p class="calibre2">对于连续输入，可以参考另一个称为连续受限玻尔兹曼机器的模型，它利用了不同类型的对比发散采样。这种模型可以处理在零和一之间标准化的图像像素或单词向量。</p>


            

            
        
    





  
    <title>RBM versus Boltzmann Machines</title>
    <meta content="urn:uuid:3fd593c7-d9c4-4d18-84e8-0b6efc257c5b" name="Adept.expected.resource"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  


  
        

                            
                    <h1 class="header-title">RBM对玻尔兹曼机器</h1>
                
            
            
                
<p class="calibre2"><strong class="calibre13">玻尔兹曼机器</strong> ( <strong class="calibre13"> BMs </strong>)可以被认为是对数线性马尔可夫随机场的一种特殊形式，对于它，能量函数在其自由参数中是线性的。为了增加它们对复杂分布的表示能力，人们可以考虑并增加从未观察到的变量的数量，即隐藏变量，或者在这种情况下，隐藏神经元。RBM是建立在BMs之上的，在BMs中，应用了限制来强制没有可见-可见和隐藏-隐藏连接。</p>


            

            
        
    





  
    <title>Recurrent neural networks (RNN/LSTM)</title>
    <meta content="urn:uuid:3fd593c7-d9c4-4d18-84e8-0b6efc257c5b" name="Adept.expected.resource"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  


  
        

                            
                    <h1 class="header-title">递归神经网络(RNN/LSTM)</h1>
                
            
            
                
<p class="calibre2">在卷积神经网络或典型的前馈网络中，信息流通过在节点上执行的一系列数学运算，而没有反馈回路或对信号顺序的任何考虑。因此，它们不能处理按顺序出现的输入。</p>
<p class="calibre2">然而，在实践中，我们有许多顺序数据，如句子和时间序列数据，其中包括文本、基因组、手写、口语或来自传感器、股票市场和政府机构的数字时间序列数据。这里重要的不仅仅是顺序。行中的下一个值通常很大程度上取决于过去的上下文(长或短)。例如，为了预测句子中的下一个单词，需要大量的信息，不仅仅是来自附近的单词，有时还包括句子中的前几个单词。这有助于设置主题和内容。</p>
<p class="calibre2"><strong class="calibre13">递归神经网络</strong> ( <strong class="calibre13"> RNN </strong>)是一种新型的人工神经网络，专门为这些类型的数据而设计。它考虑了序列顺序(该序列可以是任意长度)和其架构内的内部循环，这意味着网络的任何配置/状态都受到影响，不仅受当前输入的影响，还受其最近的过去的影响。</p>


            

            
        
    





  
    <title>Cells in RNN and unrolling</title>
    <meta content="urn:uuid:3fd593c7-d9c4-4d18-84e8-0b6efc257c5b" name="Adept.expected.resource"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  


  
        

                            
                    <h1 class="header-title">细胞在RNN和展开</h1>
                
            
            
                
<p class="calibre2">所有的递归神经网络都可以被认为是在时间维度上重复模型/细胞的链。该重复模块/电池可以简单地是单个tanh层。理解这一点的一种方法是将架构展开或解开到每个时间步骤中，并将每个时间步骤视为一层。我们可以看到，RNN的深度基本上是由时间步长或序列长度决定的。序列的第一个元素，如句子的单词，相当于第一层。</p>
<p class="calibre2">下图显示了时间轴中单个循环单元格的展开:</p>
<div><img class="alignnone48" src="img/40560af7-09f7-4237-ba4f-6ce62c0e9a75.png"/></div>
<p>RNN的展开。标准RNN中的重复模块实际上包含一个层</p>


            

            
        
    





  
    <title>Backpropagation through time</title>
    <meta content="urn:uuid:3fd593c7-d9c4-4d18-84e8-0b6efc257c5b" name="Adept.expected.resource"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  


  
        

                            
                    <h1 class="header-title">穿越时间的反向传播</h1>
                
            
            
                
<p class="calibre2">在前馈网络中，<strong class="calibre13">反向传播</strong> ( <strong class="calibre13"> BP </strong>)从计算输出层的最终误差开始，然后一层一层地向输入层返回。在每一步，它都会计算误差对权重的偏导数<img class="fm-editor-equation47" src="img/a34e445b-b02d-4dbc-a8ad-6a675d245985.png"/>。然后通过优化方法(例如梯度下降)，这些导数用于在减小误差的方向上向上或向下调整权重。</p>
<p class="calibre2">类似地，在递归网络中，在网络随着时间展开之后，BP可以被认为是时间维度上的扩展，这被称为通过时间的<strong class="calibre13">反向传播</strong>或<strong class="calibre13"> BPTT </strong>。计算非常相似，只是一系列层被时间轴中一系列相似的单元所取代。</p>


            

            
        
    





  
    <title>Vanishing gradient and LTSM</title>
    <meta content="urn:uuid:3fd593c7-d9c4-4d18-84e8-0b6efc257c5b" name="Adept.expected.resource"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  


  
        

                            
                    <h1 class="header-title">消失梯度和LTSM</h1>
                
            
            
                
<p class="calibre2">与所有深层架构类似，网络越深，消失梯度问题就越严重。现在的情况是，网络开始时的权重变化越来越小。鉴于网络的权重是随机生成的，权重不变，我们从数据中学到的东西很少。这个所谓的<em class="calibre20">消失梯度</em>问题也影响着RNN。</p>
<p class="calibre2">RNN中的每一个时间步长都可以看作是一个层。然后，在反向传播过程中，误差从一个时间步到前一个时间步。所以这个网络可以被认为和时间步数一样深。在许多实际问题中，如单词句子、段落或其他时间序列数据，输入RNN的序列可能很长。RNN擅长序列相关问题的原因是，他们擅长从以前的输入中保留重要信息，并使用这个<em class="calibre20">过去的</em>上下文信息来修改当前的输出。如果序列很长，并且在训练/BPTT期间计算的梯度或者消失(由于0 &lt;值&lt; 1的多次相乘，假设展开的RNN很深)或者爆炸，网络将学习得非常慢。</p>
<p class="calibre2">例如，如果我们从句子中学习预测接下来的单词，句子中的第一个单词可能对主体非常重要，对建立整个句子的上下文非常重要，甚至对预测句子中的最后一个单词也很重要。对于没有通过时间线正确学习的权重，我们可能已经丢失了这样的信息。</p>
<p class="calibre2">在90年代中期，德国研究人员Sepp Hochreiter和Juergen Schmidhuber提出了一种具有所谓长短期记忆单元(LSTMs)的递归网络变体，作为消失梯度问题的解决方案。</p>
<p class="calibre2">LSTM通过引入更多的门来控制对细胞状态的访问，解决了长序列训练和保持记忆的问题。新的单元结构有助于保持更恒定的误差，从而允许递归网络在许多时间步长(有时可能超过1000)上继续学习。</p>
<p class="calibre2">除了合并先前的输出(在时间线中)和当前的输入以生成输出之外，LTSM不同于典型的RNN，它保留隐藏的状态信息，合并它以生成输出，并更新新的单元状态。这意味着，RNN的当前产出由两项决定:当前投入和以前的产出。LTSM的当前输出由三项决定:当前输入、先前输出和先前状态。RNN单元格将只输出隐藏值，而LTSM单元格将输出带有新单元格状态的隐藏值。</p>


            

            
        
    





  
    <title>Cells and gates in LTSM</title>
    <meta content="urn:uuid:3fd593c7-d9c4-4d18-84e8-0b6efc257c5b" name="Adept.expected.resource"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  


  
        

                            
                    <h1 class="header-title">LTSM的牢房和大门</h1>
                
            
            
                
<p class="calibre2">LSTM网络由许多相连的LSTM细胞组成，每个细胞都可以被认为是由三个重要的门组成。这些门决定了过去/现在的信息是否能通过。</p>
<p class="calibre2">下图显示了一个标准的LSTM存储单元。在这个单元格中，向量乘法和向量加法由一个蓝色圆形表示，圆形内有一个符号。将一个向量与另一个在[0，1]范围内的向量相乘称为<strong class="calibre13">选通</strong>。在[0，1]中生成向量可以被认为是一个<em class="calibre20">滤波</em>过程。<img class="fm-editor-equation48" src="img/68f7b37d-b7d4-4005-a8e7-670a61442fe3.png"/>是前一个(在时间上)单元格状态。<img class="fm-editor-equation49" src="img/4908aee0-69a6-415d-aef8-759dce13bd5e.png"/>是由当前输入更新的单元状态。<img class="fm-editor-equation50" src="img/c996ee83-5bd3-4888-b393-7d04c7465884.png"/>是当前预测/输出，<img class="fm-editor-equation51" src="img/71f86080-e01f-4ce3-88df-96c85ff8fcc5.png"/>是先前预测/输出(如果网络中仅使用一个LSTM单元，<img class="fm-editor-equation50" src="img/c996ee83-5bd3-4888-b393-7d04c7465884.png"/>实质上是预测输出，如果架构中堆叠了一个以上的LSTM单元，<img class="fm-editor-equation50" src="img/b89383db-e806-4a8f-b304-bb4a2bfb9f2a.png"/>被认为是当前LSTM单元的隐藏输出:</p>
<div><img class="alignnone49" src="img/b8da6971-25ff-4abd-9684-c7997c1d3987.png"/></div>
<p>LSTM体系结构中包含三个门的重复模块/单元</p>
<p class="calibre2">在每个单元内，对传入的信息执行三个步骤:单元的当前输入<img class="fm-editor-equation52" src="img/e4ad1b5f-1745-4885-9533-57ad97468e1a.png"/>，单元的先前输出<img class="fm-editor-equation53" src="img/646b4703-9536-4604-b7cc-d1f143245663.png"/>，以及来自上次的单元状态<img class="fm-editor-equation18" src="img/539c831b-264d-4dc4-9ef8-32e5060ab5a1.png"/>。在图中，<img class="fm-editor-equation52" src="img/20c8f58b-c4f6-483b-b4f9-f7523f3e1949.png"/>和<img class="fm-editor-equation51" src="img/ab9da8b6-1886-4304-9516-8ddaeb8c00f5.png"/>通过串联组合在一起。</p>


            

            
        
    





  
    <title>Step 1 – The forget gate</title>
    <meta content="urn:uuid:3fd593c7-d9c4-4d18-84e8-0b6efc257c5b" name="Adept.expected.resource"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  


  
        

                            
                    <h1 class="header-title">第一步——遗忘之门</h1>
                
            
            
                
<p class="calibre2"><strong class="calibre13">遗忘门</strong>决定我们想要从过去的细胞状态<img class="fm-editor-equation54" src="img/5d3ac0ff-1709-4e57-aec0-984d77ff83a7.png"/>中保留或丢弃哪个记忆。这是通过使<img class="fm-editor-equation55" src="img/9565f97f-d939-4e61-8bed-1f844b0cefdc.png"/>通过激活/挤压函数(sigmoid)来获得指示向量，然后将该向量(门控)乘以先前的单元状态向量<img class="fm-editor-equation54" src="img/23276fe9-7a0f-4719-9ce4-9bdc435baa80.png"/>来实现的。</p>
<p class="calibre2">结果<img class="fm-editor-equation56" src="img/252819aa-09d8-489a-811c-b97238c5aad6.png"/>表示从先前状态记忆的保留信息，我们认为该信息将对当前值有用。</p>


            

            
        
    





  
    <title>Step 2 – Updating memory/cell state</title>
    <meta content="urn:uuid:3fd593c7-d9c4-4d18-84e8-0b6efc257c5b" name="Adept.expected.resource"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  


  
        

                            
                    <h1 class="header-title">步骤2–更新存储器/单元状态</h1>
                
            
            
                
<p class="calibre2">下一步是将单元状态从<img class="fm-editor-equation54" src="img/4dd26260-6726-4b68-9df2-9835caf564fa.png"/>更新到<img class="fm-editor-equation49" src="img/21d41b5c-cb2a-4d05-a9e9-936006c36b8a.png"/>。从步骤1中选择的隐藏存储器与当前输入的过滤版本相结合。所谓的<strong class="calibre13">输入门</strong>再次进行过滤，输入门是一个sigmoid层，决定我们想要更新哪些值。这个滤波决定与tanh的激活结果相乘，然后添加到来自遗忘门的所选记忆向量。结果用于更新单元状态<img class="fm-editor-equation49" src="img/176a9905-15d9-480a-8925-06ec59658a20.png"/>。</p>


            

            
        
    





  
    <title>Step 3 – The output gate</title>
    <meta content="urn:uuid:3fd593c7-d9c4-4d18-84e8-0b6efc257c5b" name="Adept.expected.resource"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  


  
        

                            
                    <h1 class="header-title">步骤3–输出门</h1>
                
            
            
                
<p class="calibre2"><strong class="calibre13">输出门</strong>决定我们要输出什么，也就是选择性地决定我们要输出当前单元格状态的哪一部分作为新的隐藏状态/输出/预测。同样，sigmoid节点用于从<img class="fm-editor-equation57" src="img/a1836400-ffbb-459e-ad83-73468b009bbb.png"/>生成过滤向量(决定当前单元格状态的哪些部分将被选择)。然后，当前单元状态<img class="fm-editor-equation58" src="img/a1c55ec2-4761-454a-a11a-5bd5f58b068a.png"/>通过一个双曲正切函数(为了将值压缩到-1和1之间),并乘以sigmoid门的输出。然后我们得到我们的最终输出<img class="fm-editor-equation43" src="img/4ef5bbfd-61b4-450a-b3de-d04eed00e77f.png"/>。</p>


            

            
        
    





  
    <title>Practical examples</title>
    <meta content="urn:uuid:3fd593c7-d9c4-4d18-84e8-0b6efc257c5b" name="Adept.expected.resource"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  


  
        

                            
                    <h1 class="header-title">实际例子</h1>
                
            
            
                
<p class="calibre2">在本节中，我们提供了一个可以使用神经网络解决的实际问题。我们将介绍问题，并使用TensorFlow建立我们的神经网络模型来解决问题。</p>


            

            
        
    





  
    <title>TensorFlow setup and key concepts</title>
    <meta content="urn:uuid:3fd593c7-d9c4-4d18-84e8-0b6efc257c5b" name="Adept.expected.resource"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  


  
        

                            
                    <h1 class="header-title">TensorFlow设置和关键概念</h1>
                
            
            
                
<p class="calibre2">我们建议读者按照https://www.tensorflow.org/install/<a href="https://www.tensorflow.org/install/" class="calibre11">的说明安装TensorFlow。我们将使用Python作为我们的编程语言。代码示例中主要使用了三个关键概念:</a></p>
<ul class="calibre7">
<li class="calibre8"><strong class="calibre1">张量</strong>:张量是TensorFlow中的中心数据单元。我们可以认为它是一个任意维数的矩阵。张量中的条目是原始值。例如，请参见以下内容:</li>
</ul>
<pre class="prettyprint1">5 is a scalar and a rank 0 tensor<br class="title-page-name"/>[[0, 1, 2], [3, 4, 5]] is a matrix with shape [2, 3] and a rank 2 tensor</pre>
<ul class="calibre7">
<li class="calibre8"><strong class="calibre1"> TensorFlow会话</strong>:TensorFlow会话封装了tensor flow运行时的控制和状态。</li>
<li class="calibre8"><strong class="calibre1">计算图</strong>:排列成节点计算图的一组张量流运算。图的边是张量。节点可以是张量或运算。在TensorFlow中，我们需要构建计算图并运行计算图。例如，下面的代码构建了一个由三个节点组成的图，其中node1和node2输出常数，node 3是一个加法运算，将node 1和node 2中的两个常数相加:</li>
</ul>
<pre class="prettyprint1">node1 = tf.constant(1.0, dtype=tf.float32)<br class="title-page-name"/>node2 = tf.constant(2.0, dtype=tf.float32)<br class="title-page-name"/>node3 = tf.add(node1, node2)]</pre>
<p class="calibre36">然后我们可以使用下面的代码来运行图形:</p>
<pre class="prettyprint1">sess = tf.Session()<br class="title-page-name"/>print("sess.run(node3):", sess.run(node3))</pre>
<p class="calibre36">上述代码的输出如下:</p>
<pre class="prettyprint1">sess.run(node3):3.0</pre>


            

            
        
    





  
    <title>Handwritten digits recognition</title>
    <meta content="urn:uuid:3fd593c7-d9c4-4d18-84e8-0b6efc257c5b" name="Adept.expected.resource"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  


  
        

                            
                    <h1 class="header-title">手写数字识别</h1>
                
            
            
                
<p class="calibre2">手写数字识别的挑战是从手写数字的图像中识别数字。这在很多情况下都很有用，例如识别信封上的邮政编码。在这个例子中，我们将使用MNIST数据集来开发和评估我们用于手写数字识别的神经网络模型。</p>
<p class="calibre2">http://yann.lecun.com/exdb/mnist/的MNIST是一个计算机视觉数据集。它由手写数字的灰度图像以及正确的数字标签组成。每个图像是28像素乘28像素。样本图像如下所示:</p>
<div><img class="alignnone50" src="img/7a637940-df0c-4df1-9005-f3826536daf8.jpg"/></div>
<p>来自MNIST数据集的样本图像</p>
<p class="calibre2">MNIST数据分为三部分:55，000幅训练数据图像、10，000幅测试数据图像和5，000幅验证数据图像。每张图片都附有标签，代表一个数字。目标是将图像分类成数字，换句话说，将每个图像与十个类别中的一个相关联。</p>
<p class="calibre2">我们可以使用介于0和1之间的浮点数的1 x 784向量来表示图像。数字784是28 x 28图像中的像素数。我们通过将2D图像展平成1D矢量来获得1×784矢量。我们可以将标签表示为一个1 x 10的二进制值向量，其中有且仅有一个元素为1，其余的为0。我们将使用TensorFlow建立一个深度学习模型，在给定1 x 784数据向量的情况下预测1 x 10标签向量。</p>
<p class="calibre2">我们首先导入数据集:</p>
<pre class="calibre21">from tensorflow.examples.tutorials.mnist import input_data<br class="title-page-name"/>mnist = input_data.read_data_sets('MNIST_data', one_hot=True)</pre>
<p class="calibre2">然后，我们定义CNN的一些基本构件:</p>
<ul class="calibre7">
<li class="calibre8">重量:</li>
</ul>
<pre class="prettyprint1">def weight_variable(shape):<br class="title-page-name"/>  # initialize weights with a small noise for symmetry breaking<br class="title-page-name"/>  initial = tf.truncated_normal(shape, stddev=0.1)<br class="title-page-name"/>  return tf.Variable(initial)</pre>
<ul class="calibre7">
<li class="calibre8">偏见:</li>
</ul>
<pre class="prettyprint1">def bias_variable(shape):<br class="title-page-name"/>  # Initialize the bias to be slightly positive to avoid dead<br class="title-page-name"/>  # neurons<br class="title-page-name"/>  initial = tf.constant(0.1, shape=shape)<br class="title-page-name"/>  return tf.Variable(initial)</pre>
<ul class="calibre7">
<li class="calibre8">卷积:</li>
</ul>
<pre class="prettyprint1">def conv2d(x, W):<br class="title-page-name"/>  # First dimension in x is batch size<br class="title-page-name"/>  return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1],<br class="title-page-name"/>                      padding='SAME')</pre>
<ul class="calibre7">
<li class="calibre8">最大池:</li>
</ul>
<pre class="prettyprint1">def max_pool_2x2(x):<br class="title-page-name"/>  return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],<br class="title-page-name"/>                        strides=[1, 2, 2, 1], padding='SAME')</pre>
<p class="calibre2">现在，我们通过使用基本构件的计算图来构建神经网络模型。我们的模型由两个卷积层组成，每个卷积层后有一个池，最后是一个完全连接的卷积层。该图表可以在下图中进行说明:</p>
<div><img class="alignnone51" src="img/0721194a-d736-49ba-9282-918c70f777dd.jpg"/></div>
<p>用于数字识别的卷积神经网络结构</p>
<p class="calibre2">以下代码实现了这种卷积神经网络架构:</p>
<pre class="calibre21">x = tf.placeholder(tf.float32, shape=[None, 784])<br class="title-page-name"/>y_ = tf.placeholder(tf.float32, shape=[None, 10]) # ground-truth label<br class="title-page-name"/># First convolution layer<br class="title-page-name"/>W_conv1 = weight_variable([5, 5, 1, 32])<br class="title-page-name"/>b_conv1 = bias_variable([32])<br class="title-page-name"/># first dimension of x_image is batch size<br class="title-page-name"/>x_image = tf.reshape(x, [-1, 28, 28, 1])<br class="title-page-name"/>h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)<br class="title-page-name"/>h_pool1 = max_pool_2x2(h_conv1)<br class="title-page-name"/># Second convolution layer<br class="title-page-name"/>W_conv2 = weight_variable([5, 5, 32, 64])<br class="title-page-name"/>b_conv2 = bias_variable([64])<br class="title-page-name"/>h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)<br class="title-page-name"/>h_pool2 = max_pool_2x2(h_conv2)<br class="title-page-name"/># Fully connected layer<br class="title-page-name"/>W_fc1 = weight_variable([7 * 7 * 64, 1024])<br class="title-page-name"/>b_fc1 = bias_variable([1024])<br class="title-page-name"/>h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])<br class="title-page-name"/>h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)</pre>
<p class="calibre2">我们还可以使用dropout来减少过度拟合:</p>
<pre class="calibre21">keep_prob = tf.placeholder(tf.float32)<br class="title-page-name"/>h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)</pre>
<p class="calibre2">我们现在构建最后一层，读出层:</p>
<pre class="calibre21">W_fc2 = weight_variable([1024, 10])<br class="title-page-name"/>b_fc2 = bias_variable([10])<br class="title-page-name"/># Readout layer<br class="title-page-name"/>y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2<br class="title-page-name"/>h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)</pre>
<p class="calibre2">现在我们定义成本函数和训练参数:</p>
<pre class="prettyprint">cross_entropy = tf.reduce_mean(<br class="title-page-name"/>    tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_conv))<br class="title-page-name"/>train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)</pre>
<p class="calibre2">接下来，我们定义评估:</p>
<pre class="calibre21">correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))<br class="title-page-name"/>accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))</pre>
<p class="calibre2">最后，我们最终可以在一个会话中运行图表:</p>
<pre class="calibre21">with tf.Session() as sess:<br class="title-page-name"/>   sess.run(tf.global_variables_initializer())<br class="title-page-name"/>   for i in range(2000):<br class="title-page-name"/>     batch = mnist.train.next_batch(50)<br class="title-page-name"/>     if i % 20 == 0:<br class="title-page-name"/>       train_accuracy = accuracy.eval(feed_dict={<br class="title-page-name"/>           x: batch[0], y_: batch[1], keep_prob: 1.0})<br class="title-page-name"/>       print('step %d, training accuracy %g' % (i, train_accuracy))<br class="title-page-name"/>     train_step.run(feed_dict={x: batch[0], y_: batch[1],<br class="title-page-name"/>                              keep_prob: 0.5})<br class="title-page-name"/>print('test accuracy %g' % accuracy.eval(<br class="title-page-name"/>      feed_dict={<br class="title-page-name"/>          x: mnist.test.images,<br class="title-page-name"/>          y_: mnist.test.labels,<br class="title-page-name"/>          keep_prob: 1.0}))</pre>
<p class="calibre2">最后，我们使用一个简单的CNN在这个MNIST数据集上取得了99.2%的准确率。</p>


            

            
        
    





  
    <title>Summary</title>
    <meta content="urn:uuid:3fd593c7-d9c4-4d18-84e8-0b6efc257c5b" name="Adept.expected.resource"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  


  
        

                            
                    <h1 class="header-title">摘要</h1>
                
            
            
                
<p class="calibre2">在这一章中，我们从基本的多层感知器网络开始。从那里，我们讨论了基本结构，例如输入/输出层以及各种类型的<em class="calibre20">激活功能</em>。我们也给出了网络如何学习的详细步骤，重点是反向传播和其他一些重要的组成部分。考虑到这些基本原理，我们介绍了三种流行的网络:CNN、受限玻尔兹曼机器和递归神经网络(及其变体，LSTM)。对于每种特定的网络类型，我们都详细解释了每种体系结构中的关键构造块。最后，我们给出了一个实际的例子来说明如何使用TensorFlow进行端到端的应用。在下一章，我们将讨论神经网络在计算机视觉中的应用，包括流行的网络架构、最佳实践和真实的工作实例。</p>
<p class="calibre2"/>
<p class="calibre2"/>
<p class="calibre2"/>
<p class="calibre2"/>
<p class="calibre2"/>
<p class="calibre2"/>
<p class="calibre2"/>
<p class="calibre2"/>


            

            
        
    


</body></html>