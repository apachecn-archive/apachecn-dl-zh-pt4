

# 十三、附录：问题的答案

# 第 1 章，从转换器的模型架构开始

1.  NLP transduction can encode and decode text representations. (True/False)

    **真**。NLP 是将序列(书面或口头)转换成数字表示、处理它们并将结果解码回文本的转换。

2.  **Natural Language Understanding** (**NLU**) is a subset of **Natural Language Processing** (**NLP**). (True/False)

    **真**。

3.  Language modeling algorithms generate probable sequences of words based on input sequences. (True/False)

    **真**。

4.  A transformer is a customized LSTM with a CNN layer. (True/False)

    **假**。《转换器》根本不包含 LSTM 或 CNN。

5.  A transformer does not contain an LSTM or CNN layers. (True/False)

    **真**。

6.  Attention examines all of the tokens in a sequence, not just the last one. (True/False)

    **真**。

7.  A transformer uses a positional vector, not positional encoding. (True/False)

    **假**。转换器使用位置编码。原始转换器模型没有额外的位置向量。位置编码会在处理后添加到输入中。

8.  A transformer contains a feedforward network. (True/False)

    **真**。

9.  The masked multi-headed attention component of the decoder of a transformer prevents the algorithm parsing a given position from seeing the rest of a sequence that is being processed. (True/False)

    **真**。

10.  Transformers can analyze long-distance dependencies better than LSTMs. (True/False)

    **真**。

# 第 2 章，微调 BERT 模型

1.  BERT stands for Bidirectional Encoder Representations from Transformers. (True/False)

    **真**。

2.  BERT is a two-step framework. *Step 1* is pretraining. *Step 2* is fine-tuning. (True/False)

    **真**。

3.  Fine-tuning a BERT model implies training parameters from scratch. (True/False)

    **假**。用预训练的训练参数初始化 BERT 微调。

4.  BERT only pretrains using all downstream tasks. (True/False)

    **假**。

5.  BERT pretrains with **Masked Language Modeling** (**MLM**). (True/False)

    **真**。

6.  BERT pretrains with **Next Sentence Predictions** (**NSP**). (True/False)

    **真**。

7.  BERT pretrains mathematical functions. (True/False)

    **假**。

8.  A question-answer task is a downstream task. (True/False)

    **真**。

9.  A BERT pretraining model does not require tokenization. (True/False)

    **假**。

10.  Fine-tuning a BERT model takes less time than pretraining. (True/False)

    **真**。

# 第三章，从零开始预训练 RoBERTa 模型

1.  RoBERTa uses a byte-level byte-pair encoding tokenizer. (True/False)

    **真**。

2.  A trained Hugging Face tokenizer produces `merges.txt` and `vocab.json`. (True/False)

    **真**。

3.  RoBERTa does not use token type IDs. (True/False)

    **真**。

4.  DistilBERT has 6 layers and 12 heads. (True/False)

    **真**。

5.  A transformer model with 80 million parameters is enormous. (True/False)

    **假**。8000 万参数是个小模型。

6.  We cannot train a tokenizer. (True/False)

    **假**。记号赋予者可以被训练。

7.  A BERT-like model has 6 decoder layers. (True/False)

    **假**。BERT 包含 6 个编码器层，而不是解码器层。

8.  Masked language modeling predicts a word contained in a mask token in a sentence. (True/False)

    **真**。

9.  A BERT-like model has no self-attention sub-layers. (True/False)

    **假**。伯特有自我关注层。

10.  Data collators are helpful for backpropagation. (True/False)

    **假**。数据排序规则是 dataset 类的一部分。

# 第 4 章，转换器的下游 NLP 任务

1.  Machine intelligence uses the same data as humans to make predictions. (True/False)

    **假**。对 NLU 来说，人类可以通过感官获得更多的信息。机器智能依赖于人类为所有类型的媒体提供的内容。

2.  SuperGLUE is more difficult than GLUE for NLP models. (True/False)

    **真**。

3.  BoolQ expects a binary answer. (True/False)

    **真**。

4.  WiC stands for Words in Context. (True/False)

    **真**。

5.  **Recognizing Textual Entailment** (**RTE**) detects if one sequence entails another sequence. (True/False)

    **真**。

6.  A Winograd Schema predicts if a verb is spelled correctly. (True/False)

    **假**。Winograd 图式主要应用于代词消歧。

7.  Transformer models now occupy the top ranks of GLUE and SuperGLUE. (True/False)

    **真**。

8.  Human Baseline Standards are not defined once and for all. They were made tougher to attain by SuperGLUE. (True/False)

    **真**。

9.  Transformer models will never beat SuperGLUE Human Baseline standards. (True/False)

    **假**。转换器模型在胶水方面胜过人类基线，在强力胶方面也是如此。随着我们不断提高超级胶水基准的水平，模型将继续进步，并超过人类基线标准。

10.  Variants of transformer models have outperformed RNN and CNN models. (True/False)

    **真**。但是你永远不知道 AI 未来会发生什么！

# 第 5 章，使用转换器进行机器翻译

1.  Machine translation has now exceeded human baselines. (True/False)

    **假**。机器翻译是最困难的 NLP ML 任务之一。

2.  Machine translation requires large datasets. (True/False)

    **真**。

3.  There is no need to compare transformer models using the same datasets. (True/False)

    **假**。比较不同模型的唯一方法是使用相同的数据集。

4.  BLEU is the French word for *blue* and is the acronym of an NLP metric. (True/False)

    **真**。BLEU 代表双语评价替角评分，便于记忆。

5.  Smoothing techniques enhance BERT. (True/False)

    **真**。

6.  German-English is the same as English-German for machine translation. (True/False)

    **假**。表示德语然后翻译成另一种语言和表示英语然后翻译成另一种语言不是一个过程。语言结构并不相同。

7.  The original Transformer multi-head attention sub-layer has 2 heads. (True/False)

    **假**。每个注意力子层有 8 个头。

8.  The original Transformer encoder has 6 layers. (True/False)

    **真**。

9.  The original Transformer encoder has 6 layers but only 2 decoder layers. (True/False)

    **假**。有 6 个解码器层。

10.  You can train transformers without decoders. (True/False)

    **真**。BERT 的架构只包含编码器。

# 第 6 章，用 OpenAI GPT-2 和 GPT-3 模型生成文本

1.  A zero-shot method trains the parameters once. (True/False)

    **假**。不，模型的参数首先通过尽可能多的情节进行训练。零触发意味着下游任务的执行无需额外的微调。

2.  Gradient updates are performed when running zero-shot models. (True/False)

    **假**。

3.  GPT models only have a decoder stack. (True/False)

    **真**。

4.  It is impossible to train a 117M GPT model on a local machine. (True/False)

    **假**。我们在本章中训练了一个。

5.  It is impossible to train the GPT-2 model with a specific dataset. (True/False)

    **假**。我们在本章中训练了一个。

6.  A GPT-2 model cannot be conditioned to generate text. (True/False)

    **假**。我们在本章中实现了这一点。

7.  A GPT-2 model can analyze the context of input and produce completion content. (True/False)

    **真**。

8.  We cannot interact with a 345M GPT parameter model on a machine with less than 8 GPUs. (True/False).

    **假**。在这一章中，我们与这样大小的模型进行了交互。

9.  Supercomputers with 285,000 CPUs do not exist. (True/False)

    **假**。

10.  Supercomputers with thousands of GPUs are game changers in AI. (True/False)

    **真**。我们将能够用越来越多的参数和连接建立模型。

# 第 7 章，将转换器应用于法律和金融文档，用于人工智能文本摘要

1.  T5 models only have encoder stacks like BERT models. (True/False)

    **假**。

2.  T5 models have both encoder and decoder stacks. (True/False)

    **真**。

3.  T5 models use relative positional encoding, not absolute positional encoding. (True/False)

    **真**。

4.  Text-to-text models are only designed for summarization. (True/False)

    **Fa** lse。

5.  Text-to-text models apply a prefix to the input sequence that determines the NLP task. (True/False)

    **真**。

6.  T5 models require specific hyperparameters for each task. (True/False)

    **假**。

7.  One of the advantages of text-to-text models is that they use the same hyperparameters for all NLP tasks. (True/False)

    **真**。

8.  T5 transformers do not contain a feedforward network. (True/False)

    **假**。

9.  NLP text summarization works for any text. (True/False)

    **假**。

10.  Hugging Face is a framework that makes transformers easier to implement. (True/False)

    **真**。

# 第 8 章，匹配记号赋予器和数据集

1.  A tokenized dictionary contains every word that exists in a language. (True/False)

    **假**。

2.  Pretrained tokenizers can encode any dataset. (True/False)

    **假**。

3.  It is good practice to check a database before using it. (True/False)

    **真**。

4.  It is good practice to eliminate obscene data from datasets. (True/False)

    **真**。

5.  It is a good practice to delete data containing discriminating assertions. (True/False)

    **真**。

6.  Raw datasets might sometimes produce relationships between noisy content and useful content. (True/False)

    **真**。

7.  A standard pretrained tokenizer contains the English vocabulary of the past 700 years. (True/False)

    **假**。

8.  Old English can create problems when encoding data with a tokenizer trained in modern English. (True/False)

    **真**。

9.  Medical and other types of jargon can create problems when encoding data with a tokenizer trained in modern English. (True/False)

    **真**。

10.  Controlling the output of the encoded data produced by a pretrained tokenizer is good practice. (True/False)

    **真**。

# 第 9 章，基于 BERT 变换器的语义角色标注

1.  **Semantic Role Labeling** (**SRL**) is a text generation task. (True/False)

    **假**。

2.  A predicate is a noun. (True/False)

    **假**。

3.  A verb is a predicate. (True/False)

    **真**。

4.  Arguments can describe who and what is doing something. (True/False)

    **真**。

5.  A modifier can be an adverb. (True/False)

    **真**。

6.  A modifier can be a location. (True/False)

    **真**。

7.  A BERT-based model contains encoder and decoder stacks. (True/False)

    **假**。

8.  A BERT-based SRL model has standard input formats. (True/False)

    **真**。

9.  Transformers can solve any SRL task. (True/False)

    **假**。

# 第十章，让你的数据说话:故事、问题和答案

1.  A trained transformer model can answer any question. (True/False)

    **假**。

2.  Question-answering requires no further research. It is perfect as it is. (True/False)

    **假**。

3.  **Named Entity Recognition** (**NER**) can provide useful information when looking for meaningful questions. (True/False)

    **真**。

4.  **Semantic Role Labeling** (**SRL**) is useless when preparing questions. (True/False)

    **假**。

5.  A question generator is an excellent way to produce questions. (True/False)

    **真**。

6.  Implementing question answering requires careful project management. (True/False)

    **真**。

7.  ELECTRA models have the same architecture as GPT-2\. (True/False)

    **假**。

8.  ELECTRA models have the same architecture as BERT but are trained as discriminators. (True/False)

    **真**。

9.  NER can recognize a location and label it as I-LOC. (True/False)

    **真**。

10.  NER can recognize a person and label that person as I-PER. (True/False)

    **真**。

# 第 11 章，检测客户情感以做出预测

1.  It is not necessary to pretrain transformers for sentiment analysis. (True/False)

    **假**。

2.  A sentence is always positive or negative. It cannot be neutral. (True/False)

    **假**。

3.  The Principle of Compositionality signifies that a transformer must grasp every part of a sentence to understand it. (True/False)

    **真**。

4.  RoBERTa-large was designed to improve the pretraining process of transformer models. (True/False)

    **真**。

5.  A transformer can provide feedback that informs us whether a customer is satisfied or not. (True/False)

    **真**。

6.  If the sentiment analysis of a product or service is consistently negative, it helps us make the proper decisions to improve our offer. (True/False)

    **真**。

7.  If a model fails to provide a good result on a task, it requires more training before changing models. (True/False)

    **真**。

# 第 12 章，用转换器分析假新闻

1.  News labeled as "fake news" is always fake. (True/False)

    **假**。

2.  News that everybody agrees with is always accurate. (True/False)

    **假**。

3.  Transformers can be used to run sentiment analysis on Tweets. (True/False)

    **真**。

4.  Key entities can be extracted from Facebook messages with a DistilBERT model running NER. (True/False)

    **真**。

5.  Key verbs can be identified from YouTube chats with BERT-based models running SRL. (True/False)

    **真**。

6.  Emotional reactions are a natural first response to fake news. (True/False)

    **真**。

7.  A rational approach to fake news can help clarify one's position. (True/False)

    **真**。

8.  Connecting transformers to reliable websites can help somebody understand why some news is fake. (True/False)

    **真**。

9.  Transformers can make summaries of reliable websites to help us understand some of the topics labeled as fake news. (True/False)

    **真**。

10.  You can change the world if you use AI for the good of us all. (True/False)

    **真**。

**分享你的经历**

感谢您花时间阅读这本书。如果你喜欢这本书，帮助别人找到它。在[https://www.amazon.com/dp/1800565798](https://www.amazon.com/dp/1800565798)留下评论