<title>Preface</title> <link href="../Styles/syntax-highlighting.css" rel="stylesheet" type="text/css"> <link href="../Styles/epub.css" rel="stylesheet" type="text/css">

# 前言

变形金刚是自然语言理解(**【NLU】**)的游戏规则改变者，是自然语言处理 ( **NLP** )的子集，已经成为全球数字经济中人工智能的支柱之一。

*全球经济已经从物理世界转向数字世界*。

我们正在见证社交网络相对于物理接触、电子商务相对于物理购物、数字报纸、流媒体相对于物理影院、远程医生咨询相对于物理访问、远程工作代替现场任务的扩张，以及数百个更多领域的类似趋势。

人工智能驱动的语言理解将继续呈指数级增长，这些活动产生的数据量也将如此。语言理解已经成为语言建模、聊天机器人、个人助理、问题回答、文本总结、语音到文本、情感分析、机器翻译等等的支柱。

如果没有人工智能语言理解，社会使用互联网将非常困难。

变压器架构既是革命性的，也是颠覆性的。transformer 和随后的 Transformer 架构和模型是革命性的，因为它们改变了我们对 NLP 和人工智能本身的看法。变压器的架构不是一种进化。它与过去决裂，将 rnn 和 CNN 抛在身后。它让我们更接近无缝的机器智能，在未来几年将与人类智能相匹配。

变压器和随后的变压器架构、概念和模型是破坏性的。我们将在本书中探索的各种变形金刚将会逐步取代 NLP，正如我们在它们到来之前所知道的那样。

想想看，需要多少人来控制每天发布在社交网络上的数十亿条信息的内容，以决定它们是否合法、合乎道德，并提取其中包含的信息。想想每天需要多少人来翻译网上发布的数百万页。或者想象一下，需要多少人来手动控制每分钟数百万条消息！最后，想想需要多少人来编写每天在网上发布的大量流媒体的文字记录。最后，想想为持续出现在网上的数十亿张图像替换人工智能图像字幕所需的人力资源。

这将我们引向人工智能的更深层次。在一个数据呈指数级增长的世界里，人工智能执行的任务比人类能够执行的还要多。想想看，翻译 10 亿条在线信息需要多少翻译人员，而机器翻译没有数量限制。

这本书将告诉你如何提高语言理解能力。每章将带你从头开始理解 Python、PyTorch 和 TensorFlow 语言的关键方面。

在许多领域，例如媒体、社交媒体和研究论文，对语言理解的需求日益增加。在数百项人工智能任务中，我们需要总结大量数据进行研究，翻译我们经济各个领域的文档，并出于道德和法律原因扫描所有社交媒体帖子。

需要取得进展。谷歌推出的 Transformer 通过一种新颖的自我关注架构，提供了一种新颖的语言理解方法。OpenAI 提供变压器技术，脸书的人工智能研究部门提供高质量的数据集。总的来说，正如我们将在本书中发现的那样，互联网巨头已经让所有人都可以使用变形金刚。

《变形金刚》可以超越今天使用的经典 RNN 和 CNN 模型。例如，英语到法语翻译和英语到德语翻译转换器模型比`ConvS2S` (RNN)、`GNMT` (CNN)和`SliceNet` (CNN)提供更好的结果。

在整本书中，您将亲自使用 Python、PyTorch 和 TensorFlow。将向您介绍关键的人工智能语言理解神经网络模型。然后，您将学习如何探索和实现变压器。

这本书的目标是为读者提供 Python 深度学习的知识和工具，这是有效开发语言理解的关键方面所需要的。

# 这本书是给谁的

这本书不是对 Python 编程或机器学习概念的介绍。相反，它专注于机器翻译、语音到文本、文本到语音、语言建模、问答和许多其他 NLP 领域的深度学习。

最能从本书中受益的读者是:

*   熟悉 Python 编程的深度学习和 NLP 从业者。
*   想要介绍人工智能语言理解以处理日益增多的语言驱动函数的数据分析师和数据科学家。

# 这本书涵盖的内容

**第一部分:变压器架构介绍**

*第 1 章*、*从《变形金刚》的模型架构入手*，通过 NLP 的背景，了解 RNN、LSTM、CNN 架构是如何被抛弃的，以及《变形金刚》架构是如何开启一个新时代的。我们将通过由谷歌研究和谷歌大脑作者发明的独特的“*注意力是你所需要的全部*”方法来浏览变压器的架构。我们将描述变压器的理论。我们将亲自使用 Python 来看看多注意力头部子层是如何工作的。在本章结束时，你将已经理解了变压器的原始结构。在接下来的章节中，您将准备好探索转换器的多种变体和用法。

*第 2 章*、*微调 BERT 模型*，建立在原始变压器的架构上。**变形金刚** ( **BERT** )的双向编码器表现将变形金刚带入了一个全新的感知 NLP 世界的方式。伯特不是通过分析过去的序列来预测未来的序列，而是关注整个序列！我们将首先浏览 BERT 架构的关键创新，然后通过在 Google 协作笔记本中浏览每个步骤来微调 BERT 模型。像人类一样，BERT 可以学习任务并执行其他新任务，而不必从头开始学习主题。

*第 3 章*，*从零开始预训练一个罗伯塔模型*，使用拥抱脸 PyTorch 模块从零开始构建一个罗伯塔变压器模型。变压器将是伯特式和蒸馏式的。首先，我们将在一个定制的数据集上从头开始训练一个标记器。然后，经过训练的转换器将在下游屏蔽语言建模任务上运行。

我们将在伊曼纽尔·康德的数据集上进行掩蔽语言建模实验，以探索概念上的 NLP 表示。

**第二部分:应用变形金刚进行自然语言理解和生成**

*第四章*、*用变压器完成下游 NLP 任务*，揭示变压器模型用下游 NLP 任务的神奇之处。经过预训练的 transformer 模型可以进行微调，以解决一系列 NLP 任务，如 BoolQ、CB、MultiRC、RTE、WiC 等，主导 GLUE 和 SuperGLUE 排行榜。我们将介绍变压器的评估流程、任务、数据集和指标。然后，我们将使用拥抱脸的变形金刚管道运行一些下游任务。

*第 5 章*，*机器翻译与变压器*，定义了机器翻译，以理解如何从人类基线到机器转换方法。然后我们将预处理来自欧洲议会的 WMT 法语-英语数据集。机器翻译需要精确的评估方法，在这一章中，我们探讨 BLEU 评分方法。最后，我们将使用 Trax 实现一个 Transformer 机器翻译模型。

*第 6 章*，*用 OpenAI GPT-2 和 GPT-3 模型生成文本*，探索 OpenAI 的 GPT-2 变形金刚的许多方面。我们将首先从项目管理的角度考察 GPT-2 和 GPT-3，研究替代解决方案，如重整器和 PET。然后，我们将探索 OpenAI 的 GPT-2 和 GPT-3 变压器模型的新颖架构，并运行 GPT-2 345M 参数模型，并与其交互以生成文本。然后，我们将在自定义数据集上训练 GPT-2 117M 参数模型，并生成自定义的文本补全。

*第 7 章*，*将 transformer 应用于法律和金融文档进行人工智能文本摘要*，介绍 T5 transformer 模型的概念和架构。我们将从拥抱脸到总结文档初始化一个 T5 模型。最后，我们将让 T5 模型总结各种文档，包括来自*权利法案*的样本，探索迁移学习方法应用于变形金刚的成功和局限性。

*第 8 章*，*匹配记号化器和数据集*，分析记号化器的局限性，并查看一些用于提高数据编码过程质量的方法。我们将首先构建一个 Python 程序来调查为什么一些单词会被`word2vector`分词器省略或曲解。接下来，我们用记号赋予器-激动法找到了预训练记号赋予器的极限。最后，我们将通过应用一些想法来改进 T5 的总结，这些想法表明仍然有很大的空间来改进标记化过程的方法。

*第九章*、*用基于 BERT 的变形金刚*进行语义角色标注，探索变形金刚如何学习理解文本的内容。**语义角色标注**(**)对人类来说是一项具有挑战性的工作。变形金刚可以产生令人惊讶的效果。我们将在 Google Colab 笔记本中实现一个由艾伦人工智能研究所设计的基于 BERT 的变压器模型。我们还将使用他们的在线资源来可视化 SRL 输出。**

 ****第三部分:高级语言理解技巧**

*第 10 章*、*让你的数据说话:故事、问题和答案*，展示了变形金刚如何学习如何推理。一个变形人必须能够理解一篇文章，一个故事，并且展示推理能力。我们将看到如何通过将 NER 和 SRL 加入到过程中来增强问题回答。我们将为问题生成器构建蓝图，该生成器可用于训练变压器或作为独立的解决方案。

*第 11 章*，*检测顾客情绪做出预测*，展示了变形金刚如何改进情绪分析。我们将使用斯坦福情感树库分析复杂的句子，挑战几种转换器模型，不仅理解序列的结构，还理解其逻辑形式。我们将看到如何使用 transformers 进行预测，根据情感分析的输出触发不同的动作。

*第 12 章*、*用变形金刚*分析假新闻，深入探讨假新闻这个热门话题，以及变形金刚如何帮助我们理解我们每天看到的网络内容的不同视角。每天，数十亿条消息、帖子和文章通过社交媒体、网站和各种可用的实时交流方式发布在网络上。利用前几章的一些技巧，我们将分析关于气候变化和枪支控制的辩论以及前总统的推文。我们将通过道德和伦理的问题来确定什么可以被认为是超越合理怀疑的假新闻，什么新闻仍然是主观的。

# 从这本书中获得最大收益

*   书中的大多数程序都是合作笔记本。你只需要一个免费的谷歌 Gmail 账户，就可以在谷歌实验室的免费虚拟机上运行笔记本。
*   你需要在你的机器上安装 Python 来运行一些教育程序。
*   花必要的时间阅读*第 1 章*、*变压器模型架构入门*。它包含了对最初的 Transformer 的描述，该 Transformer 是由整本书中将要实现的构建块构建的。如果你觉得很难，那就从这一章中挑出一般的直觉想法。几章之后，当你对变形金刚更熟悉的时候，你可以回到这一章。
*   在阅读完每一章后，考虑你如何为你的客户实现变形金刚，或者用它们来提升你的职业生涯。

## 下载示例代码文件

这本书的代码包托管在 GitHub 的 https://GitHub . com/packt publishing/Transformers-for-Natural-Language-Processing 上。我们在 https://github.com/PacktPublishing/的[也有来自我们丰富的书籍和视频目录的其他代码包。看看他们！](https://github.com/PacktPublishing/)

## 下载彩色图像

我们还提供了一个 PDF 文件，其中包含本书中使用的截图/图表的彩色图像。可以在这里下载:[https://static . packt-cdn . com/downloads/9781800565971 _ color images . pdf](https://static.packt-cdn.com/downloads/9781800565971_ColorImages.pdf)。

## 使用的惯例

本书通篇使用了几个文本约定。

`CodeInText`:表示文本中的码字、数据库表名、文件夹名、文件名、文件扩展名、路径名、伪 URL、用户输入和 Twitter 句柄。例如，“然而，如果你想探索代码，你可以在谷歌合作实验室的`positional_encoding.ipynb`笔记本和本章 GitHub 库的`text.txt`文件中找到它。”

代码块设置如下:

```py
import numpy as np

from scipy.special import softmax 
```

当我们希望将您的注意力吸引到代码块的特定部分时，相关的行或项目以粗体显示:

```py
The **black** cat sat on the couch and the **brown** dog slept on the rug. 
```

任何命令行输入或输出都按如下方式编写:

```py
[[0.99987495]] word similarity

[[0.8600013]] positional encoding vector similarity

[[0.9627094]] final positional encoding similarity 
```

**Bold** :表示一个新的术语、一个重要的单词，或者您在屏幕上看到的单词，例如，在菜单或对话框中，也像这样出现在文本中。例如:“在我们的案例中，我们正在寻找 **t5-large** ，一个我们可以在谷歌实验室顺利运行的 t5-large 模型。”

警告或重要提示如下所示。

提示和技巧是这样出现的。

# 取得联系

我们随时欢迎读者的反馈。

**总体反馈**:发送电子邮件`feedback@packtpub.com`，在邮件主题中提及书名。如果您对本书的任何方面有疑问，请发邮件至`questions@packtpub.com`联系我们。

**勘误表**:虽然我们已经尽力确保内容的准确性，但错误还是会发生。如果你在这本书里发现了一个错误，请告诉我们，我们将不胜感激。请访问 http://www.packtpub.com/submit-errata，选择您的图书，点击勘误表提交表格链接，并输入详细信息。

**盗版**:如果您在互联网上遇到我们作品的任何形式的非法拷贝，如果您能提供我们的地址或网站名称，我们将不胜感激。请通过`copyright@packtpub.com`联系我们，并提供材料链接。

**如果你有兴趣成为一名作家**:如果有你擅长的主题，并且你有兴趣写书或投稿，请访问 http://authors.packtpub.com。

## 复习

请留下评论。一旦你阅读并使用了这本书，为什么不在你购买它的网站上留下评论呢？潜在的读者可以看到并使用您不带偏见的意见来做出购买决定，我们 Packt 可以了解您对我们产品的看法，我们的作者可以看到您对他们的书的反馈。谢谢大家！

更多关于 Packt 的信息，请访问[packtpub.com](http://packtpub.com)。**