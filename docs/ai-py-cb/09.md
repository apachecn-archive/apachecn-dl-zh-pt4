<title>Deep Learning in Audio and Speech</title> <link rel="stylesheet" href="css/style.css" type="text/css">  Deep Learning in Audio and Speech

在这一章中，我们将讨论声音和语音。声音数据以波的形式出现，因此需要与其他类型的数据不同的预处理。

对音频信号的机器学习在语音增强(例如，在助听器中)、语音到文本和文本到语音、噪声消除(如在耳机中)、根据用户的偏好向用户推荐音乐(如 Spotify)以及生成音频方面找到了商业应用。在音频中可以遇到很多有趣的问题，包括音乐流派的分类，音乐的转录，音乐的生成等等。

在本章中，我们将实现几个声音和语音的应用程序。我们将首先做一个简单的分类任务的例子，其中我们试图区分不同的单词。这将是智能家居设备中区分不同命令的典型应用。然后，我们将看看一个文本到语音的架构。你可以用它来从文本中创建你自己的有声读物，或者为你自己开发的智能家居设备提供语音输出。我们将以一个制作音乐的方法来结束。从商业意义上来说，这可能更像是一个利基应用程序，但是你也可以为娱乐或娱乐你的视频游戏用户而制作你自己的音乐。

在这一章中，我们将看看下面的食谱:

*   识别语音命令
*   从文本合成语音
*   产生旋律

# 技术要求

你可以在 GitHub 上的[https://GitHub . com/packt publishing/Artificial-Intelligence-with-Python-Cookbook/tree/master/chapter 09](https://github.com/PacktPublishing/Artificial-Intelligence-with-Python-Cookbook/tree/master/chapter09)找到与本章食谱相关的笔记本源代码。

本章我们将使用`librosa`音频处理库([https://librosa.org/doc/latest/index.html](https://librosa.org/doc/latest/index.html))，您可以按如下方式安装:

```
!pip install librosa
```

Librosa 默认安装在 Colab 中。

对于本章中的食谱，请确保你有一个可用的 GPU。在 Google Colab 上，确保你激活了一个 GPU 运行时。

# 识别语音命令

在这个菜谱中，我们将在 Google 的语音命令数据集上研究一个简单的声音识别问题。我们将声音命令分为不同的类别。然后我们将建立一个深度学习模型并训练它。

## 做好准备

对于这个食谱，我们需要本章开头提到的`librosa`库。我们还需要下载语音命令数据集，为此我们需要首先安装`wget`库:

```
!pip install wget
```

或者，我们可以在 Linux 和 macOS 中使用`!wget`系统命令。我们将创建一个新目录，下载包含数据集的归档文件，并提取`tarfile`:

```
import os
import wget
import tarfile

DATA_DIR = 'sound_commands'
DATASET_URL = 'http://download.tensorflow.org/data/speech_commands_v0.01.tar.gz'
ARCHIVE = os.path.basename(DATASET_URL)
os.mkdir(DATA_DIR)
os.chdir(DATA_DIR)
wget.download(DATASET_URL)
with tarfile.open(ARCHIVE, 'r:gz') as tar:
  tar.extractall(path='data/train')
os.remove(ARCHIVE)
```

这为我们提供了`data/train`目录中的许多文件和目录:

```
_background_noise_  five     marvin        right             tree
bed                 four     nine       seven             two
bird                go       no         sheila            up
cat                 happy    off        six               validation_list.txt
dog                 house    on         stop              wow
down                left     one        testing_list.txt  yes
eight               LICENSE  README.md  three             zero
```

其中大多数指的是语音命令；例如，`bed`目录包含了`bed`命令的例子。

有了所有这些，我们现在准备开始。

## 怎么做...

在这个食谱中，我们将训练一个神经网络来识别语音命令。这个食谱的灵感来自于在 https://www.tensorflow.org/tutorials/audio/simple_audio 举办的关于语音命令的 TensorFlow 教程。

我们将首先执行数据探索，然后导入并预处理数据集以进行训练，然后我们将创建一个模型，对其进行训练，并在验证中检查其性能:

1.  让我们从一些数据探索开始:我们将听一个命令，看它的波形，然后看它的频谱。`librosa`库提供了将声音文件加载到矢量中的功能:

```
import librosa
x, sr = librosa.load('data/train/bed/58df33b5_nohash_0.wav')
```

我们还可以获得一个 Jupyter 小部件来收听声音文件或加载的矢量:

```
import IPython.display as ipd
ipd.Audio(x, rate=sr)
```

小部件如下所示:

![](assets/06c58925-04d1-4647-9a74-4f0976ea4f8d.png)

按下播放键，我们听到声音。注意，这甚至可以在远程连接上工作，例如，如果我们使用 Google Colab。

现在我们来看看声音波形:

```
%matplotlib inline
import matplotlib.pyplot as plt
import librosa.display

plt.figure(figsize=(14, 5))
librosa.display.waveplot(x, sr=sr, alpha=0.8)
```

波形看起来像这样:

![](assets/6e74df18-4205-44ba-a3a2-30486e0fdc03.png)

这也称为压力-时间图，显示了(带符号的)振幅随时间的变化。

我们可以将光谱绘制如下:

```
X = librosa.stft(x)
Xdb = librosa.amplitude_to_db(abs(X))
plt.figure(figsize=(14, 5))
librosa.display.specshow(Xdb, sr=sr, x_axis='time', y_axis='log')
plt.colorbar()
```

光谱看起来像这样:

![](assets/593a7454-c705-4231-82c5-e2720f935923.png)

请注意，我们在 *y* 轴上使用了对数刻度。

2.  现在，让我们开始数据导入和预处理。我们必须遍历文件，并将它们存储为一个向量:

```
from tqdm.notebook import tqdm

def vectorize_directory(dirpath, label=0):
 features = []
  labels = [0]
  files = os.listdir(dirpath)
  for filename in tqdm(files):
    x, _ = librosa.load(
        os.path.join(dirpath, filename)
    )
    if len(x) == 22050:
      features.append(x)
  return features, [label] * len(features)

features, labels = vectorize_directory('data/train/bed/')
f, l = vectorize_directory('data/train/bird/', 1)
features.extend(f)
labels.extend(l)
f, l = vectorize_directory('data/train/tree/', 2)
features.extend(f)
labels.extend(l)
```

为了简单起见，我们这里只采用三个命令:`bed`、`bird`和`tree`。这足以说明问题以及深度神经网络在声音分类中的应用，并且足够简单，不会花费很长时间。然而，这个过程仍然需要一段时间。在 Google Colab 上花了大概一个小时。

最后，我们需要将 Python 特性列表转换为 NumPy 数组，并且我们需要分割训练和验证数据:

```
import numpy as np
from sklearn.model_selection import train_test_split

features = np.concatenate([f.reshape(1, -1) for f in features], axis=0)
labels = np.array(labels)
X_train, X_test, y_train, y_test = train_test_split(
 features, labels, test_size=0.33, random_state=42
)
```

现在我们需要对我们的训练数据做些什么。我们需要一个可以训练的模型。

3.  让我们创建一个深度学习模型，然后进行训练和测试。首先，我们需要创建我们的模型和规范化。让我们先做标准化:

```
import tensorflow.keras as keras
from tensorflow.keras.layers import *
from tensorflow.keras.regularizers import l2
from tensorflow.keras.models import Model
import tensorflow.keras.backend as K

def preprocess(x):
    x = (x + 0.8) / 7.0
    x = K.clip(x, -5, 5)
    return x

Preprocess = Lambda(preprocess)
```

接下来是以下内容:

```
def relu6(x):
    return K.relu(x, max_value=6)

def conv_layer(x, num_filters=100, k=3, strides=2):
    x = Conv1D(
          num_filters,
          (k),
          padding='valid',
          use_bias=False,
          kernel_regularizer=l2(1e-6)
        )(x)
    x = BatchNormalization()(x)
    x = Activation(relu6)(x)
    x = MaxPool1D(pool_size=num_filters, strides=None, padding='valid')(x)
    return x

def create_model(classes, nlayers=1, filters=100, k=100):
    input_layer = Input(shape=[features.shape[1]])
    x = Preprocess(input_layer)
    x = Reshape([features.shape[1], 1])(x)
    for _ in range(nlayers):
        x = conv_layer(x, num_filters=filters, k=k)
        x = Reshape([219 * filters])(x)
        x = Dense(
            units=len(classes), activation='softmax',
            kernel_regularizer=l2(1e-2)
        )(x)
    model = Model(input_layer, x, name='conv1d_sound')
    model.compile(
        optimizer=keras.optimizers.Adam(lr=3e-4),
        loss=keras.losses.SparseCategoricalCrossentropy(),
        metrics=[keras.metrics.sparse_categorical_accuracy])
    model.summary()
    return model

model = create_model(classes
```

请注意`conv_layer()`函数，它提供了网络的核心。非常相似的卷积模块可以用在 vision 中，只是我们在这里使用 1D 卷积。

这为我们提供了一个只有大约 75，000 个参数的相对较小的模型:

```
Layer (type)                 Output Shape              Param #   
=================================================================
input_46 (InputLayer)        [(None, 22050)]           0         
_________________________________________________________________
lambda_44 (Lambda)           (None, 22050)             0         
_________________________________________________________________
reshape_86 (Reshape)         (None, 22050, 1)          0         
_________________________________________________________________
conv1d_56 (Conv1D)           (None, 21951, 100)        10000     
_________________________________________________________________
batch_normalization_43 (Batc (None, 21951, 100)        400       
_________________________________________________________________
activation_43 (Activation)   (None, 21951, 100)        0         
_________________________________________________________________
max_pooling1d_29 (MaxPooling (None, 219, 100)          0         
_________________________________________________________________
reshape_87 (Reshape)         (None, 21900)             0         
_________________________________________________________________
dense_33 (Dense)             (None, 3)                 65703     
=================================================================
Total params: 76,103
Trainable params: 75,903
Non-trainable params: 200
_________________________________________________________________
```

您会注意到最大的层(就参数而言)是最终的密集层。我们可以通过在密集层之前改变卷积或最大池操作来进一步减少参数的数量。

我们现在可以执行培训和验证:

```
import sklearn

model.fit(X_train, y_train, epochs=30)
predicted = model.predict(X_test)
print('accuracy: {:.3f}'.format(
    sklearn.metrics.accuracy_score(y_test, predicted.argmax(axis=1))
))
```

在验证集中，我们应该看到类似于 0.805 的模型精度输出。

## 它是如何工作的...

除了预处理之外，声音与其他领域没有什么不同。至少对声音在文件中的存储方式有一个基本的了解是很重要的。在最基本的层面上，声音是以振幅随时间和频率的变化来存储的。声音以离散间隔采样(这是*采样率*)。48 kHz 是 DVD 的典型录制质量，指的是每秒 48，000 次的采样频率。*位深度*(也称为*动态范围*)是信号幅度的分辨率(例如，16 位意味着 0-65535 的范围)。

对于机器学习，我们可以从波形中提取特征，并对原始波形使用 1D 卷积，或对频谱图表示使用 2D 卷积(例如，Mel spectrograms–Davis 和 Mermelstein，*基于音节的连续语音识别实验*，1980)。我们以前在[第七章](f386de9e-b56d-4b39-bf36-803860def385.xhtml)、*高级图像应用*中讨论过卷积。简而言之，卷积是前馈滤波器，应用于层输入上的矩形面片。生成的地图通常会通过合并图层进行二次采样。

卷积层可以叠得很深(如戴等，2016:【https://arxiv.org/abs/1610.00087】)。我们已经让读者很容易地试验堆叠层。层数`nlayers`是`create_model()`中的参数之一。

有趣的是，许多语音识别模型使用递归神经网络。然而，一些模型，例如脸书的 wav2 letter([https://github.com/facebookresearch/wav2letter](https://github.com/facebookresearch/wav2letter))使用了完全卷积模型，这与本配方中采用的方法没有太大不同。

## 请参见

除了`librosa`，Python 中对音频处理有用的库还有`pydub`([【https://github.com/jiaaro/pydub】](https://github.com/jiaaro/pydub))和`scipy`。pyAudioProcessing 库附带了音频的特征提取和分类功能:[https://github.com/jsingh811/pyAudioProcessing](https://github.com/jsingh811/pyAudioProcessing)。

还有一些有趣的库和存储库可供探索:

*   wav2letter++是一个开源的语音处理工具包，来自脸书人工智能研究所的语音团队，使用 Python 绑定:[https://github.com/facebookresearch/wav2letter](https://github.com/facebookresearch/wav2letter)。
*   硕士论文项目-*结构化自动编码器及其在音乐流派识别中的应用*:【https://github.com/mdeff/dlaudio】T2。
*   Erdene-Ochir Tuguldur 用 PyTorch 维护了一个用于蒙古语语音识别的 GitHub 库，其中包括从头开始的训练:[https://github.com/tugstugi/mongolian-speech-recognition](https://github.com/tugstugi/mongolian-speech-recognition)。

# 从文本合成语音

人类容易理解的文本到语音转换程序可以让有视觉或阅读障碍的人在家用电脑上听书面文字，或者让你在开车时欣赏一本书。在这个菜谱中，我们将加载一个文本到语音的模型，并让它为我们朗读文本。*它是如何工作的...*部分，我们将讨论模型实现和模型架构。

## 做好准备

对于这个食谱，请确保你有一个可用的图形处理器。在 Google Colab 上，确保你激活了一个 GPU 运行时。我们还需要`wget`库，我们可以从笔记本上安装它，如下所示:

```
!pip install wget
```

我们还需要从 GitHub 克隆`pytorch-dc-tts`存储库并安装它的需求。请从笔记本上运行(或从终端上运行，不带前导感叹号):

```
from os.path import exists

if not exists('pytorch-dc-tts'):
 !git clone --quiet https://github.com/tugstugi/pytorch-dc-tts

!pip install --ignore-installed librosa
```

请注意，您需要安装 Git，这样才能工作。如果您没有安装 Git，您可以直接从您的 web 浏览器中下载存储库。

我们已经准备好处理主菜了。

## 怎么做...

我们将下载 Torch 模型文件，将它们加载到 Torch 中，然后我们将从句子中合成语音:

1.  **下载模型文件**:我们将从`dropbox`下载数据集:

```
import wget

if not exists('ljspeech-text2mel.pth'):
    wget.download(      'https://www.dropbox.com/s/4t13ugxzzgnocbj/step-300K.pth',
        'ljspeech-text2mel.pth'
    )

if not exists('ljspeech-ssrn.pth'):
    wget.download(
   'https://www.dropbox.com/s/gw4aqrgcvccmg0g/step-100K.pth',
        'ljspeech-ssrn.pth'
    )
```

现在我们可以在 torch 中加载模型。

2.  **加载模型**:让我们把依赖关系去掉:

```
import sys
sys.path.append('pytorch-dc-tts')
import numpy as np
import torch
import IPython
from IPython.display import Audio
from hparams import HParams as hp
from audio import save_to_wav
from models import Text2Mel, SSRN
from datasets.lj_speech import vocab, idx2char, get_test_data
```

现在我们可以加载模型了:

```
torch.set_grad_enabled(False)
text2mel = Text2Mel(vocab)
text2mel.load_state_dict(torch.load('ljspeech-text2mel.pth').state_dict())
text2mel = text2mel.eval()
ssrn = SSRN()
ssrn.load_state_dict(torch.load('ljspeech-ssrn.pth').state_dict())
ssrn = ssrn.eval()
```

最后，我们可以大声朗读句子。

3.  **合成语音**:我们选择了几个简单的句子。这些句子在语法上是正确的，但是在最初的理解上误导了读者。

下面的句子是花园路径句子的例子，这些句子误导听者理解单词之间的关系。我们选择它们是因为它们短小有趣。你可以在学术文献中找到这些和更多的花园小径句子，比如在*Up the Garden Path*(Tomágráf；发表于《卡罗莱纳大学学报》，2013 年):

```
SENTENCES = [
 'The horse raced past the barn fell.',
 'The old man the boat.',
 'The florist sent the flowers was pleased.',
 'The cotton clothing is made of grows in Mississippi.',
 'The sour drink from the ocean.',
 'Have the students who failed the exam take the supplementary.',
 'We painted the wall with cracks.',
 'The girl told the story cried.',
 'The raft floated down the river sank.',
 'Fat people eat accumulates.'
]
```

我们可以从这些句子中生成语音，如下所示:

```
for i in range(len(SENTENCES)):    
    sentences = [SENTENCES[i]]
    max_N = len(sentences[0])
    L = torch.from_numpy(get_test_data(sentences, max_N))
    zeros = torch.from_numpy(np.zeros((1, hp.n_mels, 1), np.float32))
    Y = zeros
    A = None

    for t in range(hp.max_T):
      _, Y_t, A = text2mel(L, Y, monotonic_attention=True)
      Y = torch.cat((zeros, Y_t), -1)
      _, attention = torch.max(A[0, :, -1], 0)
      attention = attention.item()
      if L[0, attention] == vocab.index('E'): # EOS
          break

    _, Z = ssrn(Y)
    Z = Z.cpu().detach().numpy()
    save_to_wav(Z[0, :, :].T, '%d.wav' % (i + 1))
    IPython.display.display(Audio('%d.wav' % (i + 1), rate=hp.sr))
```

*还有更多...*部分，我们将了解如何为不同的数据集训练模型。

## 它是如何工作的...

语音合成是由一个叫做语音合成器的程序产生人类语音。从自然语言到语音的合成称为**文本到语音** ( **TTS** )。合成语音可以通过连接来自以不同声音、音素和音素对(双音素)为单位的记录片段的音频来生成。

让我们看一下两种方法的细节。

### 具有引导注意力的深度卷积网络

在这个食谱中，我们加载了 Hideyuki Tachibana 等人发表的模型，*基于深度卷积网络的可有效训练的文本到语音系统，具有引导注意力*(2017；https://arxiv.org/abs/1710.08969。我们在 https://github.com/tugstugi/pytorch-dc-tts 的[使用了这个实现。](https://github.com/tugstugi/pytorch-dc-tts)

该方法于 2017 年发布，其新颖性在于在网络中没有递归，而是依赖于卷积，这是一种比其他模型更快的训练和推理决策。事实上，他们声称，在一台配备了两个现成 GPU 的游戏 PC 上，训练他们的深度卷积 TTS 网络只需要大约 15 个小时。在对来自公共领域有声读物项目的数据集进行了 15 个小时的培训后，众包平均意见得分似乎没有增加。作者提供了一个演示页面来展示培训不同阶段的音频样本，在这里你可以听到说出的句子，如*瓦瑟斯坦甘的两人零和游戏是通过考虑坎托罗维奇-鲁宾斯坦对偶而得出的*:。

该架构由两个子网络组成，可以分别训练，一个从文本合成光谱图，另一个从光谱图创建波形。文本到语谱图部分由这些模块组成:

*   文本编码器
*   音频编码器
*   注意力
*   音频解码器

这其中有趣的部分是论文题目中提到的引导注意力，它负责字符与时间的对齐。他们将注意力矩阵限制为与时间几乎成线性关系，而不是在**引导注意力丢失**的情况下以随机顺序阅读字符:

![](assets/d73731e5-df40-4531-9499-ba2eb6aef7cc.png)

这有利于矩阵对角线上的值，而不是偏离对角线。他们认为这种限制有助于大大加快训练时间。

### 韦夫兰

*还有更多...*部分，我们将加载一个不同的模型 WaveGAN，由 Chris Donahue 等人发表为 *WaveGAN:学会用生成性对抗网络*合成原始音频(2018；【https://arxiv.org/abs/1802.04208】[。](https://arxiv.org/abs/1802.04208)

Donahue 和其他人在无人监管的环境下训练 GAN 来合成原始音频波形。他们尝试了两种不同的策略:

*   一个**频谱图——策略** ( **SpecGAN** )，在这里他们使用一个 DCGAN(请参考[第 7 章](f386de9e-b56d-4b39-bf36-803860def385.xhtml)、*高级图像应用*中的*生成图像*配方)，并将其应用于频谱图(频率随时间变化)
*   一个**波形****–策略** ( **韦弗根**)，在那里他们展平了架构(1D 卷积)

对于第一个策略，他们必须开发一个可以转换回文本的声谱图。

对于 WaveGAN，他们将 2D 卷积展平为 1D，同时保持大小不变(例如，5x5 的内核变成了 25 的 1D 内核)。2x2 的步幅变成了 4。他们移除了批次标准化层。他们使用 Wasserstein GAN-GP 策略进行训练(Ishaan Gulrajani 等人，2017；瓦塞尔斯坦甘斯的 Im *探明训练；【https://arxiv.org/abs/1704.00028】[。](https://arxiv.org/abs/1704.00028)*

他们的 WaveGAN 在人类判断(平均意见得分)方面的表现明显不如他们的 SpecGAN。你可以在[https://chrisdonahue.com/wavegan_examples/](https://chrisdonahue.com/wavegan_examples/)找到一些生成声音的例子。

## 还有更多...

我们还可以使用 WaveGAN 模型从文本中合成语音。

我们将下载一个模型的检查点，该模型根据我们在前面的菜谱中遇到的语音命令进行训练，*识别语音命令*。然后我们将运行模型来生成语音:

1.  **下载 TensorFlow 模型检查点**:我们将如下下载模型数据:

```
import wget

wget.download(
  'https://s3.amazonaws.com/wavegan-v1/models/timit.ckpt.index',
  'model.ckpt.index'
)
wget.download(
  'https://s3.amazonaws.com/wavegan-v1/models/timit.ckpt.data-00000-of-00001',
  'model.ckpt.data-00000-of-00001')
wget.download(
  'https://s3.amazonaws.com/wavegan-v1/models/timit_infer.meta',
  'infer.meta'
);
```

现在我们可以将计算图加载到内存中:

```
import tensorflow as tf

tf.reset_default_graph()
saver = tf.train.import_meta_graph('infer.meta')
graph = tf.get_default_graph()
sess = tf.InteractiveSession()
saver.restore(sess, 'model.ckpt');
```

我们现在可以产生语音。

2.  **生成语音**:模型架构涉及字母的潜在表示。我们可以听听模型基于潜在表示的随机初始化构建了什么:

```
import numpy as np
import PIL.Image
from IPython.display import display, Audio
import time as time

_z = (np.random.rand(2, 100) * 2.) - 1.
z = graph.get_tensor_by_name('z:0')
G_z = graph.get_tensor_by_name('G_z:0')[:, :, 0]
G_z_spec = graph.get_tensor_by_name('G_z_spec:0')

start = time.time()
_G_z, _G_z_spec = sess.run([G_z, G_z_spec], {z: _z})
print('Finished! (Took {} seconds)'.format(time.time() - start))

for i in range(2):
    display(Audio(_G_z[i], rate=16000))
```

这将向我们展示两个生成声音的例子，每个例子都有一个 Jupyter 小部件:

![](assets/49495435-bc36-4d82-824f-1741e83b9903.png)

如果这些听起来不是特别自然，不要害怕。毕竟，我们已经使用了潜在空间的随机初始化。

## 请参见

*基于深度卷积网络的高效可训练文本到语音转换系统*(【https://arxiv.org/abs/1710.08969】T2)。在 Erdene-Ochir Tuguldur 的 GitHub 存储库中，您可以找到该论文的 PyTorch 实现。蒙古语文本到语音转换在来自蒙古语圣经:【https://github.com/tugstugi/pytorch-dc-tts】[的 5 小时音频上被训练。](https://github.com/tugstugi/pytorch-dc-tts)

在 Chris Donahue 的 GitHub repository of WaveGAN 上，你可以看到 WaveGAN 的实现和例子，用于从 MP3、WAV、OGG 等格式的音频文件中进行训练，而无需预处理([https://github.com/chrisdonahue/wavegan](https://github.com/chrisdonahue/wavegan))。

Mozilla 开源了他们百度深度语音架构的 TensorFlow 实现(2014)，你可以在这里找到:[https://github.com/mozilla/DeepSpeech](https://github.com/mozilla/DeepSpeech)。

# 产生旋律

**音乐中的人工智能** ( **AI** )是一个引人入胜的话题。如果你最喜欢的 70 年代乐队推出新歌，但可能更现代，那不是很酷吗？索尼对甲壳虫乐队也这样做了，你可以在 YouTube 上听到一首歌，配有自动生成的歌词，名为【https://www.youtube.com/watch?v=LSHZ_b05W7o】爸爸的车:。

在这个食谱中，我们将产生一段旋律。更具体地说，我们将使用 Magenta Python 库中的功能继续一首歌曲。

## 做好准备

我们需要安装 Magenta 库和一些系统库作为依赖项。请注意，您需要管理员权限才能安装系统依赖项。如果你不在 Linux(或*nix)上，你必须找到与你的系统相对应的。

在 macOS 上，这应该相对简单。否则，在 Colab 环境中运行可能更容易:

```
!apt-get update -qq && apt-get install -qq libfluidsynth1 fluid-soundfont-gm build-essential libasound2-dev libjack-dev
!pip install -qU pyfluidsynth pretty_midi
!pip install -qU magenta
```

如果您在 Colab 上，您需要另一个调整来允许 Python 找到您的系统库:

```
import ctypes.util
orig_ctypes_util_find_library = ctypes.util.find_library
def proxy_find_library(lib):
  if lib == 'fluidsynth':
    return 'libfluidsynth.so.1'
  else:
    return orig_ctypes_util_find_library(lib)
ctypes.util.find_library = proxy_find_library
```

这是 Python 的外来库导入系统的一个巧妙的变通方法，摘自最初的 Magenta 教程，位于[https://colab . research . Google . com/notebooks/Magenta/hello _ Magenta/hello _ Magenta . ipynb](https://colab.research.google.com/notebooks/magenta/hello_magenta/hello_magenta.ipynb)。

是时候发挥创造力了！

## 怎么做...

我们首先将一段旋律的开始放在一起，然后我们将从 Magenta 加载`MelodyRNN`模型，并让它继续这段旋律:

1.  让我们一起谱写一段旋律。我们将乘坐*闪烁小星星*。Magenta 项目使用名为`NoteSequence`的音符序列表示，它附带了许多实用工具，包括与 MIDI 之间的转换。我们可以像这样给序列添加音符:

```
from note_seq.protobuf import music_pb2

twinkle_twinkle = music_pb2.NoteSequence()
twinkle_twinkle.notes.add(pitch=60, start_time=0.0, end_time=0.5, velocity=80)
twinkle_twinkle.notes.add(pitch=60, start_time=0.5, end_time=1.0, velocity=80)
twinkle_twinkle.notes.add(pitch=67, start_time=1.0, end_time=1.5, velocity=80)
twinkle_twinkle.notes.add(pitch=67, start_time=1.5, end_time=2.0, velocity=80)
twinkle_twinkle.notes.add(pitch=69, start_time=2.0, end_time=2.5, velocity=80)
twinkle_twinkle.notes.add(pitch=69, start_time=2.5, end_time=3.0, velocity=80)
twinkle_twinkle.notes.add(pitch=67, start_time=3.0, end_time=4.0, velocity=80)
twinkle_twinkle.notes.add(pitch=65, start_time=4.0, end_time=4.5, velocity=80)
twinkle_twinkle.notes.add(pitch=65, start_time=4.5, end_time=5.0, velocity=80)
twinkle_twinkle.notes.add(pitch=64, start_time=5.0, end_time=5.5, velocity=80)
twinkle_twinkle.notes.add(pitch=64, start_time=5.5, end_time=6.0, velocity=80)
twinkle_twinkle.notes.add(pitch=62, start_time=6.0, end_time=6.5, velocity=80)
twinkle_twinkle.notes.add(pitch=62, start_time=6.5, end_time=7.0, velocity=80)
twinkle_twinkle.notes.add(pitch=60, start_time=7.0, end_time=8.0, velocity=80) 
twinkle_twinkle.total_time = 8
twinkle_twinkle.tempos.add(qpm=60);
```

我们可以使用散景来显示序列，然后播放音符序列:

```
import note_seq

note_seq.plot_sequence(twinkle_twinkle)
note_seq.play_sequence(twinkle_twinkle,synth=note_seq.fluidsynth)
```

这看起来如下:

![](assets/0820a4e4-87cd-483f-8680-e38f3789ca89.png)

我们可以听这首歌的前 9 秒。

2.  让我们从`magenta`加载`MelodyRNN`模型:

```
from magenta.models.melody_rnn import melody_rnn_sequence_generator
from magenta.models.shared import sequence_generator_bundle
from note_seq.protobuf import generator_pb2
from note_seq.protobuf import music_pb2

note_seq.notebook_utils.download_bundle('attention_rnn.mag', '/content/')
bundle = sequence_generator_bundle.read_bundle_file('/content/basic_rnn.mag')
generator_map = melody_rnn_sequence_generator.get_generator_map()
melody_rnn = generator_map['basic_rnn'](checkpoint=None, bundle=bundle)
melody_rnn.initialize()
```

这应该只需要几秒钟。与我们在本书中遇到的其他模型相比，Magenta 模型非常小。

我们现在可以输入之前的旋律，以及一些参数，以便继续播放歌曲:

```
def get_options(input_sequence, num_steps=128, temperature=1.0):
    last_end_time = (max(n.end_time for n in input_sequence.notes)
                      if input_sequence.notes else 0)
    qpm = input_sequence.tempos[0].qpm 
    seconds_per_step = 60.0 / qpm / melody_rnn.steps_per_quarter
    total_seconds = num_steps * seconds_per_step

    generator_options = generator_pb2.GeneratorOptions()
    generator_options.args['temperature'].float_value = temperature
    generate_section = generator_options.generate_sections.add(
      start_time=last_end_time + seconds_per_step,
      end_time=total_seconds)
    return generator_options

sequence = melody_rnn.generate(input_sequence, get_options(twinkle_twinkle))
```

我们现在可以策划和播放新的音乐:

```
note_seq.plot_sequence(sequence)
note_seq.play_sequence(sequence, synth=note_seq.fluidsynth)
```

我们再次得到散景库图和播放窗口小部件:

![](assets/142f9df4-6d24-4be8-8268-20ea3b437323.png)

我们可以从音符序列创建一个 MIDI 文件，如下所示:

```
note_seq.sequence_proto_to_midi_file(sequence, 'twinkle_continued.mid')
```

这将在磁盘上创建一个新的 MIDI 文件。

在 Google Colab 上，我们可以像这样下载 MIDI 文件:

```
from google.colab import files
files.download('twinkle_continued.mid')
```

我们可以通过 MIDI 文件将不同的旋律输入到模型中，或者我们可以尝试其他参数；我们可以增加或减少随机性(`temperature`参数)，或者让序列持续更长时间(`num_steps`参数)。

## 它是如何工作的...

MelodyRNN 是一个基于 LSTM 的音符语言模型。为了理解 MelodyRNN，我们首先需要了解**长短期记忆** ( **LSTM** )是如何工作的。1997 年由塞普·霍克雷特和于尔根·施密德胡伯(*长短期记忆*:[https://doi.org/10.1162%2Fneco.1997.9.8.1735](https://doi.org/10.1162%2Fneco.1997.9.8.1735))发表，此后更新了无数次，LSTM 是最知名的**递归神经网络** ( **RNN** )的例子，代表了图像识别和机器学习任务的最先进模型，具有语音识别、自然语言处理和时间序列等序列。LSTMs 过去或现在都是谷歌、亚马逊、微软和脸书的语音识别和语言翻译工具的幕后推手。

LSTM 层的基本单元是 LSTM 池，它由几个调节器组成，我们可以在下面的示意图中看到:

![](assets/487e29d1-87a7-44b2-b4b2-478e2eecd894.png)

该图基于 Alex Graves 等人的*深度递归神经网络语音识别*(2013)，摘自 https://en.wikipedia.org/wiki/Long_short-term_memoryT2 lst ms 的英文维基百科文章。

调节器包括以下内容:

*   输入门
*   输出门
*   遗忘之门

我们可以解释这些门背后的直觉，而不会迷失在方程式中。输入门调节输入对细胞的影响强度，输出门抑制外向细胞的激活，遗忘门是细胞活性的衰减。

LSTMs 的优点是能够处理不同长度的序列。然而，序列越长，它们的性能越差。为了学习更长的序列，Magenta 库提供了一个模型，其中包括一个注意机制(Dzmitry Bahdanau 等人，2014，*通过联合学习对齐和翻译*进行神经机器翻译；https://arxiv.org/abs/1409.0473。Bahdanau 和其他人表明，他们的注意机制导致对较长序列的性能有很大的改善。

在 MelodyRNN 中，注意力屏蔽 *a* 应用如下:

![](assets/a2fa2dce-f0eb-485b-a755-072fafefcdba.png)

更多详情可以在[https://Magenta . tensor flow . org/2016/07/15/lookback-rnn-attention-rnn/](https://magenta.tensorflow.org/2016/07/15/lookback-rnn-attention-rnn/)的 Magenta 文档中找到。

## 请参见

请注意，Magenta 有不同版本的 MelodyRNN 型号可供选择([https://github . com/Magenta/Magenta/tree/master/Magenta/models/melody _ rnn](https://github.com/magenta/magenta/tree/master/magenta/models/melody_rnn))。除了 MelodyRNN，Magenta 还提供了更多的模型，包括一个用于音乐生成的可变自动编码器，以及许多用于探索和生成音乐的基于浏览器的工具:https://github.com/magenta/magenta。

DeepBeat 是嘻哈节拍一代的项目:[https://github.com/nicholaschiang/deepbeat](https://github.com/nicholaschiang/deepbeat)。

Jukebox 是一个开源项目，基于 Dhariwal 等人的论文 *Jukebox:一个音乐生成模型*(2020；https://arxiv.org/abs/2005.00341。你可以在 https://openai.com/blog/jukebox/的[找到很多音频样本。](https://openai.com/blog/jukebox/)

可以找到 Parag K. Mital 的 NIPS 论文的原始实现，*时域神经音频风格转移*(2017；【https://arxiv.org/abs/1711.11160】，在[https://github . com/pkmital/time-domain-neural-audio-style-transfer](https://github.com/pkmital/time-domain-neural-audio-style-transfer)。