<title>Deep Reinforcement Learning</title> <link rel="stylesheet" href="css/style.css" type="text/css">  Deep Reinforcement Learning

**强化学习**是关于开发目标驱动的代理，通过优化他们在环境中的行为来自动解决问题。这包括对可用数据进行预测和分类，并训练代理成功执行任务。通常，代理是具有与环境交互的能力的实体，并且通过应用来自环境的累积回报方面的反馈来通知未来的行动，从而完成学习。

可以区分三种不同类型的强化学习:

*   基于价值—价值函数提供了对环境当前状态的评估。
*   基于策略—功能根据状态决定操作。
*   基于模型—包括状态转换、奖励和行动计划的环境模型。

在这一章中，我们将从一个相对基本的增强学习用例开始，该用例用于多臂土匪的网站优化，在这里我们将看到一个代理和一个环境，以及它们是如何交互的。然后，我们将继续进行一个简单的控制演示，这里变得有点复杂，我们将看到一个代理环境和一个基于策略的方法，加强。最后，我们将学习如何玩 21 点，在这里我们将使用一个**深度 Q 网络** ( **DQN** )，这是一种基于价值的算法，曾被用于能够玩 DeepMind 在 2015 年创建的 Atari 游戏的造波人工智能。

在本章中，我们将介绍以下配方:

*   优化网站
*   控制侧手翻
*   玩 21 点

# 技术要求

完整的笔记本可以在 GitHub 上在线获得:[https://GitHub . com/packt publishing/Artificial-Intelligence-with-Python-Cookbook/tree/master/chapter 06](https://github.com/PacktPublishing/Artificial-Intelligence-with-Python-Cookbook/tree/master/chapter06)。

# 优化网站

在这个食谱中，我们将处理网站优化。通常，有必要在一个网站上尝试一些改变(或者更好，一个单独的改变)来看看它们将会产生的效果。在所谓的 **A/B 测试**的典型场景中，网站的两个版本将被系统地比较。通过向预定数量的用户显示网页的版本 *A* 和 *B* 来进行 *A/B* 测试。随后，计算统计显著性或置信区间，以便量化点击率的差异，目的是决定保留两个网页变体中的哪一个。

在这里，我们将从强化的角度来看网站优化，对于每个视图(或加载页面的用户)，我们选择最佳版本，给出他们加载网站时的可用数据。在每一条反馈(点击或不点击)之后，我们更新统计数据。与 A/B 测试相比，这一过程可以产生更可靠的结果，此外，随着时间的推移，我们会更频繁地显示最佳网页变体。请注意，我们不限于两个变体，而是可以比较许多变体。

这个网站优化的用例将帮助我们介绍代理和环境的概念，并向我们展示探索和利用之间的权衡。我们将在*中解释这些概念是如何工作的...*一节。

## 怎么做...

为了实现我们的配方，我们需要两个组件:

*   我们的代理决定将哪个网页呈现给用户。
*   该环境是一个测试平台，将为我们的代理提供反馈(点击或不点击)。

因为我们只使用标准 Python，所以不需要安装任何东西，我们可以直接开始实现我们的配方:

1.  我们将首先实现我们的环境。我们认为这是一个多臂强盗问题，我们将在*中解释它是如何工作的...*一节。因此，我们称我们的环境为`Bandit`:

```
import random
import numpy as np

class Bandit:
    def __init__(self, K=2, probs=None):
        self.K = K
        if probs is None:
            self.probs = [
                random.random() for _ in range(self.K)
            ]
        else:
            assert len(probs) == K
            self.probs = probs

        self.probs = list(np.array(probs) / np.sum(probs))
        self.best_probs = max(self.probs)

    def play(self, i):
        if random.random() < self.probs[i]:
            return 1
        else:
            return 0
```

这个 bandit 用可用选项的数量`K`初始化。这将为这些选择中的每一个设置获得点击的概率。在实践中，环境将是真实的用户反馈；这里，我们模拟用户行为。`play()`法玩`ith`机，返奖励`1`或`0`。

2.  现在我们需要与这个环境互动。这就是我们经纪人的切入点。代理必须做出决策，我们会给它一个做出决策的策略。我们还将包括指标收集。抽象代理看起来像这样:

```
class Agent:
    def __init__(self, env):        
        self.env = env
        self.listeners = {}
        self.metrics = {}
        self.reset()

    def reset(self):
        for k in self.metrics:
            self.metrics[k] = []

    def add_listener(self, name, fun):
        self.listeners[name] = fun
        self.metrics[name] = []

    def run_metrics(self, i):
        for key, fun in self.listeners.items():
            fun(self, i, key)

    def run_one_step(self):
        raise NotImplementedError

    def run(self, n_steps):
        raise NotImplementedError
```

任何代理都需要一个与之交互的环境。它需要做出一个单一的决策(`run_one_step(self)`)，为了看看它的决策有多好，我们需要运行一个模拟(`run(self, n_steps)`)。

代理将包含度量函数的查找列表，并且还继承度量集合功能。我们可以通过`run_metrics(self, i)`函数运行指标收集。

我们在这里使用的策略叫做`UCB1`。我们将在*中解释这一策略如何实现...*节:

```

class UCB1(Agent):
    def __init__(self, env, alpha=2.):
        self.alpha = alpha
        super(UCB1, self).__init__(env)

    def run_exploration(self):
        for i in range(self.env.K):
            self.estimates[i] = self.env.play(i)
            self.counts[i] += 1
            self.history.append(i)
            self.run_metrics(i) 
            self.t += 1

    def update_estimate(self, i, r):
        self.estimates[i] += (r - self.estimates[i]) / (self.counts[i] + 1)

    def reset(self):
        self.history = []
        self.t = 0
        self.counts = [0] * self.env.K
        self.estimates = [None] * self.env.K
        super(UCB1, self).reset()

    def run(self, n_steps):
        assert self.env is not None
        self.reset()
        if self.estimates[0] is None:
            self.run_exploration()
        for _ in range(n_steps):
            i = self.run_one_step()
            self.counts[i] += 1
            self.history.append(i)
            self.run_metrics(i)

    def upper_bound(self, i):
        return np.sqrt(
            self.alpha * np.log(self.t) / (1 + self.counts[i])
        )

    def run_one_step(self):
        i = max(
            range(self.env.K),
            key=lambda i: self.estimates[i] + self.upper_bound(i)
        )
        r = self.env.play(i)
        self.update_estimate(i, r)
        self.t += 1
        return i
```

我们的`UCB1`代理需要一个与之交互的环境(一个强盗),以及一个参数 alpha，它衡量探索行为的重要性(与利用最著名的行为相比)。代理维护其一段时间内的选择历史，以及对每个可能选择的估计记录。

我们要看的是`run_one_step(self)`法，通过挑选最乐观的选择来进行单项选择。`run(self, n_step)`方法运行一系列选择，并从环境中获得反馈。

让我们跟踪两个指标:遗憾，这是由于次优选择而发生的预期损失的总和，以及——作为代理的估计与环境的实际配置的收敛的度量 Spearman 等级相关性(`stats.spearmanr()`)。

The **Spearman rank correlation** is equal to the Pearson correlation (often briefly called just the correlation or product-moment correlation) of the ranked variables.

The Pearson correlation between two variables, ![](assets/10c6e91b-16bd-4c97-888b-61a03d42d28d.png) and ![](assets/89063572-b6e5-4e77-9352-47c3cf415b34.png), can be expressed as follows:
![](assets/857a991f-0184-48f7-830c-3cf25c3b0d13.png)
where ![](assets/fe3ffd1f-afae-4547-863d-976bfb6287b0.png) is the covariance of ![](assets/ae4fef2a-7e6e-4d50-8b18-a1e0c2e5f225.png) and ![](assets/6cf6cdb6-4a76-4c2b-8b9b-9bbd8c22cabb.png), and ![](assets/fb5c045d-c5b6-42bd-b5ba-be53ab12c994.png) is the standard deviation of ![](assets/2ece40ee-148d-44a7-aa65-fafddde4480c.png).
The Spearman correlation, instead of operating on the raw scores, is calculated on the ranked scores. A rank transformation means that a variable is sorted by value, and each entry is assigned its order. Given a rank ![](assets/e2246e67-fd3c-48de-8dad-6e80a1d6ecc2.png)of point *i* in *X*, the Spearman rank correlation is calculated like this:
![](assets/39263887-f11f-43a1-a741-a6ef20e92c22.png)
This assesses how well the relationship between two variables can be described as a monotonic, but not necessarily linear (as in the case of Pearson correlation) function. Like the Pearson correlation, the Spearman correlation ranges between ![](assets/4a6caf8b-67d4-4b56-950d-674d0d04c4d9.png) for perfectly negatively correlated and ![](assets/5ac1aeff-9205-43a0-bf15-36df665dcbd7.png) for perfectly correlated. ![](assets/e8592acc-720d-4807-8112-d31a687a20fb.png) means there's no correlation.

我们的跟踪功能是`update_regret()`和`update_rank_corr()`:

```
from scipy import stats

def update_regret(agent, i, key):
    regret = agent.env.best_probs - agent.env.probs[i]
    if agent.metrics[key]:
        agent.metrics[key].append(
            agent.metrics[key][-1] + regret
        )
    else:
        agent.metrics[key] = [regret]

def update_rank_corr(agent, i, key):
    if agent.t < agent.env.K:
        agent.metrics[key].append(0.0)
    else:
        agent.metrics[key].append(
            stats.spearmanr(agent.env.probs, agent.estimates)[0]
        )
```

我们现在可以跟踪这些指标，以便比较`alpha`参数的影响(或多或少的探索)。然后，我们可以观察随着时间的推移收敛和累积的遗憾:

```
random.seed(42.0)
bandit = Bandit(20)
agent = UCB1(bandit, alpha=2.0)
agent.add_listener('regret', update_regret)
agent.add_listener('corr', update_rank_corr)
agent.run(5000)
```

所以我们有 20 种不同的网页选择，我们收集定义好的`regret`和`corr`，并运行`5000`迭代。如果我们绘制这个图，我们可以了解这个代理的表现有多好:

![](assets/1b8966ed-fc1d-42fa-938b-3f25b488b80a.png)

对于第二次运行，我们将把 alpha 改为`0.5`，因此我们将进行更少的探索:

![](assets/773a06ca-1f83-4b15-a55d-891f1d0f067b.png)

我们可以看到，用*α= 0.5 时，*的累积遗憾较少探索，远低于用*α= 2.0 时的*；然而，估计值与环境参数的总体相关性较低。

因此，随着探索的减少，我们的代理对环境的真实参数的模拟也越来越差。这是因为随着探索的减少，较低等级特征的排序没有收敛。例如，尽管他们被列为次优，但他们没有被经常选择来确定他们是最差还是第二差。这就是我们通过较少的探索所看到的，这可能是好的，因为我们可能只关心知道哪个选择是最好的。

## 它是如何工作的...

在这个菜谱中，我们处理了网站优化的问题。我们模拟了用户对不同版本网页的选择，同时实时更新关于每个变体有多好以及应该多长时间显示一次的统计数据。此外，我们比较了探索性场景和剥削性场景的优缺点。

我们把用户对网页的反应框定为一个多臂强盗问题。多臂强盗 ( **MABP** )是一种吃角子老虎机，玩家将一枚硬币投入其中，然后拉动几个杠杆中的一个，每个杠杆都与一个不同的奖励分配相关联，而这个奖励分配对玩家来说是未知的。更一般地，MABP；也被称为 **K 臂土匪问题**)是在一种情况下，在竞争的选择之间分配资源的问题，其中每个选择的结果只是部分已知，但随着时间的推移可能会变得更为人所知。当在做决定时考虑对世界的观察时，这被称为**情境强盗**。

我们已经使用了**置信上限版本 1** ( **UCB1** )算法(奥尔等人，*多臂土匪问题的有限时间分析*，2002)，该算法易于实现。

它的工作原理如下:

*   每个动作玩一次，以获得平均回报的初步估计(探索阶段)。
*   每回合 *t* 更新 *Q(a)* 和 *N(a)* ，按照此公式进行动作*a’*:

![](assets/3d6cfe28-5d39-426b-bba6-64da27e46efa.png)

其中![](assets/b34651d6-def9-4521-ab65-c9107bdadff6.png)是平均奖励的查找表，而![](assets/e18455df-dace-4623-81ff-ac440e5fc909.png)是动作![](assets/781a087f-89c9-4a59-b207-b8424266740d.png)被玩的次数。![](assets/7db2992e-104b-42fa-bc09-a47207dd8c67.png)是参数。

UCB 算法遵循所谓的面对不确定性的乐观主义原则，选择置信区间上 UCB 最高的手臂，而不是估计奖励最高的手臂。它使用一个简单的行为回报均值估计器。

前面等式中的第二项量化了不确定性。不确定性越低，我们越依赖 *Q(a)* 。不确定性随着一个动作的次数线性减少，随着回合数的对数增加。

多臂强盗在许多领域都很有用，包括在线广告、临床试验、网络路由，或者在生产中的机器学习模型的两个或更多版本之间切换。

bandit 算法有许多变种，可以处理更复杂的情况，例如，在选择之间切换的成本，或者像秘书问题这样的有限生命周期的选择。秘书问题的基本情况是，你想从有限的申请人中雇用一名秘书。每一个申请人都被随机地依次面试，并且在面试后会立即做出一个明确的决定(是否录用)。秘书问题也叫婚姻问题。

## 请参见

Ax 库用 Python 实现了很多 bandit 算法:[https://ax.dev/](https://ax.dev/)。

https://facebook.github.io/planout/index.html 的脸书 PlanOut 是一个大规模在线实地实验的图书馆。

作为阅读材料，我们推荐这些:

*   鲁索等人，2007 年，*汤普逊取样教程*(【https://arxiv.org/pdf/1707.02038.pdf】T2)
*   斯泽佩瓦里和拉蒂摩尔，*班迪特算法*，2020 年(网络版可用:【https://tor-lattimore.com/downloads/book/book.pdf】

# 控制侧手翻

侧手翻是开放体操馆提供的一项控制任务，已经研究了很多年。虽然与其他方法相比，它相对简单，但它包含了我们实现强化学习算法所需的所有内容，并且我们在这里开发的所有内容都可以应用于其他更复杂的学习任务。它还可以作为模拟环境中机器人操纵的一个例子。接受要求较低的任务的好处是培训和周转更快。

**OpenAI Gym** is an open source library that can help to develop reinforcement algorithms by standardizing a broad range of environments for agents to interact with. OpenAI Gym comes with hundreds of environments and integrations ranging from robotic control, and walking in 3D to computer games and self-driving cars: [https://gym.openai.com/](https://gym.openai.com/).

下面的 OpenAI Gym 环境截图描述了 cartpole 任务，它包括向左或向右移动手推车，以便在直立位置平衡杆子:

![](assets/6368e342-881c-435a-bf20-f75c1baa6d5e.png)

在这个菜谱中，我们将在 PyTorch 中实现增强策略梯度方法来解决 cartpole 任务。我们开始吧。

## 做好准备

有许多库提供测试问题和环境的集合。集成最多的库之一是 OpenAI Gym，我们将在本菜谱中使用它:

```
pip install gym
```

我们现在可以在食谱中使用 OpenAI 健身房。

## 怎么做...

OpenAI Gym 为我们节省了工作——我们不必自己定义环境，并提出奖励信号，对环境进行编码，或声明哪些行为是允许的。

我们将首先加载环境，为动作选择定义深度学习策略，定义使用该策略选择要执行的动作的代理，最后我们将测试代理在我们的任务中的执行情况:

1.  首先，我们将加载环境。杆子不倒的每一个动作，我们都有奖励。我们有两个可用的移动，向左或向右，以及一个观察空间，该空间包括小车位置和速度以及杆角度和速度的表示， 如下表:

    | 中的 | **观察** | **分钟** | **最大** |
    | 零点 | 推车位置 | -2.4 | 二点四 |
    | 一 | 推车速度 | -Inf | -Inf |
    | 两个 | 磁极角度 | ~-41.8 | ~ 41.8 |
    | 三个 | 尖端的极点速度 | -Inf | -Inf |

你可以在这里找到更多关于这个环境的信息:[https://gym.openai.com/envs/CartPole-v1/](https://gym.openai.com/envs/CartPole-v1/)。

我们可以加载环境并打印这些参数，如下所示:

```
import gym

env = gym.make('CartPole-v1')
print('observation space: {}'.format(
    env.observation_space
))
print('actions: {}'.format(
    env.action_space.n
))
#observation space: Box(4,)
#actions: 2
```

因此，我们确认我们的代理必须处理四个输入和两个动作。我们的代理将被定义为类似于前面的配方，*优化一个网站*，只是这一次，我们将定义代理之外的神经网络。

代理将创建一个策略网络，并使用它来做出决策，直到达到最终状态；然后，它会将累积的奖励输入网络进行学习。先说政策网。

2.  让我们创建一个政策网络。我们将采用一个完全连接的前馈神经网络，它根据观察空间预测移动。这部分基于 PyTorch 实现，可从 https://github . com/py torch/examples/blob/master/reinforcement _ learning/reinforce . py 获得:

```
import torch as T
import torch.nn as nn
import torch.nn.functional as F
import numpy as np

class PolicyNetwork(nn.Module):
    def __init__(
        self, lr, n_inputs,
        n_hidden, n_actions
    ):
        super(PolicyNetwork, self).__init__()
        self.lr = lr
        self.fc1 = nn.Linear(n_inputs, n_hidden)
        self.fc2 = nn.Linear(n_hidden, n_actions)
        self.optimizer = optim.Adam(self.parameters(), lr=self.lr)

        self.device = T.device(
            'cuda:0'
            if T.cuda.is_available()
            else 'cpu:0'
        )
        self.to(self.device)

    def forward(self, observation):
        x = T.Tensor(observation.reshape(-1).astype('float32'),
        ).to(self.device)
        x = F.relu(self.fc1(x))
        x = F.softmax(self.fc2(x), dim=0)
        return x
```

这是一个学习策略的神经网络模块，或者换句话说，是从观察到行动的映射。这样就建立了一个两层的神经网络，一个隐藏层，一个输出层，输出层的每个神经元对应一个可能的动作。我们设置以下参数:

*   `lr`:学习率
*   `n_inputs`:输入数量
*   `n_hidden`:隐藏神经元的数量
*   `n_actions`:输入维度

3.  我们现在准备定义我们的代理:

```
class Agent:
    eps = np.finfo(
        np.float32
    ).eps.item()

    def __init__(self, env, lr, params, gamma=0.99):
        self.env = env
        self.gamma = gamma
        self.actions = []
        self.rewards = []
        self.policy = PolicyNetwork(
            lr=lr,
            **params
        )

    def choose_action(self, observation):
        output = self.policy.forward(observation)
        action_probs = T.distributions.Categorical(
            output
        )
        action = action_probs.sample()
        log_probs = action_probs.log_prob(action)
        action = action.item()
        self.actions.append(log_probs)
        return action, log_probs
```

代理评估策略以采取行动并获得奖励。`gamma`是贴现因子。

使用`choose_action(self, observation)`方法，我们的代理根据观察结果选择一个动作。根据我们网络中的分类分布对动作进行采样。

我们省略了`run()`方法，如下所示:

```
    def run(self):
        state = self.env.reset()
        probs = []
        rewards = []
        done = False
        observation = self.env.reset()
        t = 0
        while not done:
            action, prob = self.choose_action(observation.reshape(-1))
            probs.append(prob)
            observation, reward, done, _ = self.env.step(action)
            rewards.append(reward)
            t += 1

        policy_loss = []
        returns = []
        R = 0
        for r in rewards[::-1]:
            R = r + self.gamma * R
            returns.insert(0, R)
        returns = T.tensor(returns)
        returns = (returns - returns.mean()) / (returns.std() + self.eps)

        for log_prob, R in zip(probs, returns):
            policy_loss.append(-log_prob * R)

        if(len(policy_loss)) > 0:
            self.policy.optimizer.zero_grad()
            policy_loss = T.stack(policy_loss, 0).sum()
            policy_loss.backward()
            self.policy.optimizer.step()
        return t
```

与前一个方法类似的`run(self)`方法，*优化一个网站*，在环境中运行一个完整的模拟，直到结束。这是直到极点即将倒下或达到 500 步(这是`env._max_episode_steps`的默认值)。

4.  接下来，我们将测试我们的代理。我们将通过模拟与环境的交互来开始在环境中运行我们的代理。为了得到一个更清晰的学习率曲线，我们将把`env._max_episode_steps`设置为`10000`。这意味着模拟在 10，000 步后停止。如果我们把它留在默认值`500`，算法会在某个性能上停滞不前，或者一旦达到大约 500 步，它的性能就会停滞不前。相反，我们正试图进一步优化:

```
env._max_episode_steps = 10000
input_dims = env.observation_space.low.reshape(-1).shape[0]
n_actions = env.action_space.n

agent = Agent(
    env=env,
    lr=0.01,
    params=dict(
        n_inputs=input_dims,
        n_hidden=10,
        n_actions=n_actions
    ),
    gamma=0.99,
)
update_interval = 100
scores = []
score = 0
n_episodes = 25000
stop_criterion = 1000
for i in range(n_episodes):
    mean_score = np.mean(scores[-update_interval:])
    if (i>0) and (i % update_interval) == 0:
        print('Iteration {}, average score: {:.3f}'.format(
            i, mean_score
        ))
        T.save(agent.policy.state_dict(), filename)

    score = agent.run()
    scores.append(score)
    if score >= stop_criterion:
        print('Stopping. Iteration {}, average score: {:.3f}'.format(
            i, mean_score
        ))
        break
```

我们应该会看到以下输出:

```
Iteration 100, average score: 31.060
Iteration 200, average score: 132.340
Iteration 300, average score: 236.550
Stopping. Iteration 301, average score: 238.350
```

在模拟过程中，我们看到每 100 次迭代就有一次更新，平均分数是自上次更新以来的。一旦分数达到 1000，我们就停止。这是我们一段时间的得分:

![](assets/99f6fdcb-3dc2-4054-aaba-a407e01c9993.png)

我们可以看到，我们的政策在不断改进——网络正在成功地学习操纵横拉杆。请注意，您的结果可能会有所不同。网络可以学习得更快或更慢。

在下一节中，我们将研究这个算法实际上是如何工作的。

## 它是如何工作的...

在这个菜谱中，我们已经在一个 cartpole 控制场景中看到了一个基于策略的算法。让我们更详细地看看其中的一些内容。

策略梯度方法寻找具有给定梯度上升的策略，该策略相对于策略参数最大化累积回报。我们已经实现了一种无模型的基于策略的方法，即增强算法(R. Williams，*用于联结主义者增强学习的简单统计梯度跟踪算法*，1992)。

在基于策略方法的核心，我们有一个策略函数。策略函数在环境![](assets/e6b6eae1-47c1-478a-b742-0ac3598336cd.png)和动作![](assets/c5de2c55-5cb2-46e2-a90b-05143184484f.png)上定义，并返回给定环境下动作的概率。在![](assets/f6669627-9c78-4ba4-b934-f1369dffd8f9.png)离散选择的情况下，我们可以使用 softmax 函数:

![](assets/bd1ada1f-8111-43b9-b502-e5887ebbe58e.png)

这就是我们在政策网络中所做的，这有助于我们做出行动选择。

价值函数![](assets/589622ea-134f-422e-83fd-dde2b7f8bc4e.png)(有时用![](assets/080a47ab-3d88-49c8-bdcb-4208df48cb30.png)表示)返回给定环境中任何行为的回报。对策略的更新定义如下:

![](assets/bbb94709-9aa8-43cd-ab49-642190d0fe0c.png)

其中![](assets/c0ca3cce-5a56-4fff-99e0-87274906c240.png)是学习率。

在参数初始化之后，每次执行一个动作时，通过应用这个更新函数，增强算法继续进行。

您应该能够在任何健身房环境中运行我们的实现，几乎不需要做任何更改。我们特意加入了一些东西(例如，将观察结果重新整形为一个向量)以使其更容易重用；但是，您应该确保您的网络架构符合您观察的性质。例如，如果您的观察是图像，您可能希望对时间序列使用 1D 卷积网络或递归神经网络(如股票交易或声音)或 2D 卷积。

## 还有更多...

还有一些我们可以玩的东西。首先，我们希望看到代理与杆子交互，其次，我们可以使用一个库，而不是从头开始实现代理。

### 在环境中观察我们的代理

我们可以玩数百个游戏或尝试不同的控制任务。如果我们想在 Jupyter 笔记本上实际观察我们的代理与环境的交互，我们可以这样做:

```
from IPython import display
import matplotlib.pyplot as plt
%matplotlib inline

observation = env.reset()
img = plt.imshow(env.render(mode='rgb_array'))
for _ in range(100):
    img.set_data(env.render(mode='rgb_array'))
    display.display(plt.gcf())
    display.clear_output(wait=True)
    action, prob = agent.choose_action(observation)
    observation, _, done, _ = agent.env.step(action)
    if done:
        break
```

我们现在应该看到我们的代理与环境交互。

如果您使用远程连接(比如在 Google Colab 上运行)，您可能需要做一些额外的工作:

```
!sudo apt-get install -y xvfb ffmpeg 
!pip install 'gym==0.10.11'
!pip install 'imageio==2.4.0'
!pip install PILLOW
!pip install 'pyglet==1.3.2'
!pip install pyvirtualdisplay
display = pyvirtualdisplay.Display(
    visible=0, size=(1400, 900)
).start()
```

在下一节中，我们将使用一个在库中实现的强化算法，`RLlib`。

### 使用 RLlib 库

我们可以利用 Python 库和包中的实现，而不是从头开始实现算法。例如，我们可以训练 PPO 算法(舒尔曼等人，*近似策略优化算法*，2017)，它带有`RLlib`包。RLlib 是我们在*Python 人工智能入门*菜谱[第 1 章](87098651-b37f-4b05-b0ee-878193f28b95.xhtml)、*Python 人工智能入门*中遇到的`Ray`库的一部分。PPO 是一种策略梯度方法，它引入了一个可通过梯度下降进行优化的替代目标函数:

```
import ray
from ray import tune
from ray.rllib.agents.ppo import PPOTrainer

ray.init()
trainer = PPOTrainer

analysis = tune.run(
    trainer,
    stop={'episode_reward_mean': 100},
    config={'env': 'CartPole-v0'},
    checkpoint_freq=1,
)
```

这将运行培训。您的代理将存储在本地目录中，因此您可以稍后加载它们。RLlib 允许您使用 PyTorch 和 TensorFlow 和`'torch': True`选项。

## 请参见

一些强化库附带了许多深度强化学习算法的实现:

*   OpenAI 的基线(需要 TensorFlow 版本< 2): [https://github.com/openai/baselines](https://github.com/openai/baselines)
*   RLlib(是`Ray`的一部分)是一个可扩展(和分布式)强化学习的库:[https://docs.ray.io/en/master/rllib-algorithms.html](https://docs.ray.io/en/master/rllib-algorithms.html)。
*   Machin 是一个基于 PyTorch 的强化学习库:[https://github.com/iffiX/machin](https://github.com/iffiX/machin)。
*   TF-Agents 提供了许多工具和算法(它与 TensorFlow 版和更高版本兼容):[https://github.com/tensorflow/agents](https://github.com/tensorflow/agents)。

请注意，这些库的安装可能需要一段时间，并可能占用您的千兆字节的硬盘空间。

最后，OpenAI 提供了一个存储库，里面有许多与强化学习相关的教育资源:[https://spinningup.openai.com/](https://spinningup.openai.com/)。

# 玩 21 点

强化学习的基准之一是游戏。许多与游戏相关的不同环境都是由研究人员或爱好者设计的。游戏中的一些里程碑已经在[第一章](87098651-b37f-4b05-b0ee-878193f28b95.xhtml)、*Python 人工智能入门*中提到。对许多人来说，亮点肯定是击败国际象棋和围棋的人类冠军——1997 年的国际象棋冠军加里·卡斯帕罗夫和 2016 年的围棋冠军李·塞多尔——以及 2015 年在雅达利游戏中达到超人的表现。

在这个食谱中，我们从一个最简单的游戏环境开始:21 点。21 点有一个有趣的特性，它与现实世界有共同之处:非决定论。

二十一点是一种纸牌游戏，最简单的形式是你和一个发牌人对打。你面前有一副牌，你可以击中，这意味着你可以再得到一张牌，或棍子，庄家可以抽牌。为了赢，你想要尽可能接近卡片分数 21，但是不要超过 21。

在这个菜谱中，我们将在 Keras 中实现一个模型，在给定一个环境配置的情况下，不同操作的价值，一个价值函数。我们将实现的变体称为 DQN，用于 2015 年雅达利里程碑成就。我们开始吧。

## 做好准备

如果您还没有安装依赖项，我们需要安装它。

我们将使用 OpenAI Gym，我们需要安装它:

```
pip install gym
```

我们将利用体育馆的环境来玩 21 点。

## 怎么做...

我们需要一个代理来维护它的行为所产生的影响的模型。这些动作从它的记忆中回放出来，以达到学习的目的。我们将从记录过去学习经历的记忆开始:

1.  让我们实现这个内存。这个存储器本质上是一个 FIFO 队列。在 Python 中，你可以使用一个队列；然而，我们发现 PyTorch 示例中重放内存的实现非常优雅，因此这是基于 Adam Paszke 的 PyTorch 设计:

```
#this is based on https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html
from collections import namedtuple

Transition = namedtuple(
    'Transition',
    ('state', 'action', 'next_state', 'reward')
)

class ReplayMemory:
    def __init__(self, capacity=2000):
        self.capacity = capacity
        self.memory = []
        self.position = 0

    def push(self, *args):
        if len(self.memory) < self.capacity:
            self.memory.append(None)
        self.memory[self.position] = Transition(*args)
        self.position = (self.position + 1) % self.capacity

    def sample(self, batch_size):
        batch = random.sample(self.memory, batch_size)
        batch = Transition(
            *(np.array(el).reshape(batch_size, -1) for el in zip(*batch))
        )
        return batch

    def __len__(self):
        return len(self.memory)
```

我们只需要两种方法:

*   我们需要推出新的记忆，如果容量达到极限，就在这个过程中覆盖旧的记忆。
*   我们需要对记忆进行取样以便学习。

后一点值得强调:我们不是把所有的记忆都用于学习，而是只取其中的一部分。

在`sample()`方法中，我们做了一些修改，以使我们的数据具有正确的形状。

2.  让我们看看我们的代理:

```
import random
import numpy as np
import numpy.matlib
import tensorflow as tf
from tensorflow.keras import layers
from tensorflow.keras import optimizers
from tensorflow.keras import initializers

class DQNAgent():
    def __init__(self, env, epsilon=1.0, lr=0.5, batch_size=128):
        self.env = env
        self.action_size = self.env.action_space.n
        self.state_size = env.observation_space
        self.memory = ReplayMemory()
        self.epsilon = epsilon
        self.lr = lr
        self.batch_size = batch_size
        self.model = self._build_model()

    def encode(self, state, action=None):
        if action is None:
            action = np.reshape(
                list(range(self.action_size)),
                (self.action_size, 1)
            )
            return np.hstack([
                np.matlib.repmat(state, self.action_size, 1),
                action
            ])
        return np.hstack([state, action])

    def play(self, state):
        state = np.reshape(state, (1, 3)).astype(float)
        if np.random.rand() <= self.epsilon:
            action = np.random.randint(0, self.action_size)
        else:
            action_value = self.model.predict(self.encode(state)).squeeze()
            action = np.argmax(action_value)

        next_state1, reward, done, _ = self.env.step(action)
        next_state = np.reshape(next_state1, (1, 3)).astype(float)
        if done:
            self.memory.push(state, action, next_state, reward)
        return next_state1, reward, done

    def learn(self):
        if len(self.memory) < self.batch_size:
            return
        batch = self.memory.sample(
            self.batch_size
        )
        result = self.model.fit(
            self.encode(batch.state, batch.action),
            batch.reward,
            epochs=1,
            verbose=0
        )
```

请注意`play()`方法开头的动作选择。我们通过掷骰子来决定我们是想随机选择一个动作，还是想遵从模型的判断。这被称为**ε-贪婪行动选择**，它导致更多的探索和对环境更好的适应。

代理附带了一些要配置的超参数:

*   `lr`:网络的学习率。
*   `batch_size`:内存采样和网络训练的批量。
*   `epsilon`:这个因素，在`0`和`1`之间，控制着我们在回应中想要多少随机性。`1`表示随意探索，`0`表示根本不探索(剥削)。

我们发现这三个参数可以显著改变我们学习的轨迹。

我们从清单中省略了一个方法，它定义了神经网络模型:

```
    def _build_model(self):
        model = tf.keras.Sequential([
            layers.Dense(
                100,
                input_shape=(4,),
                kernel_initializer=initializers.RandomNormal(stddev=5.0),
                bias_initializer=initializers.Ones(),
                activation='relu',
                name='state'
            ),
            layers.Dense(
                2,
                activation='relu'
            ),
            layers.Dense(1, name='action', activation='tanh'),
        ])
        model.summary()
        model.compile(
            loss='hinge',
            optimizer=optimizers.RMSprop(lr=self.lr)
        )
        return model
```

这是一个三层神经网络，有两个隐藏层，一个有 100 个神经元，另一个有 2 个神经元，带有 ReLU 激活，输出层有 1 个神经元。

3.  让我们加载环境并初始化我们的代理。我们按如下方式初始化代理和环境:

```
import gym
env = gym.make('Blackjack-v0')

agent = DQNAgent(
    env=env, epsilon=0.01, lr=0.1, batch_size=100
)
```

这将加载**二十一点**开放 AI 健身房环境和我们的**dq 管理员**，如该菜谱的*步骤 2* 中所实现的。

`epsilon`参数定义了代理的随机行为。我们不想把这个定得太低。学习率是我们在实验后选择的一个值。由于我们玩的是随机纸牌游戏，如果我们设置得太高，算法会很快忘记。批量大小和记忆大小参数分别对我们每一步训练多少和我们对奖励历史的记忆多少很重要。

我们可以看到这个网络的结构(如 Keras 的`summary()`方法所示):

```
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
state (Dense)                (None, 100)               500       
_________________________________________________________________
dense_4 (Dense)              (None, 2)                 202       
_________________________________________________________________
action (Dense)               (None, 1)                 3         
=================================================================
Total params: 705
Trainable params: 705
Non-trainable params: 0
```

对于模拟，我们的一个关键问题是`epsilon`参数的值。如果我们把它定得太低，我们的代理将什么也学不到；如果我们把价格定得太高，我们会赔钱，因为代理会随机变动。

4.  现在让我们玩 21 点。我们选择以线性方式稳步减少`epsilon`,然后我们进行多轮开发。当`epsilon`到达`0`时，我们停止学习:

```
num_rounds = 5000
exploit_runs = num_rounds // 5
best_100 = -1.0

payouts = []
epsilons = np.hstack([
 np.linspace(0.5, 0.01, num=num_rounds - exploit_runs), 
 np.zeros(exploit_runs)
])
```

这是实际开始玩 21 点的代码:

```
from tqdm.notebook import trange

for sample in trange(num_rounds):
  epsilon = epsilons[sample]
  agent.epsilon = epsilon
  total_payout = 0
  state = agent.env.reset()
  for _ in range(10):
    state, payout, done = agent.play(state)
    total_payout += payout
    if done:
      break
  if epsilon > 0:
    agent.learn()

  mean_100 = np.mean(payouts[-100:])
    if mean_100 > best_100:
      best_100 = mean_100

  payouts.append(total_payout)
  if (sample % 100) == 0 and sample >= 100:
    print('average payout: {:.3f}'.format(
      mean_100
    ))
    print(agent.losses[-1])

print('best 100 average: {:.3f}'.format(best_100))
```

您可以看到，我们正在收集网络培训损失的统计数据，以便在模拟过程中进行监控，我们收集了任何连续 100 次游戏的最大支出。

在 OpenAI Gym 中，奖励，或者如果我们想保持在 21 点的术语范围内，支出可以是-1(我们输了)，0(没有)，或者 1(我们赢了)。随着时间的推移，我们学习到的策略的回报如下:

![](assets/b85f5091-03ed-459e-89a4-74ec5bd1a1a4.png)

由于可变性很大，我们没有显示原始数据，而是用 100 和 1，000 的移动平均值绘制了这条曲线，结果是两条线，一条是高度可变的，另一条是平滑的，如图所示。

我们确实看到支出随着时间的推移而增加。但是，我们还是在 0 以下，也就是平均亏损。即使我们在开发阶段停止学习，这种情况也会发生。

我们的 21 点环境没有一个奖励阈值，在这个阈值上它被认为是解决了；但是一篇写出来的列举了 100 个平均 1.0 的最佳剧集，也是我们达到的:[https://gym . open ai . com/evaluations/eval _ 21 dt 2 zxjtbka 1 tjg 9 nb 8 eg/](https://gym.openai.com/evaluations/eval_21dT2zxJTbKa1TJg9NB8eg/)。

## 它是如何工作的...

在这个食谱中，我们看到了强化学习中更高级的算法，更具体地说，是基于值的算法。在基于价值的强化学习中，算法会构建价值函数的估计器，进而让我们选择策略。

代理人值得多做一些评论。如果你读过前面的菜谱，*控制一个翻筋斗*，你可能会认为实际上没有那么多事情发生——有一个网络，一个决定行动的`play()`方法，和一个`learn()`方法。代码比较小。一个基本的阈值策略(我的牌总数是 17 吗？)已经相当成功了，但是希望我们在这个菜谱中展示的内容仍然能够对更复杂的用例有所启发和帮助。与我们之前看到的政策网络相反，这一次，网络不是直接建议最佳行动，而是将环境和行动的组合作为输入，并输出预期回报。我们的模型是两层的前馈模型，其中具有两个神经元的隐藏层在由单个神经元组成的最终层中求和。代理人以一种ε-贪婪的方式进行博弈——它以概率`epsilon`进行随机移动；否则，它会根据自己的知识做出最佳选择。`play`功能通过比较所有可用行动的预期结果，建议具有最高效用的行动。

Q 值函数![](assets/d733147e-8d36-4d57-876e-4e5fdf87333e.png)定义如下:

![](assets/51024323-ff6d-4422-8ea4-9b27fbeedc79.png)

其中![](assets/17a83479-e49b-49a6-8e1f-44a678fcda45.png)是在时间![](assets/55e196a8-e2ee-400b-ad0d-7a77a02bd949.png)的奖励、状态和动作。![](assets/90241e7a-a629-4877-ab21-16b6c7fb5ab6.png)是贴现因子；策略![](assets/e1322055-52a6-4124-b760-ac3c65497713.png)选择动作。

在最简单的情况下，![](assets/39e662a3-4545-4d6c-8ad5-096eb82d378e.png)可以是一个查找表，每个状态-动作对都有一个条目。

最佳 Q 值函数定义如下:

![](assets/43e80260-758c-4855-84fc-685a049d5ec6.png)

因此，最佳策略可以根据以下公式确定:

![](assets/1ea8b645-489f-42ab-ade1-0629413ee7ca.png)

在**神经拟合 Q-学习** ( **NFQ** )(里德米勒，*神经拟合 Q 迭代-首次体验数据高效神经强化学习方法*，2005)中，神经网络在给定状态下运行前向传递，输出对应于可用动作。可以根据平方误差用梯度下降来更新神经 Q 值函数:

![](assets/50720e42-6f66-4174-bfb3-4115291522f8.png)

![](assets/dcb166d9-47c6-4b5a-aa89-bd3ba887275c.png)

其中![](assets/84083e9d-cbc5-435f-bbae-54a060914a89.png)为迭代![](assets/25509c06-94fd-4d01-8fce-70c9bd872a88.png)时的参数，![](assets/ff3f98a7-5e15-4a58-9d7c-b14ec97e168f.png)为下一时间步的动作和状态。

DQN (Mnih 等人，*用深度强化学习玩雅达利*，2015)建立在 NFQ 的基础上，但引入了一些变化。这些包括基于来自重放存储器的随机样本，每隔几个迭代仅小批量地更新参数。由于在原始论文中，算法是从屏幕上的像素值学习的，网络的第一层是卷积的(我们将在第七章、*高级图像应用*中介绍这些)。

## 请参见

这是萨顿和巴尔托的开创性著作*强化学习:介绍*:【http://incompleteideas.net/book/the-book-2nd.html】T2 的网站。

他们在里面描述了一个简单的 21 点代理。如果你在找其他的卡牌游戏，可以看看 neuron-poker，一个 OpenAI 的扑克环境；他们实现了 https://github.com/dickreuter/neuron_poker 和其他算法:。

关于 DQNs 和如何使用它的更多细节，我们推荐阅读 Mnih 等人的文章，*用深度强化学习玩雅达利*:【https://arxiv.org/abs/1312.5602】T2。

最后，DQN 及其继任者双 DQN 和决斗 dqn 构成了 AlphaGo 的基础，alpha Go 已经在《自然:【https://www.nature.com/articles/nature24270】杂志上发表了*在没有人类知识的情况下掌握围棋*(西尔弗等人，2017)。