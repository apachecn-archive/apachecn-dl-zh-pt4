<title>Deep Reinforcement Learning</title> <link rel="stylesheet" href="css/style.css" type="text/css"> 

# 深度强化学习

本章从对**强化学习** ( **RL** )的基本介绍开始，包括代理、状态、动作、奖励、政策。它扩展到基于**深度学习** ( **DL** )的架构，用于 RL 问题，如策略梯度方法、Deep-Q 网络和行动者-批评家模型。本章将解释如何在 OpenAI Gym 环境中使用这些深度学习架构和动手代码来解决顺序决策问题。

具体而言，将涵盖以下内容:

*   RL 简介
*   用 DL 对付 RL
*   PyTorch 中的策略梯度和代码遍历
*   PyTorch 中的 Deep-Q 网络和代码遍历
*   PyTorch 中的演员-评论家网络和代码遍历
*   强化学习在现实世界中的应用

<title>Introduction to RL</title> <link rel="stylesheet" href="css/style.css" type="text/css"> 

# RL 简介

RL 是机器学习的一个分支，其中代理学习在给定环境中的最佳行为。代理执行某些动作并观察奖励/结果。它学习将情况映射到行动的过程，以最大化回报。

RL 过程可以被建模为迭代循环，并且可以被表示为被称为**马尔可夫决策过程** ( **MDP** )的数学框架。以下步骤概述了发生的过程:

1.  RL 代理从环境接收状态(*s[0])。*
2.  给定当前状态( *s [0]* )，RL 代理采取动作(*a[0])。在这个阶段，它所采取的行动是随机的，因为代理人事先不知道如果它执行了这个行动会得到什么样的回报。*

3.  在第一个动作发生后，代理现在可以被认为处于状态*s[1]。*
4.  此时，环境给代理一个奖励( *r [1]* )。

这个循环不断重复；它输出一系列的状态和动作，并观察回报。

这个过程本质上是学习 MDP 策略的算法:

![](assets/544cc9ea-b434-4e34-bc3b-3e66e7d2593f.png)

与给定行动相关的每个时间步的累积奖励可表示如下:

![](assets/933d3e32-acf8-410b-96d5-3f2c3f99c061.png)

在某些应用中，对及时收到的奖励给予更大的权重可能是有益的。例如，你今天收到 100 美元会比 5 年后收到更好。为了体现这一点，通常引入一个贴现因子，𝛾.

累积折扣奖励表示如下:

![](assets/c8d8ace2-92d3-45f2-b9e4-59a2123c6aab.png)

在 RL 中一个重要的考虑因素是奖励可能是不频繁的和延迟的。在有长期延迟奖励的情况下，追溯哪一系列行动促成了奖励可能是一个挑战。

<title>Model-based RL</title> <link rel="stylesheet" href="css/style.css" type="text/css"> 

# 基于模型的 RL

基于模型的 RL 模拟环境的行为。它预测采取行动后的下一个状态。它可以用数学方法表示为概率分布:

![](assets/e9b54c93-04ea-441c-a726-90485becbb49.png)

这里， *p* 表示模型， *x* 是状态， *a* 是控制或动作。

这个概念可以通过考虑车杆的例子来证明。目标是让连接到手推车的杆子保持直立，代理可以在两个可能的动作之间做出决定:向左移动手推车或向右移动手推车。在下面的屏幕截图中，P 模拟了采取某个动作后杆子的角度:

![](assets/b1fb2e62-88aa-4801-8152-f09c6e503f75.png)

下图描述了后续时间步中θ的概率分布输出:

![](assets/d739342a-0b14-480d-b1d7-0d627b8fab81.png)

在这种情况下，模型描述了物理定律；然而，模型可以是任何东西，这取决于应用程序。另一个例子是，模型可以建立在国际象棋游戏的规则上。

基于模型的 RL 的核心概念是使用模型和成本函数来定位动作的最优路径，或者换句话说，状态和动作的轨迹，𝝉:

![](assets/ed2bd606-48cc-42b3-9f75-93b7bad085f0.png)

基于模型的算法的缺点是，随着状态空间和动作空间变大，它们可能变得不切实际。

<title>Model-free RL</title> <link rel="stylesheet" href="css/style.css" type="text/css"> 

# 无模型 RL

无模型算法依靠试错来更新知识。因此，它们不需要空间来存储状态和动作的所有组合。策略梯度、价值学习或其他无模型 RL 用于寻找采取最佳行动以获得最大回报的策略。无模型方法和基于模型方法的一个关键区别是，无模型方法在真实环境中学习。

<title>Comparing on-policy and off-policy</title> <link rel="stylesheet" href="css/style.css" type="text/css"> 

# 比较政策内和政策外

该策略定义代理的行为方式；它告诉代理在每个状态下如何行动。每个 RL 算法必须遵循某种类型的策略来决定它将如何动作。

代理试图学习的策略函数可以表示如下，其中 *θ* 是参数向量， *s* 是特定状态， *a* 是动作:

![](assets/f9f9bd17-cf05-4a79-acb6-4019878566f9.png)

策略上的代理基于其当前动作学习价值(期望的折扣回报),并且从当前策略中导出。非策略学习基于从另一个策略获得的动作的值，例如 Q-learning 中的贪婪策略，我们接下来将介绍它。

<title>Q-learning</title> <link rel="stylesheet" href="css/style.css" type="text/css"> 

# q 学习

Q-learning 是一种无模型的 RL 算法，通过创建一个表来计算每个状态下每个行为的最大预期未来回报。它被认为是不符合策略的，因为 Q 学习功能从当前策略之外的动作中学习。

当 Q 学习被执行时，Q 表被创建，其中列代表可能的动作，行代表状态。Q 表中每个单元格的值将是给定状态和动作的最大预期未来回报:

![](assets/16a3f00d-ee83-4eca-9f42-211fbc6ff457.png)

如果采取最佳策略的行动，每个 Q 表分数将是最大的预期未来回报。为了学习 Q 表中的每个值，使用 Q 学习算法。

**Q 函数**(或**动作值函数**)接受两个输入:状态和奖励。Q 函数返回该状态下该行为的预期未来回报。它可以表示为:

![](assets/52a32172-0af4-4b43-8de4-d8ddbd66bef4.png)

Q 函数实际上是在 Q 表中滚动，以找到与当前状态相关联的行和与动作相关联的列。从这里，它返回 Q 值，也就是相应的预期未来回报。

在下图所示的手推车杆示例中考虑这一点。在其当前状态下，向左移动应该比向右移动具有更高的 Q 值:

![](assets/f82716fa-e071-471f-a14b-d0a302ed4792.png)

随着环境的探索，Q 表被更新以给出更好的近似值。Q 学习算法的过程如下:

1.  Q 表被初始化。
2.  基于当前 Q 值估计来选择当前状态下的动作( *s* )。
3.  执行一个动作( *a* )。
4.  衡量奖励( *r* )。
5.  q 使用贝尔曼方程更新。

下面是贝尔曼方程:

![](assets/3ec8ae17-bff4-4aaf-b145-c9c51d2b1f23.png)

*重复步骤 2-5* ，直到达到最大集数或手动停止训练。

Q 学习算法可以表示为下面的等式:

![](assets/187fb8a6-d2c7-4d56-8d07-b763fcb179a4.png)

<title>Value methods</title> <link rel="stylesheet" href="css/style.css" type="text/css"> 

# 价值方法

价值学习是一种方法，通常是许多 RL 方法中的关键构建模块。价值函数 *V(s)* 表示代理所处的状态有多好。它等于代理人从状态 *s* 开始的期望总报酬。总的期望报酬取决于代理人选择行动的策略。如果代理使用给定的策略(𝛑)来选择其操作，则相应的值函数由以下公式给出:

![](assets/07835c7c-d756-46bf-b0fb-282981fb0c05.png)

在手推车杆子的例子中考虑到这一点，我们可以使用杆子停留的时间长度来衡量回报。在下面的屏幕截图中，与状态 s2 相比，状态 s1 下柱子保持直立的概率更高。因此，对于大多数策略，状态 s1 可能具有更高的价值函数(更高的预期未来回报):

![](assets/cb878371-7a0e-4aa1-b524-0a0ca315abd9.png)

对于所有状态，存在一个比其他函数具有更高值的最佳值函数，这可以表示为:

![](assets/d4edadb1-6ba8-42d0-a98e-f3ba9a938e9e.png)

存在对应于最优值函数的最优策略:

![](assets/6b06efaa-6bd3-4e94-a610-3d73ae9bb079.png)

有几种不同的方法可以找到最佳策略。这被称为政策评估。

<title>Value iteration</title> <link rel="stylesheet" href="css/style.css" type="text/css"> 

# 价值迭代

值迭代是通过迭代改进 *V(s)* 的估计来计算最优状态-值函数的过程。首先，该算法将 *V(s)* 初始化为随机值。从那里，它重复更新 *Q(s，a)* 和 *V(s)* 的值，直到它们收敛。值迭代收敛到最优值。

下面是值迭代伪代码:

![](assets/817dc174-bcaa-4d4c-8ccb-b77e5a4fc248.png)

<title>Coded example – value iteration</title> <link rel="stylesheet" href="css/style.css" type="text/css"> 

# 编码示例-值迭代

考虑一个这样的例子，我们将使用 OpenAI Gym 中的冰湖环境。在这个环境中，玩家要想象他们正站在一个冰冻的湖面上，那里并不是所有的东西都结冰了。目标是从地点 S 移动到地点 G，而不掉进洞里:

![](assets/5b6c1c9d-f34d-4969-b2e8-0c617753eebc.png)

网格上的字母代表以下内容:

*   s:起点，安全
*   外宾:冰面，安全
*   洞，不安全
*   目标

代理可以采取四种可能的移动:向左、向右、向下和向上，分别表示为 0、1、2 和 3。因此，在(4 x 4)中有 16 种可能的状态。对于每个 H 状态，代理收到-1 的奖励，并且在达到目标时，收到+1 的奖励。

为了在代码中实现值迭代，我们首先导入我们希望使用的相关库，并初始化`FrozenLake`环境:

```py
import gym
import numpy as np
import time, pickle, os
env = gym.make('FrozenLake-v0')
```

现在，我们分配变量:

```py
# Epsilon for an epsilon greedy approach
epsilon = 0.95 
total_episodes = 1000
# Maximum number of steps to be run for every episode
maximum_steps = 100
learning_rate = 0.75
# The discount factor
gamma = 0.96 
```

从这里，我们初始化 Q 表，其中`env.observation_space.n`是状态的数量，`env.action_space.n`是动作的数量:

```py
Q = np.zeros((env.observation_space.n, env.action_space.n))
```

定义代理选择动作和学习的功能:

```py
def select_action(state):
    action=0
    if np.random.uniform(0, 1) < epsilon:

        # If the random number sampled is smaller than epsilon then a random action is chosen.

        action = env.action_space.sample()
    else:
        # If the random number sampled is greater than epsilon then we choose an action having the maximum value in the Q-table
        action = np.argmax(Q[state, :])
    return action

def agent_learn(state, state_next, reward, action):
    predict = Q[state, action]
    target = reward + gamma * np.max(Q[state_next, :])
    Q[state, action] = Q[state, action] + learning_rate * (target - predict)
```

从这里，我们可以开始运行剧集，并将 Q 表导出到一个 pickled 文件:

```py
for episode in range(total_episodes):
    state = env.reset()
    t = 0

    while t < maximum_steps:
        env.render()
        action = select_action(state) 
        state_next, reward, done, info = env.step(action) 
        agent_learn(state, state_next, reward, action)
        state = state_next
        t += 1       
        if done:
            break
        time.sleep(0.1)

print(Q)
with open("QTable_FrozenLake.pkl", 'wb') as f:
    pickle.dump(Q, f)
```

通过运行前面的代码，我们可以看到该流程的运行情况:

![](assets/c6990f0e-f1b0-4bb1-a372-28c471906c91.png)

<title>Policy methods</title> <link rel="stylesheet" href="css/style.css" type="text/css"> 

# 政策方法

在基于策略的 RL 中，目标是找到做出最有价值决策的策略，并且可以用数学方法表示如下:

![](assets/49c33b1b-779e-4963-8486-f7788f2e7333.png)

RL 中的策略可以是确定性的，也可以是随机的:

![](assets/4e00190f-f2b8-41a2-bd8e-557f6c6934bb.png)

随机策略输出概率分布，而不是单个离散值:

![](assets/a3f3c0c6-2914-4ae7-b160-2a25e43ad2a7.png)

我们可以用数学方法表示目标，如下所示:

![](assets/94eca5ea-42aa-4d29-8103-00b2c31419d8.png)

<title>Policy iteration</title> <link rel="stylesheet" href="css/style.css" type="text/css"> 

# 策略迭代

在值迭代期间，有时最优策略在值函数之前收敛，因为它只关心找到最优策略。可以执行另一种称为策略迭代的算法来获得最佳策略。这是在每次策略评估发生后，下一个策略基于价值函数，直到策略收敛。下图说明了策略迭代过程:

![](assets/0e0fdfc1-2128-4f6b-b2bb-0d1369355d0c.png)

因此，与值迭代算法不同，策略迭代算法保证收敛到最优策略。

以下是策略迭代伪代码:

![](assets/dd14fc6f-41c2-4711-ad78-adb17f88ed82.png)

<title>Coded example – policy iteration</title> <link rel="stylesheet" href="css/style.css" type="text/css"> 

# 编码示例–策略迭代

在这里，我们考虑一个编码的例子，使用前面的冰冻湖泊环境。

首先，导入相关的库:

```py
import numpy as np
import gym
from gym import wrappers
```

现在，我们定义运行一集并返回奖励的函数:

```py
def run_episode_return_reward(environment, policy, gamma_value = 1.0, render = False):
    """ Runs an episode and return the total reward """
    obs = environment.reset()
    total_reward = 0
    step_index = 0
    while True:
        if render:
            environment.render()
        obs, reward, done , _ = environment.step(int(policy[obs]))
        total_reward += (gamma_value ** step_index * reward)
        step_index += 1
        if done:
            break
    return total_reward
```

从这里，我们可以定义函数来评估策略:

```py
def evaluate_policy(environment, policy, gamma_value = 1.0, n = 200):
    model_scores = [run_episode_return_reward(environment, policy, gamma_value, False) for _ in range(n)]
    return np.mean(model_scores)
```

然后，定义提取策略的函数:

```py
def extract_policy(v, gamma_value = 1.0):
    """ Extract the policy for a given value function """
    policy = np.zeros(environment.nS)
    for s in range(environment.nS):
        q_sa = np.zeros(environment.nA)
        for a in range(environment.nA):
            q_sa[a] = sum([p * (r + gamma_value * v[s_]) for p, s_, r, _ in environment.P[s][a]])
        policy[s] = np.argmax(q_sa)
    return policy
```

最后，定义计算策略的函数:

```py
def compute_policy_v(environment, policy, gamma_value=1.0):
    """ Iteratively evaluate the value-function under policy.
    Alternatively, we could formulate a set of linear equations in terms of v[s] 
    and solve them to find the value function.
    """
    v = np.zeros(environment.nS)
    eps = 1e-10
    while True:
        prev_v = np.copy(v)
        for s in range(environment.nS):
            policy_a = policy[s]
            v[s] = sum([p * (r + gamma_value * prev_v[s_]) for p, s_, r, _ in environment.P[s][policy_a]])
        if (np.sum((np.fabs(prev_v - v))) <= eps):
            # value converged
            break
    return v
```

从这里，我们可以在冰冻的湖泊环境中运行策略迭代:

```py
env_name = 'FrozenLake-v0'
environment = gym.make(env_name)
optimal_policy = policy_iteration(environment, gamma_value = 1.0)
scores = evaluate_policy(environment, optimal_policy, gamma_value = 1.0)
print('Average scores = ', np.mean(scores))
```

我们观察到它已经在*步骤 5* 处收敛:

![](assets/e7d03eea-d63c-4879-b3a2-0032c72a7de6.png)

<title>Value iteration versus policy iteration</title> <link rel="stylesheet" href="css/style.css" type="text/css"> 

# 价值迭代与策略迭代

在假设代理人对其行为对环境的影响有一些先验知识的情况下，可以使用价值和策略迭代算法。这些算法假设 MDP 模型是已知的。然而，策略迭代的计算效率更高，因为它通常需要较少的迭代次数来收敛。

<title>Policy gradient algorithm</title> <link rel="stylesheet" href="css/style.css" type="text/css"> 

# 策略梯度算法

策略梯度也是一种解决强化学习问题的方法，旨在直接建模和优化策略:

![](assets/2b75081f-f07a-41b9-94a3-46ed0cf0e912.png)

在策略梯度中，采取以下步骤:

1.  代理观察环境的状态( *s* )。
2.  代理基于他们对状态的本能(一个策略，π)采取行动*u*(*s*)。
3.  代理移动，环境改变；一个新的国家形成了。
4.  代理根据观察到的状态采取进一步的行动。
5.  在一个运动轨迹(τ)之后，代理根据收到的总奖励 *R(τ)* 调整它的本能。

政策梯度定理如下:

*期望报酬的导数是策略的对数π[θ]的报酬与梯度乘积的期望:*

![](assets/86474d27-5e94-4d57-bf41-ee2165f51e90.png)

<title>Coded example – policy gradient algorithm</title> <link rel="stylesheet" href="css/style.css" type="text/css"> 

# 编码示例–策略梯度算法

在这个例子中，我们使用名为 CartPole 的 OpenAI 环境，目标是让连接到购物车的杆子尽可能长时间地保持直立。代理人在极点保持平衡的情况下，每走一个时间步就会得到一份奖励。如果杆子倒了，这一集就结束了:

![](assets/7df31996-e074-44c3-8f28-cfa7e066dd53.png)

在任一时间点，大车和杆子都处于一种状态， *s* 。这种状态由四个元素的矢量表示，即极角、极速、小车位置和小车速度。代理可以在两个可能的操作之间做出决定:向左移动购物车或向右移动购物车。

政策梯度采取小步骤，并根据与该步骤相关联的奖励来更新政策。这样做是为了训练代理，而不必映射环境中每一对状态和动作的值。

在这个例子中，我们将应用一种称为蒙特卡罗策略梯度的技术。使用这种方法，代理将在每集结束时根据获得的奖励更新策略。

我们将首先导入我们计划使用的相关库:

```py
import gym
import numpy as np
from tqdm import tqdm, trange
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.autograd import Variable
from torch.distributions import Categorical
```

现在，我们定义一个前馈神经网络，它有一个 128 个神经元的隐藏层和 0.5 的下降。我们使用 Adam 作为优化器，学习率为 0.02。使用辍学显著提高了策略的性能:

```py
class PolicyGradient(nn.Module):
    def __init__(self):
        super(PolicyGradient, self).__init__()

        # Define the action space and state space
        self.action_space = env.action_space.n
        self.state_space = env.observation_space.shape[0]

        self.l1 = nn.Linear(self.state_space, 128, bias=False)
        self.l2 = nn.Linear(128, self.action_space, bias=False)

        self.gamma_value = gamma_value

        # Episode policy and reward history 
        self.history_policy = Variable(torch.Tensor()) 
        self.reward_episode = []

        # Overall reward and loss history
        self.history_reward = []
        self.history_loss = []

    def forward(self, x): 
        model = torch.nn.Sequential(
            self.l1,
            nn.Dropout(p=0.5),
            nn.ReLU(),
            self.l2,
            nn.Softmax(dim=-1)
        )
        return model(x)

policy = PolicyGradient()
optimizer = optim.Adam(policy.parameters(), lr=l_rate)
```

现在，我们定义一个`choose_action`函数。这个函数在 PyTorch 分发包的帮助下基于策略分发选择一个动作。该策略以数组的形式返回动作空间上每个可能动作的概率。在我们的示例中，这是向左移动或向右移动，因此输出可能是[0.1，0.9]。基于这些概率，选择动作，记录历史，并返回动作:

```py
def choose_action(state):
    # Run the policy model and choose an action based on the probabilities in state
    state = torch.from_numpy(state).type(torch.FloatTensor)
    state = policy(Variable(state))
    c = Categorical(state)
    action = c.sample() 
    if policy.history_policy.dim() != 0:
        try:
            policy.history_policy = torch.cat([policy.history_policy, c.log_prob(action)])
        except:
            policy.history_policy = (c.log_prob(action))
    else:
        policy.history_policy = (c.log_prob(action))
    return action
```

为了更新策略，我们取一个 Q 函数(动作值函数)的样本。回想一下，这是在一个状态下，按照政策π采取行动的预期收益。我们可以计算每个时间步的政策梯度，利用的事实是，极点保持垂直的每一步都有 1 的回报。我们使用长期奖励( *vt* )，这是该集期间所有未来奖励的贴现总额。因此，情节越长，当前状态-行动对的奖励就越大，其中γ是折扣因子。

折扣奖励向量表示如下:

![](assets/b5ed2c83-b4b0-4607-a17f-6b5d5f734a75.png)

例如，如果一集持续 4 个时间步长，则每个步长的报酬将是[4.90，3.94，2.97，1.99]。从这里，我们可以通过减去平均值并除以标准差来调整回报向量。

为了在每集之后改进策略，我们应用蒙特卡罗策略梯度，如下所示:

![](assets/01176f23-63b1-47ff-9cee-789c93f290af.png)

然后，该策略与奖励相乘，并被馈送到优化器中，并且使用随机梯度下降来更新神经网络的权重。

以下函数定义了如何在代码中更新策略:

```py
def update_policy():
    R = 0
    rewards = []

    # Discount future rewards back to the present using gamma
    for r in policy.reward_episode[::-1]:
        R = r + policy.gamma_value * R
        rewards.insert(0,R)

    # Scale rewards
    rewards = torch.FloatTensor(rewards)
    x = np.finfo(np.float32).eps
    x = np.array(x)
    x = torch.from_numpy(x)
    rewards = (rewards - rewards.mean()) / (rewards.std() + x)
    # Calculate the loss loss
    loss = (torch.sum(torch.mul(policy.history_policy, Variable(rewards)).mul(-1), -1))

    # Update the weights of the network
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    #Save and intialize episode history counters
    policy.history_loss.append(loss.data[0])
    policy.history_reward.append(np.sum(policy.reward_episode))
    policy.history_policy = Variable(torch.Tensor())
    policy.reward_episode= []
```

从这里，我们定义主要的策略培训循环。在训练的每一步，选择一个动作，记录下新的状态和奖励。在每集结束时调用`update_policy`函数，将该集的历史输入神经网络:

```py
def main_function(episodes):
    running_total_reward = 50
    for e in range(episodes):
        # Reset the environment and record the starting state
        state = env.reset() 
        done = False 

        for time in range(1000):
            action = choose_action(state)
            # Step through environment using chosen action
            state, reward, done, _ = env.step(action.data.item())

            # Save reward
            policy.reward_episode.append(reward)
            if done:
                break

        # Used to determine when the environment is solved.
        running_total_reward = (running_total_reward * 0.99) + (time * 0.01)

        update_policy()

        if e % 50 == 0:
            print('Episode number {}, Last length: {:5d}, Average length: {:.2f}'.format(e, time, running_total_reward))

        if running_total_reward > env.spec.reward_threshold:
            print("Solved! Running reward is now {} and the last episode runs to {} time steps!".format(running_total_reward, time))
            break

episodes = 2000
main_function(episodes)
```

<title>Deep Q-networks</title> <link rel="stylesheet" href="css/style.css" type="text/css"> 

# 深度 Q-网络

**Deep-Q Networks**(**DQNs**)将深度学习和 RL 结合起来，在几种不同的应用中进行学习，特别是计算机游戏。让我们考虑一个简单的游戏例子，在这个游戏中，有一只老鼠在迷宫中，目标是让老鼠尽可能多的吃奶酪。老鼠吃的奶酪越多，在游戏中得到的分数就越多:

![](assets/9e7a42c1-ace4-4233-bebb-d1ef93fb8da2.png)

在本例中，RL 项如下:

*   代理人:鼠标是由电脑控制的
*   状态:游戏中的当前时刻
*   动作:鼠标做出的决定(向左、向右、向上或向下移动)
*   奖励:游戏中的分数/老鼠吃掉的奶酪数量——换句话说，就是代理人试图最大化的价值

dqn 使用 Q-learning 来学习给定状态下的最佳动作。他们使用**卷积神经网络** ( **卷积神经网络**)作为 Q 学习函数的函数逼近器。ConvNets 使用卷积层来查找空间特征，例如鼠标当前在网格中的位置。这意味着代理只需要学习几百万个而不是几十亿个不同游戏状态的 Q 值:

![](assets/048486f3-6d05-4c7b-b741-dab2757bf1af.png)

学习鼠标迷宫游戏时的 DQN 架构示例如下:

1.  当前状态(迷宫屏幕)作为输入输入到 DQN。
2.  输入通过卷积层来寻找图像中的空间模式。请注意，没有使用池，因为在建模计算机游戏时知道空间位置是很重要的。
3.  卷积层的输出被馈入完全连接的线性层。
4.  线性图层的输出给出了 DQN 在其当前状态下采取行动的概率(向上、向下、向左或向右移动)。

<title>DQN loss function</title> <link rel="stylesheet" href="css/style.css" type="text/css"> 

# DQN 损失函数

DQN 需要一个损失函数来进行改进并获得更高的分数。该函数可以用数学方法表示如下:

![](assets/76163220-a598-499f-98fa-842db8be916f.png)

是 Q-网络选择采取哪些行动。目标网络是用作地面真实情况的近似值。如果我们考虑一个场景，其中 Q 网络预测特定状态下的校正动作是以 80%的确定性向左移动，而目标网络建议向左移动，我们可以使用反向传播来调整 Q 网络的参数，以使其更有可能预测该状态下的“向左移动”。换句话说，我们通过 DQN 反向传播损耗，并调整 Q 网络的权重以降低总损耗。损失方程旨在使移动的概率更接近正确选择的 100%确定性。

<title>Experience replay</title> <link rel="stylesheet" href="css/style.css" type="text/css"> 

# 体验回放

一次经历包括当前状态、行动、回报和下一个状态。代理获得的每个体验都记录在体验回放记忆中。来自重放存储器的体验被随机采样以训练网络。

与传统的 Q-learning 相比，经验重放有一些关键的优势。其中一个优势是，由于每次经历都有可能被用于多次训练 DQN 神经网络，因此数据效率更高。另一个优点是，当它一获得经验就从经验中学习时，当前的参数决定了参数被训练的下一个样本。如果我们在迷宫示例中考虑这一点，如果下一个最佳动作是向左移动，那么训练样本将由屏幕左侧的样本支配。这种行为会导致 DQN 陷入局部最小值。随着经验重放的结合，用于训练 DQN 的经验来自许多不同的时间点，平滑了学习并有助于避免糟糕的表现。

<title>Coded example – DQN</title> <link rel="stylesheet" href="css/style.css" type="text/css"> 

# 编码示例-DQN

在这个例子中，我们将再次考虑来自 OpenAI Gym 的`CartPole-v0`环境。

首先，我们创建了一个类，允许我们在训练 DQN 时加入经验回放。这实际上存储了代理观察到的转换。构成一个批次的转换通过采样过程去相关:

```py
transition_type = namedtuple('transition_type',
                        ('state', 'action', 'next_state', 'reward'))

class ExperienceReplayMemory(object):
    def __init__(self, model_capacity):
        self.model_capacity = model_capacity
        self.environment_memory = []
        self.pole_position = 0

    def push(self, *args):
        """Saves a transition."""
        if len(self.environment_memory) < self.model_capacity:
            self.environment_memory.append(None)
        self.environment_memory[self.pole_position] = transition_type(*args)
        self.pole_position = (self.pole_position + 1) % self.model_capacity

    def sample(self, batch_size):
        return random.sample(self.environment_memory, batch_size)

    def __len__(self):
        return len(self.environment_memory)
```

定义 ConvNet 模型，将当前和先前屏幕补丁之间的差异输入其中。该模型有两个输出— *Q(s，左)和 Q(s，右)*。在给定当前输入的情况下，网络试图预测采取行动的预期奖励/回报:

```py
class DQNAlgorithm(nn.Module):

    def __init__(self, h, w, outputs):
        super(DQNAlgorithm, self).__init__()
        self.conv_layer1 = nn.Conv2d(3, 8, kernel_size=5, stride=2)
        self.batch_norm1 = nn.BatchNorm2d(8)
        self.conv_layer2 = nn.Conv2d(8, 32, kernel_size=5, stride=2)
        self.batch_norm2 = nn.BatchNorm2d(32)
        self.conv_layer3 = nn.Conv2d(32, 32, kernel_size=5, stride=2)
        self.batch_norm3 = nn.BatchNorm2d(32)

        # The number of linear input connections depends on the number of conv2d layers
        def conv2d_layer_size_out(size, kernel_size = 5, stride = 2):
            return (size - (kernel_size - 1) - 1) // stride + 1
        convw = conv2d_layer_size_out(conv2d_layer_size_out(conv2d_layer_size_out(w)))
        convh = conv2d_layer_size_out(conv2d_layer_size_out(conv2d_layer_size_out(h)))
        linear_input_size = convw * convh * 32
        self.head = nn.Linear(linear_input_size, outputs)

    # Determines next action during optimisation
    def forward(self, x):
        x = F.relu(self.batch_norm1(self.conv_layer1(x)))
        x = F.relu(self.batch_norm2(self.conv_layer2(x)))
        x = F.relu(self.batch_norm3(self.conv_layer3(x)))
        return self.head(x.view(x.size(0), -1))
```

设置模型的超参数以及一些用于训练的实用程序:

```py
BATCH_SIZE = 128
GAMMA_VALUE = 0.95
EPISODE_START = 0.9
EPISODE_END = 0.05
EPISODE_DECAY = 200
TARGET_UPDATE = 20

init_screen = get_screen()
dummy_1, dummy_2, height_screen, width_screen = init_screen.shape

number_actions = environment.action_space.n

policy_network = DQNAlgorithm(height_screen, width_screen, number_actions).to(device)
target_network = DQNAlgorithm(height_screen, width_screen, number_actions).to(device)
target_network.load_state_dict(policy_network.state_dict())
target_network.eval()

optimizer = optim.RMSprop(policy_network.parameters())
memory = ExperienceReplayMemory(1000)

steps_done = 0

def choose_action(state):
    global steps_done
    sample = random.random()
    episode_threshold = EPISODE_END + (EPISODE_START - EPISODE_END) * \
        math.exp(-1\. * steps_done / EPISODE_DECAY)
    steps_done += 1
    if sample > episode_threshold:
        with torch.no_grad():
            return policy_network(state).max(1)[1].view(1, 1)
    else:
        return torch.tensor([[random.randrange(number_actions)]], device=device, dtype=torch.long)

durations_per_episode = []

def plot_durations():
    plt.figure(2)
    plt.clf()
    durations_timestep = torch.tensor(durations_per_episode, dtype=torch.float)
    plt.title('Training in progress...')
    plt.xlabel('Episode')
    plt.ylabel('Duration')
    plt.plot(durations_timestep.numpy())
    if len(durations_timestep) >= 50:
        mean_values = durations_per_episode.unfold(0, 100, 1).mean(1).view(-1)
        mean_values = torch.cat((torch.zeros(99), mean_values))
        plt.plot(mean_values.numpy())

    plt.pause(0.001) 
    plt.show()
```

最后，我们有了训练模型的代码。该函数执行优化的单个步骤。首先，它对一个批次进行采样，并将所有张量连接成一个。它计算 *Q(st，at)* 和 *V(st+1)=maxaQ(st+1，a)* 并将它们组合成一个损失。根据定义，如果 *s* 是终止状态，我们设置 *V(s)=0* 。我们还使用目标网络来计算 *V(st+1)* 以增加稳定性:

```py
def optimize_model():
    if len(memory) < BATCH_SIZE:
        return
    transitions_memory = memory.sample(BATCH_SIZE)
    batch = Transition(*zip(*transitions_memory))
```

计算非最终状态的掩码。之后，我们连接批处理元素:

```py
    not_final_mask = torch.tensor(tuple(map(lambda x: x is not None,
                                          batch.next_state)), device=device, dtype=torch.uint8)
    not_final_next_states = torch.cat([x for x in batch.next_state if x is not None])

    state_b = torch.cat(batch.state)
    action_b = torch.cat(batch.action)
    reward_b = torch.cat(batch.reward)
```

计算 *Q(s_t，a)* ，然后选择采取行动的列:

```py
    state_action_values = policy_network(state_b).gather(1, action_b)

    next_state_values = torch.zeros(BATCH_SIZE, device=device)
    next_state_values[not_final_mask] = target_net(not_final_next_states).max(1)[0].detach()
```

我们计算预期 Q 值:

```py
    expected_state_action_values = (next_state_values * GAMMA_VALUE) + reward_b
```

然后，我们计算 Huber 损失函数:

```py
    hb_loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))
```

现在，我们优化模型:

```py
    optimizer.zero_grad()
    hb_loss.backward()
    for param in policy_network.parameters():
        param.grad.data.clamp_(-1, 1)
    optimizer.step()

number_episodes = 100
for i in range(number_episodes):
    environment.reset()
    last_screen = get_screen()
    current_screen = get_screen()
    current_state = current_screen - last_screen
    for t in count():
        # Here we both select and perform an action
        action = choose_action(current_state)
        _, reward, done, _ = environment.step(action.item())
        reward = torch.tensor([reward], device=device)
```

现在，我们观察新的状态:

```py
        last_screen = current_screen
        current_screen = get_screen()
        if not done:
            next_state = current_screen - last_screen
        else:
            next_state = None
```

我们将过渡存储在内存中:

```py
        memory.push(current_state, action, next_state, reward)

        # Move to the next state
        current_state = next_state
```

让我们执行优化的一个步骤(在目标网络上):

```py
        optimize_model()
        if done:
            durations_per_episode.append(t + 1)
            plot_durations()
            break
```

更新目标网络；复制 DQN 中的所有权重和偏差:

```py
    if i % TARGET_UPDATE == 0:
        target_network.load_state_dict(policy_network.state_dict())

print('Complete')
environment.render()
environment.close()
plt.ioff()
plt.show()
```

这将输出一些可视化效果，以便深入了解模型在训练过程中的表现:

![](assets/d983eb9e-2dd3-407d-9965-184d98e4f351.png)

下图总结了此编码示例中模型的工作:

![](assets/c95446c8-85a6-4cf0-8cb3-d22f43b3d57c.png)

<title>Double deep Q-learning</title> <link rel="stylesheet" href="css/style.css" type="text/css"> 

# 双重深度 Q 学习

与普通的 dqn 相比，双深度 Q 学习通常会导致 AI 代理的更好的性能。深度 Q 学习的一个常见问题是，有时，代理可以学习到不切实际的高行动值，因为它包括对估计行动值的最大化步骤。这往往倾向于高估而不是低估的价值。如果高估不一致，并且没有集中在我们希望了解更多的州，那么这些会对最终政策的质量产生负面影响。

双 Q 学习的思想就是减少这些高估。它通过将目标中的 max 操作分解为动作选择和动作评估来实现这一点。在普通 DQN 实现中，动作选择和动作评估是耦合的。它使用目标网络来选择动作，同时估计动作的质量。

我们使用目标网络来选择行动，同时评估行动的质量。双 Q 学习本质上试图将两个过程相互分离。

在双 Q 学习中，时间差异(TD)目标看起来如下:

![](assets/70934206-aa2d-46c4-b277-fbc831cef8f5.png)

新 TD 目标的计算可总结为以下步骤:

1.  Q-网络使用下一个状态， *s'* ，计算品质， *Q(s '，a)* ，对于每个可能的动作， *a* ，在状态中， *s'* 。
2.  应用于*Q(s’，a)* 的`argmax`操作选择属于最高质量的动作 *a** (动作选择)。
3.  选择属于动作 *a** 的质量*Q(s’，a*)* 来计算目标。

双 Q 学习的过程可以如下图所示。人工智能代理处于初始状态 s；基于之前的一些计算，它知道在该状态下可能的两个动作的质量 Q(s，a [1] 和 Q(s，a [2] )。代理然后决定采取行动 a [1] ，并在状态 s’结束:

![](assets/e883c737-107a-4652-b211-e9d4815d0c46.png)

<title>Actor-critic methods</title> <link rel="stylesheet" href="css/style.css" type="text/css"> 

# 演员-评论家方法

行动者-批评家方法旨在结合基于价值和基于政策的方法的优点，同时消除它们的缺点:

![](assets/0968abce-a6e7-461f-8549-37d45a09c41e.png)

actor-critics 背后的基本思想是将模型分成两部分:一部分用于基于状态计算动作，另一部分用于产生动作的 Q 值。

演员是一个神经网络，以状态为输入，输出最佳动作。通过学习最优策略，它控制代理的行为。批评家通过计算价值函数来评估行动。换句话说，行动者试图优化政策，批评家试图优化价值。随着时间的推移，这两种模型在各自的角色方面都有所改进，因此，整体架构的学习效率比这两种方法单独使用时更高:

![](assets/f65da315-87b4-4a85-9b63-15004169d6e1.png)

这两种模式本质上是相互竞争的。这种方法在机器学习领域越来越流行；例如，这也存在于生成对立网络中。

演员角色的本质是探索性的。它经常尝试新事物，探索环境。评论家的角色是批评或赞扬演员的行为。参与者接受这些反馈，并相应地调整自己的行为。随着参与者收到越来越多的反馈，它会更好地决定采取哪些行动。

像神经网络一样，参与者可以是一个函数逼近器，其任务是为给定状态产生最佳动作。例如，这可以是全连接或卷积神经网络。批评家也是一个功能近似者，它接收环境作为演员行动的输入。它连接这些输入并输出动作值(Q 值)。

这两个网络被分别训练，并且为了更新它们的权重，它们使用梯度上升而不是下降，因为它旨在确定全局最大值而不是最小值。权重在每一步更新，而不是在一集结束时更新，这与策略梯度相反。

演员评论家已经被证明能够学习复杂的环境，并被用于许多 2D 和 3D 电脑游戏，如*超级马里奥*和*末日*。

<title>Coded example – actor-critic model</title> <link rel="stylesheet" href="css/style.css" type="text/css"> 

# 编码示例-演员-评论家模型

这里，我们将考虑 PyTorch 中的一个编码实现示例。首先，我们定义了`ActorCritic`类:

```py
HistoricalAction = namedtuple('HistoricalAction', ['log_prob', 'value'])

class ActorCritic(nn.Module):
    def __init__(self):
        super(ActorCritic, self).__init__()
        self.linear = nn.Linear(4, 128)
        self.head_action = nn.Linear(128, 2)
        self.head_value = nn.Linear(128, 1)

        self.historical_actions = []
       self.rewards = []

    def forward(self, x):
        x = F.relu(self.linear(x))
        scores_actions = self.head_action(x)
        state_values = self.head_value(x)
        return F.softmax(scores_actions, dim=-1), state_values
```

现在，我们初始化模型:

```py
ac_model = ActorCritic()
optimizer = optim.Adam(ac_model.parameters(), lr=3e-2)
eps = np.finfo(np.float32).eps.item()
```

定义一个将根据状态选择最佳操作的函数:

```py
def choose_action(current_state):
    current_state = torch.from_numpy(current_state).float()
    probabilities, state_value = ac_model(current_state)
    m = Categorical(probabilities)
    action = m.sample()
    ac_model.historical_actions.append(HistoricalAction(m.log_prob(action), state_value))
    return action.item()
```

从这里开始，我们需要定义计算总回报的函数，并考虑损失函数:

```py
def end_episode():
    R = 0
    historical_actions = ac_model.historical_actions
    losses_policy = []
    losses_value = []
    returns = []
    for r in ac_model.rewards[::-1]:
        R = r + gamma * R
        returns.insert(0, R)
    returns = torch.tensor(returns)
    returns = (returns - returns.mean()) / (returns.std() + eps)
    for (log_prob, value), R in zip(historical_actions, returns):
        advantage = R - value.item()
        losses_policy.append(-log_prob * advantage)
        losses_value.append(F.smooth_l1_loss(value, torch.tensor([R])))
    optimizer.zero_grad()
    loss = torch.stack(losses_policy).sum() + torch.stack(losses_value).sum()
    loss.backward()
    optimizer.step()
    del ac_model.rewards[:]
    del ac_model.historical_actions[:]
```

最后，我们可以训练模型并检查它的表现:

```py
    running_reward = 10
    for i_episode in count(1):
        current_state, ep_reward = environment.reset(), 0
        for t in range(1, 10000): 
            action = choose_action(current_state)
            current_state, reward, done, _ = environment.step(action)
            if render:
                environment.render()
            ac_model.rewards.append(reward)
            ep_reward += reward
            if done:
                break

        running_reward = 0.05 * ep_reward + (1 - 0.05) * running_reward
        end_episode()
        if i_episode % log_interval == 0:
            print('Episode number {}\tLast reward: {:.2f}\tAverage reward: {:.2f}'.format(
                  i_episode, ep_reward, running_reward))
        if running_reward > environment.spec.reward_threshold:
            print("Solved! Running reward is {} and "
                  "the last episode runs to {} time steps!".format(running_reward, t))
            break
```

这将产生以下输出:

![](assets/776751b2-6167-4fa2-945b-a9722ced0530.png)

<title>Asynchronous actor-critic algorithm</title> <link rel="stylesheet" href="css/style.css" type="text/css"> 

# 异步演员-评论家算法

**异步优势演员评论家**或 **A3C** 是谷歌 DeepMind 提出的一种算法。该算法已被证明优于其他算法。

在 A3C 中，有多个代理实例，每个实例都在各自独立的环境中进行了不同的初始化。每个个体代理开始采取行动，并通过强化学习过程来收集他们自己独特的经验。这些独特的经验然后被用于更新全局神经网络。这个全局神经网络由所有代理共享，它影响代理的所有动作，并且来自每个代理的每一个新体验都提高了整体网络速度:

![](assets/da2bc576-fcf4-40ac-beba-6d9fe7c21e93.png)

名称中的**优势**术语是一个值，它表示与该状态的预期平均值相比，某项行动是否有所改进。优势公式如下:

*A (s，a) = Q(s，a) - V(s)*

<title>Practical applications</title> <link rel="stylesheet" href="css/style.css" type="text/css"> 

# 实际应用

RL 方法已经被应用于解决现实世界中许多领域的问题。这里，我们考虑其中的一些例子:

*   机器人学:在机器人学领域已经有了大量的应用 RL 的工作。如今，制造工厂到处都是执行各种任务的机器人，其基础是 RL 方法:

![](assets/feda1bf9-3714-4899-9f98-97915440d76d.png)

*   **交通灯控制**:在论文*基于强化学习的多智能体网络交通信号控制系统*中，研究人员设计了一个交通灯控制器来解决拥堵问题，其结果优于其他方法:

![](assets/180d2992-a7c4-48d2-a6ab-50a93e52815e.png)

*   **个性化推荐** : RL 应用在新闻推荐系统中，考虑到新闻变化快，用户注意力持续时间短，点击率本身不能反映用户的留存率；

![](assets/293bd127-5ca4-4eb7-8524-78d8385046ea.png)

*   **生成图像**:对于将 RL 与其他深度学习架构结合起来用于许多不同的应用，已经有了很多研究。其中许多已经显示出一些令人印象深刻的结果。DeepMind 表明，使用生成模型和 RL，他们能够成功地生成图像:

![](assets/3e043c7a-f238-4bee-83d6-3918928f53e7.png)

<title>Summary</title> <link rel="stylesheet" href="css/style.css" type="text/css"> 

# 摘要

在本章中，我们首先介绍了 RL 的基础知识，并介绍了更高级的算法，这些算法已经证明了在现实世界中超越人类的能力。我们还举例说明了如何在 PyTorch 中实现这些功能。

在下一章，也是最后一章，将会有本书的概述，以及一些关于如何让自己跟上数据科学领域最新进展的提示。

<title>Further reading</title> <link rel="stylesheet" href="css/style.css" type="text/css"> 

# 进一步阅读

有关更多信息，请参考以下链接:

*   *深度强化学习概述*:https://arxiv.org/pdf/1708.05866.pdf[T3](https://arxiv.org/pdf/1708.05866.pdf)
*   *用深度强化学习玩雅达利*:【https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf】T2
*   *采用双 Q 学习的深度强化学习*:【https://arxiv.org/pdf/1509.06461.pdf】T2
*   *深度强化学习的连续控制*:【https://arxiv.org/pdf/1509.02971.pdf】T2
*   *深度强化学习的异步方法*:https://arxiv.org/pdf/1602.01783.pdf[T3](https://arxiv.org/pdf/1602.01783.pdf)
*   *软行动者-批评家:带有随机行动者的非策略最大熵深度强化学习*:【https://arxiv.org/pdf/1801.01290.pdf】T2
*   *基于强化学习的网络交通信号控制多智能体系统*:【http://web.eecs.utk.edu/~itamar/Papers/IET_ITS_2010.pdf 
*   *深度视觉运动策略端到端培训*:【https://arxiv.org/pdf/1504.00702.pdf 
*   *DRN:新闻推荐深度强化学习框架*:[http://www . personal . PSU . edu/~ gjz 5038/paper/www 2018 _ reinforceRec/www 2018 _ reinforceRec . pdf](http://www.personal.psu.edu/~gjz5038/paper/www2018_reinforceRec/www2018_reinforceRec.pdf)
*   *使用强化对抗学习的图像合成程序*:【https://arxiv.org/pdf/1804.01118.pdf】T2