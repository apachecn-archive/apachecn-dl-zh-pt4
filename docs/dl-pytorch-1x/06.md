

# 六、计算机视觉的深度学习

在[第三章](f93f665d-9a2a-4d36-b442-75e7fb89d9cd.xhtml)、*深入研究神经网络、*中，我们使用一种流行的**卷积神经网络** ( **CNN** )架构构建了一个图像分类器，称为 **ResNet** ，但我们将这个模型用作黑盒。在这一章中，我们将探索如何从头构建一个架构来解决图像分类问题，这是最常见的用例。我们还将学习如何使用迁移学习，这将帮助我们使用一个非常小的数据集建立图像分类器。除了学习如何使用 CNN，我们还将探索这些卷积网络学习什么。

在本章中，我们将讨论卷积网络的重要组成部分。本章中我们将涉及的一些重要主题如下:

*   神经网络导论
*   从头开始构建 CNN 模型
*   创建和探索 VGG16 模型
*   计算预卷积特征
*   理解 CNN 模型学到了什么
*   可视化 CNN 图层的权重



# 神经网络导论

在过去的几年中，细胞神经网络已经在图像识别、物体检测、分割和计算机视觉领域的许多其他领域中变得流行。它们在自然语言处理领域也越来越受欢迎，尽管它们还没有被广泛使用。完全连接层和卷积层之间的根本区别在于中间层中权重相互连接的方式。让我们来看看下图，该图显示了完全连接的或线性的层是如何工作的:

![](img/25c3a354-5f47-40df-a2ba-813d044132f1.png)

将线性图层或全连接图层用于计算机视觉的最大挑战之一是，它们会丢失所有空间信息，并且全连接图层使用的权重数量的复杂性太大。例如，当我们将一个 224 像素的图像表示为平面阵列时，我们最终会得到 150，528 (224 x 224 x 3 通道)。当图像变平时，我们失去了所有的空间信息。让我们看看简化版的 CNN 是什么样子的:

![](img/b7865455-c3d0-4191-896f-d970ce006af4.png)

卷积层所做的就是在图像上应用一个叫做**过滤器**的权重窗口。在我们试图详细理解卷积和其他构建模块之前，让我们为 MNIST 数据集构建一个简单而强大的图像分类器。一旦我们建立了这个，我们将走过网络的每个组成部分。我们将把构建图像分类器分解为以下步骤:

1.  获取数据
2.  创建验证数据集
3.  从头开始构建我们的 CNN 模型
4.  训练和验证模型



# MNIST—获取数据

MNIST 数据集包含 60，000 个用于训练的从 0 到 9 的手写数字和 10，000 个用于测试集的图像。PyTorch `torchvision`库为我们提供了一个 MNIST 数据集，它下载数据并以一种易于使用的格式提供数据。让我们使用数据集 MNIST 函数将数据集拉到我们的本地机器上，然后包装它`DataLoader`。我们将使用`torchvision`变换将数据转换成 PyTorch 张量并进行数据归一化。以下代码负责下载、包装`DataLoader`周围的数据，并对数据进行规范化:

```py
transformation = transforms.Compose([transforms.ToTensor(),
transforms.Normalize((0.14,), (0.32,))])
training_dataset = datasets.MNIST('dataset/',train=True,transform=transformation,
download=True) test_dataset =
datasets.MNIST('dataset/',train=False,transform=transformation, download=True)
training_loader = torch.utils.data.DataLoader(training_dataset,batch_size=32,shuffle=True)
test_loader = torch.utils.data.DataLoader(test_dataset,batch_size=32,shuffle=True)
```

前面的代码为训练和测试数据集提供了一个`DataLoader`变量。让我们想象一些图像来理解我们正在处理的事情。以下代码将帮助我们可视化 MNIST 图像:

```py
def plot_img(image):
image = image.numpy()[0] mean = 0.1307
std = 0.3081
image = ((mean * image) + std) plt.imshow(image,cmap='gray')
```

现在，我们可以通过`plot_img`方法来可视化我们的数据集。我们将使用以下代码从`DataLoader`变量中提取一批记录，并绘制图像:

```py
sample_data = next(iter(training_loader)) plot_img(sample_data[0][1]) plot_img(sample_data[0][2])
```

图像可以按如下方式可视化:

![](img/61281deb-f80b-457e-8974-83367ab71034.png)



# 从头开始构建 CNN 模型

在这一节中，我们将从头开始构建我们自己的架构。我们的网络架构将包含不同层的组合，如下所示:

*   Conv2d
*   MaxPool2d
*   **整流线性单元** ( **ReLU** )
*   视角
*   线性层

让我们看一下我们将要实现的架构的图示:

![](img/70e1ba7b-d4dc-4563-af8a-cbd06265bbc4.png)

让我们在 PyTorch 中实现这个架构，然后看看每个单独的层是做什么的:

```py
class Network(nn.Module): def   init  (self):
super(). init  ()
self.conv1 = nn.Conv2d(1, 10, kernel_size=3)
self.conv2 = nn.Conv2d(10, 20, kernel_size=3) self.conv2_drop = nn.Dropout2d()
self.fullyconnected1 = nn.Linear(320, 50) self.fullyconnected2 = nn.Linear(50, 10)

def forward(self, x):
x = F.relu(F.max_pool2d(self.conv1(x), 2))
x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2)) x = x.view(-1, 320)
x = F.relu(self.fullyconnected1(x))
x = F.dropout(x, training=self.training) x = self.fullyconnected2(x)
return F.log_softmax(x)
```

我们来详细了解一下每一层是做什么的。



# Conv2d

Conv2d 负责对我们的 MNIST 图像应用卷积滤波器。让我们试着理解卷积是如何应用于一维数组的，然后学习二维卷积是如何应用于图像的。请看下图。这里，我们将把大小为 3 的过滤器(或内核)的 **Conv1d** 应用于长度为 7 的张量:

![](img/74b21364-00fa-46d5-af2a-92ad2317df10.png)

底部的方框表示七个值的输入张量，而相连的方框表示应用大小为三的卷积滤波器后的输出。在图像的右上角，三个框代表 **Conv1d** 层的权重和参数。卷积滤波器的应用类似于一个窗口，它通过跳过一个值来移动到下面的值。要跳过的值的数量称为**步距**，默认设置为 1。让我们通过记下第一个和最后一个输出的计算结果来了解输出值是如何计算的:

输出 1->(*-0.5209 x 0.2286*)+(*-0.0147 x 2.4488*)+(*-0.321 x-0.9498*)

输出 5->(*-0.5209 x-0.6791*)+(*-0.0147 x-0.6535*)+(*-0.321 x 0.6437*)

现在，卷积的作用应该很清楚了。它通过基于步幅值移动输入来对输入应用过滤器(或内核)，即一组权重。在前面的例子中，我们一次移动一个点。如果步幅值是 2，那么我们将一次移动两个点。让我们看一个 PyTorch 实现来理解它是如何工作的:

```py
conv = nn.Conv1d(1,1,3,bias=False) 
sample = torch.randn(1,1,7) 
conv(Variable(sample))

#Check the weights of our convolution filter by 
conv.weight
```

还有另一个重要的参数，称为**填充，**通常用于卷积。如前面的示例所示，如果直到数据结束时才应用筛选，也就是说，当没有足够的元素供数据跨越时，筛选就会停止。填充通过在张量的两端添加零来防止这种情况。让我们来看一个填充如何工作的一维示例:

![](img/04aef5fe-05f4-40b2-b860-4b04ad55278d.png)

在上图中，我们应用了填充为 2、步幅为 1 的 **Conv1d** 层。让我们看看 Conv2d 是如何处理图像的。

在我们了解 Conv2d 如何工作之前，我强烈建议您查看一个令人惊叹的博客([http://setosa.io/ev/image-kernels/](http://setosa.io/ev/image-kernels/))，其中包含卷积如何工作的现场演示。在您花了几分钟时间玩了演示之后，继续阅读。

让我们了解一下演示中发生了什么。在图像的中间方框中，我们有两组不同的数字:一组在方框中，另一组在方框下面。方框中表示的是像素值，如演示中左侧照片上的白色方框所示。方框下面的数字是用于锐化图像的过滤器(或内核)值。这些数字是为完成某项特定工作而精心挑选的。在这种情况下，他们正在锐化图像。就像我们前面的例子一样，我们正在做一个元素到元素的乘法，并对所有值求和，以生成右边图像中的像素值。生成的值通过图像右侧的白色框突出显示。

虽然在这个例子中内核中的值是手工挑选的，但是在 CNN 中，我们并不手工挑选这些值；相反，我们随机初始化它们，并让梯度下降和反向传播调整核的值。学习过的内核将负责识别不同的特征，如直线、曲线和眼睛。看看下面的截图，我们可以看到一个数字矩阵，看看卷积是如何工作的:

![](img/d54b8108-1fd5-4b42-8c4c-c7d371f0cc5e.png)

在前面的屏幕截图中，我们假设 6 x 6 矩阵表示一个图像，并应用大小为 3 x 3 的卷积滤波器。然后，我们展示输出是如何生成的。为了简单起见，我们只计算矩阵中突出显示的部分。通过执行以下计算生成输出:

输出–>*0.86 x 0+-0.92 x 0+-0.61 x 1+-0.32 x-1+-1.69 x-1+........*

Conv2d 函数中使用的另一个重要参数是`kernel_size`，它决定内核的大小。一些常用的内核大小有 *1* 、 *3* 、 *5* 和 *7* 。内核的大小越大，滤波器可以覆盖的区域就越大，因此常见的是将 *7* 或 *9* 的滤波器应用于早期层中的输入数据。



# 联营

在卷积图层之后添加池化图层是一种常见的做法，因为它们可以减小要素地图的大小和卷积图层的结果。

池提供了两个不同的功能:一个是减少要处理的数据的大小，另一个是迫使算法不关注图像位置的微小变化。例如，人脸检测算法应该能够检测照片中的人脸，而不管人脸在照片中的位置如何。

我们来看看 MaxPool2d 是如何工作的。它也使用了相同的内核大小和步幅概念。它不同于卷积，因为它没有任何权重，只作用于每个过滤器从前一层生成的数据。如果内核大小是 *2 x 2，*，那么它会考虑图像中的大小并选择该区域的最大值。让我们看看下面的图表，它将清楚地表明 MaxPool2d 是如何工作的:

![](img/95f76a92-ac50-401a-bf5d-5500e09e1509.png)

左手边的方框包含特征映射的值。应用最大池后，输出存储在盒子的右侧。让我们看看如何通过在输出的第一行中写下值的计算来计算输出:

![](img/b7285839-6b38-4e1c-9901-097262274765.png)

另一种常用的汇集技术是**平均汇集**。最大值函数替换为平均值函数。下图解释了平均池的工作原理:

![](img/af03c571-cf8b-48a6-8159-23132edc61d8.png)

在这个例子中，我们不是取四个值的最大值，而是取四个值的平均值。让我们把计算写下来，这样更容易理解:

![](img/f5926b79-2ee8-488a-8cf2-073ae4305415.png)



# 非线性激活–ReLU

在最大池化之后，或者在应用卷积之后，有一个非线性层是常见的和最佳的实践。大多数网络架构倾向于使用 ReLU 或不同风格的 ReLU。无论我们选择什么非线性函数，它都会应用到特征图的每个元素。为了更直观，让我们看一个例子，在这个例子中，我们在应用了最大池和平均池的同一特性图上应用 ReLU:

![](img/b67dbb6c-be51-48f4-a3f0-aa0d557d1165.png)



# 视角

对于影像分类问题，通常在大多数网络的末端使用全连通或线性图层。这里，我们使用一个二维卷积，它将一个数字矩阵作为输入，并输出另一个数字矩阵。要应用线性图层，我们需要将二维张量的矩阵展平成一维向量。下图显示了视图的工作方式:

![](img/843840b3-e32a-4e73-beab-0d371811788b.png)

让我们看看我们网络中使用的代码，它的作用完全相同:

```py
x.view(-1, 320)
```

正如我们前面看到的，view 方法将把一个 *n* 维张量展平成一个一维张量。在我们的网络中，第一维是每个图像。批处理后的输入数据将具有尺寸*32×1×28×28，*，其中第一个数字 *32，*将表示存在尺寸 *28* 高、 *28* 宽、 *1* 通道的 *32* 图像，因为它是黑白图像。当我们展平时，我们不想展平或混合不同图像的数据。因此，我们传递给 view 函数的第一个参数将指示 PyTorch 避免在第一维上展平数据。下图显示了其工作原理:

![](img/9463a041-acb1-40bb-be9a-67ba4382b0b3.png)

在上图中，我们有大小为 *2 x 1 x 2 x 2* 的数据；在我们应用视图函数后，它将它转换成大小为 *2 x 1 x 4* 的张量。让我们看另一个我们没有提到 *- 1* 的例子:

![](img/c9de8126-872d-4205-af00-77cb095097d7.png)

如果我们忘记提到要展平哪个维度，我们可能会得到意想不到的结果，所以在这一步要格外小心。



# 线性层

在我们将数据从二维张量转换成一维张量后，我们将数据通过线性层，然后是非线性激活层。在我们的架构中，我们有两个线性层，一个是 ReLU，另一个是`log_softmax`函数，它预测给定图像中包含什么数字。



# 训练模型

为了训练模型，我们需要遵循与我们之前的猫狗图像分类问题相同的过程。以下代码片段在提供的数据集上训练我们的模型:

```py
def fit_model(epoch,model,data_loader,phase='training',volatile=False): if phase == 'training':
model.train()
if phase == 'validation': model.eval() volatile=True
running_loss = 0.0
running_correct = 0
for batch_idx , (data,target) in enumerate(data_loader): if is_cuda:
data,target = data.cuda(),target.cuda()
data , target = Variable(data,volatile),Variable(target) if phase == 'training':
optimizer.zero_grad() output = model(data)
loss = F.null_loss(output,target) running_loss +=
F.null_loss(output,target,size_average=False).data[0] predictions = output.data.max(dim=1,keepdim=True)[1]
running_correct += preds.eq(target.data.view_as(predictions)).cpu().sum() if phase == 'training':
loss.backward() optimizer.step()
loss = running_loss/len(data_loader.dataset)
accuracy = 100\. * running_correct/len(data_loader.dataset) print(f'{phase} loss is {loss:{5}.{2}} and {phase} accuracy is
{running_correct}/{len(data_loader.dataset)}{accuracy:{10}.{4}}') return loss,accuracy
```

这个方法有不同的训练和验证逻辑。使用不同模式主要有两个原因:

*   在训练模式中，dropout 删除了一定百分比的值，这在验证或测试阶段是不应该发生的。
*   在训练模式中，我们计算梯度并改变模型的参数值，但是在测试或验证阶段不需要反向传播。

上一个函数中的大部分代码都是不言自明的。在函数的结尾，我们返回该特定时期的模型的损失和准确性。

让我们通过前面的函数运行模型 20 次迭代，并绘制训练和验证的损失和准确性，以了解我们的网络如何运行。以下代码对训练和测试数据集运行`fit`方法 20 次迭代:

```py
model = Network() if is_cuda:
model.cuda()

optimizer = optim.SGD(model.parameters(),lr=0.01,momentum=0.5) training_losses , training_accuracy = [],[]
validation_losses , validation_accuracy = [],[] for epoch in range(1,20):
epoch_loss, epoch_accuracy = fit(epoch,model,training_loader,phase='training')
validation_epoch_loss , validation_epoch_accuracy = fit(epoch,model,test_loader,phase='validation')
training_losses.append(epoch_loss) training_accuracy.append(epoch_accuracy) validation_losses.append(validation_epoch_loss) validation_accuracy.append(validation_epoch_accuracy)
```

以下代码绘制了培训和测试损失:

```py
plt.plot(range(1,len(training_losses)+1),training_losses,'bo',label = 'training loss')
plt.plot(range(1,len(validation_losses)+1),validation_losses,'r',label = 'validation loss')
plt.legend()
```

上述代码生成了下图:

![](img/47b5dd6a-ec5d-4e0e-8e88-fdc77d63df08.png)

以下代码绘制了训练和测试精度:

```py
plt.plot(range(1,len(training_accuracy)+1),training_accuracy,'bo',label = 'train accuracy')
plt.plot(range(1,len(validation_accuracy)+1),validation_accuracy,'r',label = 'val accuracy')
plt.legend()
```

上述代码生成了下图:

![](img/69897521-e770-43fb-a494-52a86fd48428.png)

在第 20 个周期结束时，我们达到了 98.9%的测试精度。我们已经得到了我们的简单卷积模型的工作，并几乎达到了最先进的结果。让我们看看当我们在我们的狗对猫数据集上尝试相同的网络架构时会发生什么。我们将使用前一章第 3 章中的数据、*神经网络的构建模块、*以及 MNIST 示例中的架构，并做一些小的改动。一旦我们训练了模型，我们就可以评估它，以了解我们的简单架构执行得有多好。



# 给狗和猫分类——CNN 从零开始

我们将使用相同的架构，但稍有改动，如下所示:

*   第一个线性层的输入尺寸需要改变，因为我们的猫和狗图像的尺寸是 *256，256* 。
*   我们将添加另一个线性层，以允许模型更灵活地学习。

让我们看看实现网络架构的代码:

```py
class Network(nn.Module): def   init  (self):
super(). init  ()
self.conv1 = nn.Conv2d(3, 10, kernel_size=3) self.conv2 = nn.Conv2d(10, 20, kernel_size=3) self.conv2_drop = nn.Dropout2d()
self.fc1 = nn.Linear(56180, 500) self.fc2 = nn.Linear(500,50) self.fc3 = nn.Linear(50, 2)

def forward(self, x):
x = F.relu(F.max_pool2d(self.conv1(x), 2))
x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2)) x = x.view(x.size(0),-1)
x = F.relu(self.fc1(x))
x = F.dropout(x, training=self.training) x = F.relu(self.fc2(x))
x = F.dropout(x,training=self.training) x = self.fc3(x)
return F.log_softmax(x,dim=1)
```

我们将使用与 MNIST 例子中相同的训练函数，所以我不打算在这里包含代码。然而，让我们看看当模型被训练 20 次迭代时生成的图。

训练和验证数据集的损失绘制如下:

![](img/fc0cc48b-993d-4894-b15a-1646ad638b5e.png)

训练和验证数据集的精度绘制如下:

![](img/368de14d-b697-4367-9f35-4272c130ae3f.png)

从这些图中可以清楚地看出，每次迭代的训练损失都在减少，但是验证损失却越来越严重。准确度在训练期间也增加，但是在 75%时几乎饱和。这是一个明显的例子，表明该模型不具有普遍性。在下一节中，我们将了解另一种称为**迁移学习**的技术，它帮助我们训练更准确的模型，并提供一些技巧来加快训练速度。



# 利用迁移学习对狗和猫进行分类

迁移学习是一种在类似数据集上重用已训练算法的能力，而无需从头开始训练。我们人类不会通过分析成千上万的相似图像来学习识别新图像。我们只是了解实际上区分特定动物的不同特征，比如说，狐狸和狗。我们不需要通过了解线条、眼睛和其他更小的特征来了解狐狸是什么。因此，我们将学习如何使用一个预训练的模型，用很少的数据来构建最先进的图像分类器。

CNN 架构的前几层关注较小的特征，例如直线或曲线的外观。CNN 后面几层中的过滤器学习更高层次的特征，例如眼睛和手指，最后几层学习识别确切的类别。预训练模型是在类似数据集上训练的算法。大多数流行的算法都在流行的 ImageNet 数据集上进行了预训练，以识别 1000 个不同的类别。这种预训练的模型将具有被调整以识别各种模式的滤波器权重。那么，让我们来了解如何利用这些预训练的重量。我们将研究一种叫做 **VGG16** 的算法，这是最早在 ImageNet 竞赛中获得成功的算法之一。尽管有更多的现代算法，这种算法仍然很受欢迎，因为它易于理解和用于迁移学习。让我们看看 VGG16 模型的架构，然后尝试理解该架构，以及我们如何使用它来训练我们的图像分类器:

![](img/13583159-3f95-4e41-afa9-201400628a4a.png)

VGG16 架构包含五个 VGG 模块。一个块是一组卷积层、一个非线性激活函数和一个最大汇集函数。在对 1，000 个类别进行分类时，所有算法参数都经过调整，以达到最先进的结果。该算法采用批处理形式的输入数据，这些数据通过 ImageNet 数据集的平均值和标准偏差进行标准化。

在迁移学习中，我们试图通过冻结架构大多数层的学习参数来捕捉算法学习的内容。仅微调网络的最后几层通常是很好的做法。在本例中，我们将只训练最后几个线性层，而保留卷积层不变，因为通过卷积特征学习的特征主要用于所有类型的图像问题，其中图像具有相似的属性。让我们用迁移学习训练一个 VGG16 模型，对狗和猫进行分类。在下一节中，我们将介绍实现这一点的步骤。



# 创建和探索 VGG16 模型

PyTorch 在其`torchvision`库中提供了一组经过训练的模型。他们中的大多数人接受一个名为`pretrained` when `True`的论点，该论点下载已经为 **ImageNet** 分类问题调整过的权重。我们可以使用下面的代码创建一个 VGG16 模型:

```py
from torchvision import models
vgg = models.vgg16(pretrained=True)
```

现在，我们已经有了 VGG16 模型和所有经过预训练的砝码，随时可以使用。第一次运行代码时，可能需要几分钟，这取决于您的网速。权重的大小可能在 500 MB 左右。我们可以通过打印来快速浏览一下 VGG16 模型。当我们使用现代架构时，理解这些网络是如何实现的变得非常有用。让我们来看看这个模型:

```py
VGG (
  (features): Sequential (
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU (inplace)
    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (3): ReLU (inplace)
    (4): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))
    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (6): ReLU (inplace)
   (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (8): ReLU (inplace)
    (9): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))
    (10):Conv2d(128, 256, kernel_size=(3, 3), stride=(1,1), padding=(1, 1))
    (11): ReLU (inplace)
    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (13): ReLU (inplace)
    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (15): ReLU (inplace)
    (16): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))
    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (18): ReLU (inplace)
    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (20): ReLU (inplace)
    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (22): ReLU (inplace)
    (23): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))
    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1,1))
    (25): ReLU (inplace)   
    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (27): ReLU (inplace)
    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (29): ReLU (inplace)
    (30): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))
  )
  (classifier): Sequential ( 
    (0): Linear (25088 -> 4096)
    (1): ReLU (inplace)
    (2): Dropout (p = 0.5)
    (3): Linear (4096 -> 4096)
    (4): ReLU (inplace)
    (5): Dropout (p = 0.5)
    (6): Linear (4096 -> 1000)
  )
)
```

模型摘要包含两个顺序模型:`features`和`classifiers`。`features`序列模型包含了我们将要冻结的层。



# 冷冻各层

让我们冻结特征模型的所有层，其中包含卷积模块。冻结层中的权重将防止这些卷积块的权重更新。由于模型的权重被训练来识别许多重要的特征，我们的算法将能够从第一次迭代开始就做同样的事情。使用模型权重的能力，最初是为不同的用例训练的，被称为**迁移学习**。

现在，让我们看看如何冻结层的权重或参数:

```py
for param in vgg.features.parameters(): param.requires_grad = False
```

这段代码阻止优化器更新权重。



# 微调 VGG16

VGG16 模型已经被训练来分类 1000 个类别，但是没有被训练来分类狗和猫。因此，我们需要将最后一层的输出特征从 1，000 更改为 2。我们可以使用下面的代码来做到这一点:

```py
vgg.classifier[6].out_features = 2
```

`vgg.classifier`函数让我们可以访问顺序模型中的所有层，第六个元素将包含最后一层。当我们训练 VGG16 模型时，我们只需要训练分类器参数。因此，我们只将`classifier.parameters`传递给优化器，如下所示:

```py
optimizer = optim.SGD(vgg.classifier.parameters(),lr=0.0001,momentum=0.5)
```



# 培训 VGG16 模型

到目前为止，我们已经创建了模型和优化器。由于我们使用的是 Dogs vs Cats 数据集，因此我们可以使用相同的数据加载器和 train 函数来训练我们的模型。请记住:当我们训练模型时，只有分类器内部的参数会发生变化。以下代码片段将为模型定型 20 个时期，从而达到 98.45%的验证准确性:

```py
training_losses , training_accuracy = [],[] 
validation_losses , validation_accuracy = [],[]
for epoch in range(1,20): 
    epoch_loss, epoch_accuracy =
fit(epoch,vgg,training_data_loader,phase='training')
    validation_epoch_loss , validation_epoch_accuracy =
fit(epoch,vgg,valid_data_loader,phase='validation')
    training_losses.append(epoch_loss)
    training_accuracy.append(epoch_accuracy)
    validation_losses.append(validation_epoch_loss)
    validation_accuracy.append(validation_epoch_accuracy)
```

让我们想象一下培训和验证的损失:

![](img/64929940-0c34-4bb1-9fad-2e81bb80cf7d.png)

让我们想象一下培训和验证的准确性:

![](img/ba85c972-cae6-4c39-8559-60a335dc3f3c.png)

我们可以应用一些技巧，如数据扩充，并使用不同的差值来提高模型的泛化能力。以下代码片段将 VGG 分类器模块中的 dropout 值从 0.5 更改为 0.2，并为模型定型:

```py
for layer in vgg.classifier.children(): if(type(layer) == nn.Dropout):
layer.p = 0.2

#Training
training_losses , training_accuracy = [],[] validation_losses , validation_accuracy = [],[]
for epoch in range(1,3): 
    epoch_loss, epoch_accuracy =
fit(epoch,vgg,training_data_loader,phase='training')
    validation_epoch_loss , validation_epoch_accuracy =
fit(epoch,vgg,valid_data_loader,phase='validation')
    training_losses.append(epoch_loss)
    training_accuracy.append(epoch_accuracy)
    validation_losses.append(validation_epoch_loss)
    validation_accuracy.append(validation_epoch_accuracy)
```

训练这几个时代给了我一点点改善；您可以自己尝试使用不同的 dropout 值，看看是否可以获得更好的结果。我们可以用来提高模型泛化能力的另一个重要技巧是添加更多数据或进行数据扩充。我们可以通过随机水平翻转图像或小角度旋转图像来执行数据扩充。`torchvision transforms`模块提供了不同的功能来进行数据扩充，并且它们是动态进行的，每一个时期都会发生变化。我们可以使用以下代码实现数据扩充:

```py
training_transform =transforms.Compose([transforms.Resize((224,224)),
                                     transforms.RandomHorizontalFlip(), 
                                        transforms.RandomRotation(0.2), 
                                        transforms.ToTensor(), 
                                        transforms.Normalize([0.485, 0.32, 0.406], [0.229, 0.224, 0.225])
])

train = ImageFolder('dogsandcats/train/',training_transform) valid = ImageFolder('dogsandcats/valid/',simple_transform)

#Training

training_losses , training_accuracy = [],[]
validation_losses , validation_accuracy = [],[]
for epoch in range(1,3):
    epoch_loss, epoch_accuracy = fit(epoch,vgg,training_data_loader,phase='training')
    validation_epoch_loss , validation_epoch_accuracy = fit(epoch,vgg,valid_data_loader,phase='validation')
    training_losses.append(epoch_loss)
    training_accuracy.append(epoch_accuracy)
    validation_losses.append(validation_epoch_loss)
    validation_accuracy.append(validation_epoch_accuracy)
```

上述代码的输出如下:

```py
#Results
training loss is 0.041 and training accuracy is 22657/23000 98.51 validation loss is 0.043 and validation accuracy is 1969/2000 98.45 training loss is 0.04 and training accuracy is 22697/23000 98.68 validation loss is 0.043 and validation accuracy is 1970/2000 98.5
```

通过运行两个时期，用扩充数据训练模型将模型的准确性提高了 0.1%；我们可以再运行几次来进一步改进。如果你在阅读这本书的时候一直在训练这些模型，你会意识到每个历元的训练可能需要几分钟以上，这取决于你运行的 GPU。让我们看看一种技术，我们可以在几秒钟内训练每个时期。



# 计算预卷积特征

当我们冻结卷积层和训练模型时，全连接层或密集层的输入(`vgg.classifier`)总是相同的。为了更好地理解这一点，让我们将卷积块——在我们的例子中是`vgg.features`块——视为一个已经学习了权重并且在训练期间不会改变的函数。因此，计算卷积特征并存储它们将有助于我们提高训练速度。训练模型的时间将减少，因为我们只计算这些特征一次，而不是为每个时期计算。

让我们直观地理解这一点并实现它:

![](img/27b0f4e9-1526-45ff-b025-57f6078dd457.png)

第一个框描述了一般情况下如何进行训练，这可能会很慢，因为我们计算每个历元的卷积特征，尽管这些值不会改变。在底部的框中，我们计算一次卷积特征，并且只训练线性层。为了计算预卷积特征，我们需要通过卷积块传递所有训练数据并存储它们。为此，我们需要选择 VGG 模型的卷积模块。幸运的是，VGG16 的 PyTorch 实现有两个顺序模型，因此只需挑选第一个顺序模型的特性就足够了。以下代码为我们完成了这项工作:

```py
vgg = models.vgg16(pretrained=True) vgg = vgg.cuda()
features = vgg.features

training_data_loader = torch.utils.data.DataLoader(train,batch_size=32,num_workers=3,shuffle=False)
valid_data_loader = torch.utils.data.DataLoader(valid,batch_size=32,num_workers=3,shuffle=False)

def preconvfeat(dataset,model):
    conv_features = [] 
    labels_list = []
    for data in dataset: 
        inputs,labels = data
        if is_cuda:
            inputs , labels = inputs.cuda(),labels.cuda() 
        inputs , labels = Variable(inputs),Variable(labels) 
        output = model(inputs)
        conv_features.extend(output.data.cpu().numpy())
        labels_list.extend(labels.data.cpu().numpy())
    conv_features = np.concatenate([[feat] for feat in conv_features])

    return (conv_features,labels_list)
conv_feat_train,labels_train = preconvfeat(training_data_loader,features) conv_feat_val,labels_val = preconvfeat(valid_data_loader,features)
```

在前面的代码中，`preconvfeat`方法接收数据集和`vgg`模型，并返回复杂的特征，以及与之关联的标签。其余的代码类似于我们在前面的例子中用来创建数据加载器和数据集的代码。

一旦我们有了训练和验证集的卷积特性，我们就可以创建 PyTorch 数据集和`DataLoader`类，这将简化我们的训练过程。以下代码为我们的卷积特征创建数据集和`DataLoader`:

```py
class CustomDataset(Dataset):
def init (self,feat,labels): self.conv_feat = feat self.labels = labels
def len (self):
return len(self.conv_feat) def getitem (self,idx):
return self.conv_feat[idx],self.labels[idx]

training_feat_dataset = CustomDataset(conv_feat_train,labels_train) validation_feat_dataset = CustomDataset(conv_feat_val,labels_val)

training_feat_loader = DataLoader(training_feat_dataset,batch_size=64,shuffle=True)
validation_feat_loader = DataLoader(validation_feat_dataset,batch_size=64,shuffle=True)
```

因为我们有了新的数据加载器，它可以生成批量复杂的要素和标注，所以我们可以使用在其他示例中使用的相同训练函数。现在，我们将使用`vgg.classifier`作为模型来创建优化器和拟合方法。下面的代码训练分类器模块来识别狗和猫。在 Titan X GPU 上，每个历元不到 5 秒，否则需要几分钟:

```py
training_losses , training_accuracy = [],[] validation_losses , validation_accuracy = [],[]
for epoch in range(1,20): epoch_loss, epoch_accuracy =
fit_numpy(epoch,vgg.classifier,training_feat_loader,phase='training') validation_epoch_loss , validation_epoch_accuracy =
fit_numpy(epoch,vgg.classifier,validation_feat_loader,phase='validation') training_losses.append(epoch_loss) training_accuracy.append(epoch_accuracy) validation_losses.append(validation_epoch_loss) validation_accuracy.append(validation_epoch_accuracy)
```



# 理解 CNN 模型学到了什么

深度学习模型经常被认为是不可解释的。然而，我们可以使用不同的技术来解释这些模型内部发生的事情。对于图像来说，修道院所学到的特征是可以解释的。在这一节中，我们将探讨两种流行的技术，以便我们可以了解修道院。



# 可视化中间层的输出

将中间层的输出可视化将有助于我们理解输入图像如何在不同层之间转换。通常，每一层的输出被称为**激活**。要做到这一点，我们应该从中间层提取输出，这可以通过不同的方式完成。PyTorch 提供了一个名为`register_forward_hook`的方法，它允许我们传递一个可以提取特定层的输出的函数。

默认情况下，PyTorch 模型只存储最后一层的输出，以便优化使用内存。因此，在我们检查中间层的激活看起来像什么之前，让我们学习如何从模型中提取输出。看一下下面的代码片段，它提取了输出。我们将通过它来了解发生了什么:

```py
vgg = models.vgg16(pretrained=True).cuda()

class LayerActivations(): features=None
def   init  (self,model,layer_num):
self.hook = model[layer_num].register_forward_hook(self.hook_fn) def hook_fn(self,module,input,output):
self.features = output.cpu() def remove(self):
self.hook.remove()

conv_out = LayerActivations(vgg.features,0) o = vgg(Variable(img.cuda())) conv_out.remove()
act = conv_out.features
```

我们首先创建一个预训练的 VGG 模型，从中提取特定层的输出。`LayerActivations`类指示 PyTorch 将图层的输出存储在 features 变量中。让我们遍历一下`LayerActivations`类中的每个函数。

`_init_`函数将模型和需要提取输出的图层的图层号作为参数。我们在层上调用`register_forward_hook`方法并传入一个函数。PyTorch 在向前传递时——也就是说，当图像通过各层时——调用传递给`register_forward_hook`方法的函数。该方法返回一个句柄，该句柄可用于注销传递给`register_forward_hook`方法的函数。

`register_forward_hook`方法向我们传递给它的函数传递三个值。`module`参数允许我们访问层本身。第二个参数是`input`，它指的是流经该层的数据。第三个参数是`output`，它允许我们访问层的转换输入或激活。我们将`features`变量的输出存储在`LayerActivations`类中。

第三个函数从`_init_`函数获取钩子，并取消注册该函数。现在，我们可以传递我们正在寻找的激活的模型和层数。让我们看看将为不同层的以下图像创建的激活:

![](img/7b52b298-91e3-43a8-a623-b828e727415f.png)

让我们想象一下第一个卷积层创建的一些激活以及用于此的代码:

```py
fig = plt.figure(figsize=(20,50)) fig.subplots_adjust(left=0,right=1,bottom=0,top=0.8,hspace=0,
wspace=0.2)
for i in range(30):
ax = fig.add_subplot(12,5,i+1,xticks=[],yticks=[]) ax.imshow(act[0][i])
```

让我们想象一下第五卷积层产生的一些激活:

![](img/b30d86fd-298a-4867-bd10-02ba5e7a3209.png)

让我们看看最后一层 CNN:

![](img/d84ef366-2e9b-4c37-a15c-ef24748a92cc.png)

通过查看不同层生成的内容，我们可以看到早期层检测线条和边缘，而最终层倾向于学习高级功能，可解释性较差。

在我们继续可视化权重之前，让我们了解一下 ReLU 图层之后的要素地图或激活是如何表示它们自己的。那么，让我们来看看第二层的输出。

如果您快速查看前面图像第二行中的第五幅图像，看起来滤镜正在检测图像中的眼睛。当模型不执行时，这些可视化技巧可以帮助我们理解为什么模型可能不工作。



# 可视化 CNN 图层的权重

获取特定层的模型权重非常简单。所有的模型重量都可以通过`state_dict`功能访问。`state_dict`函数返回一个字典，用`keys`作为它的层，用`weights`作为它的值。以下代码演示了如何提取特定图层的权重并将其可视化:

```py
vgg.state_dict().keys()
cnn_weights = vgg.state_dict()['features.0.weight'].cpu()
```

上述代码为我们提供了以下输出:

![](img/eeebe3c9-3388-483b-a64a-fd6b9ae61753.png)

每个盒子代表尺寸为 *3* x *3* 的过滤器的重量。每个过滤器被训练来识别图像中的特定模式。



# 摘要

在本章中，我们学习了如何使用 convents 构建图像分类器，以及如何使用预训练模型。我们讲述了如何通过使用预卷积特性来加速训练过程的技巧。我们还研究了可以用来理解 CNN 内部情况的不同技术。

在下一章，我们将学习如何使用循环神经网络处理顺序数据。