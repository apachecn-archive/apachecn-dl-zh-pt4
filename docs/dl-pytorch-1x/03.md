<title>Building Blocks of Neural Networks</title> <link rel="stylesheet" href="css/style.css" type="text/css"> 

# 神经网络的构建模块

理解神经网络的基本构建块，如张量、张量运算和梯度下降，对于构建复杂的神经网络非常重要。在这一章中，我们将提供神经网络的概述，同时深入 PyTorch API 的基础。神经网络的最初想法是受人脑中发现的生物神经元的启发，但是，在撰写本文时，它们之间的相似之处只是表面的，对这两个系统的任何比较都可能导致对这些系统中任何一个的错误假设。因此，我们不会考虑这两个系统之间的相似之处，而是直接深入分析人工智能中使用的神经网络。

在本章中，我们将讨论以下主题:

*   什么是神经网络？
*   在 PyTorch 中构建神经网络
*   理解 PyTorch 张量
*   理解张量运算

<title>What is a neural network?</title> <link rel="stylesheet" href="css/style.css" type="text/css"> 

# 什么是神经网络？

简而言之，神经网络是一种学习输入变量及其相关目标变量之间关系的算法。例如，如果您有一个由学生的 GPA、GRE 分数、大学排名和学生的大学录取状态组成的数据集，我们可以使用神经网络来预测学生的录取状态(目标变量)，给定他们的 GPA、GRE 分数和大学排名(输入变量):

![](assets/4b50745c-b26d-4788-82dc-2d9621b5afc1.png)

在上图中，每个箭头代表一个重量。这些权重是从训练数据{ ( (x1，y1)，(x2，y2)，...，(xm，ym) ) }，以便从操作中创建的复合特征可以预测学生的录取状态。

例如，网络可以从机构的排名中了解 GPA/GRE 的重要性，如下图所示:

![](assets/8b310c2d-99ad-4d4d-ae67-cc115371c46e.png)

<title> Understanding the structure of neural networks</title> <link rel="stylesheet" href="css/style.css" type="text/css"> 

# 理解神经网络的结构

神经网络中的运算是由两个基本计算建立的。一个是权重向量和它们对应的输入变量向量之间的点积，而另一个是将乘积转换到非线性空间的函数。我们将在下一章学习这些函数的几种类型。

让我们进一步细分:第一个点积学习混合概念，因为它创建了取决于每个输入变量重要性的输入变量的混合。将这些重要特征传递到非线性函数中，使我们能够构建比仅使用传统线性组合更强大的输出:

![](assets/867a40d7-5261-460f-9c56-4c9f019a182e.png)

通过使用这些操作作为构建模块，我们可以构建健壮的神经网络。让我们分解前面的神经网络例子；神经网络学习反过来是目标变量的最佳预测者的特征。因此，神经网络的每一层都学习可以帮助神经网络更好地预测目标变量的特征:

![](assets/403f4aa0-5ac4-45cd-96b0-8f1a3263f030.png)

在上图中，我们可以看到如何使用神经网络预测房价。

<title>Building a neural network in PyTorch</title> <link rel="stylesheet" href="css/style.css" type="text/css"> 

# 在 PyTorch 中构建神经网络

让我们从在 PyTorch 中构建一个神经网络开始，它将帮助我们预测大学生的录取状态。在 PyTorch 中有两种建立神经网络的方法。首先，我们可以使用更简单的`torch.nn.Sequential`类，它允许我们在实例化我们的网络时，将我们期望的神经网络中的操作序列作为参数传递。

另一种方法，也是更复杂、更强大但更优雅的方法，是将我们的神经网络定义为一个从`torch.nn.Module`类继承的类:

![](assets/210dec47-aa4a-47d3-ac15-0bf205b43c13.png)

我们将使用这两种模式构建我们的神经网络，这两种模式都是由 PyTorch API 定义的。

<title>PyTorch sequential neural network</title> <link rel="stylesheet" href="css/style.css" type="text/css"> 

# PyTorch 序列神经网络

神经网络中所有常用的操作都可以在`torch.nn`模块中找到。因此，我们需要从导入所需的模块开始:

```py
import torch
import torch.nn as nn
```

现在，让我们看看如何使用`torch.nn.Sequential`类构建一个神经网络。我们使用在`torch.nn`模块中定义的操作，并以连续的方式将它们作为参数传递给`torch.nn.Sequential`类来实例化我们的神经网络。导入操作后，我们的神经网络代码应该如下所示:

```py
My_neuralnet = nn.Sequential(operationOne,operationTwo…)
```

构建神经网络时最常用的操作是`nn.Linear()`操作。它需要两个参数:`in_features`和`out_features`。`in_features`参数是输入的大小。在我们的例子中，我们有三个输入特征:GPA、GRE 和大学排名。`out_features`参数是输出的大小，在我们的例子中是 2，因为我们想从输入中了解两个特征，它们可以帮助我们预测学生的录取状态。实际上，`nn.Linear(in_features, out_features)`操作接受输入并创建权重向量来执行点积。

在我们的例子中，`nn.Linear(in_features` `= 3,` `out_features` `= 2)`会创建两个向量:[w11，w12，w13]和[w21，w22，w23]。当输入[xGRE，xGPA，xrank]被传递到神经网络时，我们将创建两个输出的向量，[h1，h2]，这将是[w11]的结果。xGRE + w12。xGPA + w13。xrank，w21。xGRE + w22。xGPA + w23。xrank】。

当您想要继续向神经网络添加更多层时，这种模式会继续向下游发展。下图显示了神经网络被转化为`nn.Linear()`操作后的结构:

![](assets/3c7055d6-5e0d-4a87-a361-0aac3762905f.png)

太好了！但是增加更多的线性运算并不能利用神经网络的能力。我们还必须使用几个非线性函数之一将这些输出转换到非线性空间。这些函数的类型以及它们各自的优点和缺点将在下一章详细描述。现在，让我们使用一个最常用的非线性函数，即**整流线性单元**，也称为 **ReLU** 。PyTorch 为我们提供了一个内置的 ReLU 操作符，可以通过调用`nn.ReLU()`来访问。下图显示了非线性函数如何分类或解决线性函数无法解决的学习问题:

![](assets/52e8414b-b952-4ac8-9a77-660005093bcb.png)

最后，为了得到我们的预测，我们需要将输出压缩到 0 和 1 之间。这些状态分别指不准入和准入。如下图所示，sigmoid 函数是将负无穷大和正无穷大之间的连续量转换为 0 到 1 之间的值的最常用函数。在 PyTorch 中，我们只需要调用`nn.Sigmoid()`操作:

![](assets/911de9c9-979b-4f14-80df-9d889d683c91.png)

现在，让我们将我们的神经网络代码放在 PyTorch 中，这样我们就可以得到一个结构如下图所示的网络:

![](assets/9aec6633-8406-45d1-acde-3959e9c14888.png)

执行此操作的代码如下:

```py
import torch
import torch.nn as nn
my_neuralnet = nn.Sequential(nn.Linear(3,2),
  nn.ReLU(),
  nn.Linear(2, 1),
  nn.Sigmoid())
```

就是这样！在 PyTorch 中构建神经网络就是这么简单。Python 对象包含了我们的神经网络。稍后我们将看看如何使用它。现在，让我们看看如何使用更高级的 API 来构建神经网络，这是基于定义一个从`nn.Module`类继承的类。

<title>Building a PyTorch neural network using nn.Module</title> <link rel="stylesheet" href="css/style.css" type="text/css"> 

# 用神经网络建立 PyTorch 神经网络。组件

使用`nn.Module`类定义神经网络也是简单而优雅的。它首先定义一个继承自`nn.Module`类的类，并覆盖两个方法:`__init__()`和`forward()`方法。`__init__()`方法应该包括作为我们想要的神经网络的一部分的操作。另一方面，`forward()`方法应该描述通过这些期望的层操作的数据流。因此，代码的结构应该如下所示:

```py
class MyNeuralNet(nn.Module):
# define the __init__() method
def __init__(self, other_features_for_initialization):
# Initialize Operations for Layers
# define the forward() method
def forward(self, x):
# Describe the flow of data through the layers
```

让我们更详细地理解这个模式。关键字`class`有助于定义一个 Python 类，后跟您希望的任何名称。这种情况下就是`MyNeuralNet`。然后，圆括号中传递的参数是我们当前定义的类将要继承的类。所以，我们总是从`MyNeuralNet(nn.Module)`类开始。

`self`是传递给类中定义的每个方法的任意第一个参数。它表示类的实例，可用于访问类中定义的属性和方法。

`__init__()`方法是 Python 类中的保留方法。它也被称为构造函数。每当实例化该类的一个对象时，就会运行包装在`__init__()`方法中的代码。一旦我们的神经网络类的一个对象被实例化，这将帮助我们设置所有的神经网络操作。

需要注意的一点是，一旦我们在神经网络类中定义了`__init__()`方法，我们就无法访问在`nn.Module`类的`__init__()`方法中定义的所有代码。幸运的是，Python `super()`函数可以帮助我们运行`nn.Module`类的`__init__()`方法中的代码。我们需要做的就是在新的`__init__()`方法的第一行使用`super()`函数。使用`super()`函数来访问`__init__()`方法非常简单；我们就用`super(NameOfClass, self).__init__()`。在我们的例子中，这将是`super(MyNeuralNet, self).__init__()`。

现在我们知道了为我们的`__init__()`方法编写第一行代码需要什么，让我们看看在`__init__()`方法的定义中还需要包含哪些代码。我们必须将 PyTorch 中定义的操作存储为`self`的属性。在我们的例子中，我们有两个`nn.Linear`操作:一个是从输入变量到神经网络层的两个节点，另一个是从这些节点到输出节点。因此，我们的`__init__()`方法将如下所示:

```py
class MyNeuralNet(nn.Module):
def __init__(self):
    super(MyNeuralNet, self).__init__()
   self.operationOne = nn.Linear(3, 2)
    self.operationTwo = nn.Linear(2, 1)
```

在前面的代码中，我们将所需神经网络的操作作为属性存储在`self`中。您应该习惯于将 PyTorch 中的操作作为属性存储在`self`中。我们使用的模式如下:

```py
 self.desiredOperation = PyTorchOperation
```

然而，在前面的代码中有一个明显的错误:对`nn.Linear`的输入是硬编码的，所以如果输入大小改变，我们必须再次重写我们的神经网络类。因此，最好使用变量名，并在实例化对象时将它们作为参数传递。代码如下所示:

```py
def __init__(self, input_size, n_nodes, output_size):
super(MyNerualNet, self).__init__()
self.operationOne = nn.Linear(input_size, n_nodes)
self.operationTwo = nn.Linear(n_nodes, output_size)
```

现在，让我们深入研究一下`forward()`方法的实现。这个方法有两个参数:`self`参数和任意的`x`参数，后者是我们真实数据的占位符。

我们已经看过了`nn.ReLU`操作，但是 PyTorch 中还定义了更方便的函数接口，它允许我们更好地描述数据流。值得注意的是，这些功能等同物不能在顺序 API 中使用。我们的第一项工作是将由`x`参数表示的数据传递给神经网络中的第一个操作。在 PyTorch 中，将数据传递给网络中的第一个操作就像使用`self.operationOne(x)`一样简单。

然后，使用 PyTorch 函数接口，我们可以通过使用`torch.nn.functional.relu.self.operationOne(x)`将该操作的输出传递给非线性 ReLU 函数。让我们把所有的东西放在一起，定义`forward()`方法。重要的是要记住，最终输出必须伴随着`return`关键字:

```py
def forward(self, x):
x = self.operationOne(x)
x = nn.functional.relu(x)
x = self.operationTwo(x)
output = nn.functional.sigmoid(x)
return output
```

现在，让我们润色和编译一切，以便我们可以使用基于类的 API 在 PyTorch 中定义我们的神经网络。以下代码是您在开源社区中可以找到的大多数 PyTorch 代码:

```py
import torch
import torch.nn as nn
import torch.nn.functional as F
class MyNeuralNet(nn.Module):
def __init__(self, input_size, n_nodes, output_size):
    super(MyNeuralNet, self).__init__()
    self.operationOne = nn.Linear(input_size, n_nodes)
    self.operationTwo = nn.Linear(n_nodes, output_size)
def forward(self, x):
    x = F.relu(self.operationOne(x)
   x = self.operationTwo(x)
    x = F.sigmoid(x)
return x
```

最后，为了访问我们的神经网络，我们必须实例化一个`MyNeuralNet`类的对象。我们可以这样做:

```py
my_network = MyNeuralNet(input_size = 3, n_nodes = 2, output_size = 1)
```

现在，我们可以使用`my_network` Python 变量访问我们想要的神经网络。我们已经建立了我们的神经网络，那么下一步是什么？它能预测一个学生现在的录取情况吗？不。但是我们会到达那里。在此之前，我们需要了解数据应该如何在 PyTorch 中表示，以便我们的神经网络能够理解它。这就是 PyTorch 张量发挥作用的地方。

<title>Understanding PyTorch Tensors</title> <link rel="stylesheet" href="css/style.css" type="text/css"> 

# 理解 PyTorch 张量

PyTorch Tensors 是 PyTorch 中驱动计算的引擎。如果你有 Numpy 的经验，理解 PyTorch 张量将是轻而易举的事。您在 Numpy 数组中学到的大多数模式都可以转换成 PyTorch 张量。

张量是数据容器，是向量和矩阵的广义表示。向量是一阶张量，因为它只有一个轴，看起来像[x1，x2，x3，..].矩阵是二阶张量，有两个轴，看起来像[ [x11，x12，x13..]，[x21，x22，x23..] ].另一方面，标量是只包含单个元素的零阶张量，例如 x1。如下图所示:

![](assets/c6aa9a85-08c1-4327-b7e8-fd7bdd989524.png)

我们可以立即观察到，我们的数据集具有 GPA、GRE、Rank 和 Admit Status 列以及各种观察结果行，可以表示为二阶张量:

![](assets/af640e04-447c-4b73-b733-1d9c6ea82470.png)

让我们快速看一下如何从 Python 列表创建 PyTorch 张量:

```py
import torch
first_order_tensor = torch.tensor([1, 2, 3])
print(first_order_tensor)
#tensor([1, 2, 3])
```

从这个容器访问元素也很简单，索引从 0 开始，到 n - 1 结束，其中 n 是容器中元素的数量:

```py
print(first_order_tensor[0])
#tensor(1)
```

我们之前打印的`tensor(1)`，是一个零级张量。访问多个元素类似于我们在 NumPy 和 Python 中的做法，其中 0:2 从索引 0 的元素开始提取元素，直到(但不包括)索引 2 的元素:

```py
print(first_order_tensor[0:2])
#tensor([1, 2])
```

如果您想要访问从特定索引开始的张量的所有元素，您可以使用 k:，其中 k 是您想要提取的第一个元素的索引:

```py
print(first_order_tensor[1:])
#tensor([2, 3])
```

现在，让我们了解二阶张量是如何工作的:

```py
second_order_tensor = torch.tensor([ [ 11, 22, 33 ],
                                     [ 21, 22, 23 ]
                                   ])

print(second_order_tensor)

#tensor([[11, 12, 13],
         [21, 22, 23]])
```

从二阶张量获取元素稍微复杂一点。现在，让我们从之前创建的张量中访问元素 12。将二阶张量视为由两个一阶张量构成的张量是很重要的，例如，[ [一阶张量]，[一阶张量] ]。元素 12 位于一阶的第一个张量内，在该张量内，它位于第二个位置，即索引 1。因此，我们可以通过使用[0，1]来访问元素 22，其中 0 描述了一阶张量的索引，1 描述了元素在一阶张量内的索引:

```py
print(second_order_tensor[0, 1])
#tensor(12)
```

现在，让我们做一个小的心理练习:你如何从我们创建的张量中访问元素 23？是啊！你是对的！我们可以使用[1，2]来访问它。

同样的模式也适用于更高维度的张量。需要注意的是，访问所需标量元素所需的索引位置的数量等于张量的阶数。让我们用 4 阶张量做一个练习！

在我们开始之前，让我们想象一个 4 阶张量；它必须由三阶张量组成。因此，它必须看起来类似于[[3 阶张量]，[3 阶张量]，[3 阶张量]...].这些 3 阶张量中的每一个都必须依次由 2 阶张量组成，看起来像[[2 阶张量]，[2 阶张量]，…]，等等。

在这里，你会发现一个四阶张量。为了便于观察，它被优雅地隔开了。在本练习中，我们需要访问元素 1112、1221、2122 和 2221:

```py
fourth_order_tensor = torch.tensor(
[
    [
        [
            [1111, 1112],
            [1121, 1122]
        ],
        [
            [1211, 1212],
            [1221, 1222]
        ]
    ],
    [
        [
            [2111, 2112],
            [2121, 2122]
        ],
        [
            [2211, 2212],
            [2221, 2222]
        ]  
    ]
])
```

这里张量由两个三阶张量组成，每个三阶张量有两个二阶张量，它们又包含两个一阶张量。让我们看看如何访问元素 2122；其余的留给你在业余时间做。2122 元素包含在我们原张量[[3 阶张量]，[* 3 阶张量] ]中的三阶第二张量中。所以，第一个索引位置是 1。接下来在 3 阶张量中，我们想要的元素在 2 阶的第一个张量[[* 2 阶张量]，[2 阶张量]]。所以，第二个索引位置是 0。在二阶张量内部，我们想要的元素在一阶的第二个张量[[一阶张量]，[*一阶张量]，所以索引位置是 1。最后，在一阶张量中，我们想要的元素是第二个元素[2121，2122]，它的索引位置是 1。当我们把这些放在一起时，我们可以通过使用`fourth_order_tensor[1, 0, 1, 1]`来索引元素 2122。

<title>Understanding Tensor shapes and reshaping Tensors</title> <link rel="stylesheet" href="css/style.css" type="text/css"> 

# 理解张量形状和重塑张量

既然我们知道了如何从张量中获取元素，那么理解张量形状就很容易了。所有 PyTorch 张量都有一个`size()`方法来描述张量在每个轴上的大小。零阶 PyTorch 张量，或者说标量，没有轴，所以它没有可量化的大小。让我们看看 PyTorch 中几个张量的大小:

```py
my_tensor = torch.tensor([1, 2, 3, 4, 5])
print(my_tensor.size())
# torch.Size([5])
```

由于张量沿第一轴有五个元素，张量的大小为[5]:

```py
my_tensor = torch.tensor([[11, 12, 13], [21, 22, 23]])
print(my_tensor.size())
# torch.Size([2, 3])
```

因为二阶张量由两个一阶张量组成，所以第一轴的大小是 2，并且两个一阶张量中的每一个都由 3 个标量元素组成，其中沿着第二轴的大小是 3。所以张量的大小是[2，3]。

这种模式扩展到更高阶的张量。让我们用前一小节中创建的`fourth_order_tensor`来完成一个快速练习。有两个三阶张量，每个三阶张量有两个一阶张量，这两个一阶张量又有两个包含两个标量元素的一阶张量。因此，张量的大小为[2，2，2，2]:

```py
print(fourth_order_tensor.size())
# torch.Size([2, 2, 2, 2])
```

现在我们已经了解了张量的大小，我们可以使用`torch.rand()`快速生成带有我们想要的形状的随机元素的张量，并将想要的张量大小作为参数传递给它。在 PyTorch 中还有其他生成张量的方法，我们将在本书的后面介绍。张量中创建的元素可能与您在这里看到的不同:

```py
random_tensor = torch.rand([4, 2])
print(random_tensor)
#tensor([[0.9449, 0.6247],
        [0.1689, 0.4221],
        [0.9565, 0.0504],
        [0.5897, 0.9584]])
```

有时你也想改变张量的形状，也就是说，你想把张量中的元素移到不同的轴上。我们使用`.view()`方法来重塑张量。让我们深入研究一个在 PyTorch 中如何做到这一点的快速示例:

```py
random_tensor.view([2, 4])
#tensor([[0.9449, 0.6247, 0.1689, 0.4221],
         [0.9565, 0.0504, 0.5897, 0.9584]])
```

值得注意的是，这不是一个就地操作，原始的`random_tensor`仍然是大小为[4，2]。您将需要分配返回值来实际存储结果。有时，当您有很多轴时，您可以使用-1 让 PyTorch 计算一个特定轴的大小:

```py
random_tensor = torch.rand([4, 2, 4])
random_tensor.view([2, 4, -1])
#tensor([[[0.1751, 0.2434, 0.9390, 0.4585],
          [0.5018, 0.5252, 0.8161, 0.9712],
          [0.7042, 0.4778, 0.2127, 0.3466],
          [0.6339, 0.4634, 0.8473, 0.8062]],
        [[0.3456, 0.0725, 0.0054, 0.4665],
         [0.9140, 0.2361, 0.4009, 0.4276],
         [0.3073, 0.9668, 0.0215, 0.5560],
         [0.4939, 0.6692, 0.9476, 0.7543]]])

random_tensor.view([2, -1, 4])
#tensor([[[0.1751, 0.2434, 0.9390, 0.4585],
          [0.5018, 0.5252, 0.8161, 0.9712],
          [0.7042, 0.4778, 0.2127, 0.3466],
          [0.6339, 0.4634, 0.8473, 0.8062]],
        [[0.3456, 0.0725, 0.0054, 0.4665],
         [0.9140, 0.2361, 0.4009, 0.4276],
         [0.3073, 0.9668, 0.0215, 0.5560],
         [0.4939, 0.6692, 0.9476, 0.7543]]])
```

<title>Understanding tensor operations</title> <link rel="stylesheet" href="css/style.css" type="text/css"> 

# 理解张量运算

到目前为止，我们已经研究了基本的张量性质，但使它们如此特殊的是它们执行矢量化运算的能力，这对执行神经网络极其重要。让我们快速看一下 PyTorch 中的一些张量运算。

加法、减法、乘法和除法运算是按元素执行的:

![](assets/cd185c74-2cad-4315-8799-e37da228ea2c.png)

让我们快速看一下这些操作:

```py
x = torch.tensor([5, 3])
y = torch.tensor([3, 2])
torch.add(x, y)
# tensor([8, 5])
torch.sub(x, y)
# tensor([2, 1])
torch.mul(x, y)
# tensor([15,  6])
```

也可以使用+、-、*和/运算符对火炬张量执行以下操作:

```py
x + y
# tensor([8, 5])
```

让我们快速看一下 torch 张量中的矩阵乘法，可以使用`torch.matmul()`或`@`运算符来执行:

```py
torch.matmul(x, y)
# tensor(21)
x @ y
# tensor(21)
```

我们还没有对两个张量进行除法运算是有具体原因的。让我们现在就开始吧:

```py
torch.div(x, y)
# tensor([1, 1])
```

什么？这怎么可能呢？5 / 3 应该大约是 1.667，3 / 2 应该是 1.5。但是为什么我们得到的结果是`tensor([1, 1])`？如果你猜测是因为张量中存储的元素的数据类型，那么你绝对是对的！

<title>Understanding Tensor types in PyTorch</title> <link rel="stylesheet" href="css/style.css" type="text/css"> 

# 理解 PyTorch 中的张量类型

PyTorch 张量只能在张量中存储单一数据类型的元素。PyTorch 中还定义了一些需要特定数据类型的方法。因此，了解 PyTorch 张量可以存储的数据类型非常重要。根据 PyTorch 文档，PyTorch 张量可以存储以下数据类型:

![](assets/d43eb43a-47a1-4e7a-8eb1-316d84a3e3d4.png)

每个 PyTorch 张量都有一个`dtype`属性。让我们看看之前创建的张量的`dtype`:

```py
x.dtype
# torch.int64
y.dtype
# torch.int64
```

在这里，我们可以看到存储在我们创建的张量中的元素的数据类型是 int64。因此，在元素之间执行的除法是整数除法！

让我们通过将`dtype`参数传递给`torch.tensor()`来重新创建具有 32 位浮点元素的 PyTorch 张量:

```py
x_float = torch.tensor([5, 3], dtype = torch.float32)
y_float = torch.tensor([3, 2], dtype = torch.float32)
print(x_float / y_float)
# tensor([1.6667, 1.5000])
```

你也可以使用`torch.FloatTensor()`，或者前面截图中`tensor`列下的其他名字，直接创建你想要类型的张量。您还可以使用`.type()`方法将张量转换为其他数据类型:

```py
torch.FloatTensor([5, 3])
# tensor([5., 3.])
x.type(torch.DoubleTensor)
# tensor([5., 3.], dtype=torch.float64)
```

<title>Importing our dataset as a PyTorch Tensor</title> <link rel="stylesheet" href="css/style.css" type="text/css"> 

# 将数据集作为 PyTorch 张量导入

现在，让我们将我们的`admit_status.csv`数据集作为 PyTorch 张量导入，这样我们就可以将它输入到我们的神经网络中。为了导入数据集，我们将使用 Python 中的 NumPy 库。我们将使用的数据集可以在下图中看到:

![](assets/c11a11b3-1f2a-43dc-8b96-5c520abd4cb0.png)

当我们导入数据集时，我们不想导入第一行，它们是列名。我们将使用 numpy 库中的`np.genfromtext()`来读取 NumPy 数组中的数据:

```py
import numpy as np
admit_data = np.genfromtxt('../datasets/admit_status.csv',
delimiter = ',', skip_header = 1)
            print(admit_data)
```

这将为我们提供以下输出:

![](assets/176e606b-204b-405b-87f8-8aee431430a9.png)

我们可以使用`torch.from_numpy()`直接导入一个 numpy 数组作为 PyTorch 张量:

```py
admit_tensor = torch.from_numpy(admit_data)
print(admit_tensor)
```

这将为我们提供以下输出:

![](assets/06ce88c0-349b-4be3-859b-340c30611f2e.png)

<title>Training neural networks in PyTorch</title> <link rel="stylesheet" href="css/style.css" type="text/css"> 

# PyTorch 中训练神经网络

我们有 PyTorch 张量形式的数据，也有 PyTorch 神经网络。我们现在能预测学生的录取情况吗？不，还没有。首先，我们需要了解可以帮助我们预测录取状态的特定权重:

![](assets/db55a7de-3df9-41b3-ad8f-d64ba0a3745b.png)

我们之前定义的神经网络首先随机生成权重。因此，如果我们将数据直接传递给神经网络，我们将得到无意义的预测。

神经网络中在训练过程中有帮助的两个重要组件是**标准**和**优化器**。该标准生成一个损失分数，该分数与来自神经网络的预测与真实基础事实(即目标变量，在我们的情况下是录取状态)相比有多远成比例。

优化器利用这个分数来调整神经网络中的权重，以便网络的预测尽可能接近实际情况。

优化器使用来自标准的损失分数来更新神经网络的权重的这个迭代过程被称为神经网络的训练阶段。现在，我们可以训练我们的神经网络。

在我们继续训练神经网络之前，我们必须将数据集分成输入 x 和目标 y:

```py
x_train = admit_tensor[:300, 1:]
y_train = admit_tensor[:300, 0]
x_test = admit_tensor[300:, 1:]
y_test = admit_tensor[300:, 0]
```

我们需要创建一个标准和优化器的实例，这样我们就可以训练我们的神经网络。PyTorch 内置了多个标准，可从`torch.nn`模块访问。在这种情况下，我们将使用`BCELoss()`，也称为**二元交叉熵损失**，用于二元分类:

```py
criterion = nn.BCELoss()
```

PyTorch 中的`torch.optim`模块内置了几个优化器。这里，我们将使用 **SGD 优化器**，也称为**随机梯度下降优化器**。优化器将神经网络的参数或权重作为一个参数，可以通过在我们之前创建的神经网络实例上使用`parameters()`方法来访问该参数:

```py
optimizer = torch.optim.SGD(my_network.parameters(), lr=0.01)
```

我们必须编写一个 for 循环来迭代更新权重的过程。首先，我们需要传递我们的数据以从神经网络获得预测。这非常简单:我们只需要将输入数据作为参数传递给带有`y_pred = my_neuralnet(x_train)`的神经网络实例。然后，我们需要计算损失分数，这是通过将来自神经网络的预测和基础事实传递给标准而得到的。我们可以用`loss_score = criterion(y_pred, y_train)`做到这一点。

在我们继续更新神经网络中的权重之前，重要的是清除我们在优化器上使用`zero_grad()`方法时积累的任何梯度。然后，为了执行反向传播步骤，我们在计算的`loss_score`上使用`backward()`方法。最后，我们使用优化器上的`step()`方法更新参数或权重。

所有之前的逻辑必须在一个循环中，我们迭代训练过程，直到我们的网络学习到最佳参数。所以，让我们把所有的东西放在一起，变成可行的代码:

```py
for epoch in range(100):
 # Forward Propagation
 y_pred = my_network(x_train)

 # Compute and print loss
 loss_score = criterion(y_pred, y_train)
 print('epoch: ', epoch,' loss: ', loss.item())

 # Zero the gradients
 optimizer.zero_grad()

 # perform a backward pass (backpropagation)
 loss_score.backward()

 # Update the parameters
 optimizer.step()
```

瞧啊。我们已经训练了我们的神经网络，它已经准备好执行预测。在下一章中，我们将深入研究神经网络中使用的各种非线性函数，验证神经网络所学内容的想法，并深入探讨构建稳健神经网络的理念。

<title>Summary </title> <link rel="stylesheet" href="css/style.css" type="text/css"> 

# 摘要

在这一章中，我们探索了 PyTorch 提供的各种数据结构和操作。我们使用 PyTorch 的基本块实现了几个组件。对于数据准备阶段，我们创建了算法将使用的张量。我们的网络架构是一个模型，可以学习预测用户在我们的 Wondermovies 平台上花费的平均时间。我们使用损失函数来检查我们的模型的标准，并使用`optimize`函数来调整我们的模型的可学习参数，以使它表现得更好。

我们还研究了 PyTorch 如何通过抽象出一些需要我们并行化和增加数据的复杂性来简化数据管道的创建。

在下一章，我们将深入研究神经网络和深度学习算法是如何工作的。我们将探索用于构建网络架构、损失函数和优化的各种内置 PyTorch 模块。我们还将学习如何在真实数据集上使用它们。