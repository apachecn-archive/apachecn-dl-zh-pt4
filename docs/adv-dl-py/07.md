<title>Generative Models</title> <link rel="stylesheet" href="css/style.css" type="text/css"> 

# 生成模型

在前两章([第四章](433225cc-e19a-4ecb-9874-8de71338142d.xhtml)、*、*、*高级卷积网络*和[第五章](9ac88546-8662-4b23-aa96-7eb00f48fedb.xhtml)、*物体检测和图像分割*)中，我们重点讨论了有监督的计算机视觉问题，比如分类和物体检测。在这一章中，我们将讨论如何在无监督神经网络的帮助下创建新图像。毕竟，知道不需要带标签的数据要好得多。更具体地说，我们将讨论生成模型。

本章将涵盖以下主题:

*   直觉与生成模型的证明
*   **变型自动编码器** ( **VAEs** )介绍
*   生成对抗网络简介 ( **甘斯**
*   氮化镓的种类
*   艺术风格转换引论

<title>Intuition and justification of generative models</title> <link rel="stylesheet" href="css/style.css" type="text/css"> 

# 直觉与生成模型的证明

到目前为止，我们已经使用神经网络作为**判别模型**。这仅仅意味着，给定输入数据，判别模型将把它映射到某个标签(换句话说，一个分类)。一个典型的例子是将 MNIST 图像分类为 1/10 数字类，其中神经网络将输入数据特征(像素强度)映射到数字标签。我们也可以用另一种方式来说:一个判别模型给我们 [![](assets/b238f670-d7e7-44c2-8206-3eef71a5182a.png)] (类)，给定 [![](assets/d7261168-d220-4861-80e7-8703f5e681a0.png)] (输入)的概率。在 MNIST 的情况下，这是给定图像的像素强度时数字的概率。

另一方面，生成模型了解类是如何分布的。你可以把它看作是区别模型的反面。 [![](assets/2a4f73c5-6ad5-4785-b1aa-0d2305853f43.png)] 不是预测类概率，而是在给定某些输入特征的情况下，尝试预测给定类时输入特征的概率， [![](assets/0f43e650-3720-4ac8-a1a3-f729a3edd573.png)] - [![](assets/74f689dd-1cca-47e1-9348-0b06933bde17.png)] 。例如，当给定数字类时，生成模型将能够创建手写数字的图像。由于我们只有 10 个类，它将只能生成 10 个图像。然而，我们只是用这个例子来说明这个概念。实际上，[![](assets/6e740484-c601-46eb-9648-9c3ca3d93f76.png)]*类可以是任意的张量值，并且该模型将能够生成具有不同特征的无限数量的图像。如果你现在还不明白这一点，不要着急；在这一章中，我们将会看到许多例子。*

 *在本章中，我们将用小写的 *p* 来表示概率分布，而不是我们在前几章中使用的大写的 *P* 。我们这样做是为了遵循在 VAEs 和 GANs 背景下建立的惯例。在写这本书时，我找不到使用小写的明确理由，但一个可能的解释是， *P* 表示事件的概率，而 *p* 表示随机变量的质量(或密度)函数的概率。

以生成方式使用神经网络的两种最流行的方法是通过 VAEs 和 GANs。在下一节中，我们将介绍 VAEs。

<title>Introduction to VAEs</title> <link rel="stylesheet" href="css/style.css" type="text/css"> 

# VAEs 简介

为了理解 VAEs，我们需要谈谈常规的自动编码器。自动编码器是一种试图再现其输入的前馈神经网络。换句话说，自动编码器的目标值(标签)等于输入数据，**y***^I=***x***^I*，其中 *i* 是样本索引。我们可以正式地说，它试图学习一个恒等式函数， [![](assets/1f499b17-4a29-411c-b7cc-4102c4f87f1c.png)] (一个重复其输入的函数)。由于我们的标签只是输入数据，自动编码器是一种无监督的算法。

下图显示了自动编码器:

![](assets/7ab06d46-2566-4471-ad60-891eeedf7a21.png)

自动编码器

自动编码器由输入层、隐藏层(或瓶颈层)和输出层组成。类似于 U-Net ( [第四章](9ac88546-8662-4b23-aa96-7eb00f48fedb.xhtml) *，对象检测和图像分割*，我们可以把自动编码器看作是两个组件的虚拟组合:

*   **编码器**:将输入数据映射到网络的内部表示。为了简单起见，在这个例子中，编码器是单个完全连接的隐藏瓶颈层。内部状态只是它的激活向量。一般来说，编码器可以有多个隐藏层，包括卷积层。
*   **解码器**:尝试从网络的内部数据表示中重建输入。解码器也可以具有通常镜像编码器的复杂结构。当 U-Net 试图将输入图像转换成某个其他域的目标图像(例如，分割图)时，自动编码器只是试图重建其输入。

我们可以通过最小化一个损失函数来训练自动编码器，这就是所谓的**重构** **误差****[![](assets/59cf2229-1f46-4abc-9e7f-12cf96347b03.png)]**。它测量原始输入与其重建之间的距离。我们可以用通常的方法最小化它，也就是用梯度下降和反向传播。根据我们使用的方法，我们可以使用**均方误差** ( **MSE** )或二进制交叉熵(如交叉熵，但有两个类别)作为重建误差。

此时，您可能想知道自动编码器的作用是什么，因为它只是重复输入。然而，我们对网络输出不感兴趣，而是对其内部数据表示感兴趣(也称为在**潜在空间**中的表示)。潜在空间包含隐藏的数据特征，这些数据特征不是直接观察到的，而是由算法推断出的。关键是瓶颈层的神经元比输入/输出层少。这有两个主要原因:

*   因为网络试图从更小的特征空间重构其输入，所以它学习数据的紧凑表示。你可以认为这是压缩(但不是无损的)。
*   通过使用更少的神经元，网络被迫只学习数据中最重要的特征。为了说明这个概念，让我们看一下去噪自动编码器，其中我们有意使用损坏的输入数据，但在训练期间使用未损坏的目标数据。例如，如果我们训练去噪自动编码器来重建 MNIST 图像，我们可以通过将最大强度(白色)设置为图像的随机像素来引入噪声(如下面的屏幕截图所示)。为了最小化无噪声目标的损失，自动编码器被迫超越输入中的噪声并仅学习数据的重要特征。然而，如果网络的隐藏神经元多于输入，它可能会过度适应噪声。在更少的隐藏神经元的额外约束下，它只能试图忽略噪声。一旦经过训练，我们可以使用去噪自动编码器来消除真实图像中的噪声:

![](assets/7e476cb7-0952-4ef5-a08c-0778be517c82.png)

去噪自动编码器输入和目标

编码器将每个输入样本映射到潜在空间，其中潜在表示的每个属性具有离散值。这意味着一个输入样本只能有一个潜在表示。因此，解码器只能以一种可能的方式重建输入。换句话说，我们可以生成一个输入样本的单一重建。但是我们不想这样。相反，我们希望生成不同于原始图像的新图像。VAEs 是这项任务的一种可能的解决方案。

VAE 可以用概率术语描述潜在的表征。也就是说，我们将为每个潜在属性提供一个概率分布，而不是离散值，从而使潜在空间连续。这使得随机采样和插值更加容易。让我们用一个例子来说明这一点。想象一下，我们正在试图对一幅车辆的图像进行编码，我们的潜在表示是一个向量， **z** ，具有 *n* 个元素(瓶颈层中的 *n* 个神经元)。每个元素代表一个车辆属性，例如长度、高度和宽度(如下图所示)。

假设车辆的平均长度是四米。VAE 可以将该属性解码为平均值为 4 的正态分布，而不是固定值(这同样适用于其他属性)。然后，解码器可以选择从潜在变量的分布范围中对其进行采样。例如，与输入相比，它可以重建更长更低的车辆。通过这样做，VAE 可以生成输入的无限数量的修改版本:

![](assets/ca7963b1-63c9-4dfe-acec-3412d9b389b4.png)

变分编码器从潜在变量的分布范围中采样不同值的例子

让我们把它正式化:

*   编码器的目标是逼近真实概率分布![](assets/1a3c9132-2603-4a19-a100-3b9ffb3f270a.png)，其中 **z** 是潜在空间表示。然而，它是通过从各种样本的条件概率分布![](assets/f9ae7213-2ff8-4851-b65a-ac6c51503a03.png)中推断![](assets/731217cc-21d3-498b-a99e-66c5d60d5ddb.png)来间接做到这一点的，其中 **x** 是输入数据。换句话说，给定输入数据， **x** ，编码器试图学习 **z** 的概率分布。我们将用![](assets/987d3cac-ffcf-4c5c-8763-91cc83c75624.png)表示编码器对![](assets/447524c7-6c0e-4a74-9e66-2351ba3595f7.png)的近似，其中 *φ* 是网络的权重。编码器输出是可能由 **x** 产生的 **z** 的可能值的概率分布(例如高斯分布)。在训练过程中，我们不断更新权重， *φ* ，使![](assets/73ef9a85-0440-413a-a935-5a8a41a5e66d.png) 更接近真实的![](assets/c80bd4e2-4ee1-4571-93ed-63d288c45a17.png) *。*

*   解码器的目标是逼近真实的概率分布![](assets/e6ee0e0b-6516-405c-8cd4-d91e35893f01.png)。换句话说，解码器试图学习数据的条件概率分布， *x* ，给定潜在表示， **z** 。我们将用![](assets/3087226a-ff1d-475b-b8a9-645b704cb330.png)表示解码器对真实概率分布的近似，其中 *θ* 是解码器权重。该过程从从概率分布(例如，高斯分布)中随机抽样 **z** 开始。然后， **z** 通过解码器发送，解码器的输出是 *x* 的可能对应值的概率分布。在训练过程中，我们不断更新权重 *θ* ，使![](assets/063bdfe0-14c3-4117-96d6-858379046b64.png) 更接近真实的![](assets/bdb62cc6-5642-48ba-80cf-367f36e30fdd.png) *。*

VAE 使用一种特殊类型的损失函数，其中包含两项:

![](assets/375738b8-6acf-4617-b579-8eb9ff731667.png)

第一个是概率分布![](assets/0a69f896-e2f3-4a73-a147-e7e14a41870f.png)和期望概率分布![](assets/feeaa04b-54f9-4ce2-b225-29dbcaf4d4a8.png)之间的 Kullback-Leibler 散度( [)第一章](b94f711b-daab-4de7-97b7-b7efccd0b392.xhtml)，*神经网络的具体细节*。在这种情况下，它测量当我们使用![](assets/0141d506-ca77-486d-9793-10b0cf619839.png)来表示![](assets/03768c4c-840a-46b2-a849-0b935a8c36ea.png)时丢失了多少信息(换句话说，两个分布有多接近)。它鼓励自动编码器探索不同的重建。第二个是重建损失，它衡量原始输入与其重建之间的差异。他们之间的差异越大，差距就越大。因此，它鼓励自动编码器以更好的方式重建数据。

为了实现这一点，瓶颈层不会直接输出潜在状态变量。相反，它将输出两个向量，描述每个潜在变量分布的**均值**和**方差**:

![](assets/0d2e5729-4a36-4ee0-b2e3-4bd93d3a5c2b.png)

变分编码器采样

一旦我们有了均值和方差分布，我们就可以从潜在变量分布中采样一个状态 **z** ，并将其通过解码器进行重建。但是我们还不能庆祝。这给我们带来了另一个问题:反向传播不适用于像我们这里这样的随机过程。幸运的是，我们可以用所谓的**重新参数化技巧**来解决这个问题。首先，我们将从高斯分布(上图中的ε圆)中抽取一个与 **z** 维数相同的随机向量ε。然后，我们将根据潜在分布的平均值μ对其进行移位，并根据潜在分布的方差σ对其进行缩放:

![](assets/966588f8-306a-4935-b6e9-9ddfa2b931e5.png)

通过这种方式，我们将能够优化均值和方差(红色箭头),并且我们将从反向传递中省略随机生成器。同时，采样数据将具有原始分布的属性。既然我们已经介绍了 VAEs，我们将学习如何实现它。

<title>Generating new MNIST digits with VAE</title> <link rel="stylesheet" href="css/style.css" type="text/css"> 

# 用 VAE 生成新的 MNIST 数字

在本节中，我们将了解 VAE 如何为 MNIST 数据集生成新数字。我们将在 TF 2.0.0 下使用 Keras 来实现。我们选择 MNIST 是因为它将很好地展示 VAE 的生产能力。

本节代码部分基于[https://github . com/keras-team/keras/blob/master/examples/variable _ auto encoder . py](https://github.com/keras-team/keras/blob/master/examples/variational_autoencoder.py)。

让我们一步一步地完成实施:

1.  让我们从进口开始。我们将使用 Keras 模块，它集成在 TF:

```py
import matplotlib.pyplot as plt
from matplotlib.markers import MarkerStyle
import numpy as np
import tensorflow as tf
from tensorflow.keras import backend as K
from tensorflow.keras.layers import Lambda, Input, Dense
from tensorflow.keras.losses import binary_crossentropy
from tensorflow.keras.models import Model
```

2.  现在，我们将实例化 MNIST 数据集。回想一下，在 [第二章](d94e220f-820e-40da-8bb5-9593e0790b21.xhtml) *，了解卷积网络*中，我们用 TF/Keras 实现了一个迁移学习示例，其中我们使用了`tensorflow_datasets`模块来加载 CIFAR-10 数据集。在这个例子中，我们将使用`keras.datasets`模块来加载 MNIST，这也是可行的:

```py
(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()

image_size = x_train.shape[1] * x_train.shape[1]
x_train = np.reshape(x_train, [-1, image_size])
x_test = np.reshape(x_test, [-1, image_size])
x_train = x_train.astype('float32') / 255
x_test = x_test.astype('float32') / 255

```

3.  接下来，我们将实现`build_vae`函数，它将构建 VAE:
    *   我们将分别访问编码器、解码器和整个网络。该函数将把它们作为一个元组返回。
    *   瓶颈层将只有`2`个神经元(也就是说，我们将只有`2`个潜在变量)。这样，我们就能把潜在分布显示成 2D 图。
    *   编码器/解码器将包含带有`512`神经元的单个中间(隐藏)全连接层。这不是卷积网络。
    *   我们将使用交叉熵重建损失和 KL 散度。

下面显示了这是如何在全球范围内实施的:

```py
def build_vae(intermediate_dim=512, latent_dim=2):
   # encoder first
    inputs = Input(shape=(image_size,), name='encoder_input')
    x = Dense(intermediate_dim, activation='relu')(inputs)

    # latent mean and variance
    z_mean = Dense(latent_dim, name='z_mean')(x)
    z_log_var = Dense(latent_dim, name='z_log_var')(x)

    # Reparameterization trick for random sampling
    # Note the use of the Lambda layer
    # At runtime, it will call the sampling function
    z = Lambda(sampling, output_shape=(latent_dim,), 
    name='z')([z_mean, z_log_var])

    # full encoder encoder model
    encoder = Model(inputs, [z_mean, z_log_var, z], name='encoder')
    encoder.summary()

    # decoder
    latent_inputs = Input(shape=(latent_dim,), name='z_sampling')
    x = Dense(intermediate_dim, activation='relu')(latent_inputs)
    outputs = Dense(image_size, activation='sigmoid')(x)

    # full decoder model
    decoder = Model(latent_inputs, outputs, name='decoder')
    decoder.summary()

    # VAE model
    outputs = decoder(encoder(inputs)[2])
    vae = Model(inputs, outputs, name='vae')

    # Loss function
    # we start with the reconstruction loss
    reconstruction_loss = binary_crossentropy(inputs, outputs) *
    image_size

    # next is the KL divergence
    kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var)
    kl_loss = K.sum(kl_loss, axis=-1)
    kl_loss *= -0.5

    # we combine them in a total loss
    vae_loss = K.mean(reconstruction_loss + kl_loss)
    vae.add_loss(vae_loss)

    return encoder, decoder, vae
```

4.  与网络定义直接相关的是`sampling`函数，它实现了从高斯单元中随机采样潜在向量`z`(这是我们在*VAEs 简介*一节中介绍的重新参数化技巧):

```py
def sampling(args: tuple):
    """
    :param args: (tensor, tensor) mean and log of variance of 
    q(z|x)
    """

    # unpack the input tuple
    z_mean, z_log_var = args

    # mini-batch size
    mb_size = K.shape(z_mean)[0]

    # latent space size
    dim = K.int_shape(z_mean)[1]

    # random normal vector with mean=0 and std=1.0
    epsilon = K.random_normal(shape=(mb_size, dim))

    return z_mean + K.exp(0.5 * z_log_var) * epsilon
```

5.  现在，我们需要实现`plot_latent_distribution`函数。它收集测试集中所有图像的潜在表示，并在 2D 图上显示它们。我们可以这样做，因为我们的网络只有两个潜在变量(对于绘图的两个轴)。注意，要实现这一点，我们只需要`encoder`:

```py
def plot_latent_distribution(encoder, x_test, y_test, batch_size=128):
    z_mean, _, _ = encoder.predict(x_test, batch_size=batch_size)
    plt.figure(figsize=(6, 6))

    markers = ('o', 'x', '^', '<', '>', '*', 'h', 'H', 'D', 'd',
    'P', 'X', '8', 's', 'p')

    for i in np.unique(y_test):
        plt.scatter(z_mean[y_test == i, 0], z_mean[y_test == i, 1],
                                marker=MarkerStyle(markers[i], 
                                fillstyle='none'),
                                edgecolors='black')

    plt.xlabel("z[0]")
    plt.ylabel("z[1]")
    plt.show()
```

6.  接下来，我们将实现`plot_generated_images`函数。它将对两个潜在变量中的每一个在`[-4, 4]`范围内的`n*n`向量`z`进行采样。接下来，它将基于采样向量生成图像，并在 2D 网格中显示它们。注意，要做到这一点，我们只需要`decoder`:

```py
def plot_generated_images(decoder):
    # display a nxn 2D manifold of digits
    n = 15
    digit_size = 28

    figure = np.zeros((digit_size * n, digit_size * n))
    # linearly spaced coordinates corresponding to the 2D plot
    # of digit classes in the latent space
    grid_x = np.linspace(-4, 4, n)
    grid_y = np.linspace(-4, 4, n)[::-1]

    # start sampling z1 and z2 in the ranges grid_x and grid_y
    for i, yi in enumerate(grid_y):
        for j, xi in enumerate(grid_x):
            z_sample = np.array([[xi, yi]])
            x_decoded = decoder.predict(z_sample)
            digit = x_decoded[0].reshape(digit_size, digit_size)
            slice_i = slice(i * digit_size, (i + 1) * digit_size)
            slice_j = slice(j * digit_size, (j + 1) * digit_size)
            figure[slice_i, slice_j] = digit

    # plot the results
    plt.figure(figsize=(6, 5))
    start_range = digit_size // 2
    end_range = n * digit_size + start_range + 1
    pixel_range = np.arange(start_range, end_range, digit_size)
    sample_range_x = np.round(grid_x, 1)
    sample_range_y = np.round(grid_y, 1)
    plt.xticks(pixel_range, sample_range_x)
    plt.yticks(pixel_range, sample_range_y)
    plt.xlabel("z[0]")
    plt.ylabel("z[1]")
   plt.imshow(figure, cmap='Greys_r')
    plt.show()
```

7.  现在，运行整个代码。我们将使用 Adam 优化器(在[第 1 章](b94f711b-daab-4de7-97b7-b7efccd0b392.xhtml)、*神经网络的基本原理*中介绍)来训练 50 个时期的网络:

```py
if __name__ == '__main__':
    encoder, decoder, vae = build_vae()

    vae.compile(optimizer='adam')
    vae.summary()

    vae.fit(x_train,
            epochs=50,
            batch_size=128,
            validation_data=(x_test, None))

    plot_latent_distribution(encoder, x_test, y_test,
                                      batch_size=128)

    plot_generated_images(decoder)
```

8.  如果一切按计划进行，一旦训练结束，我们将看到所有测试图像的每个数字类别的潜在分布。左侧和底部的轴代表`z[1]`和`z[2]`潜在变量。不同的标记形状代表不同的数字类别:

![](assets/f973f0aa-9c8b-4034-ada5-6c70f9d98baa.png)

MNIST 测试图像的潜在分布

9.  接下来，我们将查看由`plot_generated_images`生成的图像。轴代表用于每个图像的特定潜在分布`z`:

![](assets/9b6f48b1-6347-4cc5-a5d2-ce4e5f088f34.png)

VAE 生成的图像

这就结束了我们对 VAEs 的描述。在下一节中，我们将讨论 GANs——可以说是最流行的生成模型家族。

<title>Introduction to GANs</title> <link rel="stylesheet" href="css/style.css" type="text/css"> 

# 甘斯简介

在这一节中，我们将讨论当今最流行的生成模型:GAN 框架。它于 2014 年在里程碑式的论文*Generative Adversarial Nets*([http://papers . nips . cc/paper/5423-Generative-Adversarial-Nets . pdf](http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf)中首次提出。GAN 框架可以处理任何类型的数据，但是它最流行的应用是生成图像，我们将只在这个上下文中讨论它们。让我们看看它是如何工作的:

![](assets/7c414189-7fd7-4213-8d98-b6d11719c1f8.png)

GAN 系统

GAN 是由两个组件(神经网络)组成的系统:

*   **生成器**:这是生成模型本身。它将概率分布(随机噪声)作为输入，并尝试生成逼真的输出图像。其目的类似于 VAE 的解码器部分。
*   **鉴别器**:这需要两个交替的输入:训练数据集的真实图像或从生成器生成的假样本。它试图确定输入图像是来自真实图像还是生成的图像。

这两个网络作为一个系统一起训练。一方面，鉴别器试图更好地区分真假图像。另一方面，生成器试图输出更真实的图像，这样它可以*欺骗*鉴别器，使其认为生成的图像是真实的。用原论文中的类比，你可以把生成器想象成一队造假者，试图生产假币。反过来，鉴别者充当警察，试图抓获假币，两者不断试图欺骗对方(因此得名对抗性)。系统的最终目标是让生成器好到鉴别器无法分辨真假图像。即使鉴别器执行分类，GAN 仍然是无人监督的，因为我们不需要图像的标签。在下一节中，我们将讨论 GAN 框架环境下的培训流程。

<title>Training GANs</title> <link rel="stylesheet" href="css/style.css" type="text/css"> 

# 训练甘斯

我们的主要目标是让生成器生成逼真的图像，而 GAN 框架是实现这一目标的工具。我们将分别按顺序(一个接一个)训练发生器和鉴别器，并多次在两个阶段之间交替。

在进入更多细节之前，让我们使用下图来介绍一些符号:

*   我们用![](assets/bcbbfa02-24dc-4482-b07e-8dd60e1fd713.png)来表示生成器，其中![](assets/22bb18a4-d9b9-4556-a55a-c818717dac5a.png)是网络权重， **z** 是潜在向量，作为生成器的输入。可以把它看作是启动图像生成过程的随机种子值。它类似于 VAEs 中的潜在向量。 **z** 有一个概率分布，![](assets/e32bcba0-3230-458b-9a75-ba2001ad4ab5.png)，通常是随机正态或者随机均匀。生成器输出假样本， **x** ，概率分布为![](assets/3d4a4e6e-4f50-4a75-bc4b-61e2a9191eea.png)。你可以把![](assets/7c9715b8-cb69-47a8-bcf0-b40d631cb7d4.png)想象成真实数据按照生成器的概率分布。
*   我们将用![](assets/d2aeadec-72da-4c67-9ce4-3cdf4b4c2168.png)表示鉴别器，其中![](assets/3e467c31-98b4-4b30-a4db-baec41b4d099.png)是网络权重。它将具有![](assets/d381bc4e-4576-48cd-8f6c-825cc69e46c4.png)分布的真实数据或生成的样本![](assets/e1542f41-298c-44aa-ae40-6e0fcea895de.png)作为输入。鉴别器是一个二元分类器，它输出输入图像是真实图像(网络输出 1)还是生成数据(网络输出 0)的一部分。
*   在训练期间，我们将分别用![](assets/b9f92315-ed74-4577-a08b-781e7c1a4ddb.png)和![](assets/39905801-6028-4091-90d6-6d318e06f2ad.png)表示鉴频器和发电机损耗函数。

下面是一个更详细的 GAN 框架图:

![](assets/3e65a493-05b1-43fa-8f9d-4b24b32fd6e1.png)

A detailed example of a GAN

GAN 的训练与常规的 DNN 训练不同，因为我们有两个网络。我们可以把它看作是两个局中人(生成者和鉴别者)的序贯极大极小零和博弈:

*   **顺序**:这意味着玩家一个接一个地轮流，类似于国际象棋或井字游戏(相对于同时)。首先，鉴别器试图最小化![](assets/2c43645e-8a20-4d89-a7e4-9d0464cb1aa8.png)，但它只能通过调整权重![](assets/13edf7a3-eec0-46e6-8180-7dfac3b1fae0.png)来实现。接下来，生成器试图最小化![](assets/d7aa480b-9b21-4494-9186-b50a577d937b.png)，但它只能调整权重![](assets/7a5ffc5f-0c61-4633-9950-bbf71baff3cd.png)。我们多次重复这个过程。
*   **零和**:这意味着一方玩家的得失被对方玩家的得失所平衡。也就是说，发电机损耗和鉴频器损耗之和始终为 0:

![](assets/9d13c209-7820-4a67-8eb5-47fab1eda918.png)

*   **Minimax** :这是指第一个玩家(生成器)的策略是**最小化**对手的(鉴别器)**最大化**得分(因此得名)。当我们训练鉴别器时，它在区分真假样本方面变得更好(最小化![](assets/3642e1bc-6d13-438e-9b8c-a6ff3f7d5eec.png))。接下来，当我们训练生成器时，它试图逐步提高到新的和改进的鉴别器的水平(我们最小化![](assets/b7c64658-7d82-43a6-a7b4-68478bc67d01.png)，这相当于最大化![](assets/935102ba-8da5-4087-819f-bacdebb515a6.png))。这两个网络一直在竞争。我们将用下面的公式来表示极小极大博弈，其中![](assets/909d5f59-d3b0-4185-ab8f-1aa5e7eefa6e.png)是损失函数:

![](assets/820971ea-bce7-4624-b374-b4c5722a920c.png)

让我们假设，在许多训练步骤之后，![](assets/47781deb-e5d3-49dd-be70-af2cf4d31eb1.png)和![](assets/35578969-ec91-43d8-902c-c900a29b99a6.png)都将处于某个局部最小值。在这里，极小极大博弈的解被称为纳什均衡。纳什均衡发生在一个行动者不改变自己的行动时，不管另一个行动者会做什么。当发生器变得如此之好以至于鉴别器不再能够区分产生的和真实的样本时，GAN 框架中的纳什均衡发生。也就是说，无论输入如何，鉴频器输出总是一半。

既然我们已经对 GANs 有了一个大致的了解，那就让我们来讨论一下如何训练他们吧。我们将从鉴别器开始，然后继续讨论生成器。

<title>Training the discriminator</title> <link rel="stylesheet" href="css/style.css" type="text/css"> 

# 训练鉴别器

鉴别器是一个分类神经网络，我们可以用通常的方法训练它，即使用梯度下降和反向传播。然而，训练集是由真实的和生成的样本组成的。让我们学习如何将这一点融入培训流程:

1.  根据输入样本(真实或虚假)，我们有两条路径:
    *   从真实数据![](assets/5d97e04f-dfdf-4558-a5f9-bb2a5660b9dc.png)中选择样本，用它产生![](assets/3d5522f8-1f33-450b-b459-0c47b862181b.png)。
    *   生成一个假样本，![](assets/d6b3f32c-cbcf-4eca-a9c8-b5fd2e5459d3.png)。这里，发生器和鉴别器作为一个网络工作。我们从随机向量 **z** 开始，我们用它来产生生成的样本![](assets/7e3fa3c3-4644-4293-9205-05b28d7d22b3.png)。然后，我们用它作为鉴别器的输入，产生最终的输出，![](assets/7bb391a2-3cb9-46cb-bdaa-3c3897a2067c.png)。
2.  接下来，我们计算损失函数，它反映了训练数据的双重性(稍后将详细介绍)。

3.  最后，我们反向传播误差梯度并更新权重。尽管两个网络协同工作，但发生器权重![](assets/557fb571-01a9-4f5f-8ad0-df9f2ed4e42f.png)将被锁定，我们将只更新鉴别器权重![](assets/1f4c5681-dfb1-4e14-bad0-a050832ee537.png)。这确保了我们将通过使其更好来提高区分性能，而不是使生成器更差。

为了理解鉴别器损耗，让我们回忆一下交叉熵损耗的公式:

![](assets/53a958ee-004a-4cb1-906a-d213fe2ea6be.png)

这里，![](assets/a44a36e6-b6ab-4220-9773-d625d3437bc7.png)是属于第 *i* 类(总类中的 *n* 类)的输出的估计概率，![](assets/77505bad-5b15-4195-8331-f92ca37df667.png)是实际概率。为了简单起见，我们假设对单个训练样本应用该公式。在二元分类的情况下，这个公式可以简化如下:

![](assets/a5101d66-76fb-4c3c-8aec-feb244019a92.png)

When the target probabilities are [![](assets/976fe21a-8bf0-488f-8a35-5c6d987e44ef.png)] (one-hot-encoding), one of the loss terms is always *0*.

我们可以扩展小批量 *m* 样品的公式:

![](assets/adf53eaf-b140-4666-8e44-8d0aa532bfb1.png)

了解所有这些后，让我们来定义鉴频器损耗:

![](assets/e2b14266-f49c-4547-bcf8-0c08f77f04e5.png)

虽然这看起来很复杂，但这只是带有一些特定于 GAN 的附加功能的二元分类器的交叉熵损失。让我们来讨论一下:

*   损失的两个分量反映了两个可能的类别(真实或虚假)，这两个类别在训练集中的数量是相等的。
*   ![](assets/774af71b-1b7e-4819-a8d7-029827c60db7.png)是从实际数据中对输入进行采样时的损耗。理想情况下，在这种情况下，我们会有![](assets/e5b468c6-1f4b-4095-b86e-7fb2dadcac50.png)。
*   在这个上下文中，期望项![](assets/aaf721e7-7573-481c-92f9-7493b78a36c8.png)意味着 **x** 是从![](assets/8cfa008d-275e-4f8a-9505-f162889529d1.png)开始采样的。本质上，这部分损耗意味着，当我们从![](assets/a610dfc5-80da-410c-b7d3-307d39038478.png)采样 **x** 时，我们期望鉴频器输出![](assets/14cd182d-42d2-4146-9fff-0623d9941f5c.png)。最后，0.5 是真实数据![](assets/802ad000-4795-4002-9ab3-464a238be1cb.png)的累积类别概率，因为它正好包括整个集合的一半。

*   ![](assets/f5860c78-954b-4743-ba11-b2fc0031a8d7.png)是从生成的数据中对输入进行采样时的损耗。在这里，我们可以进行与真实数据组件相同的观察。但是，这个术语在![](assets/8a323555-d661-467a-9932-7ddbe32ccc1a.png)时最大化。

总而言之，当所有![](assets/d3842269-6022-4eef-8a98-63ce3536015e.png)的![](assets/e4936295-2648-4560-b427-e2bb43070f70.png)和所有产生的![](assets/515c08e1-d0bb-4b8e-9d68-1451ab7698c1.png)(或![](assets/9e0d2db1-d438-43d9-a894-9d2065079a42.png))的![](assets/6d5ad66b-f88b-4f2e-bce5-4904241f8664.png)都为零时，鉴频器损耗为零。

<title>Training the generator</title>

<link rel="stylesheet" href="css/style.css" type="text/css">

# 培训发电机

我们将通过使其更好地欺骗鉴别器来训练生成器。为此，我们需要两个网络，类似于我们用假样本训练鉴别器的方式:

1.  我们从一个随机的潜在向量 **z** 开始，并通过生成器和鉴别器来产生输出![](assets/a51a9fe6-0b01-4404-a7f3-a15015fa4528.png)。
2.  损耗函数与鉴频器损耗相同。然而，我们这里的目标是最大化而不是最小化它，因为我们想要欺骗鉴别器。
3.  在向后传递中，鉴别器权重![](assets/04ebd62f-9d63-4b2c-ac18-15b1a60677c5.png)被锁定，我们只能调整![](assets/dd4cbf8b-b4d8-4e6e-8cbd-506c388294f0.png)。这迫使我们通过使发生器更好来最大化鉴频器损耗，而不是使鉴频器更差。

您可能已经注意到，在这个阶段，我们只使用生成的数据。由于鉴别器权重被锁定，我们可以忽略损失函数中处理真实数据的部分。因此，我们可以将其简化为:

![](assets/b29f63c3-b5b4-4280-87dd-5a723926da2e.png)

这个公式的导数(梯度)是![](assets/8f81c792-2138-4c3f-9231-384a6e0d4d2d.png)，在下图中可以看出是一条不间断的线。这对训练施加了限制。在早期，当鉴别器可以轻松区分真假样品(![](assets/f4f3790d-88d5-4cc1-abfb-72979964c0f3.png))时，梯度将接近于零。这将导致很少学习权重，![](assets/d9fa8f2a-dacd-40bf-96df-4ff3a5556821.png)(消失梯度问题的另一种表现):

![](assets/ce5af7d3-6d16-499d-9e0f-1bbb38116cff.png)

两个发电机损失函数的梯度

我们可以通过使用不同的损失函数来解决这个问题:

![](assets/b0de4813-b13a-44f5-9051-6e99ef0395d3.png)

这个函数的导数在前面的图中用虚线表示。当![](assets/fa89da70-a3a3-4e4d-9971-2e91e28cb23a.png)和坡度较大时，这种损失仍然最小；也就是发电机性能不佳的时候。有了这个损失，游戏就不再是零和的了，但这对 GAN 框架不会有实际作用。现在，我们拥有了定义 GAN 训练算法所需的所有要素。我们将在下一节中完成这项工作。

<title>Putting it all together</title>

<link rel="stylesheet" href="css/style.css" type="text/css">

# 把所有的放在一起

利用我们新发现的知识，我们可以完整地定义极小极大目标:

![](assets/aece5d87-490b-4419-9fd4-795396f47338.png)

简而言之，生成器试图最小化目标，而鉴别器试图最大化目标。请注意，虽然鉴频器应该使其损耗最小化，但 minimax 目标是鉴频器损耗的负值，因此鉴频器必须使其最大化。

下面的逐步训练算法是由 GAN 框架的作者介绍的。

重复此操作多次:

1.  重复 *k* 步，其中 *k* 为超参数:
    *   从潜在空间![](assets/ccb1ce7f-ea61-44fd-a73b-5e4e74e2a3f5.png)随机抽取一小批 *m* 个样本
    *   从真实数据![](assets/63b32472-2b4f-4365-a961-ddb74222380a.png)中抽取一小批 *m* 个样本
    *   通过提升其成本的随机梯度来更新鉴别器权重![](assets/35d83794-8b1e-4ded-a95d-705dbaa266ab.png):

![](assets/85093089-8bab-4a0c-a6b4-25c55e90ab02.png)

2.  从潜在空间![](assets/76bef6d4-3429-4816-ab7d-42970437865f.png)中随机抽取一小批 *m* 个样本。
3.  通过降低其成本的随机梯度来更新生成器:

![](assets/92b4f00d-9be0-4105-9221-7a7ce2ec0eda.png)

或者，我们可以使用在*训练生成器*一节中介绍的更新成本函数:

![](assets/3de6492b-30bf-4e87-bc3f-1093600c06ca.png)

现在我们知道了如何训练 GANs，让我们来讨论一下在训练时可能会遇到的一些问题。

<title>Problems with training GANs</title>

<link rel="stylesheet" href="css/style.css" type="text/css">

# 训练 GANs 的问题

训练 GAN 模型有一些主要的陷阱:

*   梯度下降算法是为了寻找损失函数的最小值，而不是纳什均衡，这不是一回事。因此，有时训练可能无法收敛，而是会振荡。
*   回想一下，鉴频器输出是一个 sigmoid 函数，它表示示例为真或假的概率。如果鉴别器在这项任务中变得太好，概率输出将在每个训练样本处收敛到 0 或 1。这将意味着误差梯度将总是 0，这将阻止发生器学习任何东西。另一方面，如果鉴别器不善于从真实图像中识别假货，它会将错误的信息反向传播给生成器。因此，为了训练成功，鉴别器不能太好也不能太差。在实践中，这意味着我们不能训练它，直到收敛。
*   **模式崩溃**是一个问题，发生器可以生成有限数量的图像(甚至只有一个)，而不管潜在的输入向量值。为了理解为什么会发生这种情况，让我们关注单个生成器训练集，该训练集试图在鉴别器的权重固定的同时最小化 [![](assets/5719dc8c-46a6-42d6-a170-76372b3fc78c.png)] 。换句话说，生成器试图生成一个假图像， **x** ^* ，以至于 [![](assets/3c3cd4f4-708a-4f2c-b986-8ffd203afe57.png)] 。然而，损失函数并不强制生成器为不同的输入潜在向量值创建唯一的图像， **x** ^* 。也就是说，训练可以以一种方式修改发生器，在这种方式中，它将所生成的图像 **x** ^* 与潜在向量值完全去耦合，并且同时仍然最小化损失函数。例如，用于生成新 MNIST 图像的 GAN 只能生成数字 4，而不考虑输入。一旦我们更新了鉴别器，先前的值 **x** ^* 可能不再是最佳的，这将迫使生成器生成新的不同的图像。然而，模式崩溃可能在训练过程的不同阶段重复出现。

现在我们已经熟悉了 GAN 框架，我们将讨论几种不同类型的 GAN。

<title>Types of GAN</title>

<link rel="stylesheet" href="css/style.css" type="text/css">

# 氮化镓的种类

自从 GAN 框架首次推出以来，已经出现了许多新的变体。事实上，现在有太多新的 GAN，为了脱颖而出，作者们想出了创造性的 GAN 名称，如 BicycleGAN，DiscoGAN，GANs for LIFE，and ELEGANT。在接下来的几节中，我们将讨论其中的一些。所有的例子都是用 TensorFlow 2.0 和 Keras 实现的。

DCGAN、CGAN、WGAN 和 CycleGAN 的代码部分受到了[https://github.com/eriklindernoren/Keras-GAN](https://github.com/eriklindernoren/Keras-GAN)的启发。你可以在[https://github . com/packt publishing/Advanced-Deep-Learning-with-Python/tree/master/chapter 05](https://github.com/PacktPublishing/Advanced-Deep-Learning-with-Python/tree/master/Chapter05)找到本章所有例子的完整实现。

<title>Deep Convolutional GAN</title>

<link rel="stylesheet" href="css/style.css" type="text/css">

# 深度卷积 GAN

在本节中，我们将实现**深度卷积 GAN** ( **DCGAN** 、*深度卷积生成对抗网络*、**[https://arxiv.rg/abs/1511.06434](https://arxiv.org/abs/1511.06434)**)的无监督表示学习。在最初的 GAN 框架提案中，作者仅使用全连接网络。相反，在 DCGANs 中，发生器和鉴别器都是 CNN。他们有一些帮助稳定训练过程的约束。您可以将这些视为 GAN 培训的通用指南，而不仅仅是针对 DCGANs:

*   鉴别器使用步进卷积而不是池层。
*   生成器使用转置卷积将潜在向量![](assets/a6f3fc7b-2965-4074-8e82-6daa68f0a312.png)上采样到生成图像的大小。
*   两个网络都使用批处理规范化。
*   没有完全连接的层，除了鉴别器的最后一层。
*   发生器和鉴别器所有层的 LeakyReLU 激活，除了它们的输出。生成器输出层使用 Tanh 激活(范围为(-1，1))来模拟真实世界数据的属性。鉴别器有一个单一的 sigmoid 输出(回想一下，它在(0，1)的范围内)，因为它测量样本为真或假的概率。

在下图中，我们可以看到 DCGAN 框架中的一个示例发电机网络:

![](assets/018fc328-a765-4b96-a164-b39d2ae28c76.png)

具有转置卷积的生成器网络

<title>Implementing DCGAN</title>

<link rel="stylesheet" href="css/style.css" type="text/css">

# 实施 DCGAN

在这一节中，我们将实现 DCGAN，它生成新的 MNIST 图像。在接下来的部分中，这个示例将作为所有 GAN 实施的蓝图。让我们开始吧:

1.  让我们从导入必要的模块和类开始:

```py
import matplotlib.pyplot as plt
import numpy as np
from tensorflow.keras.datasets import mnist
from tensorflow.keras.layers import \
    Conv2D, Conv2DTranspose, BatchNormalization, Dropout, Input,
    Dense, Reshape, Flatten
from tensorflow.keras.layers import LeakyReLU
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.optimizers import Adam
```

2.  实现`build_generator`功能。我们将遵循本节开始时概述的准则——使用转置卷积进行上采样、批量归一化和 LeakyReLU 激活。该模型从全连接层开始，对 1D 潜在向量进行上采样。然后，用一系列`Conv2DTranspose`对向量进行上采样。最终的`Conv2DTranspose`有一个`tanh`激活，生成的图像只有一个通道:

```py
def build_generator(latent_input: Input):
    model = Sequential([
        Dense(7 * 7 * 256, use_bias=False,
        input_shape=latent_input.shape[1:]),
        BatchNormalization(), LeakyReLU(),

        Reshape((7, 7, 256)),

        # expand the input with transposed convolutions
        Conv2DTranspose(filters=128, kernel_size=(5, 5), 
                        strides=(1, 1), 
                        padding='same', use_bias=False),
        BatchNormalization(), LeakyReLU(),

        # gradually reduce the volume depth
        Conv2DTranspose(filters=64, kernel_size=(5, 5),
                        strides=(2, 2),
                        padding='same', use_bias=False),
        BatchNormalization(), LeakyReLU(),

        Conv2DTranspose(filters=1, kernel_size=(5, 5), 
                        strides=(2, 2), padding='same', 
                        use_bias=False, activation='tanh'),
    ])

    # this is forward phase
    generated = model(latent_input)

    return Model(z, generated)
```

3.  建立鉴别器。再次，这是一个简单的 CNN 与步幅卷积:

```py
def build_discriminator():
    model = Sequential([
        Conv2D(filters=64, kernel_size=(5, 5), strides=(2, 2),
               padding='same', input_shape=(28, 28, 1)),
        LeakyReLU(), Dropout(0.3),
        Conv2D(filters=128, kernel_size=(5, 5), strides=(2, 2),
               padding='same'),
        LeakyReLU(), Dropout(0.3),
        Flatten(),
        Dense(1, activation='sigmoid'),
    ])

    image = Input(shape=(28, 28, 1))
    output = model(image)

    return Model(image, output)
```

4.  用实际的 GAN 训练实现`train`功能。该功能实现了*训练 GANs* 部分的*将所有东西放在一起*小节中概述的程序。我们将从函数声明和变量初始化开始:

```py
def train(generator, discriminator, combined, steps, batch_size):
    # Load the dataset
    (x_train, _), _ = mnist.load_data()

    # Rescale in [-1, 1] interval
    x_train = (x_train.astype(np.float32) - 127.5) / 127.5
    x_train = np.expand_dims(x_train, axis=-1)

    # Discriminator ground truths
    real = np.ones((batch_size, 1))
    fake = np.zeros((batch_size, 1))

    latent_dim = generator.input_shape[1]
```

我们将继续训练循环，其中我们将一个鉴别器训练集与一个发电机训练集交替进行。首先，我们在一批`real_images`和一批`generated_images`上训练`discriminator`。然后，我们在同一批`generated_images`上训练发电机(也包括`discriminator`)。请注意，我们将这些图像标记为真实图像，因为我们希望最大化`discriminator`损失。以下是实现(请注意缩进；这仍然是`train`功能的一部分):

```py
for step in range(steps):
    # Train the discriminator

    # Select a random batch of images
    real_images = x_train[np.random.randint(0, x_train.shape[0],
    batch_size)]

    # Random batch of noise
    noise = np.random.normal(0, 1, (batch_size, latent_dim))

    # Generate a batch of new images
    generated_images = generator.predict(noise)

    # Train the discriminator
    discriminator_real_loss = discriminator.train_on_batch
    (real_images, real)
    discriminator_fake_loss = discriminator.train_on_batch
    (generated_images, fake)
    discriminator_loss = 0.5 * np.add(discriminator_real_loss,
    discriminator_fake_loss)

    # Train the generator
    # random latent vector z
    noise = np.random.normal(0, 1, (batch_size, latent_dim))

    # Train the generator
    # Note that we use the "valid" labels for the generated images
    # That's because we try to maximize the discriminator loss
    generator_loss = combined.train_on_batch(noise, real)

    # Display progress
    print("%d [Discriminator loss: %.4f%%, acc.: %.2f%%] [Generator
    loss: %.4f%%]" % (step, discriminator_loss[0], 100 *
    discriminator_loss[1], generator_loss))
```

5.  实现一个样板函数`plot_generated_images`，在训练结束后显示一些生成的图像:
    1.  创建一个`nxn`网格(变量`figure`)。
    2.  创建`nxn`个随机潜在向量(变量`noise`)——每个生成的图像一个。
    3.  生成图像并将它们放置在网格单元中。
    4.  显示结果。

以下是实现:

```py
def plot_generated_images(generator):
    n = 10
    digit_size = 28

    # big array containing all images
    figure = np.zeros((digit_size * n, digit_size * n))

    latent_dim = generator.input_shape[1]

    # n*n random latent distributions
    noise = np.random.normal(0, 1, (n * n, latent_dim))

    # generate the images
    generated_images = generator.predict(noise)

    # fill the big array with images
    for i in range(n):
        for j in range(n):
            slice_i = slice(i * digit_size, (i + 1) * digit_size)
            slice_j = slice(j * digit_size, (j + 1) * digit_size)
            figure[slice_i, slice_j] = np.reshape
                          (generated_images[i * n + j], (28, 28))

    # plot the results
    plt.figure(figsize=(6, 5))
    plt.axis('off')
    plt.imshow(figure, cmap='Greys_r')
    plt.show()
```

6.  通过包括`generator`、`discriminator`和`combined`网络，构建完整的 GAN 模型。我们将使用大小为 64 的潜在向量(`latent_dim`变量)，我们将使用 Adam 优化器运行 50，000 个批次的训练(这可能需要一段时间)。然后，我们将绘制结果:

```py
latent_dim = 64

# Build the generator
# Generator input z
z = Input(shape=(latent_dim,))

generator = build_generator(z)

generated_image = generator(z)

# we'll use Adam optimizer
optimizer = Adam(0.0002, 0.5)

# Build and compile the discriminator
discriminator = build_discriminator()
discriminator.compile(loss='binary_crossentropy',
                      optimizer=optimizer,
                      metrics=['accuracy'])

# Only train the generator for the combined model
discriminator.trainable = False

# The discriminator takes generated image as input and determines validity
real_or_fake = discriminator(generated_image)

# Stack the generator and discriminator in a combined model
# Trains the generator to deceive the discriminator
combined = Model(z, real_or_fake)
combined.compile(loss='binary_crossentropy', optimizer=optimizer)

train(generator, discriminator, combined, steps=50000, batch_size=100)

plot_generated_images(generator)
```

如果一切按计划进行，我们应该会看到类似下面的内容:

![](assets/a2f2de4f-2637-4c56-b787-ac831df73d0e.png)

新生成的 MNIST 图像

我们对 DCGANs 的讨论到此结束。在下一节中，我们将讨论另一种类型的 GAN 模型，称为条件 GAN。

<title>Conditional GAN</title>

<link rel="stylesheet" href="css/style.css" type="text/css">

# 条件 GAN

条件 GAN (CGAN，*条件生成对抗网*，【https://arxiv.org/abs/1411.1784】)是 GAN 模型的扩展，其中生成器和鉴别器都接收一些附加的条件输入信息。这可能是当前图像的类别或一些其他属性:

![](assets/8109b440-49ad-491c-b72a-3f0768a6256b.png)

有条件的甘。 *Y* 代表发生器和鉴别器的条件输入

例如，如果我们训练一个 GAN 来生成新的 MNIST 图像，我们可以添加一个额外的输入层，其中包含独热编码图像标签的值。CGANs 的缺点是它们不是严格无人监管的，我们需要某种标签才能让它们工作。然而，它们还有其他一些优点:

*   通过使用更多结构良好的信息进行训练，模型可以学习更好的数据表示，并生成更好的样本。
*   在常规 GANs 中，所有图像信息都存储在潜在向量 **z** 中。这就产生了一个问题:由于![](assets/92040d51-daef-42af-9e62-7843f787a669.png)可能很复杂，我们无法控制生成图像的属性。例如，假设我们希望我们的 MNIST GAN 生成某个数字；比如说 7。我们将不得不尝试不同的潜在向量，直到我们达到预期的输出。但是有了 CGAN，我们可以简单地将独热向量 7 与某个随机的 **z** 组合起来，网络就会生成正确的数字。我们仍然可以为 **z** 尝试不同的值，并且模型将生成数字的不同版本，即 7。简而言之，CGAN 为我们提供了控制(调节)发电机输出的方法。

由于条件输入，我们将修改 minimax 目标，使其也包含条件， *y* :

![](assets/e603f48e-8b7d-4bbe-b73c-3d86c0025ed3.png)

<title>Implementing CGAN</title>

<link rel="stylesheet" href="css/style.css" type="text/css">

# 实现 CGAN

CGAN 实现的蓝图与*实现 DCGAN* 一节中的 DCGAN 示例非常相似。也就是说，我们将实现 CGAN 来生成 MNIST 数据集的新图像。为了简单(和多样性)，我们将使用完全连接的生成器和鉴别器。为了避免重复，我们将只显示与 DCGAN 相比修改过的代码部分。你可以在这本书的 GitHub 库中找到完整的例子。

第一个显著的区别是生成器的定义:

```py
def build_generator(z_input: Input, label_input: Input):
    model = Sequential([
        Dense(128, input_dim=latent_dim),
        LeakyReLU(alpha=0.2), BatchNormalization(momentum=0.8),
        Dense(256),
        LeakyReLU(alpha=0.2), BatchNormalization(momentum=0.8),
        Dense(512),
        LeakyReLU(alpha=0.2), BatchNormalization(momentum=0.8),
        Dense(np.prod((28, 28, 1)), activation='tanh'),
        # reshape to MNIST image size
        Reshape((28, 28, 1))
    ])
    model.summary()

    # the latent input vector z
    label_embedding = Embedding(input_dim=10, 
    output_dim=latent_dim)(label_input)
    flat_embedding = Flatten()(label_embedding)

    # combine the noise and label by element-wise multiplication
    model_input = multiply([z_input, flat_embedding])
    image = model(model_input)

    return Model([z_input, label_input], image)
```

虽然这是一个全连接网络，但我们仍然遵循*深度卷积 GAN*部分中定义的 GAN 网络设计指南。让我们讨论一下将潜在向量`z_input`与条件标签`label_input`(取值范围为 0 到 9 的整数)结合起来的方法。我们可以看到`label_input`被转换成了一个`Embedding`层。这一层做两件事:

*   将整数值`label_input`转换为长度为`input_dim`的独热码表示
*   使用独热表示作为大小为`output_dim`的全连接层的输入

嵌入层允许我们为每个可能的输入值获得唯一的向量表示。在这种情况下，`label_embedding`的输出与潜在向量和`z_input`的大小具有相同的维数。`label_embedding`与潜在向量`z_input`相结合，借助于`model_input`变量中的逐元素乘法，作为网络其余部分的输入。

接下来，我们将关注鉴别器，它也是一个全连接网络，使用与生成器相同的嵌入机制。这次嵌入输出大小为`np.prod((28, 28, 1))`，等于 784(MNIST 图像的大小):

```py
def build_discriminator():
    model = Sequential([
        Flatten(input_shape=(28, 28, 1)),
        Dense(256),
        LeakyReLU(alpha=0.2),
        Dense(128),
        LeakyReLU(alpha=0.2),
        Dense(1, activation='sigmoid'),
    ], name='discriminator')
    model.summary()

    image = Input(shape=(28, 28, 1))
    flat_img = Flatten()(image)

    label_input = Input(shape=(1,), dtype='int32')
    label_embedding = Embedding(input_dim=10, output_dim=np.prod(
    (28, 28, 1)))(label_input)
    flat_embedding = Flatten()(label_embedding)

    # combine the noise and label by element-wise multiplication
    model_input = multiply([flat_img, flat_embedding])

    validity = model(model_input)

    return Model([image, label_input], validity)
```

示例代码的其余部分与 DCGAN 示例非常相似。唯一的其他差异是微不足道的——它们说明了网络的多个输入(潜在向量和嵌入)。`plot_generated_images`函数有一个额外的参数，允许它为随机潜在向量和特定的条件标签(在本例中是一个数字)生成图像。在下面，我们可以看到为条件标签 3、8 和 9 新生成的图像:

![](assets/7e9fa126-d99e-4f28-8a98-5e9ffad73815.png)

条件标签 3、8 和 9 的 CGAN

我们对 CGANs 的讨论到此结束。在下一节中，我们将讨论另一种类型的 GAN 模型，称为 Wasserstein GAN。

<title>Wasserstein GAN</title>

<link rel="stylesheet" href="css/style.css" type="text/css">

# 瓦瑟斯坦·甘

为了理解 Wasserstein GAN (WGAN，[https://arxiv.org/abs/1701.07875](https://arxiv.org/abs/1701.07875)，让我们回忆一下，在*训练 GAN*部分，我们用![](assets/fa42aa65-e0cb-49ed-ab02-d097a3f2f710.png)表示发生器的概率分布，用![](assets/e043e680-d6bc-4abe-949a-441df0f3a05d.png)表示真实数据的概率分布。在训练 GAN 模型的过程中，我们更新了生成器权重，因此我们更改了![](assets/fa42aa65-e0cb-49ed-ab02-d097a3f2f710.png)。GAN 框架的目标是将![](assets/fa42aa65-e0cb-49ed-ab02-d097a3f2f710.png)收敛到![](assets/e043e680-d6bc-4abe-949a-441df0f3a05d.png)(这也适用于其他类型的生成模型，如 VAE)，也就是说，生成的图像的概率分布应该与真实图像的概率分布相同，这将产生逼真的图像。WGAN 使用一种新的方法来测量两种分布之间的距离，称为 Wasserstein 距离(或**推土机距离** ( **EMD** ))。为了理解它，让我们从下图开始:

![](assets/c4118fda-f908-4d33-983d-4756d840b288.png)

EMD 的一个例子。左:初始和目标分布；右图:将![](assets/fa42aa65-e0cb-49ed-ab02-d097a3f2f710.png)转换为![](assets/e043e680-d6bc-4abe-949a-441df0f3a05d.png)的两种不同方式

为了简单起见，我们假设![](assets/fa42aa65-e0cb-49ed-ab02-d097a3f2f710.png)和![](assets/e043e680-d6bc-4abe-949a-441df0f3a05d.png)是离散分布(同样的规则适用于连续分布)。我们可以通过沿着 *x* 轴向左或向右移动柱子(a、b、c、d、e)将![](assets/fa42aa65-e0cb-49ed-ab02-d097a3f2f710.png)转换成![](assets/e043e680-d6bc-4abe-949a-441df0f3a05d.png)。每转移 1 个职位的成本为 1。例如，将列 *a* 从其初始位置 2 移动到位置 6 的成本是 4。上图的右侧显示了两种实现方式。第一种情况，我们有*总成本=成本(a:2- > 6) +成本(e:6- > 3) +成本(b:3- > 2) = 4 +3 + 1 = 8* 。第二种情况，我们有*总成本=成本(a:2- > 3) +成本(b:2- > 1) = 1 + 1 = 2* 。EMD 是将一种分布转换成另一种分布所需的最小总成本。因此，在本例中，EMD = 2。

我们现在对什么是 EMD 有了一个基本的概念，但是我们仍然不知道为什么有必要在 GAN 模型中使用这个指标。WGAN 的论文为这个问题提供了一个详尽但有些复杂的答案。在这一节中，我们将尝试解释它。首先，我们注意到生成器从一个低维的潜在向量![](assets/7a361be8-949f-4b71-a9ba-641c76c67d3b.png)开始，然后将其转换为一个高维的生成图像(例如，在 MNIST 的情况下为 784)。图像的输出大小也意味着生成数据的高维分布![](assets/fa42aa65-e0cb-49ed-ab02-d097a3f2f710.png)。然而，它的内在维度(潜在向量，![](assets/7a361be8-949f-4b71-a9ba-641c76c67d3b.png))要低得多。因此![](assets/fa42aa65-e0cb-49ed-ab02-d097a3f2f710.png)将被排除在高维特征空间的大部分之外。另一方面，![](assets/e043e680-d6bc-4abe-949a-441df0f3a05d.png)是真正的高维度，因为它不是从一个潜在向量开始的；相反，它代表了真实数据的全部丰富性。因此，![](assets/fa42aa65-e0cb-49ed-ab02-d097a3f2f710.png)和![](assets/e043e680-d6bc-4abe-949a-441df0f3a05d.png)很可能在特征空间的任何地方都不相交。

为了理解为什么这很重要，让我们注意我们可以将生成器和鉴别器成本函数(参见*训练 GANs* 部分)转换为 KL 和**詹森–香农** ( **JS** ，**[https://en . Wikipedia . org/wiki/Jensen % E2 % 80% 93 Shannon _ divergence](https://en.wikipedia.org/wiki/Jensen%E2%80%93Shannon_divergence))散度的函数。这些指标的问题在于，当两个分布不相交时，它们提供的梯度为零。也就是说，无论两个分布之间的距离有多大(小或大)，如果它们不相交，度量就不会提供关于它们之间实际差异的任何信息。然而，正如我们刚才解释的，很有可能分布不会相交。与此相反，无论分布是否相交，Wasserstein 距离都有效，这使它成为 GAN 模型的更好候选。我们可以用下图直观地说明这个问题:**

 **![](assets/178670ad-2cb5-4f1c-9d7e-12c8738f532d.png)

Wasserstein 距离优于常规 GAN 鉴别器。资料来源:https://arxiv.org/abs/1701.07875

这里，我们可以看到两个不相交的高斯分布，![](assets/fa42aa65-e0cb-49ed-ab02-d097a3f2f710.png)和![](assets/e043e680-d6bc-4abe-949a-441df0f3a05d.png)(分别向左和向右)。常规 GAN 鉴频器输出是 sigmoid 函数(范围为(0，1))，它告诉我们输入是假还是假的概率。在这种情况下，sigmoid 输出在非常窄的范围内(以 0 为中心)是有意义的，并且在所有其他区域中向 0 或 1 收敛。这是我们在*培训 GANs 的问题*一节中概述的同一问题的表现。它导致梯度消失，从而防止误差反向传播到发生器。相比之下，WGAN 不会给我们关于图像是真是假的二进制反馈，而是提供两种分布之间的实际距离测量(也显示在前面的图中)。这个距离比二进制分类更有用，因为它将提供如何更新生成器的更好指示。为了反映这一点，这篇论文的作者给这个鉴别器重新命名，称之为**评论家**。

下面的屏幕截图显示了本文中描述的 WGAN 算法:

![](assets/8099c5fd-4ded-489a-af4b-1c7e85622148.png)

这里， *f [w]* 表示评论家， *g [w]* 是评论家权重更新， *g [θ]* 是发电机权重更新。虽然 WGAN 背后的理论很复杂，但在实践中，我们可以通过对常规 GAN 模型进行相对较少的更改来实现它:

*   移除鉴频器的输出 sigmoid 激活。
*   用 EMD 导出的损耗代替对数发生器/鉴别器损耗函数。
*   在每一个小批量后修剪 critic 权重，使它们的绝对值小于常数 *c* 。这一要求加强了评论家所谓的 Lipschitz 约束，这使得使用 Wasserstein 距离成为可能(在论文中有更多关于这一点的内容)。在不深入细节的情况下，我们将只提到重量削减会导致不期望的行为。这些问题的一个成功的解决方案是梯度罚分(WGAN-GP，*瓦瑟斯坦甘斯*，的改进训练)，它不会遇到同样的问题。
*   该论文的作者报告说，没有动量的优化方法(SGD，RMSProp)比那些有动量的方法效果更好。

<title>Implementing WGAN</title> <link rel="stylesheet" href="css/style.css" type="text/css"> 

# 实施 WGAN

现在我们对 Wasserstein GAN 的工作原理有了一个基本的概念，让我们来实现它。我们将再次使用 DCGAN 蓝图，并省略重复的代码片段，以便我们可以专注于不同之处。`build_generator`和`build_critic`函数分别实例化生成器和评价器。为了简单起见，这两个网络只包含完全连接的层。所有隐藏层都有 LeakyReLU 激活。按照这篇论文的指导方针，发生器有 Tanh 输出激活，而 critic 有单个标量输出(尽管没有 sigmoid 激活)。接下来，让我们实现`train`方法，因为它包含一些 WGAN 细节。我们将从方法的声明和训练过程的初始化开始:

```py
def train(generator, critic, combined, steps, batch_size, n_critic, clip_value):
    # Load the dataset
    (x_train, _), _ = mnist.load_data()

    # Rescale in [-1, 1] interval
    x_train = (x_train.astype(np.float32) - 127.5) / 127.5

    # We use FC networks, so we flatten the array
    x_train = x_train.reshape(x_train.shape[0], 28 * 28)

    # Discriminator ground truths
    real = np.ones((batch_size, 1))
    fake = -np.ones((batch_size, 1))

    latent_dim = generator.input_shape[1]
```

然后，我们将继续训练循环，它遵循我们在本节前面描述的 WGAN 算法的步骤。内环为`generator`的每个训练步骤训练`critic` `n_critic`步骤。事实上，这是在*实现 DCGAN* 部分的训练功能中训练`critic`和训练`discriminator`的主要区别，其中鉴别器和发生器在每个步骤*交替。*此外，`weights` critic 在每次小批量生产后被修剪。以下是实现(请注意缩进；该代码是`train`功能的一部分):

```py
    for step in range(steps):
        # Train the critic first for n_critic steps
        for _ in range(n_critic):
            # Select a random batch of images
            real_images = x_train[np.random.randint(0, x_train.shape[0], 
            batch_size)]

            # Sample noise as generator input
            noise = np.random.normal(0, 1, (batch_size, latent_dim))

            # Generate a batch of new images
            generated_images = generator.predict(noise)

            # Train the critic
            critic_real_loss = critic.train_on_batch(real_images, real)
            critic_fake_loss = critic.train_on_batch(generated_images,
            fake)
            critic_loss = 0.5 * np.add(critic_real_loss, critic_fake_loss)

            # Clip critic weights
            for l in critic.layers:
                weights = l.get_weights()
                weights = [np.clip(w, -clip_value, clip_value) for w in
                weights]
                l.set_weights(weights)

        # Train the generator
        # Note that we use the "valid" labels for the generated images
        # That's because we try to maximize the discriminator loss
        generator_loss = combined.train_on_batch(noise, real)

        # Display progress
        print("%d [Critic loss: %.4f%%] [Generator loss: %.4f%%]" %
              (step, critic_loss[0], generator_loss))
```

接下来，我们将实现 Wasserstein 损失本身的导数。它是一个 TF 运算，表示网络输出和标签(真或假)的乘积的平均值:

```py
def wasserstein_loss(y_true, y_pred):
    """The Wasserstein loss implementation"""
    return tensorflow.keras.backend.mean(y_true * y_pred)
```

现在，我们可以构建完整的 GAN 模型。该步骤类似于其他 GAN 模型:

```py
latent_dim = 100

# Build the generator
# Generator input z
z = Input(shape=(latent_dim,))

generator = build_generator(z)

generated_image = generator(z)

# we'll use RMSprop optimizer
optimizer = RMSprop(lr=0.00005)

# Build and compile the discriminator
critic = build_critic()
critic.compile(optimizer, wasserstein_loss,
               metrics=['accuracy'])

# The discriminator takes generated image as input and determines validity
real_or_fake = critic(generated_image)

# Only train the generator for the combined model
critic.trainable = False

# Stack the generator and discriminator in a combined model
# Trains the generator to deceive the discriminator
combined = Model(z, real_or_fake)
combined.compile(loss=wasserstein_loss, optimizer=optimizer)
```

最后，让我们开始培训和评估:

```py
# train the GAN system
train(generator, critic, combined,
      steps=40000, batch_size=100, n_critic=5, clip_value=0.01)

# display some random generated images
plot_generated_images(generator)
```

一旦我们运行此示例，WGAN 将在训练 40，000 个小批量后生成以下图像(这可能需要一段时间):

![](assets/118f62d9-b265-4744-a119-eb5a3fe348f6.png)

WGAN MNIST 发生器结果

我们对 WGANs 的讨论到此结束。在下一节中，我们将讨论如何用 CycleGAN 实现图像到图像的翻译。

<title>Image-to-image translation with CycleGAN</title> <link rel="stylesheet" href="css/style.css" type="text/css"> 

# 使用 CycleGAN 进行图像到图像的翻译

在这一节中，我们将讨论**循环一致对抗网络** ( **循环根**，*使用循环一致对抗网络*，[https://arxiv.org/abs/1703.10593](https://arxiv.org/abs/1703.10593)进行不成对的图像到图像翻译，以及它们在图像到图像翻译中的应用。引用这篇论文本身，图像到图像的翻译是一类视觉和图形问题，其目标是使用对齐图像对的训练集来学习输入图像和输出图像之间的映射。例如，如果我们有同一幅图像的灰度和 RGB 版本，我们可以训练一个 ML 算法来给灰度图像着色，反之亦然。

另一个例子是图像分割([第三章](9ac88546-8662-4b23-aa96-7eb00f48fedb.xhtml)、*物体检测和图像分割*，其中输入图像被转换成同一图像的分割图。在后一种情况下，我们用图像/分割图对训练模型(U-Net，Mask R-CNN)。然而，配对的训练数据可能无法用于许多任务。CycleGAN 为我们提供了一种在没有成对样本的情况下，将图像从源域 *X* 转换到目标域 *Y* 的方法。下图显示了成对和不成对图像的一些示例:

![](assets/7f45296e-5e6d-40af-b836-d2cce1a0c2d8.png)

左图:将训练样本与相应的源图像和目标图像配对；右图:未配对的训练样本，其中源图像和目标图像不对应。资料来源:https://arxiv.org/abs/1703.10593

来自同一个团队的*用条件敌对网络进行图像到图像的翻译*(称为 Pix2Pix，[https://arxiv.org/abs/1611.07004](https://arxiv.org/abs/1611.07004))论文也对成对的训练数据进行图像到图像的翻译。

但是 CycleGAN 是如何做到这一点的呢？首先，该算法假设，尽管两个集合中没有直接对，但两个域之间仍然存在某种关系。例如，这些可以是同一场景但从不同角度拍摄的照片。CycleGAN 的目标是学习这种集合级别的关系，而不是不同对之间的关系。理论上，GAN 模型非常适合这项任务。我们可以训练一个映射![](assets/cc8fc839-1087-4065-b914-46ecf623989b.png)的生成器，它产生一个图像![](assets/f006826f-ea20-4518-b662-fd68d2e1c38d.png)，一个鉴别器不能从目标图像![](assets/5a05feb6-7899-4a80-8011-091b7aebd4d3.png)中分辨出来。更具体地说，最佳的 *G* 应该将域 X 转换成域![](assets/fc044c73-e153-4d7c-a777-1b23fa8b114f.png)，与域 *Y* 具有相同的分布。实际上，该论文的作者发现，这样的翻译并不能保证单个输入 *x* 和输出 *y* 以有意义的方式配对——有无限多的映射 *G* ，它们将在![](assets/0b892c90-4404-4327-8bbe-32148ff90681.png)上创建相同的分布。他们还发现，这种 GAN 模型也存在我们熟悉的模式崩溃问题。

CycleGAN 试图用所谓的**周期一致性**来解决这些问题。为了理解这是什么，假设我们把一个句子从英语翻译成德语。如果我们将句子从德语翻译回英语，并且到达我们开始的原始句子，那么翻译将是循环一致的。在数学语境中，如果我们有一个译者![](assets/0c4fefb7-620d-4f2d-9106-739d207e735b.png)和另一个译者![](assets/06c75834-066b-49ee-be8f-d15acea94fc5.png)，这两者应该是互逆的。

为了解释 CycleGAN 如何实现周期一致性，让我们从下图开始:

![](assets/83c40c4d-e712-4b0f-8b07-8486589d8b64.png)

左:整体 CycleGAN 架构；中:前向周期-一致性损失；右图:反向周期一致性损失。资料来源:https://arxiv.org/abs/1703.10593

该模型有两个生成器， [![](assets/0c4fefb7-620d-4f2d-9106-739d207e735b.png)] 和 [![](assets/06c75834-066b-49ee-be8f-d15acea94fc5.png)] ，以及两个关联的鉴别器，分别为 *D [ x ]* 和 *D [ y ]* (在上图中左侧)。先来看看 *G* 第*。*它获取一个输入图像![](assets/204079ed-a5c5-4cf1-b474-ba34f7a7de2f.png)，并生成![](assets/fe49c7be-9444-4ede-b029-b9586188e0f3.png)，它看起来类似于来自域 *Y* 的图像。*D[y]旨在区分真实图像、![](assets/a242aec9-02be-4228-ab67-c1fb68d37a08.png)和生成的![](assets/98d91a20-d6b6-4ba5-ba80-acf9b5851539.png)。这部分模型的功能类似于常规 GAN，并使用常规 minimax GAN 对抗损失:*

![](assets/a11267c7-2a3e-496c-a180-c46a94d2f204.png)

第一项代表原始图像， *y* ，第二项代表由 *G* 生成的图像。同样的公式也适用于发电机， *F* 。正如我们之前提到的，这种损失仅确保了![](assets/ca6d7e8d-990f-49b2-96a7-bb18d26f4af0.png)将具有与来自 *Y* 的图像相同的分布，但并没有创建一对有意义的 **x** 和 **y** 。引用该论文:通过足够大的容量，网络可以将同一组输入图像映射到目标域中图像的任意随机排列，其中任何学习到的映射都可以导致与目标分布匹配的输出分布。因此，单独的对抗性损失不能保证所学习的函数能够将单个输入、 **x** [ *i* 、]映射到期望的输出、 **y** *[ i ]* 。

该论文的作者认为，学习到的映射函数应该是循环一致的(上图，中间)。对于每个图像，![](assets/ff116f2a-7091-4b4d-b4a3-86dfcee3af5f.png)，图像平移周期应该能够将 **x** 带回原始图像(这称为正向周期一致性)。 *G* 生成新图像![](assets/36649dd2-269f-4f3f-a035-ea23bfc54e00.png)，作为 *F* 的输入，F 又生成新图像![](assets/9d6869b8-ecf9-410f-a32a-29a9e9037cc3.png)，其中![](assets/af5abbe3-616b-4a4b-8e26-2b9842551933.png) : ![](assets/72a089ab-26c4-47f9-9d78-6a638e887967.png)。 *G* 和 *F* 也应满足反向循环一致性(上图，右图):![](assets/6e24c434-598b-4a8c-87a4-77c7ffb6b636.png)。

这条新路径产生了一个额外的周期一致性损失项:

![](assets/06594f64-84da-453e-9adf-20357b3d00ca.png)

这测量了原始图像之间的绝对差异，即 *x* 和 *y* ，以及它们生成的对应图像![](assets/ce8ce93b-93ba-426b-8d18-1f6e6f116879.png)和![](assets/f39352cb-a3a2-48e2-a1dd-35da7c23a89f.png)。注意，这些路径可以被视为联合训练两个自动编码器，![](assets/cfdf4e9a-1ea4-4749-a426-21b3fa1f6d55.png)和![](assets/987e6078-8c09-4169-968a-f0eb1dcf7bfe.png)。每个 autoencoder 都有一个特殊的内部结构:它借助中间表示(将图像转换到另一个域)将图像映射到自身。

完整周期目标是周期一致性损失和 *F* 和 *G* 的对抗性损失的组合:

![](assets/99d5230b-cef3-4d67-9ae4-624bd1176b8f.png)

这里，系数λ控制两个损失之间的相对重要性。CycleGAN 旨在解决以下 minimax 目标:

![](assets/2d6daef8-178c-4fda-884e-faf0f245ecd0.png)

<title>Implementing CycleGAN</title> <link rel="stylesheet" href="css/style.css" type="text/css"> 

# 实施循环

该示例包含位于[https://github . com/packt publishing/Advanced-Deep-Learning-with-Python/tree/master/chapter 05/cycle gan](https://github.com/PacktPublishing/Advanced-Deep-Learning-with-Python/tree/master/Chapter05/cyclegan)的几个源文件。除了 TF，代码还依赖于`tensorflow_addons`和`imageio`包。你可以用`pip`安装包来安装它们。我们将为多个训练数据集实现 CycleGAN，所有这些数据集都由论文作者提供。在运行这个示例之前，您必须在`download_dataset.sh`可执行脚本的帮助下下载相关的数据集，该脚本使用数据集名称作为参数。文件中包含可用数据集的列表。一旦你下载了这个，你就可以在`data_loader.py`模块中的`DataLoader`类的帮助下访问图片(这里我们不包括它的源代码)。可以说，该类可以将小批量和完整的标准化图像数据集加载为`numpy`数组。我们还将省略通常的导入。

<title>Building the generator and discriminator</title> <link rel="stylesheet" href="css/style.css" type="text/css"> 

# 构建生成器和鉴别器

首先，我们将实现`build_generator`函数。到目前为止，我们看到的 GAN 模型是从某种潜在向量开始的。但是在这里，发生器输入是来自其中一个域的图像，输出是来自相反域的图像。遵循论文的指导原则，生成器是一个 U-Net 风格的网络。它有一个下采样编码器、一个上采样解码器以及相应编码器/解码器模块之间的快捷连接。我们将从`build_generator`的定义开始:

```py
def build_generator(img: Input) -> Model:
```

U-Net 下采样编码器由多个卷积层组成，激活`LeakyReLU`，然后激活`InstanceNormalization`。批规范化和实例规范化之间的区别在于，批规范化在整个小批中计算其参数，而实例规范化为小批的每个图像单独计算参数。为了清楚起见，我们将实现一个名为`downsampling2d`的独立子例程，它定义了一个这样的层。当我们构建网络编码器时，我们将使用这个函数来构建必要的层数(请注意这里的缩进；`downsampling2d`是在`build_generator`中定义的子程序:

```py
    def downsampling2d(layer_input, filters: int):
        """Layers used in the encoder"""
        d = Conv2D(filters=filters,
                   kernel_size=4,
                   strides=2,
                   padding='same')(layer_input)
        d = LeakyReLU(alpha=0.2)(d)
        d = InstanceNormalization()(d)
        return d
```

接下来，让我们关注解码器，它不是用转置卷积实现的。相反，使用`UpSampling2D`操作对输入数据进行上采样，该操作只是将每个输入像素复制为 2×2 的小块。接下来是一个规则的卷积来平滑面片。该平滑输出与来自相应编码器模块的快捷方式(或`skip_input`)连接在一起。解码器由许多这样的上采样块组成。为了清楚起见，我们将实现一个名为`upsampling2d`的独立子例程，它定义了一个这样的块。我们将使用它为网络解码器构建必要数量的块(请注意这里的缩进；`upsampling2d`是在`build_generator`中定义的子程序:

```py
    def upsampling2d(layer_input, skip_input, filters: int):
        """
        Layers used in the decoder
        :param layer_input: input layer
        :param skip_input: another input from the corresponding encoder block
        :param filters: number of filters
        """
        u = UpSampling2D(size=2)(layer_input)
        u = Conv2D(filters=filters,
                   kernel_size=4,
                   strides=1,
                   padding='same',
                   activation='relu')(u)
        u = InstanceNormalization()(u)
        u = Concatenate()([u, skip_input])
        return u
```

接下来，我们将使用刚刚定义的子例程实现 U-Net 的完整定义(请注意这里的缩进；该代码是`build_generator`的一部分):

```py
    # Encoder
    gf = 32
    d1 = downsampling2d(img, gf)
    d2 = downsampling2d(d1, gf * 2)
    d3 = downsampling2d(d2, gf * 4)
    d4 = downsampling2d(d3, gf * 8)

    # Decoder
    # Note that we concatenate each upsampling2d block with
    # its corresponding downsampling2d block, as per U-Net
    u1 = upsampling2d(d4, d3, gf * 4)
    u2 = upsampling2d(u1, d2, gf * 2)
    u3 = upsampling2d(u2, d1, gf)

    u4 = UpSampling2D(size=2)(u3)
    output_img = Conv2D(3, kernel_size=4, strides=1, padding='same',
    activation='tanh')(u4)

    model = Model(img, output_img)

    model.summary()

    return model
```

然后，我们应该实现`build_discriminator`函数。这里我们将省略实现，因为它是一个相当简单的 CNN，类似于前面例子中显示的那些(你可以在本书的 GitHub 库中找到它)。唯一的区别是，它不使用批处理规范化，而是使用实例规范化。

<title>Putting it all together</title> <link rel="stylesheet" href="css/style.css" type="text/css"> 

# 把所有的放在一起

在这一点上，我们通常实现`train`方法，但是因为 CycleGAN 有更多的组件，我们将向您展示如何构建整个模型。首先，我们实例化了`data_loader`对象，您可以在其中指定训练集的名称(随意试验不同的数据集)。所有的图像将被调整到`img_res=(IMG_SIZE, IMG_SIZE)`用于网络输入，其中`IMG_SIZE = 256`(你也可以尝试`128`来加快训练过程):

```py
# Input shape
img_shape = (IMG_SIZE, IMG_SIZE, 3)

# Configure data loader
data_loader = DataLoader(dataset_name='facades',
                         img_res=(IMG_SIZE, IMG_SIZE))
```

然后，我们将定义优化器和损失权重:

```py
lambda_cycle = 10.0  # Cycle-consistency loss
lambda_id = 0.1 * lambda_cycle  # Identity loss

optimizer = Adam(0.0002, 0.5)
```

接下来，我们将创建两个生成器，`g_XY`和`g_YX`，以及它们对应的鉴别器，`d_Y`和`d_X`。我们还将创建`combined`模型来同时训练两个生成器。然后，我们将创建复合损失函数，它包含一个附加的身份映射项。您可以在相应的文章中了解更多信息，但简而言之，在将图像从绘画领域转换到照片领域时，它有助于保持输入和输出之间的颜色组合:

```py
# Build and compile the discriminators
d_X = build_discriminator(Input(shape=img_shape))
d_Y = build_discriminator(Input(shape=img_shape))
d_X.compile(loss='mse', optimizer=optimizer, metrics=['accuracy'])
d_Y.compile(loss='mse', optimizer=optimizer, metrics=['accuracy'])

# Build the generators
img_X = Input(shape=img_shape)
g_XY = build_generator(img_X)

img_Y = Input(shape=img_shape)
g_YX = build_generator(img_Y)

# Translate images to the other domain
fake_Y = g_XY(img_X)
fake_X = g_YX(img_Y)

# Translate images back to original domain
reconstr_X = g_YX(fake_Y)
reconstr_Y = g_XY(fake_X)

# Identity mapping of images
img_X_id = g_YX(img_X)
img_Y_id = g_XY(img_Y)

# For the combined model we will only train the generators
d_X.trainable = False
d_Y.trainable = False

# Discriminators determines validity of translated images
valid_X = d_X(fake_X)
valid_Y = d_Y(fake_Y)

# Combined model trains both generators to fool the two discriminators
combined = Model(inputs=[img_X, img_Y],
                 outputs=[valid_X, valid_Y,
                          reconstr_X, reconstr_Y,
                          img_X_id, img_Y_id])
```

接下来，让我们配置用于训练的`combined`模型:

```py
combined.compile(loss=['mse', 'mse',
                       'mae', 'mae',
                       'mae', 'mae'],
                 loss_weights=[1, 1,
                               lambda_cycle, lambda_cycle,
                               lambda_id, lambda_id],
                 optimizer=optimizer)
```

一旦模型准备好，我们用`train`函数启动训练过程。根据论文的指导原则，我们将使用 1:

```py
train(epochs=200, batch_size=1, data_loader=data_loader,
      g_XY=g_XY,
      g_YX=g_YX,
      d_X=d_X,
      d_Y=d_Y,
      combined=combined,
      sample_interval=200)
```

最后，我们将实现`train`函数。它与之前的 GAN 模型有些相似，但也考虑了两对发生器和鉴别器:

```py
def train(epochs: int, data_loader: DataLoader,
          g_XY: Model, g_YX: Model, d_X: Model, d_Y: Model, 
          combined:Model, batch_size=1, sample_interval=50):
    start_time = datetime.datetime.now()

    # Calculate output shape of D (PatchGAN)
    patch = int(IMG_SIZE / 2 ** 4)
    disc_patch = (patch, patch, 1)

    # GAN loss ground truths
    valid = np.ones((batch_size,) + disc_patch)
    fake = np.zeros((batch_size,) + disc_patch)

    for epoch in range(epochs):
        for batch_i, (imgs_X, imgs_Y) in
        enumerate(data_loader.load_batch(batch_size)):
            # Train the discriminators

            # Translate images to opposite domain
            fake_Y = g_XY.predict(imgs_X)
            fake_X = g_YX.predict(imgs_Y)

            # Train the discriminators (original images = real /
            translated = Fake)
            dX_loss_real = d_X.train_on_batch(imgs_X, valid)
            dX_loss_fake = d_X.train_on_batch(fake_X, fake)
            dX_loss = 0.5 * np.add(dX_loss_real, dX_loss_fake)

            dY_loss_real = d_Y.train_on_batch(imgs_Y, valid)
            dY_loss_fake = d_Y.train_on_batch(fake_Y, fake)
            dY_loss = 0.5 * np.add(dY_loss_real, dY_loss_fake)

            # Total discriminator loss
            d_loss = 0.5 * np.add(dX_loss, dY_loss)

            # Train the generators
            g_loss = combined.train_on_batch([imgs_X, imgs_Y],
                                             [valid, valid,
                                              imgs_X, imgs_Y,
                                              imgs_X, imgs_Y])

            elapsed_time = datetime.datetime.now() - start_time

            # Plot the progress
            print("[Epoch %d/%d] [Batch %d/%d] [D loss: %f, acc: %3d%%]
            [G loss: %05f, adv: %05f, recon: %05f, id: %05f] time: %s " \ 
            % (epoch, epochs, batch_i, data_loader.n_batches, d_loss[0], 
            100 * d_loss[1], g_loss[0], np.mean(g_loss[1:3]),
            np.mean(g_loss[3:5]), np.mean(g_loss[5:6]), elapsed_time))

            # If at save interval => save generated image samples
            if batch_i % sample_interval == 0:
                sample_images(epoch, batch_i, g_XY, g_YX, data_loader)
```

训练可能需要一段时间才能完成，但该过程会在每一批`sample_interval`之后生成图像。以下显示了机器感知中心正面数据库([http://cmp.felk.cvut.cz/~tylecr1/facade/](http://cmp.felk.cvut.cz/~tylecr1/facade/))生成的一些图像示例。它包含建筑立面，其中每个像素都被标记为多个与立面相关的类别之一，如窗、门、阳台等:

![](assets/2a6ef916-a9a3-4107-bc54-86806b502521.png)

CycleGAN 图像到图像转换的示例

我们对 GANs 的讨论到此结束。接下来，我们将关注一种不同类型的生成模型，称为艺术风格转移。

<title>Introducing artistic style transfer</title> <link rel="stylesheet" href="css/style.css" type="text/css"> 

# 引入艺术风格转移

在这最后一节，我们将讨论艺术风格的转换。类似于 CycleGAN 的一个应用，它允许我们使用一个图像的样式(或纹理)来再现另一个图像的语义内容。虽然可以用不同的算法实现，但最流行的方式是在 2015 年的*一篇艺术* *风格*的论文(【https://arxiv.org/abs/1508.06576】)中介绍的一种神经算法。它也被称为神经风格转移，它使用(你猜对了！)CNN。在过去的几年中，基本算法得到了改进和调整，但在这一节中，我们将探索它的原始形式，因为这将为我们理解最新版本提供一个良好的基础。

该算法将两幅图像作为输入:

*   我们想要重绘的内容图像( *C* )
*   样式图像(I ),我们将使用它的样式(纹理)来重画 *C*

算法的结果是一幅新的图像: *G = C + S* 。下面是一个神经风格转移的例子:

![](assets/5881e581-891e-4120-bdd1-38e5aec76e40.png)

神经类型转移的一个例子

为了理解神经类型转移是如何工作的，让我们回忆一下 CNN 学习它们特征的分级表示。我们知道初始卷积层学习基本特征，例如边和线。相反，更深的层学习更复杂的特征，如人脸、汽车和树木。了解了这一点，我们再来看看算法本身:

1.  像许多其他任务一样(例如，[第三章](9ac88546-8662-4b23-aa96-7eb00f48fedb.xhtml) *，物体检测和图像分割*，这个算法从一个预训练的 VGG 网络开始。

2.  向网络提供内容图像， *C* 。提取并存储网络中间的一个或多个隐藏卷积层的输出激活(或特征图或切片)。让我们用*A[c]l来表示这些激活，其中 *l* 是层的索引。我们对中间层感兴趣，因为中间层中编码的特性抽象级别最适合这项任务。*
3.  对样式图像做同样的操作， *S* 。这一次，用*A[s]l表示 *l* 层的样式激活。我们为内容和风格选择的层不一定相同。*
4.  生成单个随机图像(白噪声)， *G* 。这个随机图像会逐渐变成算法的最终结果。我们将多次重复这一过程:
    1.  通过网络传播 *G* 。这是我们将在整个过程中使用的唯一图像。像以前一样，我们将存储所有 *l* 层的激活(这里， *l* 是我们用于内容和样式图像的所有层的组合)。让我们用*A[g]l来表示这些激活。*
    2.  计算随机噪声激活之间的差异，一方面是*A[g]l，另一方面是*A[c]l和*A[s]l。这将是我们损失函数的两个组成部分:***
        *   ![](assets/40dba1df-c29d-4daf-b191-cf57362f210e.png)，被称为**内容损失**:这只是所有 *l* 层的两次激活之间的元素差异的 MSE。
        *   ![](assets/5e6ed128-6148-46ff-82e9-10b6a9f5363c.png)，被称为**风格损失**:这类似于内容损失，但我们将比较它们的**克矩阵**，而不是原始激活(我们不会深入讨论任何细节)。

这种算法使我们能够利用 CNN 强大的表达能力进行艺术风格的转换。这是通过一个新颖的损失函数和反向传播的巧妙运用来实现的。

如果你对实现神经风格转移感兴趣，可以去[https://PyTorch . org/tutorials/advanced/neural _ style _ tutorial . html](https://pytorch.org/tutorials/advanced/neural_style_tutorial.html)查看 py torch 官方教程。或者，到[https://www . tensor flow . org/beta/tutorials/generative/style _ transfer](https://www.tensorflow.org/beta/tutorials/generative/style_transfer)获取 TF 2.0 实现。

这种算法的一个缺点是速度相对较慢。通常，我们必须重复这个伪训练过程几百次，以产生视觉上吸引人的结果。幸运的是，论文*实时风格转换的感知损失和超分辨率*(【https://arxiv.org/abs/1603.08155】)建立在原始算法的基础上，提供了一个解决方案，速度快了三个数量级。

<title>Summary</title> <link rel="stylesheet" href="css/style.css" type="text/css"> 

# 摘要

在本章中，我们讨论了如何使用生成模型创建新图像，这是目前最令人兴奋的深度学习领域之一。我们学习了 VAEs 的理论基础，然后实现了一个简单的 VAE 来生成新的 MNIST 数字。然后，我们描述了 GAN 框架，讨论并实现了多种类型的 GAN，包括 DCGAN、CGAN、WGAN 和 CycleGAN。最后，我们提到了神经风格转移算法。本章总结了专门讨论计算机视觉的一系列四章，我真的希望你喜欢它们。

在接下来的几章中，我们将讨论自然语言处理和递归网络。***