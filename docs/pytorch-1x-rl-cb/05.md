<title>Solving Multi-armed Bandit Problems</title> <link rel="stylesheet" href="css/style.css" type="text/css"> 

# 解决多臂强盗问题

多臂 bandit 算法可能是强化学习中最流行的算法之一。本章将首先创建一个多臂强盗，并尝试随机政策。我们将重点讨论如何使用四种策略来解决多臂土匪问题，包括ε贪婪、softmax 探索、置信上限和 Thompson 采样。我们将看到他们如何以自己独特的方式处理探索-开发的困境。我们还将致力于一个十亿美元的问题，在线广告，并演示如何使用多臂土匪算法来解决它。最后，我们将使用上下文土匪来解决上下文广告问题，以便在广告优化中做出更明智的决策。

本章将介绍以下配方:

*   创造一个多武器的强盗环境
*   用ε-贪婪策略解决多臂土匪问题
*   用 softmax exploration 解决多臂土匪问题
*   用置信上限算法求解多臂土匪问题
*   用多臂大盗解决互联网广告问题
*   用 Thompson 采样算法求解多臂土匪问题
*   用语境强盗解决互联网广告问题

<title>Creating a multi-armed bandit environment</title> <link rel="stylesheet" href="css/style.css" type="text/css"> 

# 创造一个多武器的强盗环境

让我们从一个简单的项目开始，使用蒙特卡罗方法估计π的值，这是无模型强化学习算法的核心。

****多臂土匪**问题是最简单的强化学习问题之一。最好将其描述为具有多个杠杆(臂)的老虎机，并且每个杠杆具有不同的支付和支付概率。我们的目标是发现回报最大的最佳杠杆，这样我们以后就可以继续选择它。让我们从一个简单的多臂强盗问题开始，其中每个臂的支付和支付概率是固定的。创建环境后，我们将使用随机策略算法来解决它。**

**<title>How to do it...</title> <link rel="stylesheet" href="css/style.css" type="text/css"> 

# 怎么做...

让我们按如下方式开发多兵种土匪环境:

```py
>>> import torch
>>> class BanditEnv():
...     """
...     Multi-armed bandit environment
...     payout_list:
...         A list of probabilities of the likelihood that a 
 particular bandit will pay out
...     reward_list:
...         A list of rewards of the payout that bandit has
...     """
...     def __init__(self, payout_list, reward_list):
...         self.payout_list = payout_list
...         self.reward_list = reward_list
...
...     def step(self, action):
...         if torch.rand(1).item() < self.payout_list[action]:
...             return self.reward_list[action]
...         return 0
```

step 方法执行一个动作，如果付出就返回奖励，否则返回 0。

现在，我们将以一个多臂土匪为例，用随机策略解决它:

1.  定义三臂强盗的支付概率和奖励，并创建一个强盗环境的实例:

```py
>>> bandit_payout = [0.1, 0.15, 0.3]
>>> bandit_reward = [4, 3, 1]>>> bandit_env = BanditEnv(bandit_payout, bandit_reward)
```

比如选择 arm 0 有 10%的几率获得 4 的奖励。

2.  我们指定要运行的剧集数，并定义保存通过选择单个分支累积的总奖励的列表、单个分支被选择的次数以及每个分支的平均奖励:

```py
>>> n_episode = 100000
>>> n_action = len(bandit_payout)
>>> action_count = [0 for _ in range(n_action)]
>>> action_total_reward = [0 for _ in range(n_action)]
>>> action_avg_reward = [[] for action in range(n_action)]
```

3.  定义随机策略，随机选择一个手臂:

```py
>>> def random_policy():
...     action = torch.multinomial(torch.ones(n_action), 1).item()
...     return action
```

4.  现在，我们有 10 万集。对于每一集，我们还会更新每个手臂的统计数据:

```py
>>> for episode in range(n_episode):
...     action = random_policy()
...     reward = bandit_env.step(action)
...     action_count[action] += 1
...     action_total_reward[action] += reward
...     for a in range(n_action):
...         if action_count[a]:
...             action_avg_reward[a].append(
                     action_total_reward[a] / action_count[a])
...         else:
...             action_avg_reward[a].append(0)
```

5.  在运行了 100，000 集之后，我们绘制了平均报酬随时间变化的结果:

```py
>>> import matplotlib.pyplot as plt
 >>> for action in range(n_action):
 ...     plt.plot(action_avg_reward[action])
 >>> plt.legend([‘Arm {}’.format(action) for action in range(n_action)])
 >>> plt.title(‘Average reward over time’)
 >>> plt.xscale(‘log’)
 >>> plt.xlabel(‘Episode’)
 >>> plt.ylabel(‘Average reward’)
 >>> plt.show()
```

<title>How it works...</title> <link rel="stylesheet" href="css/style.css" type="text/css"> 

# 它是如何工作的...

在我们刚刚处理的示例中，有三台老虎机。每台机器有不同的支付(奖励)和支付概率。在每一集里，我们随机选择机器的一个手臂来拉(执行一个动作)，并以一定的概率获得支出。

运行*步骤 5* 中的代码行；你会看到下面的情节:

![](assets/b526f1a1-f16e-4d00-8f6a-c2e08fe8774e.png)

手臂 1 是平均奖励最大的最佳手臂。此外，平均回报在 10，000 集左右开始饱和。

这个解决方案看起来非常幼稚，因为我们只对所有的武器进行了探索。我们将在接下来的食谱中提出更聪明的策略。

<title>Solving multi-armed bandit problems with the epsilon-greedy policy</title> <link rel="stylesheet" href="css/style.css" type="text/css"> 

# 用ε-贪婪策略解决多臂土匪问题

我们可以通过探索和利用的结合来做得更好，而不是仅仅通过随机政策来探索。著名的ε贪婪政策来了。

epsilon-贪婪的多臂强盗在大多数时候利用最好的行动，并且不时地探索不同的行动。给定一个参数ε，其值从 0 到 1，进行勘探和开采的概率分别为ε和 1 - ε:

*   **ε**:采取每一个动作的概率计算如下:

![](assets/f07c030e-3e0d-42ba-a12c-11171c62525d.png)

这里，|A|是可能动作的数量。

*   **贪婪**:优先选择状态-动作值最高的动作，其被选中的概率增加 1 - ε:

![](assets/b76b5ffc-631a-46a9-867d-54c71bc2ab58.png)<title>How to do it...</title> <link rel="stylesheet" href="css/style.css" type="text/css"> 

# 怎么做...

我们使用ε-贪婪策略如下解决多臂土匪问题:

1.  导入 PyTorch 和我们在前一个配方中开发的 bandit 环境，*创建一个多武器 bandit 环境*(假设`BanditEnv`类在一个名为`multi_armed_bandit.py`的文件中):

```py
>>> import torch
 >>> from multi_armed_bandit import BanditEnv
```

2.  定义三臂强盗的支付概率和奖励，并创建一个强盗环境的实例:

```py
>>> bandit_payout = [0.1, 0.15, 0.3]
 >>> bandit_reward = [4, 3, 1] >>> bandit_env = BanditEnv(bandit_payout, bandit_reward)
```

3.  我们指定要运行的剧集数，并定义保存通过选择单个分支累积的总奖励的列表、单个分支被选择的次数以及每个分支的平均奖励:

```py
>>> n_episode = 100000
 >>> n_action = len(bandit_payout)
 >>> action_count = [0 for _ in range(n_action)]
 >>> action_total_reward = [0 for _ in range(n_action)]
 >>> action_avg_reward = [[] for action in range(n_action)]
```

4.  定义ε-贪婪策略函数，指定ε的值，并创建ε-贪婪策略实例:

```py
>>> def gen_epsilon_greedy_policy(n_action, epsilon):
 ...     def policy_function(Q):
 ...         probs = torch.ones(n_action) * epsilon / n_action
 ...         best_action = torch.argmax(Q).item()
 ...         probs[best_action] += 1.0 - epsilon
 ...         action = torch.multinomial(probs, 1).item()
 ...         return action
 ...     return policy_function >>> epsilon = 0.2
 >>> epsilon_greedy_policy = gen_epsilon_greedy_policy(n_action, epsilon)
```

5.  初始化`Q`函数，是单个兵种获得的平均奖励:

```py
>>> Q = torch.zeros(n_action)
```

我们将随着时间的推移更新`Q`函数。

6.  现在，我们有 10 万集。对于每一集，我们还会更新每个手臂的统计数据:

```py
>>> for episode in range(n_episode):
 ...     action = epsilon_greedy_policy(Q)
 ...     reward = bandit_env.step(action)
 ...     action_count[action] += 1
 ...     action_total_reward[action] += reward
 ...     Q[action] = action_total_reward[action] / action_count[action]
 ...     for a in range(n_action):
 ...         if action_count[a]:
 ...             action_avg_reward[a].append(
                         action_total_reward[a] / action_count[a])
 ...         else:
 ...             action_avg_reward[a].append(0)
```

7.  在运行了 100，000 集之后，我们绘制了一段时间内平均奖励的结果:

```py
>>> import matplotlib.pyplot as plt
 >>> for action in range(n_action):
 ...     plt.plot(action_avg_reward[action])
 >>> plt.legend([‘Arm {}’.format(action) for action in range(n_action)])
 >>> plt.title(‘Average reward over time’)
 >>> plt.xscale(‘log’)
 >>> plt.xlabel(‘Episode’)
 >>> plt.ylabel(‘Average reward’)
 >>> plt.show()
```

<title>How it works...</title> <link rel="stylesheet" href="css/style.css" type="text/css"> 

# 它是如何工作的...

与其他 MDP 问题类似，ε-贪婪策略以 1 - ε的概率选择最佳臂，并以ε的概率执行随机探索。艾司隆管理勘探和开采之间的权衡。

在*步骤 7* 中，你会看到如下的情节:

![](assets/5d2ef11b-8e61-4a96-9fbc-53652af32739.png)

手臂 1 是最好的手臂，末端平均奖励最大。此外，它的平均回报在大约 1000 集后开始饱和。

<title>There's more...</title> <link rel="stylesheet" href="css/style.css" type="text/css"> 

# 还有更多...

你可能想知道ε贪婪策略是否真的优于随机策略。除了ε-贪婪策略下最优臂的值收敛得更早这一事实之外，我们还可以证明，平均而言，ε-贪婪策略下我们在训练过程中获得的回报高于随机策略。

我们可以简单地平均所有剧集的奖励:

```py
>>> print(sum(action_total_reward) / n_episode)
 0.43718
```

根据 epsilon-greedy 政策，超过 100，000 集的平均支出是`0.43718`。对随机策略解决方案重复相同的计算，我们得到 0.37902 作为平均支出。

<title>Solving multi-armed bandit problems with the softmax exploration</title> <link rel="stylesheet" href="css/style.css" type="text/css"> 

# 使用 softmax exploration 解决多臂土匪问题

在本食谱中，我们将使用 softmax exploration 算法解决多臂强盗问题。我们将看到它与ε贪婪策略的不同之处。

正如我们在 epsilon-greedy 中看到的，在执行探索时，我们随机选择一个概率为ε/|A|的非最佳分支。每个非最佳臂被同等对待，而不管其在 Q 函数中的值。此外，以固定的概率选择最佳臂，而不考虑其值。在 **softmax 探索**中，基于来自 Q 函数值的 **softmax 分布**的概率选择手臂。概率计算如下:

![](assets/8c1bf157-b0ab-4725-841d-7f8299aff6e9.png)

这里，τ参数是温度因子，它指定了探索的随机性。τ值越高，越接近平等探索；τ值越低，选择最佳臂的可能性越大。

<title>How to do it...</title> <link rel="stylesheet" href="css/style.css" type="text/css"> 

# 怎么做...

我们使用 softmax 探索算法来解决多臂土匪问题，如下所示:

1.  导入 PyTorch 和我们在第一个配方中开发的强盗环境，*创建一个多武器强盗环境*(假设`BanditEnv`类在一个名为`multi_armed_bandit.py`的文件中):

```py
>>> import torch
 >>> from multi_armed_bandit import BanditEnv
```

2.  定义三臂强盗的支付概率和奖励，并创建一个强盗环境的实例:

```py
>>> bandit_payout = [0.1, 0.15, 0.3]
 >>> bandit_reward = [4, 3, 1] >>> bandit_env = BanditEnv(bandit_payout, bandit_reward)
```

3.  我们指定要运行的剧集数，并定义保存通过选择单个分支累积的总奖励的列表、单个分支被选择的次数以及每个分支的平均奖励:

```py
>>> n_episode = 100000
 >>> n_action = len(bandit_payout)
 >>> action_count = [0 for _ in range(n_action)]
 >>> action_total_reward = [0 for _ in range(n_action)]
 >>> action_avg_reward = [[] for action in range(n_action)]
```

4.  定义 softmax exploration 策略函数，指定τ的值，并创建 softmax exploration 策略实例:

```py
>>> def gen_softmax_exploration_policy(tau):
 ...     def policy_function(Q):
 ...         probs = torch.exp(Q / tau)
 ...         probs = probs / torch.sum(probs)
 ...         action = torch.multinomial(probs, 1).item()
 ...         return action
 ...     return policy_function >>> tau = 0.1
 >>> softmax_exploration_policy = gen_softmax_exploration_policy(tau)
```

5.  初始化 Q 函数，它是单个手臂获得的平均奖励:

```py
>>> Q = torch.zeros(n_action)
```

我们将随着时间的推移更新 Q 函数。

6.  现在，我们有 10 万集。对于每一集，我们还会更新每个手臂的统计数据:

```py
>>> for episode in range(n_episode):
 ...     action = softmax_exploration_policy(Q)
 ...     reward = bandit_env.step(action)
 ...     action_count[action] += 1
 ...     action_total_reward[action] += reward
 ...     Q[action] = action_total_reward[action] / action_count[action]
 ...     for a in range(n_action):
 ...         if action_count[a]:
 ...             action_avg_reward[a].append(                         action_total_reward[a] / action_count[a])
 ...         else:
 ...             action_avg_reward[a].append(0)
```

7.  在运行了 100，000 集之后，我们绘制了一段时间内平均奖励的结果:

```py
>>> import matplotlib.pyplot as plt
 >>> for action in range(n_action):
 ...     plt.plot(action_avg_reward[action])
 >>> plt.legend([‘Arm {}’.format(action) for action in range(n_action)])
 >>> plt.title(‘Average reward over time’)
 >>> plt.xscale(‘log’)
 >>> plt.xlabel(‘Episode’)
 >>> plt.ylabel(‘Average reward’)
 >>> plt.show()
```

<title>How it works...</title> <link rel="stylesheet" href="css/style.css" type="text/css"> 

# 它是如何工作的...

在 softmax 探索策略中，利用基于 Q 值的 softmax 函数来解决开发和探索的两难问题。它不是对最佳臂和非最佳臂使用一对固定的概率，而是根据 softmax 分布以τ参数作为温度因子来调整概率。τ值越高，勘探的重点就越多。

在*步骤 7* 中，你会看到如下的情节:

![](assets/85a2e1aa-29e4-4277-a3ca-811fcffeccec.png)

手臂 1 是最好的手臂，末端平均奖励最大。此外，在这个例子中，它的平均回报在大约 800 集之后开始饱和。

<title>Solving multi-armed bandit problems with the upper confidence bound algorithm</title> <link rel="stylesheet" href="css/style.css" type="text/css"> 

# 用置信上限算法求解多臂土匪问题

在前两个配方中，我们探讨了多臂土匪问题中的随机行动，其概率要么在ε贪婪策略中被指定为固定值，要么在 softmax 探索算法中基于 Q 函数值计算。在任一算法中，采取随机行动的概率不随时间调整。理想情况下，随着学习的进展，我们希望探索更少。在这个配方中，我们将使用一种叫做**置信上限**的新算法来实现这个目标。

**置信上限** ( **UCB** )算法源于置信区间的思想。一般来说，置信区间是真实值所在的数值范围。在 UCB 算法中，一只手臂的置信区间是这只手臂获得的平均回报所在的范围。区间的形式是[置信下限，置信上限]，我们只使用上限，即 UCB，来估计手臂的潜力。UCB 的计算方法如下:

![](assets/5dfdbc84-307b-4249-8a2b-0dd4f636dca4.png)

这里 t 是集数，N(a)是在 t 集中选择 a 臂的次数。随着学习的进行，置信区间缩小，越来越准确。要拉的臂是具有最高 UCB 的臂。

<title>How to do it...</title> <link rel="stylesheet" href="css/style.css" type="text/css"> 

# 怎么做...

我们使用 UCB 算法如下解决多臂土匪问题:

1.  导入 PyTorch 和我们在第一个配方中开发的强盗环境，*创建一个多武器强盗环境*(假设`BanditEnv`类在一个名为`multi_armed_bandit.py`的文件中):

```py
>>> import torch
 >>> from multi_armed_bandit import BanditEnv
```

2.  定义三臂强盗的支付概率和奖励，并创建一个强盗环境的实例:

```py
>>> bandit_payout = [0.1, 0.15, 0.3]
 >>> bandit_reward = [4, 3, 1] >>> bandit_env = BanditEnv(bandit_payout, bandit_reward)
```

3.  我们指定要运行的剧集数，并定义保存通过选择单个分支累积的总奖励的列表、单个分支被选择的次数以及每个分支的平均奖励:

```py
>>> n_episode = 100000
 >>> n_action = len(bandit_payout)
 >>> action_count = torch.tensor([0\. for _ in range(n_action)])
 >>> action_total_reward = [0 for _ in range(n_action)]
 >>> action_avg_reward = [[] for action in range(n_action)]
```

4.  定义 UCB 策略函数，该函数根据 UCB 公式计算最佳 arm:

```py
>>> def upper_confidence_bound(Q, action_count, t):
 ...     ucb = torch.sqrt((2 * torch.log(torch.tensor(float(t))))                                              / action_count) + Q
 ...     return torch.argmax(ucb)
```

5.  初始化 Q 函数，它是单个臂获得的平均奖励:

```py
>>> Q = torch.empty(n_action)
```

我们将随着时间的推移更新 Q 函数。

6.  现在，我们用我们的 UCB 政策做了 10 万集。对于每一集，我们还会更新每个手臂的统计数据:

```py
>>> for episode in range(n_episode):
 ...     action = upper_confidence_bound(Q, action_count, episode)
 ...     reward = bandit_env.step(action)
 ...     action_count[action] += 1
 ...     action_total_reward[action] += reward
 ...     Q[action] = action_total_reward[action] / action_count[action]
 ...     for a in range(n_action):
 ...         if action_count[a]:
 ...             action_avg_reward[a].append(                         action_total_reward[a] / action_count[a])
 ...         else:
 ...             action_avg_reward[a].append(0)
```

7.  在运行了 100，000 集之后，我们绘制了一段时间内平均奖励的结果:

```py
>>> import matplotlib.pyplot as plt
 >>> for action in range(n_action):
 ...     plt.plot(action_avg_reward[action])
 >>> plt.legend([‘Arm {}’.format(action) for action in range(n_action)])
 >>> plt.title(‘Average reward over time’)
 >>> plt.xscale(‘log’)
 >>> plt.xlabel(‘Episode’)
 >>> plt.ylabel(‘Average reward’)
 >>> plt.show()
```

<title>How it works...</title> <link rel="stylesheet" href="css/style.css" type="text/css"> 

# 它是如何工作的...

在这个食谱中，我们用 UCB 算法解决了多臂强盗。它根据剧集数量调整剥削-探索困境。对于一个只有少量数据点的动作，其置信区间相对较宽，因此，选择这个动作具有相对较高的不确定性。随着动作的更多情节被选择，置信区间变窄并收缩到其实际值。在这种情况下，选择(或不选择)这一行动具有很高的确定性。最后，UCB 算法在每集中拉动具有最高 UCB 的手臂，并且随着时间的推移获得越来越多的信心。

运行*步骤 7* 中的代码后，您将看到以下图形:

![](assets/34c31cbd-e4b9-4415-aa1f-0a4313d1382d.png)

手臂 1 是最好的手臂，最后平均奖励最大。

<title>There's more...</title> <link rel="stylesheet" href="css/style.css" type="text/css"> 

# 还有更多...

你可能想知道 UCB 是否真的胜过ε贪婪政策。我们可以计算整个训练过程的平均回报，平均回报最高的策略学习得更快。

我们可以简单地平均所有剧集的奖励:

```py
>>> print(sum(action_total_reward) / n_episode)
 0.44605
```

超过 100，000 集，UCB 的平均支出是 0.44605，高于ε-greedy 策略的 0.43718。

<title>See also</title> <link rel="stylesheet" href="css/style.css" type="text/css"> 

# 请参见

对于那些想重温置信区间的人，请随意查看以下内容:[http://www.stat.yale.edu/Courses/1997-98/101/confint.htm](http://www.stat.yale.edu/Courses/1997-98/101/confint.htm)

<title>Solving internet advertising problems with a multi-armed bandit</title> <link rel="stylesheet" href="css/style.css" type="text/css"> 

# 用多臂大盗解决互联网广告问题

假设你是一个在网站上做广告优化的广告商:

*   广告背景有三种不同的颜色——红色、绿色和蓝色。哪个会达到最好的点击率(CTR)？
*   这则广告有三种措辞——*学习…* ，*免费...*和*试试...*。哪个会达到最好的点击率？

对于每个访问者，我们需要选择一个广告，以便随着时间的推移最大化点击率。我们如何解决这个问题？

也许你正在考虑 A/B 测试，你将流量随机分组，并将每个广告分配到不同的组，然后经过一段时间的观察，从 CTR 最高的组中选择广告。然而，这基本上是一个完整的探索，我们通常不确定观察期应该有多长，最终会失去很大一部分潜在点击。此外，在 A/B 测试中，假设广告的未知点击率不会随时间而改变。否则，这种 A/B 测试应定期重新运行。

一个多臂土匪当然可以比 A/B 测试做得更好。每条手臂就是一个广告，一条手臂的奖励要么是 1(点击)，要么是 0(无点击)。

让我们试着用 UCB 算法来解决它。

<title>How to do it...</title> <link rel="stylesheet" href="css/style.css" type="text/css"> 

# 怎么做...

我们可以使用 UCB 算法如下解决多臂强盗广告问题:

1.  导入 PyTorch 和我们在第一个配方中开发的强盗环境，*创建一个多武器强盗环境*(假设`BanditEnv`类在一个名为`multi_armed_bandit.py`的文件中):

```py
>>> import torch
>>> from multi_armed_bandit import BanditEnv
```

2.  定义三臂强盗(例如，三个广告候选人)的支付概率和奖励，并创建一个强盗环境实例:

```py
>>> bandit_payout = [0.01, 0.015, 0.03]
>>> bandit_reward = [1, 1, 1]>>> bandit_env = BanditEnv(bandit_payout, bandit_reward)
```

这里，广告 0 的真实点击率是 1%，广告 1 的真实点击率是 1.5%，广告 2 的真实点击率是 3%。

3.  我们指定要运行的剧集数，并定义保存通过选择单个分支累积的总奖励的列表、单个分支被选择的次数以及每个分支的平均奖励:

```py
>>> n_episode = 100000
>>> n_action = len(bandit_payout)
>>> action_count = torch.tensor([0\. for _ in range(n_action)])
>>> action_total_reward = [0 for _ in range(n_action)]
>>> action_avg_reward = [[] for action in range(n_action)]
```

4.  定义 UCB 策略函数，该函数根据 UCB 公式计算最佳 arm:

```py
>>> def upper_confidence_bound(Q, action_count, t):
...     ucb = torch.sqrt((2 * torch.log(
 torch.tensor(float(t)))) / action_count) + Q
...     return torch.argmax(ucb)
```

5.  初始化 Q 函数，它是单个兵种获得的平均奖励:

```py
>>> Q = torch.empty(n_action)
```

我们将随着时间的推移更新 Q 函数。

6.  现在，我们用 UCB 政策做了 10 万集。对于每一集，我们还会更新每个手臂的统计数据:

```py
>>> for episode in range(n_episode):
...     action = upper_confidence_bound(Q, action_count, episode)
...     reward = bandit_env.step(action)
...     action_count[action] += 1
...     action_total_reward[action] += reward
...     Q[action] = action_total_reward[action] / action_count[action]
...     for a in range(n_action):
...         if action_count[a]:
...             action_avg_reward[a].append(
 action_total_reward[a] / action_count[a])
...         else:
...             action_avg_reward[a].append(0)
```

7.  在运行了 100，000 集之后，我们绘制了一段时间内平均奖励的结果:

```py
>>> import matplotlib.pyplot as plt
>>> for action in range(n_action):
...     plt.plot(action_avg_reward[action])
>>> plt.legend([‘Arm {}’.format(action) for action in range(n_action)])
>>> plt.title(‘Average reward over time’)
>>> plt.xscale(‘log’)
>>> plt.xlabel(‘Episode’)
>>> plt.ylabel(‘Average reward’)
>>> plt.show()
```

<title>How it works...</title> <link rel="stylesheet" href="css/style.css" type="text/css"> 

# 它是如何工作的...

在这个食谱中，我们以多臂强盗的方式解决了广告优化问题。它克服了 A/B 测试方法面临的挑战。我们使用 UCB 算法来解决多臂(多 ad)土匪问题；每只手臂的奖励不是 1 就是 0。UCB(或其他算法，如 epsilon-greedy 和 softmax exploration)不是纯粹的探索，也不是行动和回报之间没有互动，而是在必要时在开发和探索之间动态切换。对于具有少量数据点的广告，置信区间相对较宽，因此，选择该动作具有相对较高的不确定性。随着广告的更多集被选择，置信区间变窄并收缩到其实际值。

您可以在*步骤 7* 中看到如下结果图:

![](assets/269802e0-7738-422f-aedb-183bdbfb4505.png)

广告 2 是模型收敛后预测 CTR(平均奖励)最高的最佳广告。

最终，我们发现 ad 2 是最佳选择，这是事实。此外，我们越早发现这一点越好，因为我们会失去更少的潜在点击。在这个例子中，在大约 100 集之后，ad 2 的表现超过了其他的。

<title>Solving multi-armed bandit problems with the Thompson sampling algorithm</title> <link rel="stylesheet" href="css/style.css" type="text/css"> 

# 用 Thompson 采样算法求解多臂土匪问题

在这份食谱中，我们将使用另一种算法汤普森抽样来解决广告强盗问题中的剥削和探索困境。我们将看到它与前面的三种算法有很大的不同。

**Thompson sampling**(**TS**)也被称为贝叶斯强盗，因为它从以下角度应用了贝叶斯思维方式:

*   这是一种概率算法。
*   它计算每个臂的先验分布，并从每个分布中采样一个值。
*   然后，它选择具有最高值的手臂并观察奖励。
*   最后，它根据观察到的回报更新先验分布。这个过程叫做**贝叶斯更新**。

正如我们所看到的，在我们的广告优化案例中，每只手臂的回报要么是 1，要么是 0。我们可以使用**贝塔分布**作为我们的先验分布，因为贝塔分布的值是从 0 到 1。β分布由两个参数α和β参数化。α表示我们获得奖励 1 的次数，β表示我们获得奖励 0 的次数。

为了帮助您更好地理解 beta 分布，在实现 TS 算法之前，我们先来看几个 beta 分布。

<title>How to do it...</title> <link rel="stylesheet" href="css/style.css" type="text/css"> 

# 怎么做...

让我们通过以下步骤来探索 beta 版:

1.  导入 PyTorch 和 matplotlib，因为我们将可视化分布的形状:

```py
>>> import torch
>>> import matplotlib.pyplot as plt
```

2.  我们从可视化β分布的形状开始，起始位置为α=1 和β=1:

```py
>>> beta1 = torch.distributions.beta.Beta(1, 1)
>>> samples1 = [beta1.sample() for _ in range(100000)]
>>> plt.hist(samples1, range=[0, 1], bins=10)
>>> plt.title(‘beta(1, 1)’)
>>> plt.show()
```

你会看到下面的情节:

![](assets/0960d6c4-036e-4345-a1c1-2851ef650ac6.png)

显然，当α=1 和β=1 时，它不提供任何关于真实值位于 0 到 1 范围内的任何信息。因此，它变成了均匀分布。

3.  然后，我们将α=5，β=1 的β分布的形状形象化:

```py
>>> beta2 = torch.distributions.beta.Beta(5, 1)
>>> samples2 = [beta2.sample() for _ in range(100000)]
>>> plt.hist(samples2, range=[0, 1], bins=10)
>>> plt.title(‘beta(5, 1)’)
>>> plt.show()
```

你会看到下面的情节:

![](assets/bc882317-7591-47f4-a7d9-aec44de5bd29.png)

当α=5，β=1 时，这意味着 4 次实验中有 4 次连续奖励为 1。分布向 1 移动。

4.  现在，让我们对α=1 和β=5 进行实验:

```py
>>> beta3 = torch.distributions.beta.Beta(1, 5)
>>> samples3= [beta3.sample() for _ in range(100000)]
>>> plt.hist(samples3, range=[0, 1], bins=10)
>>> plt.title(‘beta(1, 5)’)
>>> plt.show()
```

你会看到下面的情节:

![](assets/dd081d9e-e4bb-4eeb-9605-46f95b53f71c.png)

当α=1，β=5 时，这意味着 4 次实验中有 4 次连续奖励为 0。分布向 0 移动。

5.  最后，我们来看看α=5 和β=5 时的情况:

```py
>>> beta4 = torch.distributions.beta.Beta(5, 5)
>>> samples4= [beta4.sample() for _ in range(100000)]
>>> plt.hist(samples4, range=[0, 1], bins=10)
>>> plt.title(‘beta(5, 5)’)
>>> plt.show()
```

你会看到下面的情节:

![](assets/fa2fe6f2-4782-4c9a-aab7-d5a26db84f4f.png)

当α=5 和β=5 时，我们在 8 轮中观察到相同数量的点击和不点击。分布向中间点移动， **0.5** 。

现在是使用 Thompson 采样算法解决多臂强盗广告问题的时候了:

1.  导入我们在第一个配方中开发的土匪环境，*创建一个多武器土匪环境*(假设`BanditEnv`类在一个名为`multi_armed_bandit.py`的文件中):

```py
>>> from multi_armed_bandit import BanditEnv
```

2.  定义三臂强盗(三个广告候选人)的支付概率和奖励，并创建一个强盗环境实例:

```py
>>> bandit_payout = [0.01, 0.015, 0.03]
>>> bandit_reward = [1, 1, 1]>>> bandit_env = BanditEnv(bandit_payout, bandit_reward)
```

3.  我们指定要运行的剧集数，并定义保存通过选择单个分支累积的总奖励的列表、单个分支被选择的次数以及每个分支的平均奖励:

```py
>>> n_episode = 100000
>>> n_action = len(bandit_payout)
>>> action_count = torch.tensor([0\. for _ in range(n_action)])
>>> action_total_reward = [0 for _ in range(n_action)]
>>> action_avg_reward = [[] for action in range(n_action)]
```

4.  定义 TS 函数，该函数从每个臂的β分布中采样一个值，并选择具有最高值的臂:

```py
>>> def thompson_sampling(alpha, beta):
...     prior_values = torch.distributions.beta.Beta(alpha, beta).sample()
...     return torch.argmax(prior_values)
```

5.  初始化每个臂的α和β:

```py
>>> alpha = torch.ones(n_action)
>>> beta = torch.ones(n_action)
```

注意，每个 beta 分布都应该从α=β=1 开始。

6.  现在，我们用 TS 算法运行 10 万集。对于每一集，我们还根据观察到的回报更新每一组的α和β:

```py
>>> for episode in range(n_episode):
 ...     action = thompson_sampling(alpha, beta)
 ...     reward = bandit_env.step(action)
 ...     action_count[action] += 1
 ...     action_total_reward[action] += reward
 ...     if reward > 0:
 ...         alpha[action] += 1
 ...     else:
 ...         beta[action] += 1
 ...     for a in range(n_action):
 ...         if action_count[a]:
 ...             action_avg_reward[a].append(                         action_total_reward[a] / action_count[a])
 ...         else:
 ...             action_avg_reward[a].append(0)
```

7.  在运行了 100，000 集之后，我们绘制了一段时间内平均奖励的结果:

```py
>>> import matplotlib.pyplot as plt
>>> for action in range(n_action):
...     plt.plot(action_avg_reward[action])
>>> plt.legend([‘Arm {}’.format(action) for action in range(n_action)])
>>> plt.title(‘Average reward over time’)
>>> plt.xscale(‘log’)
>>> plt.xlabel(‘Episode’)
>>> plt.ylabel(‘Average reward’)
>>> plt.show()
```

<title>How it works...</title> <link rel="stylesheet" href="css/style.css" type="text/css"> 

# 它是如何工作的...

在这个食谱中，我们用 ts 算法解决了广告强盗问题。TS 和其他三种方法的最大区别是采用了贝叶斯优化。它首先计算每个可能臂的先验分布，然后从每个分布中随机抽取一个值。然后，它选取具有最高值的臂，并使用观察到的结果来更新先验分布。TS 策略既随机又贪婪。如果一个广告更有可能收到点击，它的 beta 分布向 1 移动，因此，随机样本的值更接近 1。

运行完*步骤 7* 中的代码行后，您将看到下面的图:

![](assets/d9200fdd-cbcb-43cf-967e-ff9f581fb1e0.png)

广告 2 是最好的广告，预测的 CTR(平均报酬)最高。

<title>See also</title> <link rel="stylesheet" href="css/style.css" type="text/css"> 

# 请参见

对于那些想重温测试版的人，请随意查看以下内容:

*   [https://www . ITL . NIST . gov/div 898/handbook/EDA/section 3/EDA 366h . htm](https://www.itl.nist.gov/div898/handbook/eda/section3/eda366h.htm)
*   [http://variance explained . org/statistics/beta _ distribution _ and _ baseball/](http://varianceexplained.org/statistics/beta_distribution_and_baseball/)

<title>Solving internet advertising problems with contextual bandits</title> <link rel="stylesheet" href="css/style.css" type="text/css"> 

# 用语境强盗解决互联网广告问题

你可能会注意到，在广告优化问题中，我们只关心广告，而忽略了其他可能影响广告被点击与否的信息，比如用户信息和网页信息。在这份食谱中，我们将讨论如何考虑广告本身以外的更多信息，并解决与上下文相关的问题。

我们到目前为止处理的多兵种土匪问题不涉及国家的概念，这与 MDPs 有很大的不同。我们只有几个动作，将会产生与所选动作相关的奖励。**上下文土匪**通过引入国家的概念，扩展了多兵种土匪。状态提供了对环境的描述，这有助于代理采取更明智的行动。在广告示例中，州可以是用户的性别(两个州，男性和女性)、用户的年龄组(例如，四个州)或页面类别(例如，体育、金融或新闻)。直觉上，特定人群的用户更有可能点击特定页面上的广告。

不难理解语境土匪。多臂土匪是有多个臂的单个机器，而上下文土匪是一组这样的机器(土匪)。上下文强盗中的每台机器都是一个有多个分支的状态。学习目标是为每台机器(状态)找到最佳手臂(动作)。

为简单起见，我们将使用两种状态的广告示例。

<title>How to do it...</title> <link rel="stylesheet" href="css/style.css" type="text/css"> 

# 怎么做...

我们使用 UCB 算法如下解决上下文强盗广告问题:

1.  导入 PyTorch 和我们在第一个配方中开发的强盗环境，*创建一个多武器强盗环境*(假设`BanditEnv`类在一个名为`multi_armed_bandit.py`的文件中):

```py
>>> import torch
>>> from multi_armed_bandit import BanditEnv
```

2.  定义两个三臂强盗的支付概率和奖励:

```py
>>> bandit_payout_machines = [
...     [0.01, 0.015, 0.03],
...     [0.025, 0.01, 0.015]
... ]
>>> bandit_reward_machines = [
...     [1, 1, 1],
...     [1, 1, 1]
... ]
```

这里，ad 0 的真实 CTR 是 1%，ad 1 的真实 CTR 是 1.5%，ad 2 的真实 CTR 是第一个状态的 3%，第二个状态的真实 CTR 是[2.5%，1%，1.5%]。

在我们的例子中，吃角子老虎机的数量是两个:

```py
>>> n_machine = len(bandit_payout_machines)
```

根据相应的支付信息创建强盗列表:

```py
>>> bandit_env_machines = [BanditEnv(bandit_payout, bandit_reward)
...           for bandit_payout, bandit_reward in
...           zip(bandit_payout_machines, bandit_reward_machines)]
```

3.  我们指定要运行的剧集数，并定义保存通过在每个州选择单个分支而累积的总奖励的列表，在每个州选择单个分支的次数，以及在每个州每个分支的平均奖励:

```py
>>> n_episode = 100000
>>> n_action = len(bandit_payout_machines[0])
>>> action_count = torch.zeros(n_machine, n_action)
>>> action_total_reward = torch.zeros(n_machine, n_action)
>>> action_avg_reward = [[[] for action in range(n_action)] for _ in range(n_machine)]
```

4.  定义 UCB 策略函数，该函数根据 UCB 公式计算最佳 arm:

```py
>>> def upper_confidence_bound(Q, action_count, t):
...     ucb = torch.sqrt((2 * torch.log( 
                 torch.tensor(float(t)))) / action_count) + Q
...     return torch.argmax(ucb)
```

5.  初始化 Q 函数，它是各个状态下各个臂获得的平均奖励:

```py
>>> Q_machines = torch.empty(n_machine, n_action)
```

我们将随着时间的推移更新 Q 函数。

6.  现在，我们用 UCB 政策做了 10 万集。对于每集，我们还会更新每个州中每个手臂的统计数据:

```py
>>> for episode in range(n_episode):
...     state = torch.randint(0, n_machine, (1,)).item()
...     action = upper_confidence_bound(                 Q_machines[state], action_count[state], episode)
...     reward = bandit_env_machines[state].step(action)
...     action_count[state][action] += 1
...     action_total_reward[state][action] += reward
...     Q_machines[state][action] =                              action_total_reward[state][action]                              / action_count[state][action]
...     for a in range(n_action):
...         if action_count[state][a]:
...             action_avg_reward[state][a].append(                             action_total_reward[state][a]                              / action_count[state][a])
...         else:
...             action_avg_reward[state][a].append(0)
```

7.  在运行了 100，000 集之后，我们绘制了每个州随时间变化的平均奖励结果:

```py
>>> import matplotlib.pyplot as plt
>>> for state in range(n_machine):
...     for action in range(n_action):
...         plt.plot(action_avg_reward[state][action])
...     plt.legend([‘Arm {}’.format(action)                      for action in range(n_action)])
...     plt.xscale(‘log’)
...     plt.title(       ‘Average reward over time for state {}’.format(state))
...     plt.xlabel(‘Episode’)
...     plt.ylabel(‘Average reward’)
...     plt.show()
```

<title>How it works...</title> <link rel="stylesheet" href="css/style.css" type="text/css"> 

# 它是如何工作的...

在这个食谱中，我们用 UCB 算法解决了上下文强盗的上下文广告问题。

运行*步骤 7* 中的代码行，您将看到下面的图。

我们得到了第一种状态:

![](assets/a9a9590f-38ca-4319-819f-58b8715f2fb4.png)

我们得到了第二种状态:

![](assets/60b4910d-b5cf-41c8-be9e-1ef25c9c1e54.png)

给定第一种状态，广告 2 是最佳广告，具有最高的预测 CTR。给定第二种状态，ad 0 是最优 ad，平均奖励最高。这些都是真的。

上下文土匪是一组多武装的土匪。每一个土匪都代表了一种独特的环境状态。状态提供了对环境的描述，这有助于代理采取更明智的行动。在我们的广告示例中，男性用户可能比女性用户更有可能点击广告。我们简单地使用两个老虎机来合并两种状态，并搜索给定每种状态的最佳手臂。

需要注意的一点是，尽管涉及到状态的概念，但上下文土匪还是不同于 MDP。首先，情境强盗中的状态不是由先前的动作或状态决定的，而仅仅是对环境的观察。第二，在《盗匪》中没有延迟或打折的奖励，因为一集盗匪是一个步骤。然而，与多武装匪徒相比，背景匪徒更接近于 MDP，因为行动取决于环境中的状态。可以肯定地说，情境土匪介于多武装土匪和完全 MDP 强化学习之间。**