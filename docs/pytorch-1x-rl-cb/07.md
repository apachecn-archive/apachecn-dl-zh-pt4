<title>Deep Q-Networks in Action</title> <link rel="stylesheet" href="css/style.css" type="text/css"> 

# 行动中的深度 Q 网络

深度 Q 学习，或使用深度 Q 网络，被认为是最现代的强化学习技术。在这一章中，我们将逐步开发各种深度 Q-网络模型，并应用它们来解决几个强化学习问题。我们将从普通的 Q 网络开始，并通过体验回放来增强它们。我们将通过使用额外的目标网络来提高鲁棒性，并演示如何微调深度 Q 网络。我们也将实验决斗深 Q 网络，看看他们的价值函数如何不同于其他类型的深 Q 网络。在最后两个食谱中，我们将通过将卷积神经网络合并到深度 Q 网络中来解决复杂的 Atari 游戏问题。

本章将介绍以下配方:

*   发展深度 Q 网络
*   用经验回放改进 DQNs
*   发展双深度 Q-网络
*   为磁极调谐双 DQN 超参数
*   发展决斗深度 Q-网络
*   将深度 Q-网络应用于 Atari 游戏
*   在雅达利游戏中使用卷积神经网络

<title>Developing deep Q-networks</title> <link rel="stylesheet" href="css/style.css" type="text/css"> 

# 发展深度 Q 网络

你会回忆起**函数逼近** ( **FA** )使用从原始状态生成的一组特征来逼近状态空间。**深度 Q 网络** ( **DQNs** )非常类似于带有神经网络的 FA，但是它们使用神经网络将状态直接映射到动作值，而不是使用一组生成的特征作为媒介。

在深度 Q 学习中，神经网络被训练为在给定输入状态 s 的情况下为每个动作输出适当的 *Q(s，a)* 值。代理的动作 a 是基于遵循ε贪婪策略的输出 Q(s，a)值来选择的。下图描述了具有两个隐藏图层的 DQN 的结构:

![](assets/4217aa3a-58c0-4827-b4b2-4fbbd85d55ab.png)

您应该记得，Q-learning 是一种非策略学习算法，它基于以下等式更新 Q 函数:

![](assets/d2932660-72b6-40ed-9471-d558e4357238.png)

这里， *s'* 是采取行动后的结果状态， *a* ，in 状态，*s*； *r* 是关联奖励；α是学习率；γ是贴现因子。此外， [![](assets/781b37bb-fdcf-4f8f-a59e-7438e8d6cf73.png)] 意味着行为策略是贪婪的，其中选择状态*s’*中的最高 Q 值来生成学习数据。类似地，dqn 学习最小化以下误差项:

![](assets/2e80a59e-1381-409a-a0fd-e477bf2e485f.png)

现在，目标变成了为每个可能的动作找到最佳网络模型以最佳逼近状态值函数 *Q(s，a)* 。在这种情况下，我们试图最小化的损失函数类似于回归问题中的损失函数，即实际值和估计值之间的均方误差。

现在，我们将开发一种 DQN 模式来解决山地车([https://gym.openai.com/envs/MountainCar-v0/](https://gym.openai.com/envs/MountainCar-v0/))的问题。

<title>How to do it...</title> <link rel="stylesheet" href="css/style.css" type="text/css"> 

# 怎么做...

我们使用 DQN 开发深度 Q 学习如下:

1.  导入所有必需的包:

```
>>> import gym
>>> import torch
>>> from torch.autograd import Variable
>>> import random
```

该变量包装张量并支持反向传播。

2.  让我们从`DQN`类的`__init__`方法开始:

```
>>> class DQN():
 ...     def __init__(self, n_state, n_action, n_hidden=50, 
                     lr=0.05):
 ...         self.criterion = torch.nn.MSELoss()
 ...         self.model = torch.nn.Sequential(
 ...                         torch.nn.Linear(n_state, n_hidden),
 ...                         torch.nn.ReLU(),
 ...                         torch.nn.Linear(n_hidden, n_action)
 ...                 )
 ...         self.optimizer = torch.optim.Adam( self.model.parameters(), lr)
```

3.  我们现在开发训练方法，用数据点更新神经网络:

```
>>>     def update(self, s, y):
 ...         """
 ...         Update the weights of the DQN given a training sample
 ...         @param s: state
 ...         @param y: target value
 ...         """
 ...         y_pred = self.model(torch.Tensor(s))
 ...         loss = self.criterion(y_pred, 
                         Variable(torch.Tensor(y)))
 ...         self.optimizer.zero_grad()
 ...         loss.backward()
 ...         self.optimizer.step()
```

4.  接下来是给定状态下每个动作的状态值预测:

```
>>>     def predict(self, s):
 ...     """
 ...     Compute the Q values of the state for all 
              actions using the learning model
 ...     @param s: input state
 ...     @return: Q values of the state for all actions
 ...     """
 ...     with torch.no_grad():
 ...          return self.model(torch.Tensor(s))
```

`DQN`课到此为止！现在我们可以继续开发学习算法了。

5.  我们首先创建一个山地汽车环境:

```
>>> env = gym.envs.make("MountainCar-v0")
```

6.  然后，我们定义ε-贪婪策略:

```
>>> def gen_epsilon_greedy_policy(estimator, epsilon, n_action):
 ...     def policy_function(state):
 ...         if random.random() < epsilon:
 ...             return random.randint(0, n_action - 1)
 ...         else:
 ...             q_values = estimator.predict(state)
 ...             return torch.argmax(q_values).item()
 ...     return policy_function
```

7.  现在，用 DQN 定义深度 Q 学习算法:

```
>>> def q_learning(env, estimator, n_episode, gamma=1.0,
                   epsilon=0.1, epsilon_decay=.99):
 ...     """
 ...     Deep Q-Learning using DQN
 ...     @param env: Gym environment
 ...     @param estimator: Estimator object
 ...     @param n_episode: number of episodes
 ...     @param gamma: the discount factor
 ...     @param epsilon: parameter for epsilon_greedy
 ...     @param epsilon_decay: epsilon decreasing factor
 ...     """
 ...     for episode in range(n_episode):
 ...         policy = gen_epsilon_greedy_policy( estimator, epsilon, n_action)
 ...         state = env.reset()
 ...         is_done = False
 ...         while not is_done:
 ...             action = policy(state)
 ...             next_state, reward, is_done, _ = env.step(action)
 ...             total_reward_episode[episode] += reward
 ...             modified_reward = next_state[0] + 0.5
 ...             if next_state[0] >= 0.5:
 ...                 modified_reward += 100
 ...             elif next_state[0] >= 0.25:
 ...                 modified_reward += 20
 ...             elif next_state[0] >= 0.1:
 ...                 modified_reward += 10
 ...             elif next_state[0] >= 0:
 ...                 modified_reward += 5
 ...
 ...             q_values = estimator.predict(state).tolist()
 ...
 ...             if is_done:
 ...                 q_values[action] = modified_reward
 ...                 estimator.update(state, q_values)
 ...                 break
 ...             q_values_next = estimator.predict(next_state)
 ...             q_values[action] = modified_reward + gamma * 
                             torch.max(q_values_next).item()
 ...             estimator.update(state, q_values)
 ...             state = next_state
 ...         print('Episode: {}, total reward: {}, epsilon: 
                     {}'.format(episode,
                     total_reward_episode[episode], epsilon))
 ...         epsilon = max(epsilon * epsilon_decay, 0.01)
```

8.  然后，我们指定隐藏层的大小和学习速率，并相应地创建一个`DQN`实例:

```
 >>> n_state = env.observation_space.shape[0]
 >>> n_action = env.action_space.n
 >>> n_hidden = 50
 >>> lr = 0.001
 >>> dqn = DQN(n_state, n_action, n_hidden, lr)
```

9.  然后，我们使用我们刚刚开发的 1000 集 DQN 进行深度 Q 学习，并跟踪每集的总(原始)奖励:

```
>>> n_episode = 1000
>>> total_reward_episode = [0] * n_episode
>>> q_learning(env, dqn, n_episode, gamma=.99, epsilon=.3)
 Episode: 0, total reward: -200.0, epsilon: 0.3
 Episode: 1, total reward: -200.0, epsilon: 0.297
 Episode: 2, total reward: -200.0, epsilon: 0.29402999999999996
 ……
 ……
 Episode: 993, total reward: -177.0, epsilon: 0.01
 Episode: 994, total reward: -200.0, epsilon: 0.01
 Episode: 995, total reward: -172.0, epsilon: 0.01
 Episode: 996, total reward: -200.0, epsilon: 0.01
 Episode: 997, total reward: -200.0, epsilon: 0.01
 Episode: 998, total reward: -173.0, epsilon: 0.01
 Episode: 999, total reward: -200.0, epsilon: 0.01
```

10.  现在，我们来展示一段时间内剧集奖励的剧情:

```
>>> import matplotlib.pyplot as plt
>>> plt.plot(total_reward_episode)
>>> plt.title('Episode reward over time')
>>> plt.xlabel('Episode')
>>> plt.ylabel('Total reward')
>>> plt.show()
```

<title>How it works...</title> <link rel="stylesheet" href="css/style.css" type="text/css"> 

# 它是如何工作的...

在*步骤 2* 中，`DQN`类接受四个参数:输入状态和输出动作的数量、隐藏节点的数量(这里我们只用一个隐藏层作为例子)和学习速率。它用一个隐藏层初始化一个神经网络，后面跟着一个 ReLU 激活函数。它接收`n_state`个单元并生成一个`n_action`输出，这是各个动作的预测状态值。优化器 Adam 与每个线性模型一起初始化。损失函数是均方误差。

*步骤 3* 用于更新网络:给定一个训练数据点，预测结果与目标值一起用于计算损失和梯度。然后通过反向传播更新神经网络模型。

在*步骤 7* 中，深度 Q 学习功能执行以下任务:

*   在每一集，创建一个ε因子衰减到 99%的ε贪婪策略(例如，如果第一集的ε为 0.1，则第二集的ε为 0.099)。我们还将ε下限设为 0.01。
*   运行一集:在状态 *s* 的每一步中，通过遵循ε-贪婪策略采取行动 a；然后，使用 DQN 的`predict`方法计算前一状态的 *Q* 值`q_value`。
*   为新状态*s’*计算 *Q* 值`q_values_next`；然后，通过更新旧的 *Q* 值`q_values`，为动作![](assets/0f9dd526-9c3d-4a3c-b2de-b0a019e3a40d.png)计算目标值。
*   使用数据点 *(s，Q(s))* ，训练神经网络。注意 *Q(s)* 由所有动作的值组成。
*   运行`n_episode`集并记录每集的总奖励。

您可能会注意到，我们在训练模型时使用了一个经过修改的奖励版本。它基于汽车的位置，因为我们希望它达到+0.5 的位置。我们还对大于或等于+0.5、+0.25、+0.1 和 0 的仓位分别给予分级激励。这种修改后的奖励设置区分了不同的汽车位置，并倾向于更接近目标的位置；因此，与最初的每步 1 奖励相比，它大大加快了学习速度。

最后，在*步骤 10* 中，您将看到如下结果图:

![](assets/88967993-441e-40e8-b5ce-597a690045a0.png)

你可以看到，在最后的 200 集里，汽车在大约 170 到 180 步后到达山顶。

深度 Q 学习使用更直接的模型(神经网络)来逼近状态值，而不是使用一组中间人工特征。给定一个步骤，其中旧状态通过采取行动转移到新状态并获得奖励，训练 DQN 包括以下阶段:

*   使用神经网络模型来估计旧状态的 *Q* 值。
*   使用神经网络模型来估计新状态的 *Q* 值。
*   使用奖励和新的 *Q* 值更新行动的目标 Q 值，如 [![](assets/e7041f0b-3588-40ff-a5db-dc73000512f9.png)]
*   注意，如果是终端状态，目标 Q 值更新为 r。
*   以旧状态为输入，目标 *Q* 值为输出，训练神经网络模型。

它通过梯度下降更新网络的权重，并可以预测给定状态下的 Q 值。

DQN 极大地减少了要学习的状态的数量，而在 TD 方法中学习数百万个状态是不可行的。此外，它直接将输入状态映射到 Q 值，并且不需要任何附加函数来生成人工特征。

<title>See also</title> <link rel="stylesheet" href="css/style.css" type="text/css"> 

# 请参见

如果您不熟悉作为高级梯度下降法的 Adam 优化器，请查阅以下材料:

*   [https://towards data science . com/Adam-latest-trends-in-deep-learning-optimization-6be 9a 291375 c](https://towardsdatascience.com/adam-latest-trends-in-deep-learning-optimization-6be9a291375c)
*   [https://arxiv.org/abs/1412.6980](https://arxiv.org/abs/1412.6980)

<title>Improving DQNs with experience replay</title> <link rel="stylesheet" href="css/style.css" type="text/css"> 

# 用经验回放改进 DQNs

使用一次一个样本的神经网络来逼近 Q 值不是很稳定。你会记得，在 FA 中，我们加入了经验回放来提高稳定性。类似地，在这个菜谱中，我们将把经验回放应用到 DQNs。

通过体验回放，我们将代理在培训会话中的各集体验(一个体验由旧状态、新状态、动作和奖励组成)存储在内存队列中。每当我们获得足够的经验时，就会从内存中随机抽取一批批经验，用于训练神经网络。经验重放学习分为两个阶段:获取经验，以及基于随机选择的过去经验更新模型。否则，模型将继续从最近的经验中学习，并且神经网络模型可能陷入局部最小值。

我们将开发 DQN 与经验重播，以解决山地汽车问题。

<title>How to do it...</title> <link rel="stylesheet" href="css/style.css" type="text/css"> 

# 怎么做...

我们将开发一个具有如下经验回放的 DQN:

1.  导入必要的模块并创建山地汽车环境:

```
 >>> import gym
 >>> import torch
 >>> from collections import deque
 >>> import random
 >>> from torch.autograd import Variable >>> env = gym.envs.make("MountainCar-v0")
```

2.  为了整合体验回放，我们向`DQN`类添加了一个`replay`方法:

```
>>> def replay(self, memory, replay_size, gamma):
 ...     """
 ...     Experience replay
 ...     @param memory: a list of experience
 ...     @param replay_size: the number of samples we use to 
             update the model each time
 ...     @param gamma: the discount factor
 ...     """
 ...     if len(memory) >= replay_size:
 ...         replay_data = random.sample(memory, replay_size)
 ...         states = []
 ...         td_targets = []
 ...         for state, action, next_state, reward, 
                                     is_done in replay_data:
 ...             states.append(state)
 ...             q_values = self.predict(state).tolist()
 ...             if is_done:
 ...                 q_values[action] = reward
 ...             else:
 ...                 q_values_next = self.predict(next_state)
 ...                 q_values[action] = reward + gamma * 
                         torch.max(q_values_next).item()
 ...             td_targets.append(q_values)
 ...
 ...         self.update(states, td_targets)
```

其余的`DQN`类保持不变。

3.  我们将重用我们在*开发深度 Q 网络*配方中开发的`gen_epsilon_greedy_policy`函数，在此不再赘述。
4.  然后，我们指定神经网络的形状，包括输入、输出和隐藏层的大小，将 0.001 设置为学习率，并相应地创建 DQN:

```
 >>> n_state = env.observation_space.shape[0]
 >>> n_action = env.action_space.n
 >>> n_hidden = 50
 >>> lr = 0.001
 >>> dqn = DQN(n_state, n_action, n_hidden, lr)
```

5.  接下来，我们定义保存体验的缓冲区:

```
>>> memory = deque(maxlen=10000)
```

只要队列中有超过`10000`个样本，新样本将被添加到队列中，旧样本将被移除。

6.  现在，我们定义执行经验重放的深度 Q 学习函数:

```
>>> def q_learning(env, estimator, n_episode, replay_size, 
             gamma=1.0, epsilon=0.1, epsilon_decay=.99):
 ...     """
 ...     Deep Q-Learning using DQN, with experience replay
 ...     @param env: Gym environment
 ...     @param estimator: Estimator object
 ...     @param replay_size: the number of samples we use to 
                 update the model each time
 ...     @param n_episode: number of episodes
 ...     @param gamma: the discount factor
 ...     @param epsilon: parameter for epsilon_greedy
 ...     @param epsilon_decay: epsilon decreasing factor
 ...     """
 ...     for episode in range(n_episode):
 ...         policy = gen_epsilon_greedy_policy( estimator, epsilon, n_action)
 ...         state = env.reset()
 ...         is_done = False
 ...         while not is_done:
 ...             action = policy(state)
 ...             next_state, reward, is_done, _ = env.step(action)
 ...             total_reward_episode[episode] += reward
 ...             modified_reward = next_state[0] + 0.5
 ...             if next_state[0] >= 0.5:
 ...                 modified_reward += 100
 ...             elif next_state[0] >= 0.25:
 ...                 modified_reward += 20
 ...             elif next_state[0] >= 0.1:
 ...                 modified_reward += 10
 ...             elif next_state[0] >= 0:
 ...                 modified_reward += 5
 ...             memory.append((state, action, next_state, 
                               modified_reward, is_done))
 ...             if is_done:
 ...                 break
 ...             estimator.replay(memory, replay_size, gamma)
 ...             state = next_state
 ...         print('Episode: {}, total reward: {}, epsilon: 
             {}'.format(episode, total_reward_episode[episode],
              epsilon))
 ...         epsilon = max(epsilon * epsilon_decay, 0.01)
```

7.  然后，我们对`600`集进行深度 Q 学习和体验回放:

```
>>> n_episode = 600
```

我们将`20`设置为每个步骤的重放样本大小:

```
>>> replay_size = 20
```

我们还记录每集的总奖励:

```
 >>> total_reward_episode = [0] * n_episode
 >>> q_learning(env, dqn, n_episode, replay_size, gamma=.9, epsilon=.3)
```

8.  现在，是时候展示随时间推移的剧集奖励剧情了:

```
 >>> import matplotlib.pyplot as plt
 >>> plt.plot(total_reward_episode)
 >>> plt.title('Episode reward over time')
 >>> plt.xlabel('Episode')
 >>> plt.ylabel('Total reward')
 >>> plt.show()
```

<title>How it works...</title> <link rel="stylesheet" href="css/style.css" type="text/css"> 

# 它是如何工作的...

在*步骤 2* 中，体验回放功能首先随机选择`replay_size`个体验样本。然后，它将每个经验转换成由输入状态和输出目标值组成的训练样本。最后，它使用选定的批次更新神经网络。

在*步骤 6* 中，通过以下任务执行带有经验重放的深度 Q 学习:

*   在每集中，创建一个ε因子衰减到 99%的ε贪婪策略。
*   运行一集:在每一步中，采取一个行动， *a* ，遵循ε-贪婪策略；将这种体验(旧状态、动作、新状态、奖励)储存在记忆中。
*   在每一步中，进行经验重放来训练神经网络，前提是我们有足够的训练样本来随机挑选。
*   运行`n_episode`集并记录每集的总奖励。

执行*步骤 8* 中的代码行将会产生如下图:

![](assets/a5b6cc17-88d7-40e7-9a0d-529a6f7ee760.png)

你可以看到，在过去 200 集的大部分情节中，汽车到达山顶大约需要 120 到 160 步。

在深度 Q 学习中，经验重放意味着我们存储代理每一步的经验，并随机抽取一些过去经验的样本来训练 DQN。在这种情况下，学习分为两个阶段:积累经验，并根据过去的经验更新模型。具体来说，体验(也称为**缓冲**，或**记忆**)包括过去的状态、采取的行动、收到的奖励和下一个状态。经验回放可以通过提供一组低相关性的样本来稳定训练，从而提高学习效率。

<title>Developing double deep Q-Networks</title> <link rel="stylesheet" href="css/style.css" type="text/css"> 

# 发展双深度 Q-网络

在我们迄今为止开发的深度 Q 学习算法中，使用相同的神经网络来计算预测值和目标值。这可能会导致很多偏差，因为目标值不断变化，预测必须跟上它。在这个食谱中，我们将使用两个神经网络而不是一个来开发一个新的算法。

在 **double DQNs** 中，我们使用单独的网络来估计目标，而不是预测网络。独立网络具有与预测网络相同的结构。并且它的权重对于每一集 *T* 都是固定的( *T* 是我们可以调优的超参数)，这意味着它们只在每一集 *T* 之后更新。简单地通过复制预测网络的权重来完成更新。这样，目标函数在一段时间内是固定的，这导致了更稳定的训练过程。

在数学上，双 dqn 被训练以最小化以下误差项:

![](assets/083d4882-dad9-48b9-bb1c-7389e117c3eb.png)

这里， *s'* 是采取行动后的结果状态， *a* ，处于状态*s*； *r* 是关联奖励；α是学习率；γ是贴现因子。另外， [![](assets/86da76aa-fe81-4d29-868c-a2dac845bea2.png)] 是目标网络的函数，Q 是预测网络的函数。

现在让我们使用双 dqn 来解决山地汽车问题。

<title>How to do it...</title> <link rel="stylesheet" href="css/style.css" type="text/css"> 

# 怎么做...

我们使用如下双 dqn 开发深度 Q 学习:

1.  导入必要的模块并创建山地汽车环境:

```
 >>> import gym
 >>> import torch
 >>> from collections import deque
 >>> import random
 >>> import copy
 >>> from torch.autograd import Variable >>> env = gym.envs.make("MountainCar-v0")
```

2.  为了在体验重放阶段合并目标网络，我们首先在`DQN`类的 `__init__`方法中初始化它:

```
>>> class DQN():
 ...     def __init__(self, n_state, n_action, 
                     n_hidden=50, lr=0.05):
 ...         self.criterion = torch.nn.MSELoss()
 ...         self.model = torch.nn.Sequential(
 ...                         torch.nn.Linear(n_state, n_hidden),
 ...                         torch.nn.ReLU(),
 ...                         torch.nn.Linear(n_hidden, n_action)
 ...                 )
 ...         self.optimizer = torch.optim.Adam( self.model.parameters(), lr)
 ...         self.model_target = copy.deepcopy(self.model)
```

目标网络具有与预测网络相同的结构。

3.  因此，我们添加了使用目标网络的值的计算:

```
>>>     def target_predict(self, s):
 ...         """
 ...         Compute the Q values of the state for all actions 
             using the target network
 ...         @param s: input state
 ...         @return: targeted Q values of the state for all actions
 ...         """
 ...         with torch.no_grad():
 ...             return self.model_target(torch.Tensor(s))
```

4.  我们还添加了同步目标网络权重的方法:

```
>>>     def copy_target(self):
 ...         self.model_target.load_state_dict(self.model.state_dict())
```

5.  在经验重放中，我们使用目标网络而不是预测网络来计算目标值:

```
>>>     def replay(self, memory, replay_size, gamma):
 ...         """
 ...         Experience replay with target network
 ...         @param memory: a list of experience
 ...         @param replay_size: the number of samples 
             we use to update the model each time
 ...         @param gamma: the discount factor
 ...         """
 ...         if len(memory) >= replay_size:
 ...             replay_data = random.sample(memory, replay_size)
 ...             states = []
 ...             td_targets = []
 ...             for state, action, next_state, reward, is_done 
                                                 in replay_data:
 ...                 states.append(state)
 ...                 q_values = self.predict(state).tolist()
 ...                 if is_done:
 ...                     q_values[action] = reward
 ...                 else:
 ...                     q_values_next = self.target_predict( next_state).detach()
 ...                     q_values[action] = reward + gamma * 
                             torch.max(q_values_next).item()
 ...
 ...                 td_targets.append(q_values)
 ...
 ...             self.update(states, td_targets)
```

其余的`DQN`类保持不变。

6.  我们将重用在*开发深度 Q 网络*中开发的`gen_epsilon_greedy_policy`函数，这里不再重复。
7.  然后，我们指定神经网络的形状，包括输入、输出和隐藏层的大小，将`0.01`设置为学习速率，并相应地创建 DQN:

```
 >>> n_state = env.observation_space.shape[0]
 >>> n_action = env.action_space.n
 >>> n_hidden = 50
 >>> lr = 0.01
 >>> dqn = DQN(n_state, n_action, n_hidden, lr)
```

8.  接下来，我们定义保存体验的缓冲区:

```
>>> memory = deque(maxlen=10000)
```

只要队列中有超过`10000`个样本，新样本将被添加到队列中，旧样本将被移除。

9.  现在，我们将使用双 DQN 开发深度 Q 学习:

```
>>> def q_learning(env, estimator, n_episode, replay_size, 
         target_update=10, gamma=1.0, epsilon=0.1,
         epsilon_decay=.99):
 ...     """
 ...     Deep Q-Learning using double DQN, with experience replay
 ...     @param env: Gym environment
 ...     @param estimator: DQN object
 ...     @param replay_size: number of samples we use 
             to update the model each time
 ...     @param target_update: number of episodes before 
             updating the target network
 ...     @param n_episode: number of episodes
 ...     @param gamma: the discount factor
 ...     @param epsilon: parameter for epsilon_greedy
 ...     @param epsilon_decay: epsilon decreasing factor
 ...     """
 ...     for episode in range(n_episode):
 ...         if episode % target_update == 0:
 ...             estimator.copy_target()
 ...         policy = gen_epsilon_greedy_policy( estimator, epsilon, n_action)
 ...         state = env.reset()
 ...         is_done = False
 ...         while not is_done:
 ...             action = policy(state)
 ...             next_state, reward, is_done, _ = env.step(action)
 ...             total_reward_episode[episode] += reward
 ...             modified_reward = next_state[0] + 0.5
 ...             if next_state[0] >= 0.5:
 ...                 modified_reward += 100
 ...             elif next_state[0] >= 0.25:
 ...                 modified_reward += 20
 ...             elif next_state[0] >= 0.1:
 ...                 modified_reward += 10
 ...             elif next_state[0] >= 0:
 ...                 modified_reward += 5
 ...             memory.append((state, action, next_state, 
                             modified_reward, is_done))
 ...             if is_done:
 ...                 break
 ...             estimator.replay(memory, replay_size, gamma)
 ...             state = next_state
 ...         print('Episode: {}, total reward: {}, epsilon: {}'.format(episode, total_reward_episode[episode], epsilon))
 ...         epsilon = max(epsilon * epsilon_decay, 0.01)
```

10.  对于`1000`集，我们使用双 DQN 执行深度 Q 学习:

```
>>> n_episode = 1000
```

我们将`20`设置为每个步骤的重放样本大小:

```
>>> replay_size = 20
```

我们每 10 集更新一次目标网络:

```
>>> target_update = 10
```

我们还记录每集的总奖励:

```
 >>> total_reward_episode = [0] * n_episode
 >>> q_learning(env, dqn, n_episode, replay_size, target_update, gamma=.9, epsilon=1)
 Episode: 0, total reward: -200.0, epsilon: 1
 Episode: 1, total reward: -200.0, epsilon: 0.99
 Episode: 2, total reward: -200.0, epsilon: 0.9801
 ……
 ……
 Episode: 991, total reward: -151.0, epsilon: 0.01
 Episode: 992, total reward: -200.0, epsilon: 0.01
 Episode: 993, total reward: -158.0, epsilon: 0.01
 Episode: 994, total reward: -160.0, epsilon: 0.01
 Episode: 995, total reward: -200.0, epsilon: 0.01
 Episode: 996, total reward: -200.0, epsilon: 0.01
 Episode: 997, total reward: -200.0, epsilon: 0.01
 Episode: 998, total reward: -151.0, epsilon: 0.01
 Episode: 999, total reward: -200.0, epsilon: 0.01 
```

11.  然后，我们显示一段时间内剧集奖励的曲线图:

```
 >>> import matplotlib.pyplot as plt
 >>> plt.plot(total_reward_episode)
 >>> plt.title('Episode reward over time')
 >>> plt.xlabel('Episode')
 >>> plt.ylabel('Total reward')
 >>> plt.show()
```

<title>How it works...</title> <link rel="stylesheet" href="css/style.css" type="text/css"> 

# 它是如何工作的...

在*步骤 5* 中，体验回放功能首先随机选择`replay_size` 个体验样本。然后，它将每个经验转换成由输入状态和输出目标值组成的训练样本。最后，它使用选定的批次更新预测网络。

*第 9 步*是双 DQN 中最重要的一步:它使用不同的网络来计算目标值，然后该网络会定期更新。其余的功能类似于深度 Q 学习，带有经验回放。

*步骤 11* 中的可视化功能将产生以下图形:

![](assets/ec4c795b-ebcd-4389-a3c5-8c0431c6f9bb.png)

你可以看到，在大多数剧集中，在第一个 **400** 集之后，汽车在大约 **80** 到 **160** 的台阶到达山顶。

在使用双 DQN 的深度 Q 学习中，我们创建了两个独立的网络，分别用于预测和目标计算。第一个用于预测和检索 *Q* 值，而第二个用于提供稳定的目标 *Q* 值。并且，过一段时间(假设每 10 集，或 1500 个训练步骤)，我们将预测网络与目标网络同步。在这种双重网络设置中，目标值是暂时固定的，而不是不断修改，因此预测网络有更多稳定的目标来学习。我们得到的结果表明双 dqn 优于单 dqn。

<title>Tuning double DQN hyperparameters for CartPole</title> <link rel="stylesheet" href="css/style.css" type="text/css"> 

# 为磁极调谐双 DQN 超参数

在这个配方中，让我们使用双 dqn 来解决 CartPole 环境。我们将演示如何微调双 DQN 中的超参数，以实现最佳性能。

为了微调超参数，我们可以应用**网格搜索**技术来探索一组不同的值组合，并选择一个实现最佳平均性能的组合。我们可以从粗略的值范围开始，然后逐渐缩小范围。不要忘记为以下所有项目安装随机数发生器，以确保再现性:

*   健身房环境随机数生成器
*   ε贪婪随机数发生器
*   PyTorch 中神经网络的初始权值

<title>How to do it...</title> <link rel="stylesheet" href="css/style.css" type="text/css"> 

# 怎么做...

我们使用双 dqn 求解 CartPole 环境，如下所示:

1.  导入必要的模块并创建一个 CartPole 环境:

```
 >>> import gym
 >>> import torch
 >>> from collections import deque
 >>> import random
 >>> import copy
 >>> from torch.autograd import Variable >>> env = gym.envs.make("CartPole-v0")
```

2.  我们将重用在最后开发的`DQN`类，*开发双深度 Q 网络*配方。
3.  我们将重用在*开发深度 Q 网络*中开发的`gen_epsilon_greedy_policy`函数，这里不再重复。
4.  现在，我们将使用双 DQN 开发深度 Q 学习:

```
>>> def q_learning(env, estimator, n_episode, replay_size, 
                 target_update=10, gamma=1.0, epsilon=0.1,
                 epsilon_decay=.99):
 ...     """
 ...     Deep Q-Learning using double DQN, with experience replay
 ...     @param env: Gym environment
 ...     @param estimator: DQN object
 ...     @param replay_size: number of samples we use to 
                 update the model each time
 ...     @param target_update: number of episodes before 
                 updating the target network
 ...     @param n_episode: number of episodes
 ...     @param gamma: the discount factor
 ...     @param epsilon: parameter for epsilon_greedy
 ...     @param epsilon_decay: epsilon decreasing factor
 ...     """
 ...     for episode in range(n_episode):
 ...         if episode % target_update == 0:
 ...             estimator.copy_target()
 ...         policy = gen_epsilon_greedy_policy( estimator, epsilon, n_action)
 ...         state = env.reset()
 ...         is_done = False
 ...         while not is_done:
 ...             action = policy(state)
 ...             next_state, reward, is_done, _ = env.step(action)
 ...             total_reward_episode[episode] += reward
 ...             memory.append((state, action, 
                         next_state, reward, is_done))
 ...             if is_done:
 ...                 break
 ...             estimator.replay(memory, replay_size, gamma)
 ...             state = next_state
 ...         epsilon = max(epsilon * epsilon_decay, 0.01)
```

5.  然后，我们指定神经网络的形状，包括输入、输出、隐藏层的大小和集数，以及用于评估性能的集数:

```
 >>> n_state = env.observation_space.shape[0]
 >>> n_action = env.action_space.n
 >>> n_episode = 600
 >>> last_episode = 200
```

6.  然后，我们为以下超参数定义几个值，以便在网格搜索中探索:

```
 >>> n_hidden_options = [30, 40]
 >>> lr_options = [0.001, 0.003]
 >>> replay_size_options = [20, 25]
 >>> target_update_options = [30, 35]
```

7.  最后，我们执行网格搜索，在每次迭代中，我们根据超参数集创建 DQN，并允许它学习 600 集。然后，我们通过平均最后 200 集的总奖励来评估其性能:

```
>>> for n_hidden in n_hidden_options:
 ...     for lr in lr_options:
 ...         for replay_size in replay_size_options:
 ...             for target_update in target_update_options:
 ...                 env.seed(1)
 ...                 random.seed(1)
 ...                 torch.manual_seed(1)
 ...                 dqn = DQN(n_state, n_action, n_hidden, lr)
 ...                 memory = deque(maxlen=10000)
 ...                 total_reward_episode = [0] * n_episode
 ...                 q_learning(env, dqn, n_episode, replay_size, 
                         target_update, gamma=.9, epsilon=1)
 ...                 print(n_hidden, lr, replay_size, target_update, 
             sum(total_reward_episode[-last_episode:])/last_episode)
```

<title>How it works...</title> <link rel="stylesheet" href="css/style.css" type="text/css"> 

# 它是如何工作的...

执行完*步骤 7* 后，我们得到以下网格搜索结果:

```
30 0.001 20 30 143.15
 30 0.001 20 35 156.165
 30 0.001 25 30 180.575
 30 0.001 25 35 192.765
 30 0.003 20 30 187.435
 30 0.003 20 35 122.42
 30 0.003 25 30 169.32
 30 0.003 25 35 172.65
 40 0.001 20 30 136.64
 40 0.001 20 35 160.08
 40 0.001 25 30 141.955
 40 0.001 25 35 122.915
 40 0.003 20 30 143.855
 40 0.003 20 35 178.52
 40 0.003 25 30 125.52
 40 0.003 25 35 178.85
```

我们可以看到，最佳平均报酬`192.77`，是通过`n_hidden=30`、`lr=0.001`、`replay_size=25`、`target_update=35`的组合实现的。

请随意进一步微调超参数，以获得更好的 DQN 模型。

在这个食谱中，我们用双 dqn 解决了 CartPole 问题。我们使用网格搜索来微调超参数的值。在我们的例子中，我们优化了隐藏层的大小、学习速率、重放批量大小和目标网络更新频率。我们还可以探索其他超参数，如发作次数、初始ε和ε衰减值。对于每个实验，我们保持随机种子固定，以便健身房环境的随机性、ε-贪婪行为以及神经网络的权重初始化保持不变。这是为了确保性能的再现性和可比性。每个 DQN 模型的性能是由最后几集的平均总报酬来衡量的。

<title>Developing Dueling deep Q-Networks</title> <link rel="stylesheet" href="css/style.css" type="text/css"> 

# 发展决斗深度 Q-网络

在这个配方中，我们将开发另一种高级类型的 DQNs，**决斗 DQNs** ( **DDQNs** )。具体来说，我们将看到如何在 DDQNs 中将 Q 值的计算分成两部分。

在 DDQNs 中，Q 值通过以下两个函数计算:

![](assets/2c71596f-ff5e-49b2-9522-e0181e979d78.png)

这里， *V(s)* 是状态值函数，计算处于状态 *s* 时的值； *A(s，a)* 是状态相关的动作优势函数，估计采取一个动作 *a* 比在一个状态 *s* 采取其他动作好多少。通过解耦`value`和`advantage`函数，我们能够适应这样一个事实，即在学习过程中，我们的代理不一定同时关注价值和优势。换句话说，使用 DDQNs 的代理可以根据自己的喜好有效地优化一个或两个函数。

<title>How to do it...</title> <link rel="stylesheet" href="css/style.css" type="text/css"> 

# 怎么做...

我们使用 DDQNs 解决山地汽车问题，如下所示:

1.  导入必要的模块并创建山地汽车环境:

```
 >>> import gym
 >>> import torch
 >>> from collections import deque
 >>> import random
 >>> from torch.autograd import Variable
 >>> import torch.nn as nn >>> env = gym.envs.make("MountainCar-v0")
```

2.  接下来，我们将 DDQN 模型定义如下:

```
>>> class DuelingModel(nn.Module):
 ...     def __init__(self, n_input, n_output, n_hidden):
 ...         super(DuelingModel, self).__init__()
 ...         self.adv1 = nn.Linear(n_input, n_hidden)
 ...         self.adv2 = nn.Linear(n_hidden, n_output)
 ...         self.val1 = nn.Linear(n_input, n_hidden)
 ...         self.val2 = nn.Linear(n_hidden, 1)
 ...
 ...     def forward(self, x):
 ...         adv = nn.functional.relu(self.adv1(x))
 ...         adv = self.adv2(adv)
 ...         val = nn.functional.relu(self.val1(x))
 ...         val = self.val2(val)
 ...         return val + adv - adv.mean()
```

3.  因此，我们在`DQN`类中使用 DDQN 模型:

```
>>> class DQN():
 ...     def __init__(self, n_state, n_action, n_hidden=50, lr=0.05):
 ...         self.criterion = torch.nn.MSELoss()
 ...         self.model = DuelingModel(n_state, n_action, n_hidden)
 ...         self.optimizer = torch.optim.Adam(self.model.parameters(), lr)
```

其余的`DQN`类保持不变。

4.  我们将重用在*开发深度 Q 网络*中开发的`gen_epsilon_greedy_policy`函数，这里不再重复。
5.  我们将重复使用我们在“用经验重放的*改进 DQNs”配方中开发的`q_learning`函数，在此不再赘述。*
6.  然后，我们指定神经网络的形状，包括输入、输出和隐藏层的大小，将`0.001`设置为学习速率，并相应地创建 DQN:

```
 >>> n_state = env.observation_space.shape[0]
 >>> n_action = env.action_space.n
 >>> n_hidden = 50
 >>> lr = 0.001
 >>> dqn = DQN(n_state, n_action, n_hidden, lr)
```

7.  接下来，我们定义保存体验的缓冲区:

```
>>> memory = deque(maxlen=10000)
```

只要队列中有超过`10000`个样本，新样本将被添加到队列中，旧样本将被移除。

8.  然后，我们使用 DDQN 对`600`集执行深度 Q 学习:

```
>>> n_episode = 600
```

我们将`20`设置为每个步骤的重放样本大小:

```
>>> replay_size = 20
```

我们还记录每集的总奖励:

```
 >>> total_reward_episode = [0] * n_episode
 >>> q_learning(env, dqn, n_episode, replay_size, gamma=.9,
 epsilon=.3)
```

9.  现在，我们可以显示剧集奖励随时间变化的情况:

```
 >>> import matplotlib.pyplot as plt
 >>> plt.plot(total_reward_episode)
 >>> plt.title('Episode reward over time')
 >>> plt.xlabel('Episode')
 >>> plt.ylabel('Total reward')
 >>> plt.show()
```

<title>How it works...</title> <link rel="stylesheet" href="css/style.css" type="text/css"> 

# 它是如何工作的...

*第二步*是决斗 DQN 的核心部分。它由两部分组成，动作**优势** ( **优势**)，状态值**(**val**)。同样，我们使用一个隐藏层作为例子。**

执行*步骤 9* 将会产生以下图形:

![](assets/1b252d17-a5a3-4320-b706-357a6844d87a.png)

在 DDQNs 中，预测的 Q 值是两个元素的结果:状态值和动作优势。第一个估计在某个状态有多好。第二个表明采取一个特定的行动比其他选择好多少。这两个元素分别计算，并合并到 DQN 的最后一层。您会记得，传统的 dqn 只在一个状态下更新给定动作的 Q 值。DDQNs 更新所有动作(不仅仅是给定的动作)可以利用的状态值，以及动作的优势。因此，DDQNs 被认为更加健壮。

<title>Applying Deep Q-Networks to Atari games</title> <link rel="stylesheet" href="css/style.css" type="text/css"> 

# 将深度 Q-网络应用于 Atari 游戏

到目前为止，我们所处理的问题相当简单，应用 dqn 有时有些过头了。在这个和下一个配方中，我们将使用 dqn 来解决 Atari 游戏，这是一个复杂得多的问题。

在这个食谱中，我们将以乒乓球([https://gym.openai.com/envs/Pong-v0/](https://gym.openai.com/envs/Pong-v0/))为例。它模拟了 Atari 2600 游戏 Pong，其中代理与另一个玩家打乒乓球。这种环境下的观察是屏幕的 RGB 图像(参考下面的截图):

![](assets/8a405812-c25f-4905-8952-45313c8aeccf.png)

这是一个形状(210，160，3)的矩阵，这意味着图像的大小为 *210 * 160* ，并且在三个 RGB 通道中。

代理人(在右手边)在游戏中上下移动来击球。如果它错过了，另一个玩家(在左手边)将得到 1 分；同样，如果对方球员错过了，代理人将得到 1 分。谁先得 21 分，谁就是这场比赛的赢家。代理可以在 Pong 环境中采取以下 6 种可能的操作:

*   **0: NOOP** :代理保持静止
*   **1:开火**:没有意义的动作
*   **2:右**:代理上移
*   **3:左**:代理向下移动
*   **4:右击**:同 2
*   **5:左击**:同 5

每个动作在 *k* 帧的持续时间内重复执行( *k* 可以是 2、3、4 或 16，取决于 Pong 环境的具体变化)。奖励是以下任何一种:

*   *-1* :代理人漏球。
*   *1* :对手漏球。
*   *0* :否则。

Pong 中的观察空间 *210 * 160 * 3* 比我们习惯处理的要大很多。因此，在使用 DQNs 解决它之前，我们将缩小图像到 *84 * 84* 并将其转换为灰度。

<title>How to do it...</title> <link rel="stylesheet" href="css/style.css" type="text/css"> 

# 怎么做...

我们将从探索 Pong 环境开始，如下所示:

1.  导入必要的模块并创建一个 Pong 环境:

```
 >>> import gym
 >>> import torch
 >>> import random >>> env = gym.envs.make("PongDeterministic-v4")
```

在这种 Pong 环境变体中，动作是确定性的，并且在 16 帧的持续时间内重复执行。

2.  看一看观察空间和行动空间:

```
 >>> state_shape = env.observation_space.shape
 >>> n_action = env.action_space.n
 >>> print(state_shape)
 (210, 160, 3)
 >>> print(n_action)
 6
 >>> print(env.unwrapped.get_action_meanings())
 ['NOOP', 'FIRE', 'RIGHT', 'LEFT', 'RIGHTFIRE', 'LEFTFIRE']
```

3.  指定三个动作:

```
 >>> ACTIONS = [0, 2, 3]
 >>> n_action = 3
```

这些动作不是移动，是向上移动，是向下移动。

4.  让我们采取随机行动，并呈现屏幕:

```
 >>> env.reset()
 >>> is_done = False
 >>> while not is_done:
 ...     action = ACTIONS[random.randint(0, n_action - 1)]
 ...     obs, reward, is_done, _ = env.step(action)
 ...     print(reward, is_done)
 ...     env.render()
 0.0 False
 0.0 False
 0.0 False
 ……
 ……
 0.0 False
 0.0 False
 0.0 False
 -1.0 True
```

你会在屏幕上看到两个球员在打乒乓球，尽管代理人输了。

5.  现在，我们开发一个屏幕处理功能来缩小图像并将其转换为灰度:

```
 >>> import torchvision.transforms as T
 >>> from PIL import Image
 >>> image_size = 84
 >>> transform = T.Compose([T.ToPILImage(),
 ...                        T.Grayscale(num_output_channels=1),
 ...                        T.Resize((image_size, image_size),
                                 interpolation=Image.CUBIC),
 ...                        T.ToTensor(),
 ...                        ])
```

现在，我们只需定义一个 resizer，将图像缩小到 *84 * 84* :

```
 >>> def get_state(obs):
 ...     state = obs.transpose((2, 0, 1))
 ...     state = torch.from_numpy(state)
 ...     state = transform(state)
 ...     return state
```

此函数将调整大小后的图像重新调整到大小(1，84，84):

```
 >>> state = get_state(obs)
 >>> print(state.shape)
 torch.Size([1, 84, 84])
```

现在，我们可以开始使用双 dqn 求解环境，如下所示:

1.  此时，我们将使用具有两个隐藏层的更大的神经网络，因为输入大小约为 21，000:

```
 >>> from collections import deque 
 >>> import copy
 >>> from torch.autograd import Variable
 >>> class DQN():
 ...     def __init__(self, n_state, n_action, n_hidden, lr=0.05):
 ...         self.criterion = torch.nn.MSELoss()
 ...         self.model = torch.nn.Sequential(
 ...                  torch.nn.Linear(n_state, n_hidden[0]),
 ...                  torch.nn.ReLU(),
 ...                  torch.nn.Linear(n_hidden[0], n_hidden[1]),
 ...                  torch.nn.ReLU(),
 ...                  torch.nn.Linear(n_hidden[1], n_action)
 ...                  )
 ...         self.model_target = copy.deepcopy(self.model)
 ...         self.optimizer = torch.optim.Adam( self.model.parameters(), lr)
```

2.  其余的`DQN`类与*中的相同，开发双重深度 Q-网络*配方，对`replay`方法有一个小的改变:

```
>>> def replay(self, memory, replay_size, gamma):
 ...     """
 ...     Experience replay with target network
 ...     @param memory: a list of experience
 ...     @param replay_size: the number of samples we use 
                         to update the model each time
 ...     @param gamma: the discount factor
 ...     """
 ...     if len(memory) >= replay_size:
 ...         replay_data = random.sample(memory, replay_size)
 ...         states = []
 ...         td_targets = []
 ...         for state, action, next_state, reward, 
                                 is_done in replay_data:
 ...             states.append(state.tolist())
 ...             q_values = self.predict(state).tolist()
 ...             if is_done:
 ...                 q_values[action] = reward
 ...             else:
 ...                 q_values_next = self.target_predict( next_state).detach()
 ...                 q_values[action] = reward + gamma * 
                         torch.max(q_values_next).item()
 ...             td_targets.append(q_values)
 ...         self.update(states, td_targets)
```

3.  我们将重用在*开发深度 Q 网络*中开发的`gen_epsilon_greedy_policy`函数，这里不再重复。
4.  现在，我们将使用双 DQN 开发深度 Q 学习:

```
>>> def q_learning(env, estimator, n_episode, replay_size, 
             target_update=10, gamma=1.0, epsilon=0.1,
             epsilon_decay=.99):
 ...     """
 ...     Deep Q-Learning using double DQN, with experience replay
 ...     @param env: Gym environment
 ...     @param estimator: DQN object
 ...     @param replay_size: number of samples we use to 
                             update the model each time
 ...     @param target_update: number of episodes before 
                             updating the target network
 ...     @param n_episode: number of episodes
 ...     @param gamma: the discount factor
 ...     @param epsilon: parameter for epsilon_greedy
 ...     @param epsilon_decay: epsilon decreasing factor
 ...     """
 ...     for episode in range(n_episode):
 ...         if episode % target_update == 0:
 ...             estimator.copy_target()
 ...         policy = gen_epsilon_greedy_policy( estimator, epsilon, n_action)
 ...         obs = env.reset()
 ...         state = get_state(obs).view(image_size * image_size)[0]
 ...         is_done = False
 ...         while not is_done:
 ...             action = policy(state)
 ...             next_obs, reward, is_done, _ = 
                                 env.step(ACTIONS[action])
 ...             total_reward_episode[episode] += reward
 ...             next_state = get_state(obs).view( image_size * image_size)
 ...             memory.append((state, action, next_state, 
                                 reward, is_done))
 ...             if is_done:
 ...                 break
 ...             estimator.replay(memory, replay_size, gamma)
 ...             state = next_state
 ...         print('Episode: {}, total reward: {}, epsilon: 
             {}'.format(episode, total_reward_episode[episode],
             epsilon))
 ...         epsilon = max(epsilon * epsilon_decay, 0.01)
```

给定一个大小为*【210，160，3】*的观察值，这将它转换为一个更小的灰度矩阵*【84，84】*，并将其展平，以便我们可以将其输入到我们的网络中。

5.  现在，我们指定神经网络的形状，包括输入和隐藏层的大小:

```
 >>> n_state = image_size * image_size
 >>> n_hidden = [200, 50]
```

其余的超参数如下:

```
 >>> n_episode = 1000
 >>> lr = 0.003
 >>> replay_size = 32
 >>> target_update = 10
```

现在，我们相应地创建一个 DQN:

```
>>> dqn = DQN(n_state, n_action, n_hidden, lr)
```

6.  接下来，我们定义保存体验的缓冲区:

```
>>> memory = deque(maxlen=10000)
```

7.  最后，我们执行深度 Q 学习，并跟踪每集的总回报:

```
 >>> total_reward_episode = [0] * n_episode
 >>> q_learning(env, dqn, n_episode, replay_size, target_update, gamma=.9, epsilon=1)
```

<title>How it works...</title> <link rel="stylesheet" href="css/style.css" type="text/css"> 

# 它是如何工作的...

Pong 中的观察比我们在这一章中已经讨论过的环境要复杂得多。是 *210 * 160* 屏幕尺寸的三通道图像。所以，我们首先将其转换为灰度图像，缩小到 *84 * 84* ，然后将其展平，以便可以馈入完全连接的神经网络。由于我们有大约 6000 维的输入，我们使用两个隐藏层来适应复杂性。

<title>Using convolutional neural networks for Atari games</title> <link rel="stylesheet" href="css/style.css" type="text/css"> 

# 在雅达利游戏中使用卷积神经网络

在前面的配方中，我们将 Pong 环境中的每个观察到的图像视为一个灰度数组，并将其馈送到一个完全连接的神经网络。拼合图像实际上可能会导致信息丢失。为什么我们不用图像作为输入呢？在这个食谱中，我们将把**卷积神经网络**(**CNN**)并入 DQN 模型。

CNN 是处理图像输入的最佳神经网络架构之一。在 CNN 中，卷积层能够有效地从图像中提取特征，这些特征将被传递到下游的完全连接的层。这里描述了具有两个卷积层的 CNN 的例子:

![](assets/67e6e37a-cbce-468f-bbd7-d89a59f2355e.png)

你可以想象，如果我们简单地把一个图像展平成一个向量，我们会丢失一些球在哪里，两个球员在哪里的信息。这些信息对模型学习非常重要。对于 CNN 中的卷积运算，这种信息由从多个滤波器生成的一组特征图来表示。

同样，我们将图像从 *210 * 160* 缩小到 *84 * 84* ，但保留了三个 RGB 通道，这次没有将它们展平为一个数组。

<title>How to do it...</title> <link rel="stylesheet" href="css/style.css" type="text/css"> 

# 怎么做...

让我们使用基于 CNN 的 DQN 来解决 Pong 环境，如下所示:

1.  导入必要的模块并创建一个 Pong 环境:

```
 >>> import gym
 >>> import torch
 >>> import random >>> from collections import deque
 >>> import copy
 >>> from torch.autograd import Variable
 >>> import torch.nn as nn
 >>> import torch.nn.functional as F
 >>> env = gym.envs.make("PongDeterministic-v4")
```

2.  然后，我们指定三个动作:

```
 >>> ACTIONS = [0, 2, 3]
 >>> n_action = 3
```

这些动作不是移动，是向上移动，是向下移动。

3.  现在，我们开发一个图像处理函数来缩小图像:

```
 >>> import torchvision.transforms as T
 >>> from PIL import Image
 >>> image_size = 84
 >>> transform = T.Compose([T.ToPILImage(),
 ...                        T.Resize((image_size, image_size), 
                              interpolation=Image.CUBIC),
 ...                        T.ToTensor()])
```

我们现在定义一个 resizer，将图像缩小到 *84 * 84* ，然后我们将图像整形为( *3，84，84* ):

```
>>> def get_state(obs):
 ...     state = obs.transpose((2, 0, 1))
 ...     state = torch.from_numpy(state)
 ...     state = transform(state).unsqueeze(0)
 ...     return state
```

4.  现在，我们开始通过开发 CNN 模型来解决 Pong 环境:

```
>>> class CNNModel(nn.Module):
 ...     def __init__(self, n_channel, n_action):
 ...         super(CNNModel, self).__init__()
 ...         self.conv1 = nn.Conv2d(in_channels=n_channel, 
                     out_channels=32, kernel_size=8, stride=4)
 ...         self.conv2 = nn.Conv2d(32, 64, 4, stride=2)
 ...         self.conv3 = nn.Conv2d(64, 64, 3, stride=1)
 ...         self.fc = torch.nn.Linear(7 * 7 * 64, 512)
 ...         self.out = torch.nn.Linear(512, n_action)
 ...
 ...     def forward(self, x):
 ...         x = F.relu(self.conv1(x))
 ...         x = F.relu(self.conv2(x))
 ...         x = F.relu(self.conv3(x))
 ...         x = x.view(x.size(0), -1)
 ...         x = F.relu(self.fc(x))
 ...         output = self.out(x)
 ...         return output
```

5.  我们现在将使用我们刚刚在`DQN`模型中定义的 CNN 模型:

```
>>> class DQN():
 ...     def __init__(self, n_channel, n_action, lr=0.05):
 ...         self.criterion = torch.nn.MSELoss()
 ...         self.model = CNNModel(n_channel, n_action)
 ...         self.model_target = copy.deepcopy(self.model)
 ...         self.optimizer = torch.optim.Adam( self.model.parameters(), lr)
```

6.  其余的`DQN`类与*中的相同，开发双重深度 Q-网络*配方，对`replay`方法有一个小的改变:

```
>>> def replay(self, memory, replay_size, gamma):
 ...     """
 ...     Experience replay with target network
 ...     @param memory: a list of experience
 ...     @param replay_size: the number of samples we use 
                         to update the model each time
 ...     @param gamma: the discount factor
 ...     """
 ...     if len(memory) >= replay_size:
 ...         replay_data = random.sample(memory, replay_size)
 ...         states = []
 ...         td_targets = []
 ...         for state, action, next_state, reward,     
                                 is_done in replay_data:
 ...             states.append(state.tolist()[0])
 ...             q_values = self.predict(state).tolist()[0]
 ...             if is_done:
 ...                 q_values[action] = reward
 ...             else:
 ...                 q_values_next = self.target_predict( next_state).detach()
 ...                 q_values[action] = reward + gamma *         
                             torch.max(q_values_next).item()
 ...             td_targets.append(q_values)
 ...         self.update(states, td_targets)
```

7.  我们将重用在*开发深度 Q 网络*中开发的`gen_epsilon_greedy_policy`函数，这里不再重复。
8.  现在，我们用双 DQN 开发深度 Q 学习:

```
 >>> def q_learning(env, estimator, n_episode, replay_size, 
             target_update=10, gamma=1.0, epsilon=0.1,   
             epsilon_decay=.99):
 ...     """
 ...     Deep Q-Learning using double DQN, with experience replay
 ...     @param env: Gym environment
 ...     @param estimator: DQN object
 ...     @param replay_size: number of samples we use to 
                             update the model each time
 ...     @param target_update: number of episodes before 
                             updating the target network
 ...     @param n_episode: number of episodes
 ...     @param gamma: the discount factor
 ...     @param epsilon: parameter for epsilon_greedy
 ...     @param epsilon_decay: epsilon decreasing factor
 ...     """
 ...     for episode in range(n_episode):
 ...         if episode % target_update == 0:
 ...             estimator.copy_target()
 ...         policy = gen_epsilon_greedy_policy( estimator, epsilon, n_action)
 ...         obs = env.reset()
 ...         state = get_state(obs)
 ...         is_done = False
 ...         while not is_done:
 ...             action = policy(state)
 ...             next_obs, reward, is_done, _ = 
                             env.step(ACTIONS[action])
 ...             total_reward_episode[episode] += reward
 ...             next_state = get_state(obs)
 ...             memory.append((state, action, next_state, 
                                 reward, is_done))
 ...             if is_done:
 ...                 break
 ...             estimator.replay(memory, replay_size, gamma)
 ...             state = next_state
 ...         print('Episode: {}, total reward: {}, epsilon: {}' .format(episode, total_reward_episode[episode], epsilon))
 ...         epsilon = max(epsilon * epsilon_decay, 0.01)
```

9.  然后，我们将剩余的超参数指定如下:

```
 >>> n_episode = 1000
 >>> lr = 0.00025
 >>> replay_size = 32
 >>> target_update = 10
```

相应地创建 DQN:

```
 >>> dqn = DQN(3, n_action, lr)
```

10.  接下来，我们定义保存体验的缓冲区:

```
>>> memory = deque(maxlen=100000)
```

11.  最后，我们执行深度 Q 学习，并跟踪每集的总回报:

```
 >>> total_reward_episode = [0] * n_episode >>> q_learning(env, dqn, n_episode, replay_size, target_update, gamma=.9, epsilon=1)
```

<title>How it works...</title> <link rel="stylesheet" href="css/style.css" type="text/css"> 

# 它是如何工作的...

*步骤 3* 中的图像预处理功能首先将每个通道的图像缩小到 *84 * 84* ，然后将其尺寸改为 *(3，84，84)* 。这是为了确保具有正确尺寸的图像被馈送到网络。

在*步骤 4* 中，CNN 模型有三个卷积层，每个卷积层后面都有一个 ReLU 激活函数。从最后一个卷积层得到的特征图然后被展平并馈送到具有 512 个节点的全连接隐藏层，随后是输出层。

将 CNN 并入 DQNs 最早是由 DeepMind 提出的，发表在*用深度强化学习玩雅达利**(*【https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf】)中。该模型将图像像素作为输入，并输出估计的未来奖励值。它也适用于其他 Atari 游戏环境，其中观察是游戏屏幕的图像。卷积组件是一组有效的分层特征提取器。他们可以从复杂环境中的原始图像数据中学习特征表示，并将完全连接的层提供给他们，以学习成功的控制策略。

请记住，前面示例中的培训通常需要几天时间，即使是在 GPU 上，在 2.9 GHz 英特尔 i7 四核 CPU 上大约需要 90 个小时。

<title>See also</title> <link rel="stylesheet" href="css/style.css" type="text/css"> 

# 请参见

如果您不熟悉 CNN，请查看以下材料:

*   第四章， *CNN 架构*来自*用 Python 实践深度学习架构* (Packt 出版，作者 Yuxi (Hayden) Liu 和 Saransh Mehta)
*   第一章，*使用卷积神经网络的手写数字识别*、和第二章， *R 深度学习项目*的【智能车辆的交通标志识别】(Packt 出版，Yuxi (Hayden) Liu 和 Pablo Maldonado)