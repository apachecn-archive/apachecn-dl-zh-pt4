<title>Preface</title> <link rel="stylesheet" href="css/style.css" type="text/css"> 

# 前言

对强化学习的兴趣激增是由于这样一个事实，即它通过学习在一个环境中采取最佳行动以最大化累积奖励的概念，从而彻底改变了自动化。

*PyTorch 1.x 强化学习食谱*向您介绍 PyTorch 中重要的强化学习概念和算法实现。这本书的每一章都带领你通过一种不同类型的强化学习方法和它在行业中的应用。借助包含真实世界示例的食谱，您会发现增强您在动态编程、蒙特卡罗方法、时间差异和 Q 学习、多臂 bandit、函数逼近、深度 Q 网络和策略梯度等领域的强化学习技术的知识和熟练程度是非常有趣的，它们并不比您想象的更加晦涩难懂。有趣且易于理解的例子，如雅达利游戏、21 点、Gridworld 环境、互联网广告、山地车和 Flappy Bird，会让你一直保持兴趣，直到达到目标。

到本书结束时，您将已经掌握了流行的强化学习算法的实现，并学习了应用强化学习技术解决其他现实世界问题的最佳实践。

<title>Who this book is for</title> <link rel="stylesheet" href="css/style.css" type="text/css"> 

# 这本书是给谁的

寻找强化学习中不同问题的快速解决方案的机器学习工程师、数据科学家和人工智能研究人员会发现这本书很有用。需要事先接触机器学习概念，而以前的 PyTorch 经验将是一个奖金。

<title>What this book covers</title> <link rel="stylesheet" href="css/style.css" type="text/css"> 

# 这本书涵盖的内容

第 1 章，*强化学习和 PyTorch* 入门，是期待开始阅读本书 PyTorch 强化学习分步指南的读者的起点。我们将设置工作环境和 OpenAI 健身房，并使用 Atari 和 CartPole 操场熟悉强化学习环境。本章还将介绍几种基本强化学习算法的实现，包括随机搜索、爬山和策略梯度。最后，读者还将有机会回顾 PyTorch 的要点，并为即将到来的学习示例和项目做好准备。

第二章、*马尔可夫决策过程和动态规划*从创建马尔可夫链和马尔可夫决策过程开始，这是大多数强化学习算法的核心。然后，它将移动到两种方法来解决马尔可夫决策过程(MDP)，价值迭代和政策迭代。通过实践政策评估，我们将更加熟悉 MDP 和贝尔曼方程。我们还将一步步演示如何解决有趣的抛硬币赌博问题。最后，我们将学习如何执行动态编程来扩大学习范围。

第 3 章，*进行数值估计的蒙特卡罗方法*，重点介绍蒙特卡罗方法。我们将从用蒙特卡洛估算圆周率的值开始。接下来，我们将学习如何使用蒙特卡罗方法来预测状态值和状态动作值。我们将使用蒙特卡洛演示如何训练一个代理人在 21 点中获胜。此外，我们将通过开发各种算法来探索政策上、首次访问蒙特卡罗控制和政策外蒙特卡罗控制。蒙特卡罗控制与ε-贪婪政策和加权重要性抽样也将涵盖。

第四章、*时差与 Q-Learning* 从搭建悬崖行走和风 Gridworld 环境操场开始，将用于时差与 Q-Learning。通过我们一步一步的指导，读者将探索预测的时间差异，并将获得策略外控制的 Q-Learning 和策略内控制的 SARSA 的实践经验。我们还将致力于一个有趣的项目，出租车问题，并演示如何使用 Q-Learning 和 SARSA 算法来解决它。最后，我们将把双 Q 学习算法作为一个附加部分。

第 5 章，*解决多臂土匪问题*，涵盖了多臂土匪算法，这可能是强化学习中最流行的算法之一。这就要从多兵种土匪问题的产生说起。我们将看到如何使用四种策略来解决多臂土匪问题，这四种策略是ε贪婪策略、softmax 探索、置信上限算法和 Thompson 采样算法。我们还将致力于一个十亿美元的问题，在线广告，并演示如何使用多臂土匪算法来解决它。最后，我们将开发一个更复杂的算法，即 contextual bandit 算法，并使用它来优化显示广告。

[第 6 章](6371b431-5738-4267-966d-eb3be840d471.xhtml)、*用函数逼近扩大学习*，重点是函数逼近，将从设置山地汽车环境游乐场开始。通过我们的分步指南，我们将涵盖函数逼近优于表查找的动机，并获得将函数逼近融入 Q-Learning 和 SARSA 等现有算法的经验。我们还将介绍一种高级技术，使用经验回放进行批处理。最后，我们将从整体上讲述如何利用我们在本章中学到的知识解决横竿问题。

第 7 章，*深度 Q 网络在行动*，涵盖深度 Q 学习，或**深度 Q 网络** ( **DQN** )，被认为是最现代的强化学习技术。我们将逐步开发一个 DQN 模型，并理解经验回放和目标网络在实践中使深度 Q 学习工作的重要性。为了帮助读者解决 Atari 游戏，我们将演示如何将卷积神经网络纳入 dqn。我们也将涵盖两个 DQN 变种，双 dqn 和决斗 dqn。我们将以双 dqn 为例，介绍如何微调 Q 学习算法。

第 8 章，*实施策略梯度和策略优化*，重点介绍策略梯度和优化，并从实施增强算法开始。然后，我们将开发带有爬坡基线的增强算法。我们还将实现演员-评论家算法，并应用它来解决悬崖行走问题。为了扩大确定性策略梯度算法的规模，我们应用了 DQN 的技巧，开发了深度确定性策略梯度。作为一点乐趣，我们基于交叉熵方法训练一个代理来玩横竿游戏。最后，我们将讨论如何使用异步行动者-批评家方法和神经网络来扩展政策梯度方法。

第 9 章，*顶点项目——与 DQN 一起玩 Flappy Bird*，带我们看一个顶点项目——使用强化学习玩 Flappy Bird。我们将应用我们在本书中学到的知识来构建一个智能机器人。我们将专注于构建 DQN、微调模型参数和部署模型。让我们看看这只鸟能在空中飞多长时间。

<title>To get the most out of this book</title> <link rel="stylesheet" href="css/style.css" type="text/css"> 

# 从这本书中获得最大收益

寻找强化学习中不同问题的快速解决方案的数据科学家、机器学习工程师和人工智能研究人员会发现这本书很有用。需要事先接触机器学习概念，而以前的 PyTorch 经验不是必需的，但将是一个奖金。

<title>Download the example code files</title> <link rel="stylesheet" href="css/style.css" type="text/css"> 

# 下载示例代码文件

你可以从你在[www.packt.com](http://www.packt.com)的账户下载本书的示例代码文件。如果你在其他地方购买了这本书，你可以访问 www.packtpub.com/support 的[并注册，让文件直接通过电子邮件发送给你。](https://www.packtpub.com/support)

您可以按照以下步骤下载代码文件:

1.  在[www.packt.com](http://www.packt.com)登录或注册。
2.  选择支持选项卡。
3.  点击代码下载。
4.  在搜索框中输入图书名称，然后按照屏幕指示进行操作。

下载文件后，请确保使用最新版本的解压缩或解压文件夹:

*   WinRAR/7-Zip for Windows
*   适用于 Mac 的 Zipeg/iZip/UnRarX
*   用于 Linux 的 7-Zip/PeaZip

该书的代码包也托管在 GitHub 的 https://GitHub . com/packt publishing/py torch-1 . x-Reinforcement-Learning-Cookbook 上。如果代码有更新，它将在现有的 GitHub 库中更新。

我们在也有丰富的书籍和视频目录中的其他代码包。看看他们！

<title>Download the color images</title> <link rel="stylesheet" href="css/style.css" type="text/css"> 

# 下载彩色图像

我们还提供了一个 PDF 文件，其中有本书中使用的截图/图表的彩色图像。可以在这里下载:[https://static . packt-cdn . com/downloads/9781838551964 _ color images . pdf](https://static.packt-cdn.com/downloads/9781838551964_ColorImages.pdf)[。](http://www.packtpub.com/sites/default/files/downloads/Bookname_ColorImages.pdf)

<title>Conventions used</title> <link rel="stylesheet" href="css/style.css" type="text/css"> 

# 使用的惯例

本书通篇使用了许多文本约定。

`CodeInText`:表示文本中的码字、数据库表名、文件夹名、文件名、文件扩展名、路径名、伪 URL、用户输入和 Twitter 句柄。这里有一个例子:“说`empty`，并不意味着所有的元素都有一个`Null`的值。”

代码块设置如下:

```py
>>> def random_policy():
...     action = torch.multinomial(torch.ones(n_action), 1).item()
...     return action
```

任何命令行输入或输出都按如下方式编写:

```py
conda install pytorch torchvision -c pytorch
```

**Bold** :表示一个新术语、一个重要单词或您在屏幕上看到的单词。例如，菜单或对话框中的单词出现在文本中，如下所示。这里有一个例子:“这种方法被称为**随机搜索**，因为权重是在每次试验中随机选取的，希望通过大量试验找到最佳权重。”

警告或重要提示如下所示。

提示和技巧是这样出现的。

<title>Sections</title> <link rel="stylesheet" href="css/style.css" type="text/css"> 

# 部分

在这本书里，你会发现几个经常出现的标题(*做好准备*，*怎么做...*、*工作原理...*，*还有更多...*和*参见*。

要给出如何完成配方的明确说明，请使用以下章节:

<title>Getting ready</title> <link rel="stylesheet" href="css/style.css" type="text/css"> 

# 做好准备

本节将告诉您制作方法的内容，并介绍如何设置制作方法所需的任何软件或任何初步设置。

<title>How to do it...</title> <link rel="stylesheet" href="css/style.css" type="text/css"> 

# 怎么做...

本节包含遵循配方所需的步骤。

<title>How it works...</title> <link rel="stylesheet" href="css/style.css" type="text/css"> 

# 它是如何工作的...

这一部分通常包括对前一部分发生的事情的详细解释。

<title>There's more...</title> <link rel="stylesheet" href="css/style.css" type="text/css"> 

# 还有更多...

这一部分包含了关于配方的附加信息，以使你对配方有更多的了解。

<title>See also</title> <link rel="stylesheet" href="css/style.css" type="text/css"> 

# 请参见

这个部分提供了一些有用的链接，可以链接到食谱的其他有用信息。

<title>Get in touch</title> <link rel="stylesheet" href="css/style.css" type="text/css"> 

# 取得联系

我们随时欢迎读者的反馈。

**总体反馈**:如果您对这本书的任何方面有疑问，请在邮件主题中提及书名，并在`customercare@packtpub.com`发送电子邮件给我们。

**勘误表**:虽然我们已经尽力确保内容的准确性，但错误还是会发生。如果你在这本书里发现了一个错误，请告诉我们，我们将不胜感激。请访问 www.packtpub.com/support/errata，选择您的图书，点击勘误表提交表格链接，并输入详细信息。

**盗版**:如果您在互联网上遇到我们作品的任何形式的非法拷贝，如果您能提供我们的地址或网站名称，我们将不胜感激。请通过`copyright@packt.com`联系我们，并提供材料链接。

**如果你有兴趣成为一名作家**:如果有你擅长的主题，并且你有兴趣写书或投稿，请访问 authors.packtpub.com。

<title>Reviews</title> <link rel="stylesheet" href="css/style.css" type="text/css"> 

# 复习

请留下评论。一旦你阅读并使用了这本书，为什么不在你购买它的网站上留下评论呢？潜在的读者可以看到并使用您不带偏见的意见来做出购买决定，我们 Packt 可以了解您对我们产品的看法，我们的作者可以看到您对他们的书的反馈。谢谢大家！

更多关于 Packt 的信息，请访问[packt.com](http://www.packt.com/)。