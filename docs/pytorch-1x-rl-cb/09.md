<title>Capstone Project – Playing Flappy Bird with DQN</title> <link rel="stylesheet" href="css/style.css" type="text/css"> 

# 顶点项目——和 DQN 一起玩“拍打鸟”

在这最后一章，我们将致力于一个顶点项目——使用强化学习玩 Flappy Bird。我们将应用我们在本书中学到的知识来构建一个智能机器人。我们还将重点构建**深度 Q 网络** ( **DQNs** )，微调模型参数，并部署模型。让我们看看这只鸟能在空中停留多久。

顶点工程将在以下配方中一部分一部分地构建:

*   设置游戏环境
*   建立一个深度 Q 网络来玩 Flappy Bird
*   训练和调整网络
*   部署模型和玩游戏

因此，每个配方中的代码都要建立在前面配方的基础上。

<title>Setting up the game environment</title> <link rel="stylesheet" href="css/style.css" type="text/css"> 

# 设置游戏环境

要用 DQN 玩 Flappy Bird，我们首先需要设置环境。

我们将使用 Pygame 模拟 Flappy Bird 游戏。pygame([https://www.pygame.org](https://www.pygame.org/))包含了一组为创建视频游戏而开发的 Python 模块。还包括游戏中需要的图形和声音库。我们可以如下安装`Pygame`包:

```
pip install pygame
```

Flappy Bird 是由 Dong Nguyen 开发的著名手机游戏。你可以在[https://flappybird.io/](https://flappybird.io/)用键盘自己尝试一下。这个游戏的目的是尽可能长时间地活着。当小鸟碰到地板或管子时，游戏结束。所以，这只鸟需要在适当的时候扇动翅膀来通过随机的管道，以避免掉到地上。可能的动作包括拍打和不拍打。在游戏环境中，每走一步奖励+0.1，以下两个例外:

*   发生冲突时为-1
*   +1 当鸟穿过两根管子之间的缝隙时。最初的 Flappy Bird 游戏是根据通过的间隙数来评分的。

<title>Getting ready</title> <link rel="stylesheet" href="css/style.css" type="text/css"> 

# 做好准备

从[https://github . com/yanpan lau/Keras-FlappyBird/tree/master/assets/sprites](https://github.com/yanpanlau/Keras-FlappyBird/tree/master/assets/sprites)下载我们需要的游戏环境的资产。为了简单起见，我们将只使用`sprites` 文件夹中的图像。具体来说，我们需要以下图像:

*   `background-black.png`:屏幕的背景图像
*   `base.png`:地板的图像
*   `pipe-green.png`:鸟需要远离的管道的图像
*   当小鸟向下拍打翅膀时的图像
*   鸟儿不振翅时的图像
*   鸟儿振翅高飞时的图像

如果你有兴趣，你也可以使用音频文件来增加游戏的趣味性。

<title>How to do it...</title> <link rel="stylesheet" href="css/style.css" type="text/css"> 

# 怎么做...

我们将如下使用`Pygame`开发 Flappy Bird 游戏环境:

1.  我们从开发一个实用函数开始，该函数加载图像并将其转换为正确的格式:

```
>>> from pygame.image import load
 >>> from pygame.surfarray import pixels_alpha
 >>> from pygame.transform import rotate
 >>> def load_images(sprites_path):
 ...     base_image = load(sprites_path + 
                             'base.png').convert_alpha()
 ...     background_image = load(sprites_path + 
                             'background-black.png').convert()
 ...     pipe_images = [rotate(load(sprites_path + 
                        'pipe-green.png').convert_alpha(), 180),
 ...                    load(sprites_path + 
                             'pipe-green.png').convert_alpha()]
 ...     bird_images = [load(sprites_path + 
                           'redbird-upflap.png').convert_alpha(),
 ...                    load(sprites_path + 
                         'redbird-midflap.png').convert_alpha(),
 ...                    load(sprites_path + 
                         'redbird-downflap.png').convert_alpha()]
 ...     bird_hitmask = [pixels_alpha(image).astype(bool) 
                             for image in bird_images]
 ...     pipe_hitmask = [pixels_alpha(image).astype(bool) 
                             for image in pipe_images]
 ...     return base_image, background_image, pipe_images, 
                 bird_images, bird_hitmask, pipe_hitmask
```

2.  导入环境所需的所有软件包:

```
>>> from itertools import cycle
>>> from random import randint
>>> import pygame
```

3.  初始化游戏和时钟，并将屏幕刷新频率设置为每秒 30 帧:

```
>>> pygame.init()
>>> fps_clock = pygame.time.Clock() >>> fps = 30
```

4.  指定屏幕大小并相应地创建屏幕，然后给屏幕添加标题:

```
>>> screen_width = 288
 >>> screen_height = 512
 >>> screen = pygame.display.set_mode((screen_width, screen_height)) >>> pygame.display.set_caption('Flappy Bird')
```

5.  然后，我们用以下函数加载必要的图像(在`sprites`文件夹中):

```
>>> base_image, background_image, pipe_images, bird_images, bird_hitmask, pipe_hitmask = load_images('sprites/')
```

6.  获取游戏变量，包括鸟和管道的大小，并将两个管道之间的垂直间隙设置为 100:

```
>>> bird_width = bird_images[0].get_width()
>>> bird_height = bird_images[0].get_height()
>>> pipe_width = pipe_images[0].get_width()
>>> pipe_height = pipe_images[0].get_height() >>> pipe_gap_size = 100
```

7.  鸟的拍动旋转通过上、中、下、中、上等等:

```
>>> bird_index_gen = cycle([0, 1, 2, 1])
```

这只是为了让游戏看起来更有趣。

8.  定义完所有常量后，我们从游戏环境的`FlappyBird` 类的`__init__method`开始:

```
>>> class FlappyBird(object):
 ...     def __init__(self):
 ...         self.pipe_vel_x = -4
 ...         self.min_velocity_y = -8
 ...         self.max_velocity_y = 10
 ...         self.downward_speed = 1
 ...         self.upward_speed = -9
 ...         self.cur_velocity_y = 0
 ...         self.iter = self.bird_index = self.score = 0
 ...         self.bird_x = int(screen_width / 5)
 ...         self.bird_y = int((screen_height - bird_height) / 2)
 ...         self.base_x = 0
 ...         self.base_y = screen_height * 0.79
 ...         self.base_shift = base_image.get_width() - 
                             background_image.get_width()
 ...         self.pipes = [self.gen_random_pipe(screen_width), 
                         self.gen_random_pipe(screen_width * 1.5)]
 ...         self.is_flapped = False
```

9.  我们继续定义`gen_random_pipe`方法，该方法在给定的水平位置和随机的垂直位置生成一对管道(一上一下):

```
>>>     def gen_random_pipe(self, x):
 ...         gap_y = randint(2, 10) * 10 + int(self.base_y * 0.2)
 ...         return {"x_upper": x,
 ...                 "y_upper": gap_y - pipe_height,
 ...                 "x_lower": x,
 ...                 "y_lower": gap_y + pipe_gap_size}
```

上下管分别处于`gap_y - pipe_height`和`gap_y + pipe_gap_size`的`y`位置。

10.  我们开发的下一个方法是`check_collision,`,如果鸟与底座或管道发生碰撞，它将返回`True`:

```
>>>     def check_collision(self):
 ...         if bird_height + self.bird_y >= self.base_y - 1:
 ...             return True
 ...         bird_rect = pygame.Rect(self.bird_x, self.bird_y, 
                                     bird_width, bird_height)
 ...         for pipe in self.pipes:
 ...             pipe_boxes = [pygame.Rect(pipe["x_upper"], 
                          pipe["y_upper"], pipe_width, pipe_height),
 ...                           pygame.Rect(pipe["x_lower"], 
                          pipe["y_lower"], pipe_width, pipe_height)]
 ...             # Check if the bird's bounding box overlaps to 
                     the bounding box of any pipe
 ...             if bird_rect.collidelist(pipe_boxes) == -1:
 ...                 return False
 ...             for i in range(2):
 ...                 cropped_bbox = bird_rect.clip(pipe_boxes[i])
 ...                 x1 = cropped_bbox.x - bird_rect.x
 ...                 y1 = cropped_bbox.y - bird_rect.y
 ...                 x2 = cropped_bbox.x - pipe_boxes[i].x
 ...                 y2 = cropped_bbox.y - pipe_boxes[i].y
 ...                 for x in range(cropped_bbox.width):
 ...                     for y in range(cropped_bbox.height):
 ...                         if bird_hitmask[self.bird_index][x1+x, 
                                    y1+y] and pipe_hitmask[i][
                                    x2+x, y2+y]:
 ...                             return True
 ...         return False
```

11.  我们需要的最后也是最重要的方法是`next_step`，它执行一个动作，返回游戏的更新图像帧，收到的奖励，以及剧集是否结束:

```
>>>     def next_step(self, action):
 ...         pygame.event.pump()
 ...         reward = 0.1
 ...         if action == 1:
 ...             self.cur_velocity_y = self.upward_speed
 ...             self.is_flapped = True
 ...         # Update score
 ...         bird_center_x = self.bird_x + bird_width / 2
 ...         for pipe in self.pipes:
 ...             pipe_center_x = pipe["x_upper"] + 
                                     pipe_width / 2
 ...             if pipe_center_x < bird_center_x 
                                 < pipe_center_x + 5:
 ...                 self.score += 1
 ...                 reward = 1
 ...                 break
 ...         # Update index and iteration
 ...         if (self.iter + 1) % 3 == 0:
 ...             self.bird_index = next(bird_index_gen)
 ...         self.iter = (self.iter + 1) % fps
 ...         self.base_x = -((-self.base_x + 100) % 
                                 self.base_shift)
 ...         # Update bird's position
 ...         if self.cur_velocity_y < self.max_velocity_y 
                             and not self.is_flapped:
 ...             self.cur_velocity_y += self.downward_speed
 ...         self.is_flapped = False
 ...         self.bird_y += min(self.cur_velocity_y, 
                 self.bird_y - self.cur_velocity_y - bird_height)
 ...         if self.bird_y < 0:
 ...             self.bird_y = 0
 ...         # Update pipe position
 ...         for pipe in self.pipes:
 ...             pipe["x_upper"] += self.pipe_vel_x
 ...             pipe["x_lower"] += self.pipe_vel_x
 ...         # Add new pipe when first pipe is     
                 about to touch left of screen
 ...         if 0 < self.pipes[0]["x_lower"] < 5:
 ...             self.pipes.append(self.gen_random_pipe( screen_width + 10))
 ...         # remove first pipe if its out of the screen
 ...         if self.pipes[0]["x_lower"] < -pipe_width:
 ...             self.pipes.pop(0)
 ...         if self.check_collision():
 ...             is_done = True
 ...             reward = -1
 ...             self.__init__()
 ...         else:
 ...             is_done = False
 ...         # Draw sprites
 ...         screen.blit(background_image, (0, 0))
 ...         screen.blit(base_image, (self.base_x, self.base_y))
 ...         screen.blit(bird_images[self.bird_index], 
                             (self.bird_x, self.bird_y))
 ...         for pipe in self.pipes:
 ...             screen.blit(pipe_images[0], (pipe["x_upper"], pipe["y_upper"]))
 ...             screen.blit(pipe_images[1], 
                       (pipe["x_lower"], pipe["y_lower"]))
 ...         image = pygame.surfarray.array3d( pygame.display.get_surface())
 ...         pygame.display.update()
 ...         fps_clock.tick(fps)
 ...         return image, reward, is_done
```

这就是 Flappy Bird 环境的全部内容。

<title>How it works...</title> <link rel="stylesheet" href="css/style.css" type="text/css"> 

# 它是如何工作的...

在*步骤 8* 中，我们定义了管道的速度(随着时间的推移向左移动 4 个单位)、鸟的最小和最大垂直速度(`-8`和`10`)、其向上和向下的加速度(`-9`和`1`)、其默认垂直速度(`0`)、鸟图像的起始索引(`0`)、初始分数、鸟的初始水平和垂直位置、底座的位置以及使用`gen_random_pipe`方法随机生成的管道的坐标。

在*第 11 步*中，默认情况下，一步的奖励是`+0.1`。如果动作是拍打，我们通过它向上的加速度来增加鸟的垂直速度。然后，我们检查这只鸟是否碰巧通过了一对管道。如果是，游戏分数增加 1，等级奖励变为+ 1。我们更新鸟的位置，它的图像索引，以及管道的位置。如果旧管道对将要离开屏幕的左侧，将生成一对新管道，一旦旧管道对离开屏幕，旧管道对将被删除。如果发生碰撞，该集结束，奖励-1；游戏也将重置。最后，我们将在游戏屏幕上显示更新后的画面。

<title>Building a Deep Q-Network to play Flappy Bird</title> <link rel="stylesheet" href="css/style.css" type="text/css"> 

# 建立一个深度 Q 网络来玩 Flappy Bird

既然 Flappy Bird 环境已经准备好了，我们可以通过建立一个 DQN 模型来解决这个问题。

正如我们所看到的，在采取一个动作后，每一步都会返回一个屏幕图像。CNN 是处理图像输入的最佳神经网络架构之一。在 CNN 中，卷积层能够有效地从图像中提取特征，这些特征将被传递给下游完全连接的层。在我们的解决方案中，我们将使用具有三个卷积层和一个全连接隐藏层的 CNN。CNN 架构的一个例子如下:

![](assets/ecfb19a6-6585-40eb-ade5-8f24dc904ebf.png)<title>How to do it...</title> <link rel="stylesheet" href="css/style.css" type="text/css"> 

# 怎么做...

让我们开发一个基于 CNN 的 DQN 模型如下:

1.  导入必要的模块:

```
>>> import torch
>>> import torch.nn as nn
>>> import torch.nn.functional as F
>>> import numpy as np
>>> import random
```

2.  我们从 CNN 模型开始:

```
>>> class DQNModel(nn.Module):
 ...     def __init__(self, n_action=2):
 ...         super(DQNModel, self).__init__()
 ...         self.conv1 = nn.Conv2d(4, 32, 
                             kernel_size=8, stride=4)
 ...         self.conv2 = nn.Conv2d(32, 64, 4, stride=2)
 ...         self.conv3 = nn.Conv2d(64, 64, 3, stride=1)
 ...         self.fc = nn.Linear(7 * 7 * 64, 512)
 ...         self.out = nn.Linear(512, n_action)
 ...         self._create_weights()
 ...
 ...     def _create_weights(self):
 ...         for m in self.modules():
 ...             if isinstance(m, nn.Conv2d) or 
                                 isinstance(m, nn.Linear):
 ...                 nn.init.uniform(m.weight, -0.01, 0.01)
 ...                 nn.init.constant_(m.bias, 0)
 ...
 ...     def forward(self, x):
 ...         x = F.relu(self.conv1(x))
 ...         x = F.relu(self.conv2(x))
 ...         x = F.relu(self.conv3(x))
 ...         x = x.view(x.size(0), -1)
 ...         x = F.relu(self.fc(x))
 ...         output = self.out(x)
 ...         return output
```

3.  现在，使用我们刚刚构建的 CNN 模型开发一个带有经验回放的 DQN:

```
>>> class DQN():
 ...     def __init__(self, n_action, lr=1e-6):
 ...         self.criterion = torch.nn.MSELoss()
 ...         self.model = DQNModel(n_action)
 ...         self.optimizer = torch.optim.Adam( self.model.parameters(), lr)
```

4.  给定输入状态，`predict`方法估计输出 Q 值:

```
>>>     def predict(self, s):
 ...         """
 ...         Compute the Q values of the state for all 
                 actions using the learning model
 ...         @param s: input state
 ...         @return: Q values of the state for all actions
 ...         """
 ...         return self.model(torch.Tensor(s))
```

5.  给定一个训练样本，`update`方法更新神经网络的权重，并返回当前损失:

```
>>>     def update(self, y_predict, y_target):
 ...         """
 ...         Update the weights of the DQN given a training sample
 ...         @param y_predict:
 ...         @param y_target:
 ...         @return:
 ...         """
 ...         loss = self.criterion(y_predict, y_target)
 ...         self.optimizer.zero_grad()
 ...         loss.backward()
 ...         self.optimizer.step()
 ...         return loss
```

6.  `DQN`类的最后一部分是`replay` 方法，它在给定一组过去经验的情况下执行经验重放:

```
>>>     def replay(self, memory, replay_size, gamma):
 ...         """
 ...         Experience replay
 ...         @param memory: a list of experience
 ...         @param replay_size: the number of samples we 
                 use to update the model each time
 ...         @param gamma: the discount factor
 ...         @return: the loss
 ...         """
 ...         if len(memory) >= replay_size:
 ...             replay_data = random.sample(memory, replay_size)
 ...             state_batch, action_batch, next_state_batch, 
                     reward_batch, done_batch = zip(*replay_data)
 ...             state_batch = torch.cat( tuple(state for state in state_batch))
 ...             next_state_batch = torch.cat(    
                         tuple(state for state in next_state_batch))
 ...             q_values_batch = self.predict(state_batch)
 ...             q_values_next_batch = 
                         self.predict(next_state_batch)
 ...             reward_batch = torch.from_numpy(np.array( reward_batch, dtype=np.float32)[:, None])
 ...             action_batch = torch.from_numpy(
 ...                 np.array([[1, 0] if action == 0 else [0, 1] 
                     for action in action_batch], dtype=np.float32))
 ...             q_value = torch.sum( q_values_batch * action_batch, dim=1)
 ...             td_targets = torch.cat(
 ...             tuple(reward if terminal else reward + 
                         gamma * torch.max(prediction) for
                         reward, terminal, prediction
 ...                 in zip(reward_batch, done_batch, 
                         q_values_next_batch)))
 ...             loss = self.update(q_value, td_targets)
 ...             return loss
```

DQN 班到此为止。在下一个配方中，我们将在多次迭代中训练 DQN 模型。

<title>How it works...</title> <link rel="stylesheet" href="css/style.css" type="text/css"> 

# 它是如何工作的...

在第二步第一步中，我们把 CNN 的 DQN 的骨干放在一起。它有三个不同配置的卷积层。每个卷积层之后都有一个 ReLU 激活函数。来自最后一个卷积层的结果特征图然后被展平并馈送到具有 512 个节点的全连接隐藏层，随后是输出层。

请注意，我们还为权重的初始随机值和零偏差设置了一个边界，以便模型更有可能更快地收敛。

*第 6 步*是带经验回放的分步训练。如果经验足够多，我们随机抽取一组`replay_size` 经验进行训练。然后，在给定输入状态的情况下，我们将每个经验转换为由预测值和输出目标值组成的训练样本。目标值计算如下:

*   使用奖励和新的 Q 值更新动作的目标 Q 值，如: [![](assets/1a6a21ec-ee29-4f78-881a-3647f1e04b91.png)]

*   如果是终止状态，目标 Q 值更新为`r`。

最后，我们使用选定的一批训练样本更新神经网络。

<title>Training and tuning the network</title> <link rel="stylesheet" href="css/style.css" type="text/css"> 

# 训练和调整网络

在这份食谱中，我们将训练 DQN 模特扮演 Flappy Bird。

在训练的每一步中，我们采取一个遵循ε-贪婪策略的动作:在某个概率(ε)下，我们会采取一个随机动作，在我们的情况下拍打或不拍打；否则，我们选择具有最高值的动作。我们还调整了每一步的ε值，因为我们倾向于在开始时进行更多的勘探，当 DQN 模型变得更加成熟时进行更多的开采。

正如我们所看到的，对每一步的观察都是屏幕的二维图像。我们需要将观察图像转换成状态。简单地使用一个步骤中的一个图像不会提供足够的信息来指导代理如何反应。因此，我们使用来自四个相邻步骤的图像形成一个状态。我们将首先将图像调整到预期的大小，然后将当前帧的图像与前三帧的图像连接起来。

<title>How to do it...</title> <link rel="stylesheet" href="css/style.css" type="text/css"> 

# 怎么做...

我们训练 DQN 模型如下:

1.  导入必要的模块:

```
>>> import random
>>> import torch
>>> from collections import deque
```

2.  我们从发展ε贪婪策略开始:

```
>>> def gen_epsilon_greedy_policy(estimator, epsilon, n_action):
 ...     def policy_function(state):
 ...         if random.random() < epsilon:
 ...             return random.randint(0, n_action - 1)
 ...         else:
 ...             q_values = estimator.predict(state)
 ...             return torch.argmax(q_values).item()
 ...     return policy_function
```

3.  我们指定预处理图像的大小、批量大小、学习速率、gamma、动作数量、初始和最终ε、迭代次数以及内存大小:

```
>>> image_size = 84
 >>> batch_size = 32
 >>> lr = 1e-6
 >>> gamma = 0.99
 >>> init_epsilon = 0.1
 >>> final_epsilon = 1e-4
 >>> n_iter = 2000000
 >>> memory_size = 50000
 >>> n_action = 2
```

我们还会定期保存训练好的模型，因为这将是一个非常漫长的过程:

```
>>> saved_path = 'trained_models'
```

不要忘记创建一个名为`trained_models`的文件夹。

4.  我们为实验再现性指定了随机进料:

```
>>> torch.manual_seed(123)
```

5.  我们相应地创建了一个 DQN 模型:

```
>>> estimator = DQN(n_action)
```

我们还创建了一个内存队列:

```
>>> memory = deque(maxlen=memory_size)
```

只要队列中有超过 50，000 个样本，新的样本将被添加到队列中，旧的样本将被删除。

6.  接下来，我们初始化一个 Flappy Bird 环境:

```
>>> env = FlappyBird()
```

然后我们获得初始图像:

```
>>> image, reward, is_done = env.next_step(0)
```

7.  如前所述，我们应该将原始图像的大小调整为`image_size * image_size`:

```
>>> import cv2
 >>> import numpy as np
 >>> def pre_processing(image, width, height):
 ...     image = cv2.cvtColor(cv2.resize(image, 
                     (width, height)), cv2.COLOR_BGR2GRAY)
 ...     _, image = cv2.threshold(image, 1, 255, cv2.THRESH_BINARY)
 ...     return image[None, :, :].astype(np.float32)
```

如果没有安装`cv2`包，您可以使用以下命令进行安装:

```
pip install opencv-python
```

让我们对图像进行相应的预处理:

```
>>> image = pre_processing(image[:screen_width, :int(env.base_y)], image_size, image_size)
```

8.  现在，我们通过连接四幅图像来构建一个状态。因为我们现在只有第一帧，我们简单地复制它四次:

```
>>> image = torch.from_numpy(image) >>> state = torch.cat(tuple(image for _ in range(4)))[None, :, :, :]
```

9.  然后，我们开始`n_iter`步骤的训练循环:

```
>>> for iter in range(n_iter):
 ...     epsilon = final_epsilon + (n_iter - iter) 
                 * (init_epsilon - final_epsilon) / n_iter
 ...     policy = gen_epsilon_greedy_policy( estimator, epsilon, n_action)
 ...     action = policy(state)
 ...     next_image, reward, is_done = env.next_step(action)
 ...     next_image = pre_processing(next_image[ :screen_width, :int(env.base_y)], image_size, image_size)
 ...     next_image = torch.from_numpy(next_image)
 ...     next_state = torch.cat(( state[0, 1:, :, :], next_image))[None, :, :, :]
 ...     memory.append([state, action, next_state, reward, is_done])
 ...     loss = estimator.replay(memory, batch_size, gamma)
 ...     state = next_state
 ...     print("Iteration: {}/{}, Action: {}, 
                 Loss: {}, Epsilon {}, Reward: {}".format(
 ...             iter + 1, n_iter, action, loss, epsilon, reward))
 ...     if iter+1 % 10000 == 0:
 ...         torch.save(estimator.model, "{}/{}".format( saved_path, iter+1))
```

在我们运行这部分代码后，我们将看到以下日志:

```
Iteration: 1/2000000, Action: 0, Loss: None, Epsilon 0.1, Reward: 0.1 Iteration: 2/2000000, Action: 0, Loss: None, Epsilon 0.09999995005000001, Reward: 0.1
 Iteration: 3/2000000, Action: 0, Loss: None, Epsilon 0.0999999001, Reward: 0.1
 Iteration: 4/2000000, Action: 0, Loss: None, Epsilon 0.09999985015, Reward: 0.1
 ...
 ...
 Iteration: 201/2000000, Action: 1, Loss: 0.040504034608602524, Epsilon 0.09999001000000002, Reward: 0.1
 Iteration: 202/2000000, Action: 1, Loss: 0.010011588223278522, Epsilon 0.09998996005, Reward: 0.1
 Iteration: 203/2000000, Action: 1, Loss: 0.07097195833921432, Epsilon 0.09998991010000001, Reward: 0.1
 Iteration: 204/2000000, Action: 1, Loss: 0.040418840944767, Epsilon 0.09998986015000001, Reward: 0.1
 Iteration: 205/2000000, Action: 1, Loss: 0.00999421812593937, Epsilon 0.09998981020000001, Reward: 0.1
```

培训需要一段时间。当然，你可以用 GPU 加速训练。

10.  最后，我们保存最后训练的模式:

```
>>> torch.save(estimator.model, "{}/final".format(saved_path))
```

<title>How it works...</title> <link rel="stylesheet" href="css/style.css" type="text/css"> 

# 它是如何工作的...

在*步骤 9* 中，对于每个训练步骤，我们执行以下任务:

*   稍微降低ε，并相应地创建ε贪婪策略。
*   采取使用ε-贪婪策略计算的动作。
*   预处理生成的图像，并通过将图像附加到前面三个步骤中的图像来构造新的状态。
*   记录这一步的经历，包括状态，动作，下一个状态，收到的奖励，是否结束。

*   用经验回放更新模型。
*   打印出培训状态并更新状态。
*   定期保存已训练的模型，以避免从头开始重新训练。

<title>Deploying the model and playing the game</title> <link rel="stylesheet" href="css/style.css" type="text/css"> 

# 部署模型和玩游戏

现在我们已经训练了 DQN 模型，让我们应用它来玩 Flappy Bird 游戏。

用训练好的模型玩游戏很简单。我们将只采取与每一步中最高值相关的行动。我们将播放几集，看看它的表现如何。不要忘记预处理原始屏幕图像和构造状态。

<title>How to do it...</title> <link rel="stylesheet" href="css/style.css" type="text/css"> 

# 怎么做...

我们在新剧集中测试 DQN 模型，如下所示:

1.  我们首先加载最终模型:

```
>>> model = torch.load("{}/final".format(saved_path))
```

2.  我们播放 100 集，每集执行以下操作:

```
>>> n_episode = 100 >>> for episode in range(n_episode):
 ...     env = FlappyBird()
 ...     image, reward, is_done = env.next_step(0)
 ...     image = pre_processing(image[:screen_width, 
                :int(env.base_y)], image_size, image_size)
 ...     image = torch.from_numpy(image)
 ...     state = torch.cat(tuple(image for _ in range(4)))[ None, :, :, :]
 ...     while True:
 ...         prediction = model(state)[0]
 ...         action = torch.argmax(prediction).item()
 ...         next_image, reward, is_done = env.next_step(action)
 ...         if is_done:
 ...             break
 ...         next_image = pre_processing(next_image[:screen_width, :int(env.base_y)], image_size, image_size)
 ...         next_image = torch.from_numpy(next_image)
 ...         next_state = torch.cat((state[0, 1:, :, :], 
                           next_image))[None, :, :, :]
 ...         state = next_state
```

希望你会看到类似下图的东西，图中小鸟穿过一系列管道:

![](assets/6864a7d8-989a-4840-9655-8f728aa78bd6.png)<title>How it works...</title> <link rel="stylesheet" href="css/style.css" type="text/css"> 

# 它是如何工作的...

在*步骤 2* 中，我们为每集执行以下任务:

*   初始化 Flappy Bird 环境。
*   观察初始图像并生成其状态。
*   给定状态，使用模型计算 Q 值，并采取具有最高 Q 值的行动
*   观察新的图像，以及该集是否结束。
*   如果情节继续，计算下一个图像的状态，并将其分配到当前状态。
*   重复直到这一集结束。