

# é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹å’ŒåŠ¨æ€è§„åˆ’

åœ¨æœ¬ç« ä¸­ï¼Œæˆ‘ä»¬å°†é€šè¿‡æŸ¥çœ‹**é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹** ( **MDPs** )å’ŒåŠ¨æ€ç¼–ç¨‹ï¼Œç»§ç»­æˆ‘ä»¬çš„ PyTorch å¼ºåŒ–å­¦ä¹ å®žè·µä¹‹æ—…ã€‚æœ¬ç« å°†ä»Žåˆ›å»ºé©¬å°”å¯å¤«é“¾å’Œ MDP å¼€å§‹ï¼Œè¿™æ˜¯å¤§å¤šæ•°å¼ºåŒ–å­¦ä¹ ç®—æ³•çš„æ ¸å¿ƒã€‚é€šè¿‡ç»ƒä¹ æ”¿ç­–è¯„ä¼°ï¼Œä½ ä¹Ÿå°†æ›´åŠ ç†Ÿæ‚‰è´å°”æ›¼æ–¹ç¨‹ã€‚ç„¶åŽï¼Œæˆ‘ä»¬å°†ç»§ç»­åº”ç”¨ä¸¤ç§æ–¹æ³•æ¥è§£å†³ MDP:ä»·å€¼è¿­ä»£å’Œç­–ç•¥è¿­ä»£ã€‚æˆ‘ä»¬å°†ä»¥ FrozenLake çŽ¯å¢ƒä¸ºä¾‹ã€‚åœ¨æœ¬ç« çš„æœ€åŽï¼Œæˆ‘ä»¬å°†é€æ­¥æ¼”ç¤ºå¦‚ä½•ç”¨åŠ¨æ€è§„åˆ’æ¥è§£å†³æœ‰è¶£çš„æŠ›ç¡¬å¸èµŒåšé—®é¢˜ã€‚

æœ¬ç« å°†ä»‹ç»ä»¥ä¸‹é…æ–¹:

*   åˆ›å»ºé©¬å°”å¯å¤«é“¾
*   åˆ›å»º MDP
*   æ‰§è¡Œç­–ç•¥è¯„ä¼°
*   æ¨¡æ‹Ÿå†°å†»æ¹–çŽ¯å¢ƒ
*   ç”¨æ•°å€¼è¿­ä»£ç®—æ³•æ±‚è§£ MDP
*   ç”¨ç­–ç•¥è¿­ä»£ç®—æ³•æ±‚è§£ MDP
*   è§£å†³æŠ›ç¡¬å¸èµŒåšé—®é¢˜



# æŠ€æœ¯è¦æ±‚

æ‚¨éœ€è¦åœ¨ç³»ç»Ÿä¸Šå®‰è£…ä»¥ä¸‹ç¨‹åºï¼Œä»¥æˆåŠŸæ‰§è¡Œæœ¬ç« ä¸­çš„é…æ–¹:

*   Python 3.6ã€3.7 æˆ–æ›´é«˜ç‰ˆæœ¬
*   èŸ’è›‡
*   PyTorch 1.0 æˆ–ä»¥ä¸Š
*   OpenAI Gym



# åˆ›å»ºé©¬å°”å¯å¤«é“¾

è®©æˆ‘ä»¬ä»Žåˆ›å»ºä¸€ä¸ªé©¬å°”å¯å¤«é“¾å¼€å§‹ï¼ŒMDP å°±æ˜¯åœ¨è¿™ä¸ªé©¬å°”å¯å¤«é“¾ä¸Šå‘å±•èµ·æ¥çš„ã€‚

é©¬å°”å¯å¤«é“¾æè¿°äº†ç¬¦åˆ**é©¬å°”å¯å¤«å±žæ€§**çš„äº‹ä»¶åºåˆ—ã€‚å®ƒç”±ä¸€ç»„å¯èƒ½çš„çŠ¶æ€å®šä¹‰ï¼Œ *S = {s0ï¼Œs1ï¼Œ...ï¼Œsm}* ï¼Œä»¥åŠè½¬ç§»çŸ©é˜µ *T(sï¼Œsâ€™)*ï¼Œç”±çŠ¶æ€ *s* è½¬ç§»åˆ°çŠ¶æ€ sâ€™çš„æ¦‚çŽ‡ç»„æˆã€‚æ ¹æ®é©¬å°”å¯å¤«ç‰¹æ€§ï¼Œç»™å®šå½“å‰çŠ¶æ€ï¼Œè¿‡ç¨‹çš„æœªæ¥çŠ¶æ€æœ‰æ¡ä»¶åœ°ç‹¬ç«‹äºŽè¿‡åŽ»çš„çŠ¶æ€ã€‚æ¢å¥è¯è¯´ï¼Œåœ¨ *t+1* çš„è¿‡ç¨‹çŠ¶æ€ä»…å–å†³äºŽåœ¨ *t* çš„çŠ¶æ€ã€‚è¿™é‡Œï¼Œæˆ‘ä»¬ä»¥å­¦ä¹ å’Œç¡çœ çš„è¿‡ç¨‹ä¸ºä¾‹ï¼Œå¹¶åŸºäºŽä¸¤ç§çŠ¶æ€åˆ›å»ºé©¬å°”å¯å¤«é“¾ï¼Œ *s0* (å­¦ä¹ )å’Œ *s1* (ç¡çœ )ã€‚å‡è®¾æˆ‘ä»¬æœ‰ä»¥ä¸‹è½¬æ¢çŸ©é˜µ:

![](img/4bb4c9e3-4d16-402a-af8a-a586d8db69a1.png)

åœ¨ä¸‹ä¸€èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†è®¡ç®— k ä¸ªæ­¥éª¤åŽçš„è½¬ç§»çŸ©é˜µï¼Œä»¥åŠåœ¨ç»™å®šåˆå§‹çŠ¶æ€åˆ†å¸ƒçš„æƒ…å†µä¸‹å¤„äºŽæ¯ä¸ªçŠ¶æ€çš„æ¦‚çŽ‡ï¼Œä¾‹å¦‚ *[0.7ï¼Œ0.3]* ï¼Œè¿™æ„å‘³ç€è¯¥è¿‡ç¨‹æœ‰ 70%çš„å¯èƒ½æ€§ä»Žå­¦ä¹ å¼€å§‹ï¼Œæœ‰ 30%çš„å¯èƒ½æ€§ä»Žç¡çœ å¼€å§‹ã€‚



# æ€Žä¹ˆåš...

è¦ä¸ºå­¦ä¹ å’Œç¡çœ è¿‡ç¨‹åˆ›å»ºé©¬å°”å¯å¤«é“¾å¹¶å¯¹å…¶è¿›è¡Œåˆ†æžï¼Œè¯·æ‰§è¡Œä»¥ä¸‹æ­¥éª¤:

1.  å¯¼å…¥åº“å¹¶å®šä¹‰è½¬æ¢çŸ©é˜µ:

```py
>>> import torch
>>> T = torch.tensor([[0.4, 0.6],
...                   [0.8, 0.2]])
```

2.  è®¡ç®— k æ­¥åŽçš„è½¬ç§»æ¦‚çŽ‡ã€‚è¿™é‡Œï¼Œæˆ‘ä»¬ä»¥ k = `2`ã€`5`ã€`10`ã€`15`ã€`20`ä¸ºä¾‹:

```py
>>> T_2 = torch.matrix_power(T, 2)
>>> T_5 = torch.matrix_power(T, 5)
>>> T_10 = torch.matrix_power(T, 10)
>>> T_15 = torch.matrix_power(T, 15)
>>> T_20 = torch.matrix_power(T, 20)
```

3.  å®šä¹‰ä¸¤ç§çŠ¶æ€çš„åˆå§‹åˆ†å¸ƒ:

```py
>>> v = torch.tensor([[0.7, 0.3]])
```

4.  è®¡ç®— k = `1`ã€`2`ã€`5`ã€`10`ã€`15`ã€`20`æ­¥éª¤åŽçš„çŠ¶æ€åˆ†å¸ƒ:

```py
>>> v_1 = torch.mm(v, T)
>>> v_2 = torch.mm(v, T_2)
>>> v_5 = torch.mm(v, T_5)
>>> v_10 = torch.mm(v, T_10)
>>> v_15 = torch.mm(v, T_15)
>>> v_20 = torch.mm(v, T_20)
```



# å®ƒæ˜¯å¦‚ä½•å·¥ä½œçš„...

åœ¨*ç¬¬äºŒæ­¥*ä¸­ï¼Œæˆ‘ä»¬è®¡ç®—äº† k æ­¥åŽçš„è½¬ç§»æ¦‚çŽ‡ï¼Œä¹Ÿå°±æ˜¯è½¬ç§»çŸ©é˜µçš„ k æ¬¡^å¹‚ã€‚æ‚¨å°†çœ‹åˆ°ä»¥ä¸‹è¾“å‡º:

```py
>>> print("Transition probability after 2 steps:\n{}".format(T_2))
Transition probability after 2 steps:
tensor([[0.6400, 0.3600],
 [0.4800, 0.5200]])
>>> print("Transition probability after 5 steps:\n{}".format(T_5))
Transition probability after 5 steps:
tensor([[0.5670, 0.4330],
 [0.5773, 0.4227]])
>>> print(
"Transition probability after 10 steps:\n{}".format(T_10))
Transition probability after 10 steps:
tensor([[0.5715, 0.4285],
 [0.5714, 0.4286]])
>>> print(
"Transition probability after 15 steps:\n{}".format(T_15))
Transition probability after 15 steps:
tensor([[0.5714, 0.4286],
 [0.5714, 0.4286]])
>>> print(
"Transition probability after 20 steps:\n{}".format(T_20))
Transition probability after 20 steps:
tensor([[0.5714, 0.4286],
 [0.5714, 0.4286]])
```

æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œåœ¨ 10 åˆ° 15 æ­¥ä¹‹åŽï¼Œè·ƒè¿æ¦‚çŽ‡æ”¶æ•›ã€‚è¿™æ„å‘³ç€ï¼Œæ— è®ºè¿›ç¨‹å¤„äºŽä»€ä¹ˆçŠ¶æ€ï¼Œéƒ½æœ‰ç›¸åŒçš„æ¦‚çŽ‡è¿‡æ¸¡åˆ° s0 (57.14%)å’Œ s1 (42.86%)ã€‚

åœ¨*æ­¥éª¤ 4* ä¸­ï¼Œæˆ‘ä»¬è®¡ç®—äº† k = `1`ã€`2`ã€`5`ã€`10`ã€`15`ã€`20`æ­¥éª¤åŽçš„çŠ¶æ€åˆ†å¸ƒï¼Œå®ƒæ˜¯åˆå§‹çŠ¶æ€åˆ†å¸ƒå’Œè½¬ç§»æ¦‚çŽ‡çš„ä¹˜ç§¯ã€‚ä½ å¯ä»¥åœ¨è¿™é‡Œçœ‹åˆ°ç»“æžœ:

```py
>>> print("Distribution of states after 1 step:\n{}".format(v_1))
Distribution of states after 1 step:
tensor([[0.5200, 0.4800]])
>>> print("Distribution of states after 2 steps:\n{}".format(v_2))
Distribution of states after 2 steps:
tensor([[0.5920, 0.4080]])
>>> print("Distribution of states after 5 steps:\n{}".format(v_5))
Distribution of states after 5 steps:
tensor([[0.5701, 0.4299]])
>>> print(
 "Distribution of states after 10 steps:\n{}".format(v_10))
Distribution of states after 10 steps:
tensor([[0.5714, 0.4286]])
>>> print(
 "Distribution of states after 15 steps:\n{}".format(v_15))
Distribution of states after 15 steps:
tensor([[0.5714, 0.4286]])
>>> print(
 "Distribution of states after 20 steps:\n{}".format(v_20))
Distribution of states after 20 steps:
tensor([[0.5714, 0.4286]])
```

æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œ10 æ­¥ä¹‹åŽï¼ŒçŠ¶æ€åˆ†å¸ƒæ”¶æ•›ã€‚åœ¨ s0 çš„æ¦‚çŽ‡(57.14%)å’Œåœ¨ s1 çš„æ¦‚çŽ‡(42.86%)é•¿æœŸä¸å˜ã€‚

ä»Ž[0.7ï¼Œ0.3]å¼€å§‹ï¼Œä¸€æ¬¡è¿­ä»£åŽçš„çŠ¶æ€åˆ†å¸ƒå˜æˆ[0.52ï¼Œ0.48]ã€‚å…¶è®¡ç®—ç»†èŠ‚å¦‚ä¸‹å›¾æ‰€ç¤º:

![](img/19ed17b5-c90e-42d9-92f8-adbd951bb10b.png)

ç»è¿‡å¦ä¸€æ¬¡è¿­ä»£åŽï¼ŒçŠ¶æ€åˆ†å¸ƒå˜ä¸º[0.592ï¼Œ0.408]ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤º:

![](img/4e6a9d0f-ddda-4d73-b19b-2f3b56883a05.png)

éšç€æ—¶é—´çš„æŽ¨ç§»ï¼ŒçŠ¶æ€åˆ†å¸ƒè¾¾åˆ°å¹³è¡¡ã€‚



# è¿˜æœ‰æ›´å¤š...

äº‹å®žä¸Šï¼Œä¸ç®¡è¿›ç¨‹çš„åˆå§‹çŠ¶æ€æ˜¯ä»€ä¹ˆï¼ŒçŠ¶æ€åˆ†å¸ƒæ€»æ˜¯æ”¶æ•›åˆ°[0.5714ï¼Œ0.4286]ã€‚æ‚¨å¯ä»¥ä½¿ç”¨å…¶ä»–åˆå§‹å‘è¡Œç‰ˆè¿›è¡Œæµ‹è¯•ï¼Œæ¯”å¦‚[0.2ï¼Œ0.8]å’Œ[1ï¼Œ0]ã€‚10 æ­¥ä¹‹åŽåˆ†å¸ƒå°†ä¿æŒ[0.5714ï¼Œ0.4286]ã€‚

é©¬å°”å¯å¤«é“¾ä¸ä¸€å®šæ”¶æ•›ï¼Œå°¤å…¶æ˜¯å½“å®ƒåŒ…å«çž¬æ€æˆ–å½“å‰çŠ¶æ€æ—¶ã€‚ä½†å¦‚æžœå®ƒç¡®å®žæ”¶æ•›äº†ï¼Œä¸ç®¡èµ·å§‹åˆ†å¸ƒå¦‚ä½•ï¼Œå®ƒéƒ½ä¼šè¾¾åˆ°ç›¸åŒçš„å‡è¡¡ã€‚



# è¯·å‚è§

å¦‚æžœä½ æƒ³äº†è§£æ›´å¤šå…³äºŽé©¬å°”å¯å¤«é“¾çš„çŸ¥è¯†ï¼Œä¸‹é¢æ˜¯ä¸¤ç¯‡å¾ˆæ£’çš„åšå®¢æ–‡ç« ï¼Œæœ‰å¾ˆå¥½çš„å¯è§†åŒ–æ•ˆæžœ:

*   [https://brilliant.org/wiki/markov-chains/](https://brilliant.org/wiki/markov-chains/)
*   [http://setosa.io/ev/markov-chains/](http://setosa.io/ev/markov-chains/)



# åˆ›å»º MDP

åŸºäºŽé©¬å°”å¯å¤«é“¾å¼€å‘çš„ MDP åŒ…æ‹¬ä¸€ä¸ªä»£ç†å’Œä¸€ä¸ªå†³ç­–è¿‡ç¨‹ã€‚è®©æˆ‘ä»¬ç»§ç»­å¼€å‘ MDP å¹¶è®¡ç®—æœ€ä¼˜ç­–ç•¥ä¸‹çš„ä»·å€¼å‡½æ•°ã€‚

é™¤äº†ä¸€ç»„å¯èƒ½çš„çŠ¶æ€å¤–ï¼Œ *S = {s0ï¼Œs1ï¼Œ...ï¼Œsm}* ï¼Œä¸€ä¸ª MDP ç”±ä¸€ç»„åŠ¨ä½œå®šä¹‰ï¼Œ *A = {a0ï¼Œa1ï¼Œ...ï¼Œan }*ï¼›ä¸€ä¸ªè¿‡æ¸¡æ¨¡åž‹ï¼Œ *T(sï¼Œaï¼Œsâ€™)*ï¼›ä¸€ä¸ªå¥–åŠ±å‡½æ•°ï¼Œ*R(s)*ï¼›å’Œä¸€ä¸ªè´´çŽ°å› å­ï¼Œð².è½¬ç§»çŸ©é˜µ *T(sï¼Œaï¼Œsâ€™)*åŒ…å«ä»ŽçŠ¶æ€ s é‡‡å–è¡ŒåŠ¨ a ç„¶åŽåˆ°è¾¾ sâ€™çš„æ¦‚çŽ‡ã€‚è´´çŽ°å› å­ð²æŽ§åˆ¶ç€æœªæ¥å›žæŠ¥å’Œçœ¼å‰å›žæŠ¥ä¹‹é—´çš„æƒè¡¡ã€‚

ä¸ºäº†ä½¿æˆ‘ä»¬çš„ MDP ç¨å¾®å¤æ‚ä¸€ç‚¹ï¼Œæˆ‘ä»¬ç”¨ä¸€ä¸ªçŠ¶æ€æ¥æ‰©å±•å­¦ä¹ å’Œç¡çœ è¿‡ç¨‹ï¼Œ`s2 play` gamesã€‚å‡è®¾æˆ‘ä»¬æœ‰ä¸¤ä¸ªåŠ¨ä½œï¼Œ`a0 work` å’Œ`a1 slack`ã€‚ *3 * 2 * 3* è½¬ç§»çŸ©é˜µ *T(sï¼Œaï¼Œsâ€™)*å¦‚ä¸‹:

![](img/c142bd78-673a-4dc7-a222-014889c5cc5f.png)

è¿™æ„å‘³ç€ï¼Œä¾‹å¦‚ï¼Œå½“ä»ŽçŠ¶æ€ s0 ç ”ç©¶ä¸­é‡‡å– a1 æ‡ˆæ€ è¡ŒåŠ¨æ—¶ï¼Œæœ‰ 60%çš„å¯èƒ½æ€§ä¼šå˜æˆ s1 ç¡çœ (å¯èƒ½ä¼šç´¯)ï¼Œæœ‰ 30%çš„å¯èƒ½æ€§ä¼šå˜æˆ s2 çŽ©æ¸¸æˆ(å¯èƒ½æƒ³æ”¾æ¾)ï¼Œæœ‰ 10%çš„å¯èƒ½æ€§ä¼šç»§ç»­å­¦ä¹ (å¯èƒ½æ˜¯çœŸæ­£çš„å·¥ä½œç‹‚)ã€‚æˆ‘ä»¬å°†ä¸‰ç§çŠ¶æ€çš„å¥–åŠ±å‡½æ•°å®šä¹‰ä¸º[+1ï¼Œ0ï¼Œ-1]ï¼Œä»¥è¡¥å¿è¾›è‹¦çš„å·¥ä½œã€‚æ˜¾ç„¶ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œ**æœ€ä¼˜ç­–ç•¥**æ˜¯ä¸ºæ¯ä¸€æ­¥é€‰æ‹© 0 å·¥ä½œ(ç»§ç»­å­¦ä¹ â€”â€”æ²¡æœ‰ä»˜å‡ºå°±æ²¡æœ‰æ”¶èŽ·ï¼Œå¯¹å—ï¼Ÿ).æ­¤å¤–ï¼Œæˆ‘ä»¬é¦–å…ˆé€‰æ‹© 0.5 ä½œä¸ºæŠ˜æ‰£ç³»æ•°ã€‚åœ¨ä¸‹ä¸€èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†è®¡ç®—æœ€ä¼˜ç­–ç•¥ä¸‹çš„**çŠ¶æ€-ä»·å€¼å‡½æ•°**(ä¹Ÿç§°ä¸º**ä»·å€¼å‡½æ•°**ï¼Œç®€ç§°**å€¼**ï¼Œæˆ–**æœŸæœ›æ•ˆç”¨**)ã€‚



# æ€Žä¹ˆåš...

åˆ›å»º MDP å¯é€šè¿‡ä»¥ä¸‹æ­¥éª¤å®Œæˆ:

1.  å¯¼å…¥ PyTorch å¹¶å®šä¹‰è½¬æ¢çŸ©é˜µ:

```py
 >>> import torch
 >>> T = torch.tensor([[[0.8, 0.1, 0.1],
 ...                    [0.1, 0.6, 0.3]],
 ...                   [[0.7, 0.2, 0.1],
 ...                    [0.1, 0.8, 0.1]],
 ...                   [[0.6, 0.2, 0.2],
 ...                    [0.1, 0.4, 0.5]]]
 ...                  )
```

2.  å®šä¹‰å¥–åŠ±å‡½æ•°å’ŒæŠ˜æ‰£ç³»æ•°:

```py
 >>> R = torch.tensor([1., 0, -1.])
 >>> gamma = 0.5
```

3.  è¿™ç§æƒ…å†µä¸‹çš„æœ€ä½³ç­–ç•¥æ˜¯åœ¨æ‰€æœ‰æƒ…å†µä¸‹é€‰æ‹©åŠ¨ä½œ`a0`:

```py
>>> action = 0
```

4.  æˆ‘ä»¬åœ¨ä»¥ä¸‹å‡½æ•°ä¸­ä½¿ç”¨**çŸ©é˜µæ±‚é€†**æ–¹æ³•è®¡ç®—æœ€ä¼˜ç­–ç•¥çš„å€¼`V`:

```py
 >>> def cal_value_matrix_inversion(gamma, trans_matrix, rewards):
 ...     inv = torch.inverse(torch.eye(rewards.shape[0]) 
 - gamma * trans_matrix)
 ...     V = torch.mm(inv, rewards.reshape(-1, 1))
 ...     return V
```

æˆ‘ä»¬å°†åœ¨ä¸‹ä¸€èŠ‚ä¸­æ¼”ç¤ºå¦‚ä½•å¯¼å‡ºè¯¥å€¼ã€‚

5.  æˆ‘ä»¬å°†æ‰€æœ‰çš„å˜é‡è¾“å…¥åˆ°å‡½æ•°ä¸­ï¼ŒåŒ…æ‹¬ä¸ŽåŠ¨ä½œ`a0`ç›¸å…³çš„è½¬ç§»æ¦‚çŽ‡:

```py
 >>> trans_matrix = T[:, action]
 >>> V = cal_value_matrix_inversion(gamma, trans_matrix, R)
 >>> print("The value function under the optimal 
 policy is:\n{}".format(V))
 The value function under the optimal policy is:
 tensor([[ 1.6787],
 [ 0.6260],
 [-0.4820]])
```



# å®ƒæ˜¯å¦‚ä½•å·¥ä½œçš„...

åœ¨è¿™ä¸ªè¿‡äºŽç®€åŒ–çš„å­¦ä¹ -ç¡çœ -æ¸¸æˆè¿‡ç¨‹ä¸­ï¼Œæœ€ä¼˜ç­–ç•¥ï¼Œå³èŽ·å¾—æœ€é«˜æ€»æŠ¥é…¬çš„ç­–ç•¥ï¼Œåœ¨æ‰€æœ‰æ­¥éª¤ä¸­éƒ½é€‰æ‹©è¡ŒåŠ¨ a0ã€‚ç„¶è€Œï¼Œåœ¨å¤§å¤šæ•°æƒ…å†µä¸‹ï¼Œäº‹æƒ…ä¸ä¼šé‚£ä¹ˆç®€å•ã€‚æ­¤å¤–ï¼Œæ¯ä¸ªæ­¥éª¤ä¸­é‡‡å–çš„è¡ŒåŠ¨ä¸ä¸€å®šç›¸åŒã€‚å®ƒä»¬é€šå¸¸ä¾èµ–äºŽçŠ¶æ€ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¿…é¡»é€šè¿‡åœ¨çŽ°å®žä¸–ç•Œä¸­å¯»æ‰¾æœ€ä¼˜ç­–ç•¥æ¥è§£å†³ MDP é—®é¢˜ã€‚

ç»™å®šæ‰€éµå¾ªçš„ç­–ç•¥ï¼Œç­–ç•¥çš„ä»·å€¼å‡½æ•°æµ‹é‡ä»£ç†åœ¨æ¯ä¸ªçŠ¶æ€ä¸‹æœ‰å¤šå¥½ã€‚æ•°å€¼è¶Šå¤§ï¼ŒçŠ¶æ€è¶Šå¥½ã€‚

åœ¨*æ­¥éª¤ 4* ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨**çŸ©é˜µæ±‚é€†**è®¡ç®—æœ€ä¼˜ç­–ç•¥çš„å€¼`V`ã€‚æ ¹æ®**è´å°”æ›¼æ–¹ç¨‹**ï¼Œæ­¥éª¤ *t+1* å¤„çš„å€¼ä¸Žæ­¥éª¤ *t* å¤„çš„å€¼ä¹‹é—´çš„å…³ç³»å¯ä»¥è¡¨ç¤ºä¸º:

![](img/56fc727f-bb72-4413-8ebf-104b07f358b8.png)

å½“å€¼æ”¶æ•›æ—¶ï¼Œè¿™æ„å‘³ç€ *Vt+1 = Vt* ï¼Œæˆ‘ä»¬å¯ä»¥å¯¼å‡ºå€¼`V`ï¼Œå¦‚ä¸‹æ‰€ç¤º:

![](img/7f19bce0-3a09-4649-87f8-f2e13badfd54.png)

è¿™é‡Œï¼Œ *I* æ˜¯ä¸»å¯¹è§’çº¿ä¸Šä¸º 1 çš„å•ä½çŸ©é˜µã€‚

ç”¨çŸ©é˜µæ±‚é€†æ±‚è§£ MDP çš„ä¸€ä¸ªä¼˜ç‚¹æ˜¯ä½ æ€»èƒ½å¾—åˆ°ç²¾ç¡®çš„ç­”æ¡ˆã€‚ä½†æ˜¯ç¼ºç‚¹æ˜¯å®ƒçš„å¯æ‰©å±•æ€§ã€‚ç”±äºŽæˆ‘ä»¬éœ€è¦è®¡ç®— m * m çŸ©é˜µçš„é€†çŸ©é˜µ(å…¶ä¸­ *m* æ˜¯å¯èƒ½çŠ¶æ€çš„æ•°é‡)ï¼Œå¦‚æžœæœ‰å¤§é‡çš„çŠ¶æ€ï¼Œè®¡ç®—å°†å˜å¾—æ˜‚è´µã€‚



# è¿˜æœ‰æ›´å¤š...

æˆ‘ä»¬å†³å®šè¯•éªŒä¸åŒçš„æŠ˜æ‰£å› å­å€¼ã€‚è®©æˆ‘ä»¬ä»Ž 0 å¼€å§‹ï¼Œè¿™æ„å‘³ç€æˆ‘ä»¬åªå…³å¿ƒçœ¼å‰çš„å›žæŠ¥:

```py
 >>> gamma = 0
 >>> V = cal_value_matrix_inversion(gamma, trans_matrix, R)
 >>> print("The value function under the optimal policy is:\n{}".format(V))
 The value function under the optimal policy is:
 tensor([[ 1.],
 [ 0.],
 [-1.]])
```

è¿™ä¸Žå¥–åŠ±å‡½æ•°æ˜¯ä¸€è‡´çš„ï¼Œå› ä¸ºæˆ‘ä»¬åªçœ‹ä¸‹ä¸€æ­¥æ£‹å¾—åˆ°çš„å¥–åŠ±ã€‚

éšç€æŠ˜æ‰£ç³»æ•°å‘ 1 å¢žåŠ ï¼Œå°†è€ƒè™‘æœªæ¥çš„å¥–åŠ±ã€‚è®©æˆ‘ä»¬çœ‹çœ‹ð²=0.99:

```py
 >>> gamma = 0.99
 >>> V = cal_value_matrix_inversion(gamma, trans_matrix, R)
 >>> print("The value function under the optimal policy is:\n{}".format(V))
 The value function under the optimal policy is:
 tensor([[65.8293],
 [64.7194],
 [63.4876]])
```



# è¯·å‚è§

æœ¬å¤‡å¿˜å•[https://cs-cheat sheet . readthedocs . io/en/latest/subjects/ai/mdp . html](https://cs-cheatsheet.readthedocs.io/en/latest/subjects/ai/mdp.html)æ˜¯ MDP çš„å¿«é€Ÿå‚è€ƒã€‚



# æ‰§è¡Œç­–ç•¥è¯„ä¼°

æˆ‘ä»¬åˆšåˆšå¼€å‘äº†ä¸€ä¸ª MDPï¼Œå¹¶ä½¿ç”¨çŸ©é˜µæ±‚é€†è®¡ç®—äº†æœ€ä¼˜ç­–ç•¥çš„ä»·å€¼å‡½æ•°ã€‚æˆ‘ä»¬ä¹Ÿæåˆ°äº† m * m çŸ©é˜µæ±‚å¤§ m å€¼(æ¯”å¦‚è¯´ 1000ï¼Œ10000ï¼Œæˆ–è€… 100000)çš„é€†çŸ©é˜µçš„å±€é™æ€§ã€‚åœ¨è¿™ä¸ªèœè°±ä¸­ï¼Œæˆ‘ä»¬å°†è®¨è®ºä¸€ç§æ›´ç®€å•çš„æ–¹æ³•ï¼Œç§°ä¸º**ç­–ç•¥è¯„ä¼°**ã€‚

ç­–ç•¥è¯„ä¼°æ˜¯ä¸€ç§è¿­ä»£ç®—æ³•ã€‚å®ƒä»Žä»»æ„ç­–ç•¥å€¼å¼€å§‹ï¼Œç„¶åŽåŸºäºŽ**è´å°”æ›¼æœŸæœ›æ–¹ç¨‹**è¿­ä»£æ›´æ–°è¿™äº›å€¼ï¼Œç›´åˆ°å®ƒä»¬æ”¶æ•›ã€‚åœ¨æ¯æ¬¡è¿­ä»£ä¸­ï¼Œå¯¹äºŽçŠ¶æ€ *s* ï¼Œç­–ç•¥çš„å€¼ *Ï€* è¢«æ›´æ–°å¦‚ä¸‹:

![](img/3f9f1117-0f84-4327-808c-1923adad27b8.png)

è¿™é‡Œï¼Œ *Ï€(sï¼Œa)* è¡¨ç¤ºåœ¨ç­–ç•¥ *Ï€* ä¸‹ï¼Œåœ¨çŠ¶æ€ *s* é‡‡å–è¡ŒåŠ¨ *a* çš„æ¦‚çŽ‡ã€‚ *T(sï¼Œaï¼Œsâ€™)*æ˜¯é€šè¿‡é‡‡å–è¡ŒåŠ¨ *a* ä»ŽçŠ¶æ€ *s* åˆ°çŠ¶æ€*sâ€™*çš„è½¬ç§»æ¦‚çŽ‡ï¼Œå¹¶ä¸” *R(sï¼Œa)* æ˜¯é€šè¿‡é‡‡å–è¡ŒåŠ¨ *a* åœ¨çŠ¶æ€ *s* ä¸­æ”¶åˆ°çš„å¥–åŠ±ã€‚

æœ‰ä¸¤ç§æ–¹æ³•å¯ä»¥ç»ˆæ­¢è¿­ä»£æ›´æ–°è¿‡ç¨‹ã€‚ä¸€ç§æ˜¯é€šè¿‡è®¾ç½®å›ºå®šçš„è¿­ä»£æ¬¡æ•°ï¼Œæ¯”å¦‚ 1ï¼Œ000 å’Œ 10ï¼Œ000ï¼Œè¿™æœ‰æ—¶å¯èƒ½å¾ˆéš¾æŽ§åˆ¶ã€‚å¦ä¸€ç§æ–¹æ³•åŒ…æ‹¬æŒ‡å®šä¸€ä¸ªé˜ˆå€¼(é€šå¸¸ä¸º 0.0001ã€0.00001 æˆ–ç±»ä¼¼çš„å€¼)ï¼Œå¹¶ä¸”åªæœ‰å½“æ‰€æœ‰çŠ¶æ€çš„å€¼å˜åŒ–åˆ°ä½ŽäºŽæŒ‡å®šé˜ˆå€¼çš„ç¨‹åº¦æ—¶æ‰ç»ˆæ­¢è¯¥è¿‡ç¨‹ã€‚

åœ¨ä¸‹ä¸€èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†åœ¨æœ€ä¼˜ç­–ç•¥å’Œéšæœºç­–ç•¥ä¸‹å¯¹å­¦ä¹ -ç¡çœ -æ¸¸æˆè¿‡ç¨‹è¿›è¡Œç­–ç•¥è¯„ä¼°ã€‚



# æ€Žä¹ˆåš...

è®©æˆ‘ä»¬å¼€å‘ä¸€ä¸ªç­–ç•¥è¯„ä¼°ç®—æ³•ï¼Œå¹¶å°†å…¶åº”ç”¨äºŽæˆ‘ä»¬çš„å­¦ä¹ -ç¡çœ -æ¸¸æˆæµç¨‹ï¼Œå¦‚ä¸‹æ‰€ç¤º:

1.  å¯¼å…¥ PyTorch å¹¶å®šä¹‰è½¬æ¢çŸ©é˜µ:

```py
 >>> import torch
 >>> T = torch.tensor([[[0.8, 0.1, 0.1],
 ...                    [0.1, 0.6, 0.3]],
 ...                   [[0.7, 0.2, 0.1],
 ...                    [0.1, 0.8, 0.1]],
 ...                   [[0.6, 0.2, 0.2],
 ...                    [0.1, 0.4, 0.5]]]
 ...                  )
```

2.  å®šä¹‰å¥–åŠ±å‡½æ•°å’ŒæŠ˜æ‰£å› å­(çŽ°åœ¨æˆ‘ä»¬ç”¨`0.5`):

```py
 >>> R = torch.tensor([1., 0, -1.])
 >>> gamma = 0.5
```

3.  å®šä¹‰ç”¨äºŽç¡®å®šä½•æ—¶åœæ­¢è¯„ä¼°æµç¨‹çš„é˜ˆå€¼:

```py
 >>> threshold = 0.0001
```

4.  å®šä¹‰åœ¨æ‰€æœ‰æƒ…å†µä¸‹é€‰æ‹©è¡ŒåŠ¨ a0 çš„æœ€ä½³ç­–ç•¥:

```py
 >>> policy_optimal = torch.tensor([[1.0, 0.0],
 ...                                [1.0, 0.0],
 ...                                [1.0, 0.0]])
```

5.  å¼€å‘ä¸€ä¸ªæ”¿ç­–è¯„ä¼°å‡½æ•°ï¼Œè¯¥å‡½æ•°åŒ…å«æ”¿ç­–ã€è½¬ç§»çŸ©é˜µã€å¥–åŠ±ã€æŠ˜æ‰£ç³»æ•°å’Œé˜ˆå€¼ï¼Œå¹¶è®¡ç®—`value`å‡½æ•°:

```py
>>> def policy_evaluation(
 policy, trans_matrix, rewards, gamma, threshold):
...     """
...     Perform policy evaluation
...     @param policy: policy matrix containing actions and their 
 probability in each state
...     @param trans_matrix: transformation matrix
...     @param rewards: rewards for each state
...     @param gamma: discount factor
...     @param threshold: the evaluation will stop once values 
 for all states are less than the threshold
...     @return: values of the given policy for all possible states
...     """
...     n_state = policy.shape[0]
...     V = torch.zeros(n_state)
...     while True:
...         V_temp = torch.zeros(n_state)
...         for state, actions in enumerate(policy):
...             for action, action_prob in enumerate(actions):
...                 V_temp[state] += action_prob * (R[state] + 
 gamma * torch.dot(
 trans_matrix[state, action], V))
...         max_delta = torch.max(torch.abs(V - V_temp))
...         V = V_temp.clone()
...         if max_delta <= threshold:
...             break
...     return V
```

6.  çŽ°åœ¨è®©æˆ‘ä»¬æ’å…¥æœ€ä¼˜ç­–ç•¥å’Œæ‰€æœ‰å…¶ä»–å˜é‡:

```py
>>> V = policy_evaluation(policy_optimal, T, R, gamma, threshold)
>>> print(
 "The value function under the optimal policy is:\n{}".format(V)) The value function under the optimal policy is:
tensor([ 1.6786,  0.6260, -0.4821])
```

è¿™å‡ ä¹Žå’Œæˆ‘ä»¬ç”¨çŸ©é˜µæ±‚é€†å¾—åˆ°çš„ä¸€æ ·ã€‚

7.  æˆ‘ä»¬çŽ°åœ¨è¯•éªŒå¦ä¸€ç§ç­–ç•¥ï¼Œä¸€ç§éšæœºç­–ç•¥ï¼Œå…¶ä¸­ä»¥ç›¸åŒçš„æ¦‚çŽ‡é€‰æ‹©åŠ¨ä½œ:

```py
>>> policy_random = torch.tensor([[0.5, 0.5],
...                               [0.5, 0.5],
...                               [0.5, 0.5]])
```

8.  æ’å…¥éšæœºç­–ç•¥å’Œæ‰€æœ‰å…¶ä»–å˜é‡:

```py
>>> V = policy_evaluation(policy_random, T, R, gamma, threshold)
>>> print(
 "The value function under the random policy is:\n{}".format(V))
The value function under the random policy is:
tensor([ 1.2348,  0.2691, -0.9013])
```



# å®ƒæ˜¯å¦‚ä½•å·¥ä½œçš„...

æˆ‘ä»¬åˆšåˆšçœ‹åˆ°äº†ä½¿ç”¨ç­–ç•¥è¯„ä¼°æ¥è®¡ç®—ç­–ç•¥çš„ä»·å€¼æ˜¯å¤šä¹ˆæœ‰æ•ˆã€‚è¿™æ˜¯ä¸€ç§ç®€å•çš„æ”¶æ•›è¿­ä»£æ–¹æ³•ï¼Œåœ¨**åŠ¨æ€è§„åˆ’æ—**ä¸­ï¼Œæˆ–è€…æ›´å…·ä½“åœ°è¯´ï¼Œ**è¿‘ä¼¼åŠ¨æ€è§„åˆ’**ã€‚å®ƒä»Žå¯¹å€¼çš„éšæœºçŒœæµ‹å¼€å§‹ï¼Œç„¶åŽæ ¹æ®è´å°”æ›¼æœŸæœ›æ–¹ç¨‹è¿­ä»£åœ°æ›´æ–°å®ƒä»¬ï¼Œç›´åˆ°å®ƒä»¬æ”¶æ•›ã€‚

åœ¨æ­¥éª¤ 5 ä¸­ï¼Œç­–ç•¥è¯„ä¼°åŠŸèƒ½æ‰§è¡Œä»¥ä¸‹ä»»åŠ¡:

*   å°†ç­–ç•¥å€¼åˆå§‹åŒ–ä¸ºå…¨é›¶ã€‚
*   åŸºäºŽè´å°”æ›¼æœŸæœ›æ–¹ç¨‹æ›´æ–°å€¼ã€‚
*   è®¡ç®—æ‰€æœ‰çŠ¶æ€å€¼çš„æœ€å¤§å˜åŒ–ã€‚
*   å¦‚æžœæœ€å¤§å˜åŒ–å¤§äºŽé˜ˆå€¼ï¼Œå®ƒä¼šä¸æ–­æ›´æ–°å€¼ã€‚å¦åˆ™ï¼Œå®ƒç»ˆæ­¢è¯„ä¼°è¿‡ç¨‹å¹¶è¿”å›žæœ€æ–°å€¼ã€‚

ç”±äºŽç­–ç•¥è¯„ä¼°ä½¿ç”¨è¿­ä»£è¿‘ä¼¼ï¼Œå…¶ç»“æžœå¯èƒ½ä¸Žä½¿ç”¨ç²¾ç¡®è®¡ç®—çš„çŸ©é˜µæ±‚é€†æ–¹æ³•çš„ç»“æžœä¸å®Œå…¨ç›¸åŒã€‚äº‹å®žä¸Šï¼Œæˆ‘ä»¬çœŸçš„ä¸éœ€è¦ä»·å€¼å‡½æ•°é‚£ä¹ˆç²¾ç¡®ã€‚æ­¤å¤–ï¼Œå®ƒå¯ä»¥è§£å†³ç»´åº¦çš„**è¯…å’’** **é—®é¢˜ï¼Œè¿™å¯ä»¥å¯¼è‡´å°†è®¡ç®—è§„æ¨¡æ‰©å¤§åˆ°æ•°åä¸‡ä¸ªçŠ¶æ€ã€‚å› æ­¤ï¼Œæˆ‘ä»¬é€šå¸¸æ›´å–œæ¬¢æ”¿ç­–è¯„ä¼°ã€‚**

è¿˜æœ‰ä¸€ç‚¹è¦è®°ä½çš„æ˜¯ï¼Œæ”¿ç­–è¯„ä¼°æ˜¯ç”¨æ¥**é¢„æµ‹**æˆ‘ä»¬å°†ä»Žä¸€ä¸ªç»™å®šçš„æ”¿ç­–ä¸­èŽ·å¾—å¤šå¤§çš„å¥½å¤„ï¼›å®ƒä¸æ˜¯ç”¨äºŽ**æŽ§åˆ¶**çš„é—®é¢˜ã€‚



# è¿˜æœ‰æ›´å¤š...

ä¸ºäº†è¿›ä¸€æ­¥äº†è§£ï¼Œæˆ‘ä»¬è¿˜ç»˜åˆ¶äº†æ•´ä¸ªè¯„ä¼°è¿‡ç¨‹ä¸­çš„ç­–ç•¥å€¼ã€‚

æˆ‘ä»¬é¦–å…ˆéœ€è¦è®°å½•`policy_evaluation`å‡½æ•°ä¸­æ¯æ¬¡è¿­ä»£çš„å€¼:

```py
>>> def policy_evaluation_history(
 policy, trans_matrix, rewards, gamma, threshold):
...     n_state = policy.shape[0]
...     V = torch.zeros(n_state)
...     V_his = [V]
...     i = 0
...     while True:
...         V_temp = torch.zeros(n_state)
...         i += 1
...         for state, actions in enumerate(policy):
...             for action, action_prob in enumerate(actions):
...                 V_temp[state] += action_prob * (R[state] + gamma * 
 torch.dot(trans_matrix[state, action], V))
...         max_delta = torch.max(torch.abs(V - V_temp))
...         V = V_temp.clone()
...         V_his.append(V)
...         if max_delta <= threshold:
...             break
...     return V, V_his
```

çŽ°åœ¨ï¼Œæˆ‘ä»¬å‘`policy_evaluation_history`å‡½æ•°è¾“å…¥æœ€ä¼˜ç­–ç•¥ã€è´´çŽ°å› å­`0.5`å’Œå…¶ä»–å˜é‡:

```py
>>> V, V_history = policy_evaluation_history(
 policy_optimal, T, R, gamma, threshold)
```

ç„¶åŽï¼Œæˆ‘ä»¬ä½¿ç”¨ä»¥ä¸‹ä»£ç è¡Œç»˜åˆ¶ç»“æžœå€¼çš„åŽ†å²è®°å½•:

```py
>>> import matplotlib.pyplot as plt
>>> s0, = plt.plot([v[0] for v in V_history])
>>> s1, = plt.plot([v[1] for v in V_history])
>>> s2, = plt.plot([v[2] for v in V_history])
>>> plt.title('Optimal policy with gamma = {}'.format(str(gamma)))
>>> plt.xlabel('Iteration')
>>> plt.ylabel('Policy values')
>>> plt.legend([s0, s1, s2],
...            ["State s0",
...             "State s1",
...             "State s2"], loc="upper left")
>>> plt.show()
```

æˆ‘ä»¬çœ‹åˆ°ä»¥ä¸‹ç»“æžœ:

![](img/51417194-cf66-4db6-8e61-2d782b6981f6.png)

æœ‰è¶£çš„æ˜¯åœ¨æ”¶æ•›æœŸé—´çœ‹åˆ°è¿­ä»£ 10 åˆ° 14 ä¹‹é—´çš„ç¨³å®šã€‚

æŽ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬è¿è¡Œç›¸åŒçš„ä»£ç ï¼Œä½†æ˜¯ä½¿ç”¨ä¸¤ä¸ªä¸åŒçš„æŠ˜æ‰£å› å­ï¼Œ0.2 å’Œ 0.99ã€‚æˆ‘ä»¬å¾—åˆ°äº†è´´çŽ°å› å­ä¸º 0.2 çš„ä¸‹å›¾:

![](img/f874a0f1-4024-4877-ad18-4240810a31ae.png)

å°†æŠ˜æ‰£ç³»æ•°ä¸º 0.5 çš„å›¾ä¸Žæ­¤å›¾è¿›è¡Œæ¯”è¾ƒï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œç³»æ•°è¶Šå°ï¼Œç­–ç•¥å€¼æ”¶æ•›è¶Šå¿«ã€‚

æˆ‘ä»¬è¿˜å¾—åˆ°äº†è´´çŽ°å› å­ä¸º 0.99 çš„ä¸‹å›¾:

![](img/e9ddb167-e202-49ec-baa5-8f8e2d317e95.png)

é€šè¿‡æ¯”è¾ƒæŠ˜æ‰£å› å­ä¸º 0.5 çš„å›¾å’ŒæŠ˜æ‰£å› å­ä¸º 0.99 çš„å›¾ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œå› å­è¶Šå¤§ï¼Œç­–ç•¥å€¼æ”¶æ•›æ‰€éœ€çš„æ—¶é—´è¶Šé•¿ã€‚æŠ˜æ‰£å› å­æ˜¯çŽ°åœ¨çš„å¥–åŠ±å’Œæœªæ¥çš„å¥–åŠ±ä¹‹é—´çš„æƒè¡¡ã€‚



# æ¨¡æ‹Ÿå†°å†»æ¹–çŽ¯å¢ƒ

åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘ä»¬å¤„ç†çš„ MDP çš„æœ€ä½³ç­–ç•¥æ˜¯éžå¸¸ç›´è§‚çš„ã€‚ç„¶è€Œï¼Œåœ¨å¤§å¤šæ•°æƒ…å†µä¸‹ä¸ä¼šé‚£ä¹ˆç®€å•ï¼Œæ¯”å¦‚ FrozenLake çŽ¯å¢ƒã€‚åœ¨è¿™ä¸ªé£Ÿè°±ä¸­ï¼Œè®©æˆ‘ä»¬çŽ©çŽ© FrozenLake çŽ¯å¢ƒï¼Œå¹¶ä¸ºå³å°†åˆ°æ¥çš„é£Ÿè°±åšå¥½å‡†å¤‡ï¼Œåœ¨é‚£é‡Œæˆ‘ä»¬å°†æ‰¾åˆ°å®ƒçš„æœ€ä½³ç­–ç•¥ã€‚

FrozenLake æ˜¯ä¸€ä¸ªå…¸åž‹çš„å¥èº«æˆ¿çŽ¯å¢ƒï¼Œå…·æœ‰ä¸€ä¸ª**ç¦»æ•£çš„**çŠ¶æ€ç©ºé—´ã€‚å®ƒæ˜¯å…³äºŽåœ¨ä¸€ä¸ªç½‘æ ¼ä¸–ç•Œä¸­å°†ä¸€ä¸ªä»£ç†ä»Žèµ·å§‹ä½ç½®ç§»åŠ¨åˆ°ç›®æ ‡ä½ç½®ï¼ŒåŒæ—¶é¿å…é™·é˜±ã€‚ç½‘æ ¼è¦ä¹ˆæ˜¯å››ä¹˜å››(ã€https://gym.openai.com/envs/FrozenLake-v0/ã€‘)è¦ä¹ˆæ˜¯å…«ä¹˜å…«ã€‚

t([https://gym.openai.com/envs/FrozenLake8x8-v0/](https://gym.openai.com/envs/FrozenLake8x8-v0/))ã€‚ç½‘æ ¼ç”±ä»¥ä¸‹å››ç§ç±»åž‹çš„å•å¹…å›¾å—ç»„æˆ:

*   **S** :èµ·å§‹ä½ç½®
*   **G** :ç›®æ ‡ä½ç½®ï¼Œç»“æŸä¸€é›†
*   **F** :å†°å†»çš„ç“·ç –ï¼Œè¿™æ˜¯ä¸€ä¸ªå¯ä»¥è¡Œèµ°çš„ä½ç½®
*   **H** :ç»“æŸä¸€é›†çš„æ´žä½

å¾ˆæ˜Žæ˜¾ï¼Œæœ‰å››ä¸ªåŠ¨ä½œ:å‘å·¦ç§»åŠ¨(0)ã€å‘ä¸‹ç§»åŠ¨(1)ã€å‘å³ç§»åŠ¨(2)å’Œå‘ä¸Šç§»åŠ¨(3)ã€‚å¦‚æžœä»£ç†æˆåŠŸåˆ°è¾¾ç›®æ ‡ä½ç½®ï¼Œå¥–åŠ±ä¸º+1ï¼Œå¦åˆ™ä¸º 0ã€‚è¿˜æœ‰ï¼Œè§‚å¯Ÿç©ºé—´ç”¨ 16 ç»´æ•´æ•°æ•°ç»„è¡¨ç¤ºï¼Œæœ‰ 4 ä¸ªå¯èƒ½çš„åŠ¨ä½œ(æœ‰æ„ä¹‰)ã€‚

åœ¨è¿™ç§çŽ¯å¢ƒä¸‹ï¼Œæ£˜æ‰‹çš„æ˜¯ï¼Œç”±äºŽå†°é¢å¾ˆæ»‘ï¼Œä»£ç†äººä¸ä¼šæ€»æ˜¯æœç€å®ƒæƒ³è¦çš„æ–¹å‘ç§»åŠ¨ã€‚ä¾‹å¦‚ï¼Œå½“å®ƒæ‰“ç®—å‘ä¸‹ç§»åŠ¨æ—¶ï¼Œå®ƒå¯ä»¥å‘å·¦æˆ–å‘å³ç§»åŠ¨ã€‚



# åšå¥½å‡†å¤‡

è¦è¿è¡Œ FrozenLake çŽ¯å¢ƒï¼Œè®©æˆ‘ä»¬é¦–å…ˆåœ¨è¿™é‡Œçš„çŽ¯å¢ƒè¡¨ä¸­æœç´¢å®ƒ:[https://github.com/openai/gym/wiki/Table-of-environments](https://github.com/openai/gym/wiki/Table-of-environments)ã€‚æœç´¢ç»™äº†æˆ‘ä»¬`FrozenLake-v0`ã€‚



# æ€Žä¹ˆåš...

è®©æˆ‘ä»¬æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤æ¨¡æ‹Ÿå››ä¹˜å››çš„ FrozenLake çŽ¯å¢ƒ:

1.  æˆ‘ä»¬å¯¼å…¥`gym`åº“å¹¶åˆ›å»ºä¸€ä¸ª FrozenLake çŽ¯å¢ƒçš„å®žä¾‹:

```py
>>> import gym
>>> import torch
>>> env = gym.make("FrozenLake-v0")
>>> n_state = env.observation_space.n
>>> print(n_state)
16
>>> n_action = env.action_space.n
>>> print(n_action)
4
```

2.  é‡ç½®çŽ¯å¢ƒ:

```py
>>> env.reset()
0
```

ä»£ç†ä»ŽçŠ¶æ€`0`å¼€å§‹ã€‚

3.  æ¸²æŸ“çŽ¯å¢ƒ:

```py
>>> env.render()
```

4.  è®©æˆ‘ä»¬åšä¸€ä¸ªå‘ä¸‹çš„è¿åŠ¨ï¼Œå› ä¸ºå®ƒæ˜¯å¯ä»¥è¡Œèµ°çš„:

```py
>>> new_state, reward, is_done, info = env.step(1)
>>> env.render()
```

6.  æ‰“å°å‡ºæ‰€æœ‰è¿”å›žä¿¡æ¯ï¼Œç¡®è®¤ä»£ç†ä»¥ 33.33%çš„æ¦‚çŽ‡é™è½åœ¨çŠ¶æ€`4`:

```py
>>> print(new_state)
4
>>> print(reward)
0.0
>>> print(is_done)
False
>>> print(info)
{'prob': 0.3333333333333333}
```

ä½ å¾—åˆ°`0`ä½œä¸ºå¥–åŠ±ï¼Œå› ä¸ºå®ƒè¿˜æ²¡æœ‰è¾¾åˆ°ç›®æ ‡ï¼Œè¿™ä¸€é›†è¿˜æ²¡æœ‰å®Œæˆã€‚åŒæ ·ï¼Œä½ å¯èƒ½ä¼šçœ‹åˆ°ä»£ç†é™è½åœ¨çŠ¶æ€ 1ï¼Œæˆ–åœç•™åœ¨çŠ¶æ€ 0ï¼Œå› ä¸ºå…‰æ»‘çš„è¡¨é¢ã€‚

7.  ä¸ºäº†æ¼”ç¤ºåœ¨å†°å†»çš„æ¹–é¢ä¸Šè¡Œèµ°æœ‰å¤šå›°éš¾ï¼Œå®žæ–½éšæœºç­–ç•¥å¹¶è®¡ç®— 1000 é›†ä»¥ä¸Šçš„å¹³å‡æ€»å¥–åŠ±ã€‚é¦–å…ˆï¼Œå®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œæ¨¡æ‹Ÿç»™å®šç­–ç•¥çš„ FrozenLake æƒ…èŠ‚ï¼Œå¹¶è¿”å›žæ€»æŠ¥é…¬(æˆ‘ä»¬çŸ¥é“å®ƒä¸æ˜¯ 0 å°±æ˜¯ 1):

```py
>>> def run_episode(env, policy):
...     state = env.reset()
...     total_reward = 0
...     is_done = False
...     while not is_done:
...         action = policy[state].item()
...         state, reward, is_done, info = env.step(action)
...         total_reward += reward
...         if is_done:
...             break
...     return total_reward
```

8.  çŽ°åœ¨è¿è¡Œ`1000`é›†ï¼Œå°†éšæœºç”Ÿæˆä¸€ä¸ªç­–ç•¥ï¼Œå¹¶å°†åœ¨æ¯é›†ä¸­ä½¿ç”¨:

```py
>>> n_episode = 1000
>>> total_rewards = []
>>> for episode in range(n_episode):
...     random_policy = torch.randint(
 high=n_action, size=(n_state,))
...     total_reward = run_episode(env, random_policy)
...     total_rewards.append(total_reward)
...
>>> print('Average total reward under random policy: {}'.format(
 sum(total_rewards) / n_episode))
Average total reward under random policy: 0.014
```

è¿™åŸºæœ¬ä¸Šæ„å‘³ç€ï¼Œå¦‚æžœæˆ‘ä»¬éšæœºè¡ŒåŠ¨ï¼Œä»£ç†äººå¹³å‡åªæœ‰ 1.4%çš„æœºä¼šè¾¾åˆ°ç›®æ ‡ã€‚

9.  æŽ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬ç”¨éšæœºæœç´¢ç­–ç•¥è¿›è¡Œå®žéªŒã€‚åœ¨è®­ç»ƒé˜¶æ®µï¼Œæˆ‘ä»¬éšæœºç”Ÿæˆä¸€ç»„ç­–ç•¥ï¼Œå¹¶è®°å½•ç¬¬ä¸€ä¸ªè¾¾åˆ°ç›®æ ‡çš„ç­–ç•¥:

```py
>>> while True:
...     random_policy = torch.randint(
 high=n_action, size=(n_state,))
...     total_reward = run_episode(env, random_policy)
...     if total_reward == 1:
...         best_policy = random_policy
...         break
```

10.  çœ‹ä¸€çœ‹æœ€ä½³ç­–ç•¥:

```py
>>> print(best_policy)
tensor([0, 3, 2, 2, 0, 2, 1, 1, 3, 1, 3, 0, 0, 1, 1, 1])
```

11.  çŽ°åœ¨ç”¨æˆ‘ä»¬åˆšåˆšæŒ‘é€‰çš„ç­–ç•¥è¿è¡Œ 1000 é›†:

```py
>>> total_rewards = []
>>> for episode in range(n_episode):
...     total_reward = run_episode(env, best_policy)
...     total_rewards.append(total_reward)
...
>>> print('Average total reward under random search 
     policy: {}'.format(sum(total_rewards) / n_episode))
Average total reward under random search policy: 0.208
```

ä½¿ç”¨éšæœºæœç´¢ç®—æ³•ï¼Œå¹³å‡æœ‰ 20.8%çš„æœºä¼šè¾¾åˆ°ç›®æ ‡ã€‚

è¯·æ³¨æ„ï¼Œè¿™ä¸ªç»“æžœå¯èƒ½ä¼šæœ‰å¾ˆå¤§çš„å˜åŒ–ï¼Œå› ä¸ºæˆ‘ä»¬é€‰æ‹©çš„ç­–ç•¥å¯èƒ½ä¼šå› ä¸ºå…‰æ»‘çš„å†°è€Œç¢°å·§è¾¾åˆ°ç›®æ ‡ï¼Œå¹¶ä¸”å¯èƒ½ä¸æ˜¯æœ€ä½³çš„ã€‚



# å®ƒæ˜¯å¦‚ä½•å·¥ä½œçš„...

åœ¨è¿™ä¸ªèœè°±ä¸­ï¼Œæˆ‘ä»¬éšæœºç”Ÿæˆäº†ä¸€ä¸ªç­–ç•¥ï¼Œå®ƒç”± 16 ä¸ªå·žçš„ 16 ä¸ªæ“ä½œç»„æˆã€‚è¯·è®°ä½ï¼Œåœ¨ FrozenLake ä¸­ï¼Œç§»åŠ¨æ–¹å‘åªæ˜¯éƒ¨åˆ†å–å†³äºŽæ‰€é€‰çš„åŠ¨ä½œã€‚è¿™å¢žåŠ äº†æŽ§åˆ¶çš„ä¸ç¡®å®šæ€§ã€‚

åœ¨è¿è¡Œäº†*æ­¥éª¤ 4* ä¸­çš„ä»£ç åŽï¼Œæ‚¨å°†çœ‹åˆ°å¦‚ä¸‹ 4 * 4 çš„çŸ©é˜µï¼Œä»£è¡¨å†»ç»“çš„æ¹–å’Œä»£ç†æ‰€åœ¨çš„ç“¦ç‰‡(çŠ¶æ€ 0):

![](img/aa71c6fa-1ea2-4769-9588-2e08637c775c.png)

åœ¨è¿è¡Œäº†*æ­¥éª¤ 5* ä¸­çš„ä»£ç è¡Œä¹‹åŽï¼Œæ‚¨å°†çœ‹åˆ°å¦‚ä¸‹çš„ç»“æžœç½‘æ ¼ï¼Œå…¶ä¸­ä»£ç†å‘ä¸‹ç§»åŠ¨åˆ°çŠ¶æ€ 4:

![](img/c1deaef9-3cbb-4ce1-9181-1ca34373ce78.png)

å¦‚æžœæ»¡è¶³ä»¥ä¸‹ä¸¤ä¸ªæ¡ä»¶ä¸­çš„ä»»ä½•ä¸€ä¸ªï¼Œå‰§é›†å°†ä¼šç»ˆæ­¢:

*   ç§»åŠ¨åˆ° H å›¾å—(çŠ¶æ€ 5ã€7ã€11ã€12)ã€‚è¿™å°†äº§ç”Ÿæ€»å¥–åŠ± 0ã€‚
*   ç§»åŠ¨åˆ° G ç“¦ç‰‡(çŠ¶æ€ 15)ã€‚è¿™å°†äº§ç”Ÿ+1 çš„æ€»å¥–åŠ±ã€‚



# è¿˜æœ‰æ›´å¤š...

æˆ‘ä»¬å¯ä»¥é€šè¿‡ä½¿ç”¨ P å±žæ€§æ¥æŸ¥çœ‹ FrozenLake çŽ¯å¢ƒçš„ç»†èŠ‚ï¼ŒåŒ…æ‹¬æ¯ä¸ªçŠ¶æ€å’ŒåŠ¨ä½œçš„è½¬æ¢çŸ©é˜µå’Œå¥–åŠ±ã€‚ä¾‹å¦‚ï¼Œå¯¹äºŽçŠ¶æ€ 6ï¼Œæˆ‘ä»¬å¯ä»¥æ‰§è¡Œä»¥ä¸‹æ“ä½œ:

```py
>>> print(env.env.P[6])
{0: [(0.3333333333333333, 2, 0.0, False), (0.3333333333333333, 5, 0.0, True), (0.3333333333333333, 10, 0.0, False)], 1: [(0.3333333333333333, 5, 0.0, True), (0.3333333333333333, 10, 0.0, False), (0.3333333333333333, 7, 0.0, True)], 2: [(0.3333333333333333, 10, 0.0, False), (0.3333333333333333, 7, 0.0, True), (0.3333333333333333, 2, 0.0, False)], 3: [(0.3333333333333333, 7, 0.0, True), (0.3333333333333333, 2, 0.0, False), (0.3333333333333333, 5, 0.0, True)]}
```

è¿™å°†è¿”å›žä¸€ä¸ªåŒ…å«é”® 0ã€1ã€2 å’Œ 3 çš„å­—å…¸ï¼Œä»£è¡¨å››ç§å¯èƒ½çš„æ“ä½œã€‚è¯¥å€¼æ˜¯é‡‡å–åŠ¨ä½œåŽçš„åŠ¨ä½œåˆ—è¡¨ã€‚ç§»åŠ¨åˆ—è¡¨çš„æ ¼å¼å¦‚ä¸‹:(è½¬åŒ–æ¦‚çŽ‡ï¼Œæ–°çŠ¶æ€ï¼Œæ”¶åˆ°å¥–åŠ±ï¼Œå®Œæˆ)ã€‚ä¾‹å¦‚ï¼Œå¦‚æžœä»£ç†é©»ç•™åœ¨çŠ¶æ€ 6 å¹¶ä¸”æ‰“ç®—é‡‡å–åŠ¨ä½œ 1(å‘ä¸‹)ï¼Œåˆ™æœ‰ 33.33%çš„æœºä¼šå®ƒå°†åˆ°è¾¾çŠ¶æ€ 5ï¼ŒæŽ¥æ”¶å¥–åŠ± 0 å¹¶ä¸”ç»ˆæ­¢è¯¥æƒ…èŠ‚ï¼›æœ‰ 33.33%çš„å‡ çŽ‡é™è½åœ¨çŠ¶æ€ 10ï¼ŒèŽ·å¾—å¥–åŠ± 0ï¼›å¹¶ä¸”æœ‰ 33.33%çš„å‡ çŽ‡é™è½åœ¨çŠ¶æ€ 7ï¼ŒèŽ·å¾—å¥–åŠ± 0ï¼Œç»ˆæ­¢å‰§é›†ã€‚

å¯¹äºŽçŠ¶æ€ 11ï¼Œæˆ‘ä»¬å¯ä»¥æ‰§è¡Œä»¥ä¸‹æ“ä½œ:

```py
>>> print(env.env.P[11])
{0: [(1.0, 11, 0, True)], 1: [(1.0, 11, 0, True)], 2: [(1.0, 11, 0, True)], 3: [(1.0, 11, 0, True)]}
```

ç”±äºŽè¸©ä¸€ä¸ªæ´žä¼šç»ˆæ­¢ä¸€é›†ï¼Œä¹‹åŽå®ƒä¸ä¼šæœ‰ä»»ä½•åŠ¨ä½œã€‚

è¯·éšæ„æŸ¥çœ‹å…¶ä»–å·žã€‚



# ç”¨æ•°å€¼è¿­ä»£ç®—æ³•æ±‚è§£ MDP

å¦‚æžœæ‰¾åˆ°äº† MDP çš„æœ€ä¼˜ç­–ç•¥ï¼Œåˆ™è®¤ä¸ºè¯¥é—®é¢˜å·²è§£å†³ã€‚åœ¨è¿™ä¸ªèœè°±ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ä¸€ä¸ª**å€¼è¿­ä»£**ç®—æ³•æ¥è®¡ç®—å‡º FrozenLake çŽ¯å¢ƒçš„æœ€ä½³ç­–ç•¥ã€‚

ä»·å€¼è¿­ä»£èƒŒåŽçš„æ€æƒ³ä¸Žæ”¿ç­–è¯„ä¼°éžå¸¸ç›¸ä¼¼ã€‚å®ƒä¹Ÿæ˜¯ä¸€ç§è¿­ä»£ç®—æ³•ã€‚å®ƒä»Žä»»æ„ç­–ç•¥å€¼å¼€å§‹ï¼Œç„¶åŽåŸºäºŽ**è´å°”æ›¼æœ€ä¼˜æ–¹ç¨‹**è¿­ä»£æ›´æ–°è¿™äº›å€¼ï¼Œç›´åˆ°å®ƒä»¬æ”¶æ•›ã€‚å› æ­¤ï¼Œåœ¨æ¯æ¬¡è¿­ä»£ä¸­ï¼Œå®ƒä¸æ˜¯å–æ‰€æœ‰æ“ä½œçš„æœŸæœ›å€¼(å¹³å‡å€¼)ï¼Œè€Œæ˜¯é€‰å–å®žçŽ°æœ€å¤§ç­–ç•¥å€¼çš„æ“ä½œ:

![](img/aa401157-f4e2-414b-9843-6b221c86fa9f.png)

è¿™é‡Œï¼ŒV*(s)è¡¨ç¤ºæœ€ä¼˜å€¼ï¼Œå³æœ€ä¼˜ç­–ç•¥çš„å€¼ï¼›T(sï¼Œaï¼Œsâ€™)æ˜¯é€šè¿‡é‡‡å–åŠ¨ä½œ a ä»ŽçŠ¶æ€ s åˆ°çŠ¶æ€ sâ€™çš„è½¬ç§»æ¦‚çŽ‡ï¼›R(sï¼Œa)æ˜¯åœ¨çŠ¶æ€ s é‡‡å–è¡ŒåŠ¨ a å¾—åˆ°çš„å›žæŠ¥ã€‚

ä¸€æ—¦è®¡ç®—å‡ºæœ€ä¼˜å€¼ï¼Œæˆ‘ä»¬å°±å¯ä»¥å¾ˆå®¹æ˜“åœ°èŽ·å¾—ç›¸åº”çš„æœ€ä¼˜ç­–ç•¥:

![](img/94ddcae0-0acc-4516-a10b-c27bb79080ea.png)

# æ€Žä¹ˆåš...

è®©æˆ‘ä»¬ä½¿ç”¨å€¼è¿­ä»£ç®—æ³•æ±‚è§£ FrozenLake çŽ¯å¢ƒï¼Œå¦‚ä¸‹æ‰€ç¤º:

1.  æˆ‘ä»¬å¯¼å…¥å¿…è¦çš„åº“å¹¶åˆ›å»ºä¸€ä¸ª FrozenLake çŽ¯å¢ƒçš„å®žä¾‹:

```py
>>> import torch
>>> import gym
>>> env = gym.make('FrozenLake-v0')
```

2.  çŽ°åœ¨å°†`0.99`è®¾ä¸ºæŠ˜æ‰£å› å­ï¼Œå°†`0.0001`è®¾ä¸ºæ”¶æ•›é˜ˆå€¼:

```py
>>> gamma = 0.99
>>> threshold = 0.0001
```

3.  çŽ°åœ¨å®šä¹‰åŸºäºŽå€¼è¿­ä»£ç®—æ³•è®¡ç®—æœ€ä½³å€¼çš„å‡½æ•°:

```py
>>> def value_iteration(env, gamma, threshold):
...     """
...     Solve a given environment with value iteration algorithm
...     @param env: OpenAI Gym environment
...     @param gamma: discount factor
...     @param threshold: the evaluation will stop once values for 
 all states are less than the threshold
...     @return: values of the optimal policy for the given 
 environment
...     """
...     n_state = env.observation_space.n
...     n_action = env.action_space.n
...     V = torch.zeros(n_state)
...     while True:
...         V_temp = torch.empty(n_state)
...         for state in range(n_state):
...             v_actions = torch.zeros(n_action)
...             for action in range(n_action):
...                 for trans_prob, new_state, reward, _ in 
 env.env.P[state][action]:
...                     v_actions[action] += trans_prob * (reward 
 + gamma * V[new_state])
...             V_temp[state] = torch.max(v_actions)
...         max_delta = torch.max(torch.abs(V - V_temp))
...         V = V_temp.clone()
...         if max_delta <= threshold:
...             break
...     return V
```

4.  æ’å…¥çŽ¯å¢ƒã€æŠ˜æ‰£å› å­å’Œæ”¶æ•›é˜ˆå€¼ï¼Œç„¶åŽæ‰“å°æœ€ä½³å€¼:

```py
>>> V_optimal = value_iteration(env, gamma, threshold)
>>> print('Optimal values:\n{}'.format(V_optimal))
Optimal values:
tensor([0.5404, 0.4966, 0.4681, 0.4541, 0.5569, 0.0000, 0.3572, 0.0000, 0.5905,
 0.6421, 0.6144, 0.0000, 0.0000, 0.7410, 0.8625, 0.0000])
```

5.  çŽ°åœ¨æˆ‘ä»¬æœ‰äº†æœ€ä½³å€¼ï¼Œæˆ‘ä»¬å¼€å‘äº†ä»Žä¸­æå–æœ€ä½³ç­–ç•¥çš„å‡½æ•°:

```py
>>> def extract_optimal_policy(env, V_optimal, gamma):
...     """
...     Obtain the optimal policy based on the optimal values
...     @param env: OpenAI Gym environment
...     @param V_optimal: optimal values
...     @param gamma: discount factor
...     @return: optimal policy
...     """
...     n_state = env.observation_space.n
...     n_action = env.action_space.n
...     optimal_policy = torch.zeros(n_state)
...     for state in range(n_state):
...         v_actions = torch.zeros(n_action)
...         for action in range(n_action):
...             for trans_prob, new_state, reward, _ in 
                                   env.env.P[state][action]:
...                 v_actions[action] += trans_prob * (reward 
 + gamma * V_optimal[new_state])
...         optimal_policy[state] = torch.argmax(v_actions)
...     return optimal_policy
```

6.  æ’å…¥çŽ¯å¢ƒã€æŠ˜æ‰£ç³»æ•°å’Œæœ€ä¼˜å€¼ï¼Œç„¶åŽæ‰“å°æœ€ä¼˜ç­–ç•¥:

```py
>>> optimal_policy = extract_optimal_policy(env, V_optimal, gamma)
>>> print('Optimal policy:\n{}'.format(optimal_policy))
Optimal policy:
tensor([0., 3., 3., 3., 0., 3., 2., 3., 3., 1., 0., 3., 3., 2., 1., 3.])
```

7.  æˆ‘ä»¬æƒ³è¡¡é‡æœ€ä¼˜ç­–ç•¥æœ‰å¤šå¥½ã€‚æ‰€ä»¥ï¼Œè®©æˆ‘ä»¬ç”¨æœ€ä¼˜ç­–ç•¥è¿è¡Œ 1000 é›†ï¼Œå¹¶æ£€æŸ¥å¹³å‡å›žæŠ¥ã€‚è¿™é‡Œï¼Œæˆ‘ä»¬å°†é‡ç”¨æˆ‘ä»¬åœ¨å‰ä¸€ä¸ªé…æ–¹ä¸­å®šä¹‰çš„`run_episode`å‡½æ•°:

```py
>>> n_episode = 1000
>>> total_rewards = []
>>> for episode in range(n_episode):
...     total_reward = run_episode(env, optimal_policy)
...     total_rewards.append(total_reward)
>>> print('Average total reward under the optimal 
 policy: {}'.format(sum(total_rewards) / n_episode))
Average total reward under the optimal policy: 0.75
```

åœ¨æœ€ä¼˜ç­–ç•¥ä¸‹ï¼Œä»£ç†å¹³å‡æœ‰ 75%çš„æœºä¼šè¾¾åˆ°ç›®æ ‡ã€‚è¿™æ˜¯æˆ‘ä»¬èƒ½å¾—åˆ°çš„æœ€å¥½çš„äº†ï¼Œå› ä¸ºå†°å¾ˆæ»‘ã€‚



# å®ƒæ˜¯å¦‚ä½•å·¥ä½œçš„...

åœ¨å€¼è¿­ä»£ç®—æ³•ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡è¿­ä»£åº”ç”¨è´å°”æ›¼æœ€ä¼˜æ€§æ–¹ç¨‹æ¥èŽ·å¾—æœ€ä¼˜å€¼å‡½æ•°ã€‚

ä¸‹é¢æ˜¯è´å°”æ›¼æœ€ä¼˜æ–¹ç¨‹çš„å¦ä¸€ä¸ªç‰ˆæœ¬ï¼Œå®ƒå¯ä»¥å¤„ç†å¥–åŠ±éƒ¨åˆ†ä¾èµ–äºŽæ–°çŠ¶æ€çš„çŽ¯å¢ƒ:

![](img/9f8162d0-dc85-4f29-9da6-927b2ca44e6c.png)

è¿™é‡Œï¼ŒR(sï¼Œaï¼Œsâ€™)æ˜¯é€šè¿‡é‡‡å–è¡ŒåŠ¨ a ä»ŽçŠ¶æ€ s ç§»åŠ¨åˆ°çŠ¶æ€ sâ€™çš„ç»“æžœæ‰€æ”¶åˆ°çš„å¥–åŠ±ã€‚ç”±äºŽè¿™ä¸ªç‰ˆæœ¬æ›´å…¼å®¹ï¼Œæˆ‘ä»¬æ ¹æ®å®ƒå¼€å‘äº†æˆ‘ä»¬çš„`value_iteration`å‡½æ•°ã€‚æ­£å¦‚æ‚¨åœ¨*æ­¥éª¤ 3* ä¸­çœ‹åˆ°çš„ï¼Œæˆ‘ä»¬æ‰§è¡Œä»¥ä¸‹ä»»åŠ¡:

*   å°†ç­–ç•¥å€¼åˆå§‹åŒ–ä¸ºå…¨é›¶ã€‚
*   æ ¹æ®è´å°”æ›¼æœ€ä¼˜æ–¹ç¨‹æ›´æ–°è¿™äº›å€¼ã€‚
*   è®¡ç®—æ‰€æœ‰çŠ¶æ€å€¼çš„æœ€å¤§å˜åŒ–ã€‚
*   å¦‚æžœæœ€å¤§å˜åŒ–å¤§äºŽé˜ˆå€¼ï¼Œæˆ‘ä»¬å°±ä¸æ–­æ›´æ–°è¿™äº›å€¼ã€‚å¦åˆ™ï¼Œæˆ‘ä»¬å°†ç»ˆæ­¢è¯„ä¼°è¿‡ç¨‹ï¼Œå¹¶å°†æœ€æ–°å€¼ä½œä¸ºæœ€ä½³å€¼è¿”å›žã€‚



# è¿˜æœ‰æ›´å¤š...

æˆ‘ä»¬èŽ·å¾—äº† 75%çš„æˆåŠŸçŽ‡ï¼ŒæŠ˜æ‰£ç³»æ•°ä¸º 0.99ã€‚è´´çŽ°å› å­å¦‚ä½•å½±å“ä¸šç»©ï¼Ÿè®©æˆ‘ä»¬åšä¸€äº›ä¸åŒå› ç´ çš„å®žéªŒï¼ŒåŒ…æ‹¬`0`ã€`0.2`ã€`0.4`ã€`0.6`ã€`0.8`ã€`0.99`å’Œ`1.`:

```py
>>> gammas = [0, 0.2, 0.4, 0.6, 0.8, .99, 1.]
```

å¯¹äºŽæ¯ä¸ªæŠ˜æ‰£ç³»æ•°ï¼Œæˆ‘ä»¬è®¡ç®— 10ï¼Œ000 é›†çš„å¹³å‡æˆåŠŸçŽ‡:

```py
>>> avg_reward_gamma = []
>>> for gamma in gammas:
...     V_optimal = value_iteration(env, gamma, threshold)
...     optimal_policy = extract_optimal_policy(env, V_optimal, gamma)
...     total_rewards = []
...     for episode in range(n_episode):
...         total_reward = run_episode(env, optimal_policy)
...         total_rewards.append(total_reward)
...     avg_reward_gamma.append(sum(total_rewards) / n_episode)
```

æˆ‘ä»¬ç»˜åˆ¶äº†å¹³å‡æˆåŠŸçŽ‡ä¸ŽæŠ˜çŽ°ç³»æ•°çš„å…³ç³»å›¾:

```py
>>> import matplotlib.pyplot as plt
>>> plt.plot(gammas, avg_reward_gamma)
>>> plt.title('Success rate vs discount factor')
>>> plt.xlabel('Discount factor')
>>> plt.ylabel('Average success rate')
>>> plt.show()
```

æˆ‘ä»¬å¾—åˆ°å¦‚ä¸‹çš„æƒ…èŠ‚:

![](img/650d388b-391a-4be4-8537-d49d31b08315.png)

ç»“æžœè¡¨æ˜Žï¼Œå½“æŠ˜æ‰£å› å­å¢žåŠ æ—¶ï¼Œæ€§èƒ½æé«˜ã€‚è¿™éªŒè¯äº†ä¸€ä¸ªäº‹å®žï¼Œå³å°çš„æŠ˜æ‰£å› å­é‡è§†çŽ°åœ¨çš„å›žæŠ¥ï¼Œè€Œå¤§çš„æŠ˜æ‰£å› å­é‡è§†æœªæ¥æ›´å¥½çš„å›žæŠ¥ã€‚



# ç”¨ç­–ç•¥è¿­ä»£ç®—æ³•æ±‚è§£ MDP

è§£å†³ MDP çš„å¦ä¸€ç§æ–¹æ³•æ˜¯ä½¿ç”¨**ç­–ç•¥è¿­ä»£**ç®—æ³•ï¼Œæˆ‘ä»¬å°†åœ¨æœ¬èœè°±ä¸­è®¨è®ºã€‚

ç­–ç•¥è¿­ä»£ç®—æ³•å¯ä»¥ç»†åˆ†ä¸ºä¸¤ä¸ªéƒ¨åˆ†:ç­–ç•¥è¯„ä¼°å’Œç­–ç•¥æ”¹è¿›ã€‚å®ƒå§‹äºŽä¸€é¡¹æ­¦æ–­çš„æ”¿ç­–ã€‚åœ¨æ¯æ¬¡è¿­ä»£ä¸­ï¼Œå®ƒé¦–å…ˆæ ¹æ®è´å°”æ›¼æœŸæœ›æ–¹ç¨‹è®¡ç®—ç»™å®šæœ€æ–°ç­–ç•¥çš„ç­–ç•¥å€¼ï¼›ç„¶åŽï¼Œå®ƒæ ¹æ®è´å°”æ›¼æœ€ä¼˜æ–¹ç¨‹ä»Žç»“æžœç­–ç•¥å€¼ä¸­æå–ä¸€ä¸ªæ”¹è¿›çš„ç­–ç•¥ã€‚å®ƒè¿­ä»£åœ°è¯„ä¼°ç­–ç•¥å¹¶ç”Ÿæˆä¸€ä¸ªæ”¹è¿›çš„ç‰ˆæœ¬ï¼Œç›´åˆ°ç­–ç•¥ä¸å†æ”¹å˜ã€‚

è®©æˆ‘ä»¬å¼€å‘ä¸€ä¸ªç­–ç•¥è¿­ä»£ç®—æ³•ï¼Œå¹¶ç”¨å®ƒæ¥è§£å†³ FrozenLake çŽ¯å¢ƒã€‚ä¹‹åŽï¼Œæˆ‘ä»¬å°†è§£é‡Šå®ƒæ˜¯å¦‚ä½•å·¥ä½œçš„ã€‚



# æ€Žä¹ˆåš...

è®©æˆ‘ä»¬ä½¿ç”¨å¦‚ä¸‹ç­–ç•¥è¿­ä»£ç®—æ³•æ¥æ±‚è§£ FrozenLake çŽ¯å¢ƒ:

1.  æˆ‘ä»¬å¯¼å…¥å¿…è¦çš„åº“å¹¶åˆ›å»ºä¸€ä¸ª FrozenLake çŽ¯å¢ƒçš„å®žä¾‹:

```py
>>> import torch
>>> import gym
>>> env = gym.make('FrozenLake-v0')
```

2.  çŽ°åœ¨å°†`0.99`è®¾ä¸ºæŠ˜æ‰£å› å­ï¼Œå°†`0.0001`è®¾ä¸ºæ”¶æ•›é˜ˆå€¼:

```py
>>> gamma = 0.99
>>> threshold = 0.0001
```

3.  çŽ°åœ¨æˆ‘ä»¬å®šä¹‰`policy_evaluation`å‡½æ•°ï¼Œå®ƒè®¡ç®—ç»™å®šç­–ç•¥çš„å€¼:

```py
>>> def policy_evaluation(env, policy, gamma, threshold):
...     """
...     Perform policy evaluation
...     @param env: OpenAI Gym environment
...     @param policy: policy matrix containing actions and 
 their probability in each state
...     @param gamma: discount factor
...     @param threshold: the evaluation will stop once values 
 for all states are less than the threshold
...     @return: values of the given policy
...     """
...     n_state = policy.shape[0]
...     V = torch.zeros(n_state)
...     while True:
...         V_temp = torch.zeros(n_state)
...         for state in range(n_state):
...             action = policy[state].item()
...             for trans_prob, new_state, reward, _ in 
 env.env.P[state][action]:
...                 V_temp[state] += trans_prob * (reward 
 + gamma * V[new_state])
...         max_delta = torch.max(torch.abs(V - V_temp))
...         V = V_temp.clone()
...         if max_delta <= threshold:
...             break
...     return V
```

è¿™ç±»ä¼¼äºŽæˆ‘ä»¬åœ¨*æ‰§è¡Œæ”¿ç­–è¯„ä¼°*æ–¹æ³•ä¸­æ‰€åšçš„ï¼Œä½†æ˜¯ä½¿ç”¨å¥èº«æˆ¿çŽ¯å¢ƒä½œä¸ºè¾“å…¥ã€‚

4.  æŽ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å¼€å‘ç­–ç•¥è¿­ä»£ç®—æ³•çš„ç¬¬äºŒä¸ªä¸»è¦ç»„ä»¶ï¼Œå³ç­–ç•¥æ”¹è¿›éƒ¨åˆ†:

```py
>>> def policy_improvement(env, V, gamma):
...     """
...     Obtain an improved policy based on the values
...     @param env: OpenAI Gym environment
...     @param V: policy values
...     @param gamma: discount factor
...     @return: the policy
...     """
...     n_state = env.observation_space.n
...     n_action = env.action_space.n
...     policy = torch.zeros(n_state)
...     for state in range(n_state):
...         v_actions = torch.zeros(n_action)
...         for action in range(n_action):
...             for trans_prob, new_state, reward, _ in 
 env.env.P[state][action]:
...                 v_actions[action] += trans_prob * (reward 
 + gamma * V[new_state])
...         policy[state] = torch.argmax(v_actions)
...     return policy
```

è¿™åŸºäºŽè´å°”æ›¼æœ€ä¼˜æ–¹ç¨‹ä»Žç»™å®šçš„ç­–ç•¥å€¼ä¸­æå–æ”¹è¿›çš„ç­–ç•¥ã€‚

5.  çŽ°åœ¨æˆ‘ä»¬å·²ç»å‡†å¤‡å¥½äº†ä¸¤ä¸ªç»„ä»¶ï¼Œæˆ‘ä»¬å¼€å‘ç­–ç•¥è¿­ä»£ç®—æ³•å¦‚ä¸‹:

```py
>>> def policy_iteration(env, gamma, threshold):
...     """
...     Solve a given environment with policy iteration algorithm
...     @param env: OpenAI Gym environment
...     @param gamma: discount factor
...     @param threshold: the evaluation will stop once values 
 for all states are less than the threshold
...     @return: optimal values and the optimal policy for the given 
 environment
...     """
...     n_state = env.observation_space.n
...     n_action = env.action_space.n
...     policy = torch.randint(high=n_action, size=(n_state,)).float()
...     while True:
...         V = policy_evaluation(env, policy, gamma, threshold)
...         policy_improved = policy_improvement(env, V, gamma)
...         if torch.equal(policy_improved, policy):
...             return V, policy_improved
...         policy = policy_improved
```

6.  æ’å…¥çŽ¯å¢ƒã€æŠ˜æ‰£ç³»æ•°å’Œæ”¶æ•›é˜ˆå€¼:

```py
>>> V_optimal, optimal_policy = 
 policy_iteration(env, gamma, threshold)
```

7.  æˆ‘ä»¬å·²ç»èŽ·å¾—äº†æœ€ä¼˜å€¼å’Œæœ€ä¼˜ç­–ç•¥ã€‚è®©æˆ‘ä»¬æ¥çœ‹çœ‹å®ƒä»¬:

```py
>>> print('Optimal values:\n{}'.format(V_optimal))
Optimal values:
tensor([0.5404, 0.4966, 0.4681, 0.4541, 0.5569, 0.0000, 0.3572, 0.0000, 0.5905,
 0.6421, 0.6144, 0.0000, 0.0000, 0.7410, 0.8625, 0.0000])
>>> print('Optimal policy:\n{}'.format(optimal_policy))
Optimal policy:
tensor([0., 3., 3., 3., 0., 3., 2., 3., 3., 1., 0., 3., 3., 2., 1., 3.])
```

å®ƒä»¬ä¸Žæˆ‘ä»¬ä½¿ç”¨å€¼è¿­ä»£ç®—æ³•å¾—åˆ°çš„ç»“æžœå®Œå…¨ç›¸åŒã€‚



# å®ƒæ˜¯å¦‚ä½•å·¥ä½œçš„...

ç­–ç•¥è¿­ä»£åœ¨æ¯æ¬¡è¿­ä»£ä¸­ç»“åˆäº†ç­–ç•¥è¯„ä¼°å’Œç­–ç•¥æ”¹è¿›ã€‚åœ¨ç­–ç•¥è¯„ä¼°ä¸­ï¼Œç»™å®šç­–ç•¥(éžæœ€ä½³ç­–ç•¥)çš„å€¼æ˜¯åŸºäºŽè´å°”æ›¼æœŸæœ›æ–¹ç¨‹è®¡ç®—çš„ï¼Œç›´åˆ°å®ƒä»¬æ”¶æ•›:

![](img/6408dcd4-b641-4ef3-bacb-6ceb32d75026.png)

è¿™é‡Œï¼Œa = Ï€(s)ï¼Œè¿™æ˜¯åœ¨çŠ¶æ€ s ä¸‹æ ¹æ®ç­–ç•¥Ï€é‡‡å–çš„è¡ŒåŠ¨ã€‚

åœ¨ç­–ç•¥æ”¹è¿›ä¸­ï¼ŒåŸºäºŽè´å°”æ›¼æœ€ä¼˜æ–¹ç¨‹ï¼Œä½¿ç”¨æ‰€å¾—çš„æ”¶æ•›ç­–ç•¥å€¼ V(s)æ¥æ›´æ–°ç­–ç•¥:

![](img/f60962b4-570b-4954-8da6-79733626a594.png)

è¿™å°†é‡å¤ç­–ç•¥è¯„ä¼°å’Œç­–ç•¥æ”¹è¿›æ­¥éª¤ï¼Œç›´åˆ°ç­–ç•¥æ”¶æ•›ã€‚åœ¨æ”¶æ•›æ—¶ï¼Œæœ€æ–°çš„ç­–ç•¥åŠå…¶ä»·å€¼å‡½æ•°æ˜¯æœ€ä¼˜ç­–ç•¥å’Œæœ€ä¼˜ä»·å€¼å‡½æ•°ã€‚å› æ­¤ï¼Œåœ¨æ­¥éª¤ 5 ä¸­ï¼Œ`policy_iteration`å‡½æ•°æ‰§è¡Œä»¥ä¸‹ä»»åŠ¡:

*   åˆå§‹åŒ–éšæœºç­–ç•¥ã€‚
*   ä½¿ç”¨ç­–ç•¥è¯„ä¼°ç®—æ³•è®¡ç®—ç­–ç•¥çš„å€¼ã€‚
*   åŸºäºŽç­–ç•¥å€¼èŽ·å–æ”¹è¿›çš„ç­–ç•¥ã€‚
*   å¦‚æžœæ–°ç­–ç•¥ä¸Žæ—§ç­–ç•¥ä¸åŒï¼Œå®ƒä¼šæ›´æ–°ç­–ç•¥å¹¶è¿è¡Œå¦ä¸€ä¸ªè¿­ä»£ã€‚å¦åˆ™ï¼Œå®ƒç»ˆæ­¢è¿­ä»£è¿‡ç¨‹ï¼Œå¹¶è¿”å›žç­–ç•¥å€¼å’Œç­–ç•¥ã€‚



# è¿˜æœ‰æ›´å¤š...

æˆ‘ä»¬åˆšåˆšç”¨ç­–ç•¥è¿­ä»£ç®—æ³•è§£å†³äº† FrozenLake çŽ¯å¢ƒã€‚å› æ­¤ï¼Œæ‚¨å¯èƒ½æƒ³çŸ¥é“ä»€ä¹ˆæ—¶å€™ä½¿ç”¨ç­–ç•¥è¿­ä»£æ¯”ä½¿ç”¨å€¼è¿­ä»£æ›´å¥½ï¼Œåä¹‹äº¦ç„¶ã€‚åŸºæœ¬ä¸Šæœ‰ä¸‰ç§æƒ…å†µï¼Œå…¶ä¸­ä¸€ç§æ¯”å¦ä¸€ç§æœ‰ä¼˜åŠ¿:

*   å¦‚æžœæœ‰å¤§é‡çš„æ“ä½œï¼Œä½¿ç”¨ç­–ç•¥è¿­ä»£ï¼Œå› ä¸ºå®ƒå¯ä»¥æ›´å¿«åœ°æ”¶æ•›ã€‚
*   å¦‚æžœæœ‰å°‘é‡çš„åŠ¨ä½œï¼Œä½¿ç”¨å€¼è¿­ä»£ã€‚
*   å¦‚æžœå·²ç»æœ‰ä¸€ä¸ªå¯è¡Œçš„ç­–ç•¥(é€šè¿‡ç›´è§‰æˆ–è€…é¢†åŸŸçŸ¥è¯†èŽ·å¾—)ï¼Œä½¿ç”¨ç­–ç•¥è¿­ä»£ã€‚

åœ¨é‚£äº›åœºæ™¯ä¹‹å¤–ï¼Œç­–ç•¥è¿­ä»£å’Œä»·å€¼è¿­ä»£é€šå¸¸æ˜¯å¯æ¯”è¾ƒçš„ã€‚

åœ¨ä¸‹ä¸€ä¸ªé£Ÿè°±ä¸­ï¼Œæˆ‘ä»¬å°†åº”ç”¨æ¯ä¸ªç®—æ³•æ¥è§£å†³æŽ·ç¡¬å¸èµŒåšé—®é¢˜ã€‚æˆ‘ä»¬å°†çœ‹åˆ°å“ªä¸ªç®—æ³•æ”¶æ•›å¾—æ›´å¿«ã€‚



# è¯·å‚è§

éšæ„ä½¿ç”¨æˆ‘ä»¬åœ¨è¿™ä¸¤ä¸ªé£Ÿè°±ä¸­å­¦åˆ°çš„çŸ¥è¯†æ¥è§£å†³ä¸€ä¸ªæ›´å¤§çš„å†°æ ¼ï¼Œå³`FrozenLake8x8-v0`çŽ¯å¢ƒ([https://gym.openai.com/envs/FrozenLake8x8-v0/](https://gym.openai.com/envs/FrozenLake8x8-v0/))ã€‚



# è§£å†³æŠ›ç¡¬å¸èµŒåšé—®é¢˜

æŠ›ç¡¬å¸èµŒåšï¼Œå¤§å®¶åº”è¯¥å¬èµ·æ¥å¾ˆç†Ÿæ‚‰ã€‚åœ¨æ¸¸æˆçš„æ¯ä¸€è½®ä¸­ï¼Œæ¸¸æˆè€…å¯ä»¥æ‰“èµŒæŽ·ç¡¬å¸æ˜¯å¦ä¼šæ­£é¢æœä¸Šã€‚å¦‚æžœç»“æžœæ˜¯æ­£é¢æœä¸Šï¼ŒèµŒå¾’å°†èµ¢å¾—ä¸Žä»–ä»¬ä¸‹æ³¨ç›¸åŒçš„é‡‘é¢ï¼›å¦åˆ™ï¼Œä»–ä»¬å°†å¤±åŽ»è¿™ç¬”é’±ã€‚æ¸¸æˆç»§ç»­è¿›è¡Œï¼Œç›´åˆ°èµŒå¾’è¾“äº†(æœ€ç»ˆä¸€æ— æ‰€æœ‰)æˆ–èµ¢äº†(æ¯”å¦‚èµ¢äº† 100 å¤šç¾Žå…ƒ)ã€‚å‡è®¾ç¡¬å¸æ˜¯ä¸å…¬å¹³çš„ï¼Œå®ƒæœ‰ 40%çš„æœºä¼šæ­£é¢æœä¸Šã€‚ä¸ºäº†æœ€å¤§åŒ–èŽ·èƒœçš„æœºä¼šï¼ŒèµŒå¾’åº”è¯¥åœ¨æ¯ä¸€è½®ä¸­æ ¹æ®ä»–ä»¬å½“å‰çš„èµ„æœ¬ä¸‹æ³¨å¤šå°‘ï¼Ÿè¿™è‚¯å®šä¼šæ˜¯ä¸€ä¸ªæœ‰è¶£çš„é—®é¢˜éœ€è¦è§£å†³ã€‚

å¦‚æžœç¡¬å¸è½åœ¨å¤´ä¸Šçš„å‡ çŽ‡è¶…è¿‡ 50%ï¼Œé‚£å°±æ²¡ä»€ä¹ˆå¥½è®¨è®ºçš„äº†ã€‚èµŒå¾’å¯ä»¥æ¯è½®åªèµŒä¸€ç¾Žå…ƒï¼Œå¤§éƒ¨åˆ†æ—¶é—´åº”è¯¥ä¼šèµ¢ã€‚å¦‚æžœè¿™æ˜¯ä¸€ä¸ªå…¬å¹³çš„ç¡¬å¸ï¼ŒèµŒå¾’å¯ä»¥æ¯è½®èµŒä¸€ç¾Žå…ƒï¼Œæœ€ç»ˆèµ¢å¾—å¤§çº¦ 50%çš„æœºä¼šã€‚å½“æ­£é¢çš„æ¦‚çŽ‡ä½ŽäºŽ 50%æ—¶ï¼Œäº‹æƒ…å°±å˜å¾—æ£˜æ‰‹äº†ï¼›å®‰å…¨ä¸‹æ³¨ç­–ç•¥å°†ä¸å†æœ‰æ•ˆã€‚éšæœºç­–ç•¥ä¹Ÿä¸è¡Œã€‚æˆ‘ä»¬éœ€è¦å€ŸåŠ©æˆ‘ä»¬åœ¨æœ¬ç« ä¸­å­¦åˆ°çš„å¼ºåŒ–å­¦ä¹ æŠ€æœ¯æ¥è¿›è¡Œæ˜Žæ™ºçš„æŠ¼æ³¨ã€‚

è®©æˆ‘ä»¬ä»Žå°†æŠ›ç¡¬å¸èµŒåšé—®é¢˜å…¬å¼åŒ–ä¸º MDP å¼€å§‹ã€‚å®ƒåŸºæœ¬ä¸Šæ˜¯ä¸€ä¸ªæœªè´´çŽ°çš„ã€æƒ…èŠ‚æ€§çš„ã€æœ‰é™çš„ MDPï¼Œå…·æœ‰ä»¥ä¸‹æ€§è´¨:

*   ä»¥ç¾Žå…ƒè®¡ç®—ï¼Œå›½å®¶æ˜¯èµŒå¾’çš„èµ„æœ¬ã€‚å…±æœ‰ 101 ç§çŠ¶æ€:0ã€1ã€2ã€â€¦ã€98ã€99 å’Œ 100+ã€‚
*   è¾¾åˆ°çŠ¶æ€ 100+å¥–åŠ± 1ï¼›å¦åˆ™ï¼Œå¥–åŠ±ä¸º 0ã€‚
*   åŠ¨ä½œæ˜¯æ¸¸æˆè€…åœ¨ä¸€è½®ä¸­å¯èƒ½ä¸‹çš„èµŒæ³¨ã€‚ç»™å®šçŠ¶æ€ sï¼Œå¯èƒ½çš„åŠ¨ä½œåŒ…æ‹¬ 1ï¼Œ2ï¼Œâ€¦ï¼Œå’Œ min(sï¼Œ100 - s)ã€‚ä¾‹å¦‚ï¼Œå½“èµŒå¾’æœ‰ 60 ç¾Žå…ƒæ—¶ï¼Œä»–ä»¬å¯ä»¥ä¸‹æ³¨ 1 åˆ° 40 ä¹‹é—´çš„ä»»ä½•é‡‘é¢ã€‚ä»»ä½•é«˜äºŽ 40 çš„æ•°é¢éƒ½æ²¡æœ‰ä»»ä½•æ„ä¹‰ï¼Œå› ä¸ºå®ƒå¢žåŠ äº†æŸå¤±ï¼Œå¹¶æ²¡æœ‰å¢žåŠ èŽ·èƒœçš„æœºä¼šã€‚
*   é‡‡å–è¡ŒåŠ¨åŽçš„ä¸‹ä¸€ä¸ªçŠ¶æ€å–å†³äºŽç¡¬å¸æ­£é¢æœä¸Šçš„æ¦‚çŽ‡ã€‚å‡è®¾æ˜¯ 40%ã€‚æ‰€ä»¥ï¼ŒçŠ¶æ€ s åœ¨é‡‡å–è¡ŒåŠ¨ *a* ä¹‹åŽçš„ä¸‹ä¸€ä¸ªçŠ¶æ€å°†æ˜¯*s+a*40%ï¼Œ*s-a*60%ã€‚
*   è¯¥è¿‡ç¨‹åœ¨çŠ¶æ€ 0 å’ŒçŠ¶æ€ 100+å¤„ç»ˆæ­¢ã€‚



# æ€Žä¹ˆåš...

æˆ‘ä»¬é¦–å…ˆé€šè¿‡ä½¿ç”¨å€¼è¿­ä»£ç®—æ³•å¹¶æ‰§è¡Œä»¥ä¸‹æ­¥éª¤æ¥è§£å†³æŠ›ç¡¬å¸èµŒåšé—®é¢˜:

1.  æ±‡å…¥æ¸¸æ ‡:

```py
>>> import torch
```

2.  æŒ‡å®šæŠ˜æ‰£ç³»æ•°å’Œæ”¶æ•›é˜ˆå€¼:

```py
>>> gamma = 1
>>> threshold = 1e-10
```

è¿™é‡Œï¼Œæˆ‘ä»¬è®¾ç½® 1 ä¸ºè´´çŽ°å› å­ï¼Œå› ä¸º MDP æ˜¯ä¸€ä¸ªä¸è´´çŽ°çš„è¿‡ç¨‹ï¼›æˆ‘ä»¬è®¾ç½®ä¸€ä¸ªå°çš„é˜ˆå€¼ï¼Œå› ä¸ºæˆ‘ä»¬æœŸæœ›å°çš„ç­–ç•¥å€¼ï¼Œå› ä¸ºé™¤äº†æœ€åŽä¸€ä¸ªçŠ¶æ€ï¼Œæ‰€æœ‰çš„å¥–åŠ±éƒ½æ˜¯ 0ã€‚

3.  å®šä¹‰ä»¥ä¸‹çŽ¯å¢ƒå˜é‡ã€‚

æ€»å…±æœ‰ 101 ä¸ªå·ž:

```py
>>> capital_max = 100
>>> n_state = capital_max + 1
```

ç›¸åº”çš„å¥–åŠ±æ˜¾ç¤ºå¦‚ä¸‹:

```py
>>> rewards = torch.zeros(n_state)
>>> rewards[-1] = 1
>>> print(rewards)
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])
```

å‡è®¾èŽ·å¾—æ­£é¢çš„æ¦‚çŽ‡æ˜¯ 40%:

```py
>>> head_prob = 0.4
```

å°†è¿™äº›å˜é‡æ”¾å…¥å­—å…¸ä¸­:

```py
>>> env = {'capital_max': capital_max,
...        'head_prob': head_prob,
...        'rewards': rewards,
...        'n_state': n_state}
```

4.  çŽ°åœ¨ï¼Œæˆ‘ä»¬å¼€å‘ä¸€ä¸ªåŸºäºŽå€¼è¿­ä»£ç®—æ³•è®¡ç®—æœ€ä½³å€¼çš„å‡½æ•°:

```py
>>> def value_iteration(env, gamma, threshold):
...     """
...     Solve the coin flipping gamble problem with 
 value iteration algorithm
...     @param env: the coin flipping gamble environment
...     @param gamma: discount factor
...     @param threshold: the evaluation will stop once values 
 for all states are less than the threshold
...     @return: values of the optimal policy for the given 
 environment
...     """
...     head_prob = env['head_prob']
...     n_state = env['n_state']
...     capital_max = env['capital_max']
...     V = torch.zeros(n_state)
...     while True:
...         V_temp = torch.zeros(n_state)
...         for state in range(1, capital_max):
...             v_actions = torch.zeros(
 min(state, capital_max - state) + 1)
...             for action in range(
 1, min(state, capital_max - state) + 1):
...                 v_actions[action] += head_prob * (
 rewards[state + action] +
 gamma * V[state + action])
...                 v_actions[action] += (1 - head_prob) * (
 rewards[state - action] +
 gamma * V[state - action])
...             V_temp[state] = torch.max(v_actions)
...         max_delta = torch.max(torch.abs(V - V_temp))
...         V = V_temp.clone()
...         if max_delta <= threshold:
...             break
...     return V
```

æˆ‘ä»¬åªéœ€è¦è®¡ç®—çŠ¶æ€ 1 åˆ° 99 çš„å€¼ï¼Œå› ä¸ºçŠ¶æ€ 0 å’ŒçŠ¶æ€ 100+çš„å€¼éƒ½æ˜¯ 0ã€‚å¹¶ä¸”ç»™å®šçŠ¶æ€ *s* ï¼Œå¯èƒ½çš„åŠ¨ä½œå¯ä»¥æ˜¯ä»Ž 1 åˆ° *min(sï¼Œ100 - s)* çš„ä»»ä½•å€¼ã€‚åœ¨è®¡ç®—è´å°”æ›¼æœ€ä¼˜æ–¹ç¨‹æ—¶ï¼Œæˆ‘ä»¬åº”è¯¥è®°ä½è¿™ä¸€ç‚¹ã€‚

5.  æŽ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å¼€å‘ä¸€ä¸ªåŸºäºŽæœ€ä½³å€¼æå–æœ€ä½³ç­–ç•¥çš„å‡½æ•°:

```py
>>> def extract_optimal_policy(env, V_optimal, gamma):
...     """
...     Obtain the optimal policy based on the optimal values
...     @param env: the coin flipping gamble environment
...     @param V_optimal: optimal values
...     @param gamma: discount factor
...     @return: optimal policy
...     """
...     head_prob = env['head_prob']
...     n_state = env['n_state']
...     capital_max = env['capital_max']
...     optimal_policy = torch.zeros(capital_max).int()
...     for state in range(1, capital_max):
...         v_actions = torch.zeros(n_state)
...         for action in range(1, 
 min(state, capital_max - state) + 1):
...             v_actions[action] += head_prob * (
 rewards[state + action] +
 gamma * V_optimal[state + action])
...             v_actions[action] += (1 - head_prob) * (
 rewards[state - action] +
 gamma * V_optimal[state - action])
...         optimal_policy[state] = torch.argmax(v_actions)
...     return optimal_policy
```

6.  æœ€åŽï¼Œæˆ‘ä»¬å¯ä»¥æ’å…¥çŽ¯å¢ƒã€æŠ˜æ‰£å› å­å’Œæ”¶æ•›é˜ˆå€¼æ¥è®¡ç®—æœ€ä¼˜å€¼å’Œæœ€ä¼˜ç­–ç•¥ã€‚è¿˜æœ‰ï¼Œæˆ‘ä»¬è®¡æ—¶ç”¨ä»·å€¼è¿­ä»£è§£å†³èµŒåš MDP éœ€è¦å¤šé•¿æ—¶é—´ï¼›æˆ‘ä»¬å°†æ­¤ä¸Žç­–ç•¥è¿­ä»£å®Œæˆæ‰€éœ€çš„æ—¶é—´è¿›è¡Œæ¯”è¾ƒ:

```py
>>> import time
>>> start_time = time.time()
>>> V_optimal = value_iteration(env, gamma, threshold)
>>> optimal_policy = extract_optimal_policy(env, V_optimal, gamma)
>>> print("It takes {:.3f}s to solve with value 
 iteration".format(time.time() - start_time))
It takes 4.717s to solve with value iteration
```

æˆ‘ä»¬åœ¨`4.717`ç§’å†…ç”¨å€¼è¿­ä»£è§£å†³äº†èµŒåšé—®é¢˜ã€‚

7.  çœ‹çœ‹æœ€ä¼˜ç­–ç•¥å€¼å’Œæˆ‘ä»¬å¾—åˆ°çš„æœ€ä¼˜ç­–ç•¥:

```py
>>> print('Optimal values:\n{}'.format(V_optimal))
>>> print('Optimal policy:\n{}'.format(optimal_policy))
```

8.  æˆ‘ä»¬å¯ä»¥ç»˜åˆ¶ç­–ç•¥å€¼ä¸ŽçŠ¶æ€çš„å…³ç³»ï¼Œå¦‚ä¸‹æ‰€ç¤º:

```py
>>> import matplotlib.pyplot as plt
>>> plt.plot(V_optimal[:100].numpy())
>>> plt.title('Optimal policy values')
>>> plt.xlabel('Capital')
>>> plt.ylabel('Policy value')
>>> plt.show()
```

çŽ°åœ¨æˆ‘ä»¬å·²ç»ç”¨ä»·å€¼è¿­ä»£è§£å†³äº†èµŒåšé—®é¢˜ï¼Œé‚£ä¹ˆç­–ç•¥è¿­ä»£å‘¢ï¼Ÿè®©æˆ‘ä»¬çœ‹çœ‹ã€‚

9.  æˆ‘ä»¬é¦–å…ˆå¼€å‘`policy_evaluation`å‡½æ•°ï¼Œå®ƒè®¡ç®—ç»™å®šç­–ç•¥çš„å€¼:

```py
>>> def policy_evaluation(env, policy, gamma, threshold):
...     """
...     Perform policy evaluation
...     @param env: the coin flipping gamble environment
...     @param policy: policy tensor containing actions taken 
 for individual state
...     @param gamma: discount factor
...     @param threshold: the evaluation will stop once values 
 for all states are less than the threshold
...     @return: values of the given policy
...     """
...     head_prob = env['head_prob']
...     n_state = env['n_state']
...     capital_max = env['capital_max']
...     V = torch.zeros(n_state)
...     while True:
...         V_temp = torch.zeros(n_state)
...         for state in range(1, capital_max):
...             action = policy[state].item()
...             V_temp[state] += head_prob * (
 rewards[state + action] +
 gamma * V[state + action])
...             V_temp[state] += (1 - head_prob) * (
 rewards[state - action] +
 gamma * V[state - action])
...         max_delta = torch.max(torch.abs(V - V_temp))
...         V = V_temp.clone()
...         if max_delta <= threshold:
...             break
...     return V
```

10.  æŽ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å¼€å‘ç­–ç•¥è¿­ä»£ç®—æ³•çš„å¦ä¸€ä¸ªä¸»è¦ç»„ä»¶ï¼Œç­–ç•¥æ”¹è¿›éƒ¨åˆ†:

```py
>>> def policy_improvement(env, V, gamma):
...     """
...     Obtain an improved policy based on the values
...     @param env: the coin flipping gamble environment
...     @param V: policy values
...     @param gamma: discount factor
...     @return: the policy
...     """
...     head_prob = env['head_prob']
...     n_state = env['n_state']
...     capital_max = env['capital_max']
...     policy = torch.zeros(n_state).int()
...     for state in range(1, capital_max):
...         v_actions = torch.zeros(
 min(state, capital_max - state) + 1)
...         for action in range(
 1, min(state, capital_max - state) + 1):
...             v_actions[action] += head_prob * (
 rewards[state + action] + 
 gamma * V[state + action])
...             v_actions[action] += (1 - head_prob) * (
 rewards[state - action] +
 gamma * V[state - action])
...         policy[state] = torch.argmax(v_actions)
...     return policy
```

11.  å‡†å¤‡å¥½è¿™ä¸¤ä¸ªç»„ä»¶åŽï¼Œæˆ‘ä»¬å¯ä»¥å¼€å‘ç­–ç•¥è¿­ä»£ç®—æ³•çš„ä¸»è¦æ¡ç›®ï¼Œå¦‚ä¸‹æ‰€ç¤º:

```py
>>> def policy_iteration(env, gamma, threshold):
...     """
...     Solve the coin flipping gamble problem with policy 
 iteration algorithm
...     @param env: the coin flipping gamble environment
...     @param gamma: discount factor
...     @param threshold: the evaluation will stop once values
 for all states are less than the threshold
...     @return: optimal values and the optimal policy for the 
 given environment
...     """
...     n_state = env['n_state']
...     policy = torch.zeros(n_state).int()
...     while True:
...         V = policy_evaluation(env, policy, gamma, threshold)
...         policy_improved = policy_improvement(env, V, gamma)
...         if torch.equal(policy_improved, policy):
...             return V, policy_improved
...         policy = policy_improved
```

12.  æœ€åŽï¼Œæˆ‘ä»¬æ’å…¥çŽ¯å¢ƒã€æŠ˜æ‰£å› å­å’Œæ”¶æ•›é˜ˆå€¼æ¥è®¡ç®—æœ€ä¼˜å€¼å’Œæœ€ä¼˜ç­–ç•¥ã€‚æˆ‘ä»¬è¿˜è®°å½•äº†æ±‚è§£ MDP èŠ±è´¹çš„æ—¶é—´:

```py
>>> start_time = time.time()
>>> V_optimal, optimal_policy 
 = policy_iteration(env, gamma, threshold)
>>> print("It takes {:.3f}s to solve with policy 
 iteration".format(time.time() - start_time))
It takes 2.002s to solve with policy iteration
```

13.  æŸ¥çœ‹æˆ‘ä»¬åˆšåˆšèŽ·å¾—çš„æœ€ä½³å€¼å’Œæœ€ä½³ç­–ç•¥:

```py
>>> print('Optimal values:\n{}'.format(V_optimal))
>>> print('Optimal policy:\n{}'.format(optimal_policy))
```



# å®ƒæ˜¯å¦‚ä½•å·¥ä½œçš„...

åœ¨æ‰§è¡Œäº†*æ­¥éª¤ 7* ä¸­çš„ä»£ç è¡ŒåŽï¼Œæ‚¨å°†çœ‹åˆ°æœ€ä½³ç­–ç•¥å€¼:

```py
Optimal values:
tensor([0.0000, 0.0021, 0.0052, 0.0092, 0.0129, 0.0174, 0.0231, 0.0278, 0.0323,
 0.0377, 0.0435, 0.0504, 0.0577, 0.0652, 0.0695, 0.0744, 0.0807, 0.0866,
 0.0942, 0.1031, 0.1087, 0.1160, 0.1259, 0.1336, 0.1441, 0.1600, 0.1631,
 0.1677, 0.1738, 0.1794, 0.1861, 0.1946, 0.2017, 0.2084, 0.2165, 0.2252,
 0.2355, 0.2465, 0.2579, 0.2643, 0.2716, 0.2810, 0.2899, 0.3013, 0.3147,
 0.3230, 0.3339, 0.3488, 0.3604, 0.3762, 0.4000, 0.4031, 0.4077, 0.4138,
 0.4194, 0.4261, 0.4346, 0.4417, 0.4484, 0.4565, 0.4652, 0.4755, 0.4865,
 0.4979, 0.5043, 0.5116, 0.5210, 0.5299, 0.5413, 0.5547, 0.5630, 0.5740,
 0.5888, 0.6004, 0.6162, 0.6400, 0.6446, 0.6516, 0.6608, 0.6690, 0.6791,
 0.6919, 0.7026, 0.7126, 0.7248, 0.7378, 0.7533, 0.7697, 0.7868, 0.7965,
 0.8075, 0.8215, 0.8349, 0.8520, 0.8721, 0.8845, 0.9009, 0.9232, 0.9406,
 0.9643, 0.0000])
```

æ‚¨è¿˜å°†çœ‹åˆ°æœ€ä½³ç­–ç•¥:

```py
Optimal policy:
tensor([ 0,  1, 2, 3, 4,  5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17,
 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 22, 29, 30, 31, 32, 33,  9, 35,
 36, 37, 38, 11, 40,  9, 42, 43, 44, 5, 4,  3, 2, 1, 50, 1, 2, 47,
 4, 5, 44,  7, 8, 9, 10, 11, 38, 12, 36, 35, 34, 17, 32, 19, 30,  4,
 3, 2, 26, 25, 24, 23, 22, 21, 20, 19, 18, 17, 16, 15, 14, 13, 12, 11,
 10, 9, 8,  7, 6, 5, 4,  3, 2, 1], dtype=torch.int32)
```

*ç¬¬ 8 æ­¥*ä¸ºæœ€ä½³ç­–ç•¥å€¼ç”Ÿæˆä»¥ä¸‹å›¾è¡¨:

![](img/030a3b7f-e8f7-4a1d-8536-44740a2774d3.png)

æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œéšç€èµ„æœ¬(çŠ¶æ€)çš„å¢žåŠ ï¼Œä¼°è®¡çš„æŠ¥é…¬(ä¿å•ä»·å€¼)ä¹Ÿå¢žåŠ ï¼Œè¿™æ˜¯æœ‰é“ç†çš„ã€‚

æˆ‘ä»¬åœ¨*æ­¥éª¤ 9* ä¸­æ‰€åšçš„ä¸Žæˆ‘ä»¬åœ¨*ä¸­ç”¨ç­–ç•¥è¿­ä»£ç®—æ³•*è§£å†³ MDP çš„æ–¹æ³•éžå¸¸ç›¸ä¼¼ï¼Œä½†è¿™æ¬¡æ˜¯åœ¨æŠ›ç¡¬å¸çš„èµŒåšçŽ¯å¢ƒä¸­ã€‚

åœ¨*æ­¥éª¤ 10* ä¸­ï¼Œç­–ç•¥æ”¹è¿›åŠŸèƒ½åŸºäºŽè´å°”æ›¼æœ€ä¼˜æ–¹ç¨‹ä»Žç»™å®šçš„ç­–ç•¥å€¼ä¸­æå–æ”¹è¿›çš„ç­–ç•¥ã€‚

æ­£å¦‚æ‚¨åœ¨*æ­¥éª¤ 12* ä¸­çœ‹åˆ°çš„ï¼Œæˆ‘ä»¬åœ¨`2.002`ç§’å†…ç”¨ç­–ç•¥è¿­ä»£è§£å†³äº†èµŒåšé—®é¢˜ï¼Œè¿™æ¯”ç”¨å€¼è¿­ä»£èŠ±è´¹çš„æ—¶é—´å°‘äº†ä¸€åŠã€‚

æˆ‘ä»¬ä»Ž*æ­¥éª¤ 13* ä¸­å¾—åˆ°çš„ç»“æžœåŒ…æ‹¬ä»¥ä¸‹æœ€ä½³å€¼:

```py
Optimal values:
tensor([0.0000, 0.0021, 0.0052, 0.0092, 0.0129, 0.0174, 0.0231, 0.0278, 0.0323,
 0.0377, 0.0435, 0.0504, 0.0577, 0.0652, 0.0695, 0.0744, 0.0807, 0.0866,
 0.0942, 0.1031, 0.1087, 0.1160, 0.1259, 0.1336, 0.1441, 0.1600, 0.1631,
 0.1677, 0.1738, 0.1794, 0.1861, 0.1946, 0.2017, 0.2084, 0.2165, 0.2252,
 0.2355, 0.2465, 0.2579, 0.2643, 0.2716, 0.2810, 0.2899, 0.3013, 0.3147,
 0.3230, 0.3339, 0.3488, 0.3604, 0.3762, 0.4000, 0.4031, 0.4077, 0.4138,
 0.4194, 0.4261, 0.4346, 0.4417, 0.4484, 0.4565, 0.4652, 0.4755, 0.4865,
 0.4979, 0.5043, 0.5116, 0.5210, 0.5299, 0.5413, 0.5547, 0.5630, 0.5740,
 0.5888, 0.6004, 0.6162, 0.6400, 0.6446, 0.6516, 0.6608, 0.6690, 0.6791,
 0.6919, 0.7026, 0.7126, 0.7248, 0.7378, 0.7533, 0.7697, 0.7868, 0.7965,
 0.8075, 0.8215, 0.8349, 0.8520, 0.8721, 0.8845, 0.9009, 0.9232, 0.9406,
 0.9643, 0.0000])
```

å®ƒä»¬è¿˜åŒ…æ‹¬æœ€ä¼˜ç­–ç•¥:

```py
Optimal policy:
tensor([ 0,  1, 2, 3, 4,  5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17,
 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 22, 29, 30, 31, 32, 33,  9, 35,
 36, 37, 38, 11, 40,  9, 42, 43, 44, 5, 4,  3, 2, 1, 50, 1, 2, 47,
 4, 5, 44,  7, 8, 9, 10, 11, 38, 12, 36, 35, 34, 17, 32, 19, 30,  4,
 3, 2, 26, 25, 24, 23, 22, 21, 20, 19, 18, 17, 16, 15, 14, 13, 12, 11,
 10, 9, 8,  7, 6, 5, 4,  3, 2, 1, 0], dtype=torch.int32)
```

ä»·å€¼è¿­ä»£å’Œç­–ç•¥è¿­ä»£è¿™ä¸¤ç§æ–¹æ³•çš„ç»“æžœæ˜¯ä¸€è‡´çš„ã€‚

æˆ‘ä»¬å·²ç»é€šè¿‡ä½¿ç”¨å€¼è¿­ä»£å’Œç­–ç•¥è¿­ä»£è§£å†³äº†èµŒåšé—®é¢˜ã€‚è¦å¤„ç†å¼ºåŒ–å­¦ä¹ é—®é¢˜ï¼Œæœ€æ£˜æ‰‹çš„ä»»åŠ¡ä¹‹ä¸€æ˜¯å°†è¿‡ç¨‹å…¬å¼åŒ–ä¸º MDPã€‚åœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­ï¼Œæ”¿ç­–é€šè¿‡ä¸‹æ³¨æŸäº›èµŒæ³¨(è¡ŒåŠ¨)ä»Žå½“å‰èµ„æœ¬(å·ž)è½¬æ¢åˆ°æ–°èµ„æœ¬(æ–°å·ž)ã€‚æœ€ä¼˜ç­–ç•¥æœ€å¤§åŒ–æ¸¸æˆèŽ·èƒœæ¦‚çŽ‡(+1 å¥–åŠ±)ï¼Œè¯„ä¼°æœ€ä¼˜ç­–ç•¥ä¸‹çš„èŽ·èƒœæ¦‚çŽ‡ã€‚

å¦ä¸€ä»¶æœ‰è¶£çš„äº‹æƒ…æ˜¯ï¼Œåœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­ï¼Œè½¬æ¢æ¦‚çŽ‡å’Œæ–°çŠ¶æ€æ˜¯å¦‚ä½•åœ¨è´å°”æ›¼æ–¹ç¨‹ä¸­ç¡®å®šçš„ã€‚åœ¨çŠ¶æ€ s ä¸­é‡‡å–è¡ŒåŠ¨ a(æ‹¥æœ‰èµ„æœ¬ s å¹¶ä¸‹æ³¨ 1 ç¾Žå…ƒ)å°†æœ‰ä¸¤ç§å¯èƒ½çš„ç»“æžœ:

*   ç§»åŠ¨åˆ°æ–°çš„çŠ¶æ€ s+aï¼Œå¦‚æžœç¡¬å¸æ­£é¢æœä¸Šã€‚å› æ­¤ï¼Œè½¬æ¢æ¦‚çŽ‡ç­‰äºŽæ­£é¢æ¦‚çŽ‡ã€‚
*   å¦‚æžœç¡¬å¸è½åœ¨åé¢ï¼Œåˆ™ç§»åŠ¨åˆ°æ–°çš„å·ž s-aã€‚å› æ­¤ï¼Œè½¬æ¢æ¦‚çŽ‡ç­‰äºŽå°¾éƒ¨çš„æ¦‚çŽ‡ã€‚

è¿™ä¸Ž FrozenLake çŽ¯å¢ƒéžå¸¸ç›¸ä¼¼ï¼Œåœ¨è¯¥çŽ¯å¢ƒä¸­ï¼Œä»£ç†åªä»¥ä¸€å®šçš„æ¦‚çŽ‡è½åœ¨é¢„æœŸçš„ç“·ç –ä¸Šã€‚

æˆ‘ä»¬è¿˜éªŒè¯äº†åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œç­–ç•¥è¿­ä»£æ¯”å€¼è¿­ä»£æ”¶æ•›å¾—æ›´å¿«ã€‚è¿™æ˜¯å› ä¸ºæœ‰å¤šè¾¾ 50 ä¸ªå¯èƒ½çš„åŠ¨ä½œï¼Œæ¯” FrozenLake ä¸­çš„ 4 ä¸ªåŠ¨ä½œå¤šã€‚å¯¹äºŽå…·æœ‰å¤§é‡æ“ä½œçš„ MDPï¼Œç”¨ç­–ç•¥è¿­ä»£æ±‚è§£æ¯”ç”¨å€¼è¿­ä»£æ±‚è§£æ›´æœ‰æ•ˆã€‚



# è¿˜æœ‰æ›´å¤š...

ä½ å¯èƒ½æƒ³çŸ¥é“æœ€ä¼˜ç­–ç•¥æ˜¯å¦çœŸçš„æœ‰æ•ˆã€‚è®©æˆ‘ä»¬åƒèªæ˜Žçš„èµŒå¾’ä¸€æ ·ï¼ŒçŽ©ä¸€ä¸‡é›†æ¸¸æˆã€‚æˆ‘ä»¬å°†æœ€ä¼˜ç­–ç•¥ä¸Žå¦å¤–ä¸¤ç§ç­–ç•¥è¿›è¡Œæ¯”è¾ƒ:ä¿å®ˆç­–ç•¥(æ¯è½®ä¸‹æ³¨ä¸€ç¾Žå…ƒ)å’Œéšæœºç­–ç•¥(ä¸‹æ³¨éšæœºé‡‘é¢):

1.  æˆ‘ä»¬é¦–å…ˆå®šä¹‰å‰é¢æåˆ°çš„ä¸‰ç§ä¸‹æ³¨ç­–ç•¥ã€‚

æˆ‘ä»¬é¦–å…ˆå®šä¹‰æœ€ä¼˜ç­–ç•¥:

```py
>>> def optimal_strategy(capital):
...     return optimal_policy[capital].item()
```

ç„¶åŽæˆ‘ä»¬å®šä¹‰ä¿å®ˆç­–ç•¥:

```py
>>> def conservative_strategy(capital):
...     return 1
```

æœ€åŽï¼Œæˆ‘ä»¬å®šä¹‰éšæœºç­–ç•¥:

```py
>>> def random_strategy(capital):
...     return torch.randint(1, capital + 1, (1,)).item()
```

2.  å®šä¹‰ä¸€ä¸ªåŒ…è£…å‡½æ•°ï¼Œè¯¥å‡½æ•°è¿è¡Œä¸€é›†çš„ç­–ç•¥å¹¶è¿”å›žæ¸¸æˆæ˜¯å¦èŽ·èƒœ:

```py
>>> def run_episode(head_prob, capital, policy):
...     while capital > 0:
...         bet = policy(capital)
...         if torch.rand(1).item() < head_prob:
...             capital += bet
...             if capital >= 100:
...                 return 1
...         else:
...             capital -= bet
...     return 0
```

3.  æŒ‡å®šä¸€ç¬”å¯åŠ¨èµ„é‡‘(æ¯”å¦‚è¯´`50`ç¾Žå…ƒ)å’Œè‹¥å¹²é›†(`10000`):

```py
>>> capital = 50
>>> n_episode = 10000
```

4.  è¿è¡Œ 10ï¼Œ000 é›†å¹¶è®°å½•èŽ·å¥–æ¬¡æ•°:

```py
>>> n_win_random = 0
>>> n_win_conservative = 0
>>> n_win_optimal = 0
>>> for episode in range(n_episode):
...     n_win_random += run_episode(
 head_prob, capital, random_strategy)
...     n_win_conservative += run_episode(
 head_prob, capital, conservative_strategy)
...     n_win_optimal += run_episode(
 head_prob, capital, optimal_strategy)
```

5.  æ‰“å°å‡ºä¸‰ç§ç­–ç•¥çš„èŽ·èƒœæ¦‚çŽ‡:

```py
>>> print('Average winning probability under the random 
 policy: {}'.format(n_win_random/n_episode))
Average winning probability under the random policy: 0.2251
>>> print('Average winning probability under the conservative 
 policy: {}'.format(n_win_conservative/n_episode))
Average winning probability under the conservative policy: 0.0
>>> print('Average winning probability under the optimal 
 policy: {}'.format(n_win_optimal/n_episode))
Average winning probability under the optimal policy: 0.3947
```

æˆ‘ä»¬çš„æœ€ä¼˜ç­–ç•¥æ˜¾ç„¶æ˜¯èµ¢å®¶ï¼