  <title>2Off the Ground with the First Deep Learning Model</title> <link rel="stylesheet" type="text/css" href="../styles/stylesheet1.css"> <link rel="stylesheet" type="text/css" href="../styles/stylesheet2.css">

# 2 推出第一个深度学习模型

深度学习模型最近获得了极大的流行。它们引起了学术界和工业界数据科学家的关注。他们取得巨大成功的原因是他们有能力解决计算机科学中最简单也是最古老的问题——计算机视觉。计算机科学家长久以来的梦想是找到一种算法，让机器像人类一样看东西……或者至少能够识别物体。深度学习模型不仅可以识别物体，还可以用于任何事情，从预测图像中的人到预测和生成文本的自然语言处理，从理解语音，甚至是制作视频等深度视频。在其核心，所有深度学习模型都是使用神经网络算法构建的；然而，它们不仅仅是一个神经网络。虽然神经网络自 20 世纪 50 年代以来就已经被使用，但只是在最近几年，深度学习才能够对行业产生重大影响。神经网络在 1955 年随着 IBM 在世界博览会上的一次演示而得到普及，一台机器做出了预测，全世界都看到了**人工智能** ( **AI** )让机器学习任何东西并预测任何东西的巨大潜力。这个系统是一个基于**感知器**的网络(后来被称为**多层感知器** ( **MLPs** )的基本表亲)，它成为了神经网络和深度学习模型的基础。

随着神经网络的成功，许多人试图将它们用于更高级的问题，如图像识别。计算机视觉的第一次正式尝试是由麻省理工学院的一位教授在 1965 年做出的(是的，**机器学习** ( **ML** )在 20 世纪 50 年代末和 60 年代初也是一件大事)。他给学生布置了一项任务，寻找图像识别的算法。毫无疑问，尽管他们尽了最大的努力和无与伦比的聪明，但还是没能解决问题。事实上，计算机视觉物体识别的问题在未来几年甚至几十年都没有解决，这一时期被普遍称为**人工智能冬天**。随着**卷积神经网络**(**CNN**)的发明，20 世纪 90 年代中期出现了重大突破，这是一种非常进化的神经网络形式。2012 年，经过大规模训练的 CNN 的更高级版本赢得了 ImageNet 比赛，并在测试集上取得了与人类一样好的准确率，成为了计算机视觉的首选。从那时起，CNN 不仅成为了图像识别问题的基石，还创造了机器学习的整个分支，称为深度学习。在过去的几年里，更先进的 CNN 形式已经被设计出来，新的深度学习算法每天都在不断出现，推动了艺术的发展，并将人类对人工智能的掌握提升到了新的水平。在这一章中，我们将考察深度学习模型的基础，理解这些基础对于最好地利用最新的算法是极其重要的。这不仅仅是一个实践章节，因为 MLP 和 CNN 对于大多数商业应用仍然非常有用。我们将在本章中讨论以下主题:

*   神经网络入门
*   构建“你好世界”MLP 模式
*   构建我们的第一个深度学习模型
*   使用 CNN 模型进行图像识别

## 技术要求

在本章中，我们将主要使用以下 Python 模块:

*   py torch Lightning–版本:1.2.3
*   火炬-版本:1.7.1
*   可选–GPU-1
*   torchvision–0 . 8 . 2 版

所有上述软件包预计将被导入到 Jupyter 笔记本电脑。你可以参考*第 1 章*、 *PyTorch 闪电冒险*获得导入包的指导。

你可以在下面的 GitHub 链接找到本章的工作示例:[https://GitHub . com/packt publishing/Deep-Learning-with-py torch-Lightning](https://github.com/PacktPublishing/Deep-Learning-with-PyTorch-Lightning)

源数据集链接–猫狗数据集:[https://www.kaggle.com/tongpython/cat-and-dog](https://www.kaggle.com/tongpython/cat-and-dog)

## 神经网络入门

在本节中，我们将通过了解神经网络的基础知识开始我们的旅程。

### 为什么是神经网络？

在我们深入研究神经网络之前，回答一个简单的问题很重要。为什么我们甚至需要一个新的分类算法，当有这么多伟大的算法，如决策树？简单的答案是，因为有些分类问题是决策树永远无法解决的。正如您可能知道的，决策树的工作方式是在一个类中找到一组对象，然后在该组对象中创建分裂，以继续创建一个纯类。当数据集中的不同类之间有明显的区别时，这种方法很有效，但是当它们混合在一起时，这种方法就失效了。决策树无法解决的一个非常基本的问题就是`XOR`问题。

### 关于 XOR 运算符

`XOR`门/操作符也称为独占`OR`。它来自一个数字逻辑门。`XOR`门是一种数字逻辑门，当它有不同的输入时会产生一个真输出。下图显示了`XOR`门产生的输入和输出:

![Figure 2.1 – XOR gate input and output truth table](../media/file0.png)

Figure 2.1 – XOR gate input and output truth table

简单地说，`XOR`门是一个函数，它接受两个输入，例如`A`和`B`，并产生一个输出。从上表中，您可以看到`XOR` gate 函数给出了以下输出:

*   当输入`A`和`B`不同时，输出为`1`。
*   当`A`和`B`相同时，输出为`0`。

正如你所看到的，如果我们想要建立一个决策树分类器，它将隔离`0`和`1`，并且总是给出 50%错误的预测。为了解决这个问题，我们需要一种完全不同的模型，它不仅仅训练输入值，而是通过概念化输入和输出对以及学习它们的关系。MLP 算法就是这样一个基本却又无比强大的算法。

在本章中，我们将尝试构建我们自己的简单 MLP 模型来模拟`XOR`门的行为。

### 多层感知器架构

让我们使用 PyTorch Lightning 框架来构建 XOR 神经网络架构。我们的目标是建立一个简单的 MLP 模型，类似于下图所示:

![Figure 2.2 – MLP architecture](../media/file1.png)

Figure 2.2 – MLP architecture

前面的神经网络体系结构图显示了以下内容:

*   输入层、中间层和输出层。
*   这种架构有两个输入。这是我们传递`XOR`输入的地方。
*   中间层有四个节点。
*   最后一层只有一个节点。这就是我们期望的`XOR`操作的输出。

我们将要建立的这个神经网络旨在模仿`XOR`门。现在让我们开始编写我们的第一个 PyTorch 闪电模型！

## “你好，世界”MLP 模型

欢迎来到 PyTorch 闪电的世界！

最后，是我们使用 PyTorch Lightning 构建第一个模型的时候了。在本节中，我们将构建一个简单的 MLP 模型来完成`XOR`操作符。这就像是 **Hello World** 对神经网络世界以及 PyTorch Lightning 的介绍。我们将按照这些步骤来构建我们的第一个`XOR`操作符:

1.  准备数据
2.  配置模型
3.  训练模型
4.  加载模型
5.  做预测

### 准备数据

正如您在*图 2.1* 中看到的，`XOR`门接受两个输入，有四行数据。在数据科学术语中，我们可以将列 **A** 和 **B** 称为我们的特征，将 **Out** 列称为我们的目标变量。

在我们开始为`XOR`准备输入和目标数据之前，了解 PyTorch Lightning 接受数据加载器来训练模型是很重要的。在本节中，我们将构建最简单的数据加载器，它有输入和目标。我们将在训练模型时使用它:

1.  是时候建立我们的数据集了。创建输入要素的代码如下:

    ```
    inputs = [Variable(torch.Tensor([0, 0])),
              Variable(torch.Tensor([0, 1])),
              Variable(torch.Tensor([1, 0])),
              Variable(torch.Tensor([1, 1]))]
    ```

    因为 PyTorch Lightning 是建立在 PyTorch 框架之上的，所以传递到模型中的所有数据都必须是张量形式的。在前面的代码中，我们创建了四个张量，每个张量有两个值。也就是说，它有两个特征，`A`和`B`。我们已经准备好了所有的输入特性。总共有四排准备喂给我们的`XOR`模型。

2.  既然输入特性已经准备好了，就该构建我们的目标变量了，如下面的代码所示:

    ```
    targets = [Variable(torch.Tensor([0])),
               Variable(torch.Tensor([1])),
               Variable(torch.Tensor([1])),
               Variable(torch.Tensor([0]))]
    ```

    前面的目标代码类似于输入要素代码。唯一的区别是每个目标变量都是一个值。输入和目标将在准备数据集的最后一步准备就绪。是时候创建一个数据加载器了。我们可以通过不同的方式创建数据集，并将其作为数据加载器传递给 PyTorch Lightning。在接下来的章节中，我们将展示使用数据加载器的不同方式。

3.  这里，我们将使用最简单的方式为我们的`XOR`模型构建数据集。以下代码用于为我们的`XOR`模型构建数据集:

    ```
    data_inputs_targets = list(zip(inputs, targets))
    data_inputs_targets
    ```

    PyTorch Lightning 中的数据加载器寻找两个主要的东西，键和值，在我们的例子中是特性和目标值。这里，我们使用 Python `zip()`函数在输入和目标变量之间创建元组，然后将它们转换成列表对象。这是前面代码的输出:

    ```
    [(tensor([0., 0.]), tensor([0.])),
     (tensor([0., 1.]), tensor([1.])),
     (tensor([1., 0.]), tensor([1.])),
     (tensor([1., 1.]), tensor([0.]))]
    ```

    在前面的代码中，我们创建了一个数据集，它是一个元组列表，每个元组有两个值。第一个值是两个特征/输入，第二个值是给定输入的目标值。

### 配置模型

最后，是时候建立我们的第一个 PyTorch 闪电模型了。PyTorch Lightning 中的模型与 PyTorch 中的相似。PyTorch Lightning 的一个额外优势是，它将通过其生命周期方法使您的代码更加结构化，并且大多数模型训练代码由框架处理，这有助于我们避免样板代码。

此外，这一框架的另一个巨大优势是人们可以轻松地跨多个 GPU 和**TPU**(**张量处理单元**)扩展深度学习模型，我们将在接下来的章节中使用。

我们使用 PyTorch Lightning 构建的每个模型都必须从一个名为`LightningModule`的类继承。这是防止样板代码的类，也是我们拥有闪电生命周期方法的地方。简单来说，我们可以说`PyTorch LightningModule`和`PyTorch nn.Module`是一样的，但是增加了生命周期方法和其他操作。如果我们看一下源代码，`PyTorch LightningModule`是继承自`PyTorch nn.Module`。这意味着`PyTorch nn.Module`中的大部分功能在`LightningModule`中也是可用的。

任何 PyTorch Lightning 模型都需要至少两种生命周期方法，一种用于训练循环来训练模型，称为`training_step`，另一种用于为模型配置优化器，称为`configure_optimizers`。除了这两种生命周期方法，我们还使用了`forward`方法。这是我们接收输入数据并将其传递给模型的地方。

为了构建我们的第一个简单 PyTorch 闪电模型，我们将主要使用我们刚刚讨论过的两种生命周期方法。在本章的下一节中，我们将使用其他生命周期方法，我们将在该节中单独讨论。

我们的 MLP 模型制作遵循这一流程，我们将详细介绍每一步:

1.  初始化模型
2.  将输入映射到模型
3.  配置优化程序
4.  设置培训参数

#### 初始化模型

要初始化模型，请按照下列步骤操作:

1.  首先创建一个名为`XOR`的类，它继承自 PyTorch `Lightning Module`，如下面的代码片段所示:

    ```
    class XOR(pl. Lightning  Module)
    ```

2.  我们将开始创建我们的层。这可以在`__init__`方法中初始化，如以下代码所示:

    ```
    def __init__(self):
        super(XOR,self).__init__()
        self.input_layer = nn.Linear(2, 4)
    self.output_layer = nn.Linear(4,1)
    self.sigmoid = nn.Sigmoid()
    self.loss = nn.MSELoss()
    ```

在前面的代码中，我们执行了以下操作:

*   设置隐藏层，其中第一层接受两个输入并返回四个输出，该输出成为我们的中间层。中间层与单个节点合并，成为我们的输出节点。
*   初始化激活功能。这里，我们使用`sigmoid`函数来构建我们的`XOR`门。
*   初始化损失函数。这里，我们使用**均方差** ( **MSE** )损失函数来构建我们的`XOR`模型。

#### 将输入映射到模型

这是我们使用`forward`方法的简单步骤，它获取输入并生成模型的输出。以下代码片段演示了该过程:

```
def forward(self, input):
  x = self.linear1(input)
  x = self.sigmoid(x)
  output = self.final_layer(x)
  return output
```

方法就像一个映射器或媒介，在多层和激活函数之间传递数据。在前面的`forward`方法中，它主要执行以下操作:

1.  获取`XOR`门的输入，并将其传递给我们的第一个输入层。
2.  第一输入层产生的输出被输入到`sigmoid`激活功能。
3.  来自`sigmoid`激活函数的输出被提供给最后一层，同样的输出由`forward`方法返回。

#### 配置优化程序

PyTorch Lightning 中的所有优化器都可以用一种叫做`configure_optimizers`的生命周期方法来配置。在这种方法中，可以配置一个或多个优化器。对于这个例子，我们可以使用单个优化器，在以后的章节中，会有一些模型使用多个优化器。

对于我们的`XOR`模型，我们将使用`Adam`优化器，下面的代码演示了我们的`configure_optimizers`生命周期方法:

```
  def configure_optimizers(self):
    params = self.parameters()
    optimizer = optim.Adam(params=params, lr = 0.01)
    return optimizer
```

所有的模型参数都可以通过使用带有`self.parameters()`方法的`self`对象来访问。这里，我们创建了`Adam`优化器，它采用学习率为`0.01`的模型参数，并返回相同的优化器。

#### 设置培训参数

这是重要的生命周期方法之一。这是所有模型训练发生的地方。我们来试着详细了解一下这个方法。这是`training_step`的代码片段:

```
def training_step(self, batch, batch_idx):
  inputs, targets = batch
  outputs = self(inputs) 
  loss = self.loss(outputs, targets)
  return loss 
```

这个`training_step`生命周期方法采用以下两个输入:

*   `batch`:正在数据加载器中传递的数据被批量访问。每批有两项:一项是输入/特征数据，另一项是`targets`。
*   `batch_idx`:这是数据批次的索引号或顺序号。

在前面的方法中，我们从批处理中访问输入和目标，然后将输入传递给`self`方法。当输入被传递给`self`方法时，它间接调用我们的`forward`方法，后者返回`XOR`多层神经网络输出。我们使用`MSE`损失函数来计算损失，并返回该方法的损失值。

> **重要提示**
> 
> 传递给`self`方法的输入间接调用`forward`方法，其中数据映射发生在层之间，激活函数和来自模型的输出正在生成。
> 
> 训练步骤的输出是单损失值，而在下一章中，我们将介绍不同的方法和技巧，帮助我们建立和研究我们的神经网络。
> 
> PyTorch Lightning 中还有许多其他的生命周期方法。我们将在接下来的章节中讨论它们，这取决于用例以及场景。

我们已经完成了制造第一个`XOR` MLP 模型所需的所有步骤。我们的`XOR`模型的完整代码块如下:

```
class XOR(pl. Lightning  Module):
  def __init__(self):
    super(XOR,self).__init__()

    self.input_layer = nn.Linear(2, 4)
    self.output_layer = nn.Linear(4,1)
    self.sigmoid = nn.Sigmoid()
    self.loss = nn.MSELoss()
  def forward(self, input):
    x = self.input_layer(input)
    x = self.sigmoid(x)
    output = self.output_layer(x)
    return output
  def configure_optimizers(self):
    params = self.parameters()
    optimizer = optim.Adam(params=params, lr = 0.01)
    return optimizer
  def training_step(self, batch, batch_idx):
    inputs, targets = batch
    outputs = self(inputs) 
    loss = self.loss(outputs, targets)
    return loss 
```

总而言之，在前面的代码中，我们有以下内容:

*   `XOR`模型接受大小为 2 的`XOR`输入。
*   数据被传递到中间层，中间层有四个节点，只返回一个输出。
*   在这个过程中，我们使用`sigmoid`作为我们的激活函数，`MSE`作为我们的损失函数，`Adam`作为我们的优化器。

如果您观察，我们没有设置任何反向传播、清除梯度或优化器参数更新，许多其他事情都由 PyTorch Lightning 框架负责。

### 训练模型

PyTorch Lightning 内置的所有模型都可以使用`Trainer`类进行训练。下面我们来详细了解一下`Trainer`类。

`Trainer`类是一些关键事物的抽象，比如数据集循环、反向传播、清除渐变和优化器步骤。PyTorch 的所有样板代码都由 PyTorch Lightning 中的`Trainer`类获取。此外，`Trainer`类支持许多其他功能，帮助我们轻松地构建模型，其中一些功能是各种回调、模型检查点、提前停止、单元测试的开发运行、对 GPU 和 TPU 的支持、记录器、日志、时期等等。在本书的各个章节中，我们将尝试涵盖`Trainer`类支持的大多数重要特性。

用于训练我们的`XOR`模型的代码片段如下:

```
checkpoint_callback = ModelCheckpoint()
model = XOR()
trainer = pl.Trainer(max_epochs=500, callbacks=[checkpoint_callback])
```

在 PyTorch Lightning 中，我们看到的一个优势是，每当我们多次训练一个模型时，所有不同的模型版本都会保存到磁盘上一个名为`Lightning _logs`的默认文件夹中，并且一旦所有不同版本的模型都准备好了，我们总是有机会从文件中加载不同的模型版本并比较结果。例如，这里我们已经运行了两次`XOR`模型，当我们查看`lightning _logs`文件夹时，我们可以看到`XOR`模型的两个版本:

![Figure 2.3 – List of files in the Lightning _logs folder](../media/file2.png)

Figure 2.3 – List of files in the Lightning _logs folder

在这些版本子文件夹中，我们有关于正在被训练和构建的模型的所有信息，这些信息可以很容易地被加载，并且可以执行预测。这些文件夹中的文件有一些有用的信息，比如超参数，它们被保存为`hparams.yaml`，我们还有一个名为`checkpoints`的子文件夹。这是我们的`XOR`模型以序列化形式存储的地方。以下是这些版本文件夹中所有文件的屏幕截图:

![Figure 2.4 – A list of subfolders and files within the Lightning _logs folder](../media/file3.png)

Figure 2.4 – A list of subfolders and files within the Lightning _logs folder

> **重要提示**
> 
> 请为 Collab 内部运行的 Shell 命令添加`!`。

下面是`checkpoints`子文件夹中文件的截图，其中`version_0`模型运行于`99`时期，而`version_1`运行于`499`时期:

![Figure 2.5 – A list of files within the checkpoint folder](../media/file4.png)

Figure 2.5 – A list of files within the checkpoint folder

如果您运行多个版本并且想要加载模型的最新版本，从前面的代码片段中，模型的最新版本被存储在`version_1`文件夹中。我们可以手动找到模型最新版本的路径，或者使用模型检查点回调。

在下一步中，我们将创建一个教练对象。我们最多运行 500 个时期的模型，并通过回调传递模型检查点。在最后一步，一旦模型训练器准备好了，我们通过传递模型和输入数据来调用`fit`方法，如下面的代码片段所示:

```
trainer.fit(model, train_dataloader=data_inputs_targets)
```

在运行模型 500 个时期后，我们将得到以下输出:

![Figure 2.6 – Model output after 500 epochs](../media/file5.png)

Figure 2.6 – Model output after 500 epochs

如果我们仔细观察模型训练的进度，最后我们可以看到损失值正在显示。PyTorch Lightning 支持一种良好而灵活的方式来配置进度条上显示的值，我们将在接下来的章节中介绍这一点。

总结这一节，我们已经创建了一个`XOR`模型对象，并使用`Trainer`类来训练 500 个时期的模型。

### 加载模型

一旦我们构建了模型，下一步就是加载模型。正如前面提到的，识别模型的最新版本可以使用前面步骤中创建的`checkpoint_callback`来完成。这里，我们已经运行了两个版本的模型，以获得模型的最新版本的路径，如下面的代码片段所示:

```
print(checkpoint_callback.best_model_path)
```

下面是前面代码的输出，其中显示了模型的最新文件路径。这稍后用于从检查点加载回模型并进行预测:

![Figure 2.7 – Output of the file path for the latest model version file](../media/file6.png)

Figure 2.7 – Output of the file path for the latest model version file

使用模型对象的`load_from_checkpoint`方法，通过传递模型检查点路径，可以很容易地从检查点加载模型，如下面的代码片段所示:

```
train_model = model.load_from_checkpoint(checkpoint_callback.best_model_path)
```

上述代码将从检查点加载模型。在这一步中，我们已经为两个不同的版本构建和训练了模型，并从检查点加载了最新的模型。

### 做预测

现在我们的模型已经准备好了，是时候做一些预测了。下面的代码片段演示了这个过程:

```
for input in inputs:
  result = train_model(input)
  print("Input: ",[int(input[0]),int(input[1])], "Model_output:",int(result.round()))
```

前面的代码是进行预测的一种简单方法，迭代我们的`XOR`数据集，将输入值传递给模型，然后进行预测。这是前面代码片段的输出:

![Figure 2.8 – XOR model output](../media/file7.png)

Figure 2.8 – XOR model output

从前面的输出中，我们可以看到输入模型预测了正确的结果。

我们总结一下:

1.  我们首先为`XOR`模型创建一个数据集。
2.  接下来，我们使用`LightningModule`类构建模型。
3.  然后我们使用`Trainer`类训练模型 500 个历元。
4.  最后，我们使用`callback`函数加载最佳模型，然后进行预测。

有不同的方法和技术来建立一个模型，我们将在接下来的章节中介绍。

## 构建我们的第一个深度学习模型

现在是时候使用我们创建 MLP 的知识来建立深度学习模型了。

### 那么，是什么让它“深”呢？

虽然最早使用深度学习的确切起源经常被争论，但一个流行的误解是，深度学习只是涉及一个具有数百或数千层的非常大的神经网络模型。虽然大多数深度学习模型都很大，但理解真正的秘密是一个叫做反向传播的概念是很重要的。

正如我们所见，像 MLPs 这样的神经网络已经存在了很长时间，它们本身就可以解决以前未解决的分类问题，例如`XOR`，或者比传统的分类器给出更好的预测。然而，在处理大型非结构化数据(如图像)时，它们仍然不准确。为了在高维空间中学习，使用了一种简单的方法，称为反向传播，它向系统提供反馈。这种反馈使模型**了解**在预测方面做得是好是坏，错误在模型的每次迭代中都会受到惩罚。慢慢地，通过使用优化方法的多次迭代，系统学会了最小化错误并实现收敛。我们通过使用反馈回路的损失函数来收敛，并不断减少损失，从而实现期望的优化。有各种各样的损失函数可用，最流行的是`log loss`或`cosine loss`函数。

反向传播，当与云提供的大量数据和计算能力相结合时，可以创造奇迹，这就是最近 ML 复兴的原因。自 2012 年以来，当一个 **CNN** 架构通过实现接近人类的准确性赢得 ImageNet 竞赛时，它只会变得更好。在这一节中，我们将看到如何建立一个 CNN 模型。让我们从 CNN 架构的概述开始。

### CNN 架构

众所周知，计算机只理解位的语言，这意味着它接受数字形式的输入。但是你如何将图像转换成数字呢？CNN 架构由代表卷积的不同层组成。CNN 的简单目标是获取一个高维对象，如图像，并将其转换为一个低维实体，如数字的数学形式，它以矩阵形式表示(也称为张量)。

当然，CNN 不仅仅是将图像转换成张量。它还学习使用反向传播和优化方法来识别图像中的对象。一旦对图像的数量进行训练，它就可以很容易地准确识别出看不见的图像。CNN 的成功在于其在规模方面的灵活性，只需添加更多的硬件，然后随着规模的扩大，在准确性方面表现出色。

我们将为`Cats Dogs`数据集构建一个 CNN 模型，以确定图像中包含的是狗还是猫:

![Figure 2.9 – A CNN architecture for the cats and dogs use case](../media/file8.png)

Figure 2.9 – A CNN architecture for the cats and dogs use case

我们将使用一个简单的 CNN 架构作为例子:

*   源图像数据集以具有 *3* 颜色通道的 *64x64* 图像开始。我们用内核大小为 *3* 和步长为 *1* 对其进行第一次卷积。
*   第一个卷积层之后是 MaxPool 层，在这里图像被转换为低维的 *32x32* 对象。
*   其后是另一个具有 *6* 信道的卷积层。这个卷积层之后是特征长度为 1000*、 *250* 和 *60* 的 *3* 全连接层，最终得到给出最终预测的 SoftMax 层。*

我们将使用`ReLU`作为我们的激活函数，`Adam`作为我们的优化器，`Cross-Entropy`作为我们的损失函数。

## 用于图像识别的 CNN 模型

PyTorch Lightning 是一个很酷的框架，它使得编写和扩展深度学习模型变得很容易。PyTorch Lightning 捆绑了许多有用的功能和选项，用于构建深度学习模型。很难在一章中涵盖所有主题，因此我们将继续探索和使用 PyTorch Lightning 的重要主题和不同功能。

以下是使用 CNN 构建图像分类器的步骤:

1.  加载数据
2.  构建模型
3.  训练模型
4.  计算精确度

### 加载数据

该数据集由大量的狗和猫组成，具有不同的颜色、角度、品种和不同的年龄组。它有以下两个子文件夹:

*   `cat-and-dog/training_set/training_set`
*   `cat-and-dog/test_set/test_set`

第一条路径有大约 8000 张猫和狗的图片。这是我们将用来训练 CNN 模型的数据。第二条路径有大约 2000 张猫和狗的图片；我们将使用这个数据集来测试我们的 CNN `ImageClassifier`模型。

以下是一些猫和狗的图片:

![Figure 2.10 – Sample images of cats and dogs](../media/file9.png)

Figure 2.10 – Sample images of cats and dogs

对于我们的 CNN 模型，我们将创建两个数据加载器，一个用于测试，另一个用于训练。每个数据加载器提供批量大小为 *64* 的图像，图像大小为 *64* 像素。下面的代码演示了数据的加载和转换:

```
image_size = 64
batch_size = 256
data_path_train = "cat-and-dog/training_set/training_set"
data_path_test= "cat-and-dog/test_set/test_set"
```

在前面的代码中，我们首先初始化图像大小为 *64* ，批处理大小为 *256* ，以及用于`train`和`test`数据集的子文件夹路径。

我们将从导入 torchvision 库开始这个过程。我们将使用来自`torchvision.datasets`的`torchvision.transforms`、`ImageFolder`和来自`torch.utils.data`的`DataLoader`。

初始化变量后，加载数据集和应用文件夹转换的一个简单方法是使用 torchvision 的内置库:

```
train_dataset = ImageFolder(data_path_train, transform=T.Compose([
    T.Resize(image_size),
    T.CenterCrop(image_size),
    T.ToTensor()]))
test_dataset = ImageFolder(data_path_test, transform=T.Compose([
    T.Resize(image_size),
    T.CenterCrop(image_size),
    T.ToTensor()]))
train_dataloader = DataLoader(train_dataset, batch_size, num_workers=2, pin_memory=True, shuffle=True)
test_dataloader = DataLoader(test_dataset, batch_size, num_workers=2, pin_memory=True)
```

在前面的代码中，我们有以下内容:

*   我们从使用`torchvision.datasets`包中的`ImageFolder`开始。
*   使用`ImageFolder`模块，我们通过从`test`和`train`文件夹中读取图像来创建两个数据集。
*   在创建数据集的过程中，我们还使用 torchvision `transform`模块将图像转换为 *64* 像素，并将图像裁剪到中心，即将图像转换为 *64 x 64* 像素的平方，并将图像转换为张量。
*   在最后一步中，创建的两个`train`和`test`数据集用于为它们创建两个数据加载器。

在这一点上，我们已经准备好了我们的列车数据加载器，它包含大约*8000 张*图像，并测试包含大约【2,000 张图像的数据加载器。所有图像的尺寸都是 *64 x 64* ，转换成张量形式，并批量提供 *256* 张图像。我们将使用训练数据加载器来训练我们的模型，并使用测试数据加载器来测量我们的模型的准确性。

> **重要提示**
> 
> 在某些情况下，我们可能需要创建自己的定制数据加载器。在接下来的章节中，我们将介绍这些技术。

### 构建模型

为了构建我们的 CNN 图像分类器，让我们将这个过程分为多个步骤:

1.  模型初始化
2.  配置优化程序
3.  配置培训和测试步骤

#### 模型初始化

类似于`XOR`模型，让我们首先创建一个名为`ImageClassifier`的类，它继承自`PyTorch LightningModule`类，如下面的代码片段所示:

```
class ImageClassifier(pl.LightningModule)
```

> **重要提示**
> 
> 正如我们将在本书中看到的，用 PL 构建的每个模型都必须从`PyTorch LightningModule`继承。

1.  让我们从设置我们的`ImageClassifier`类开始。这可以在`__init__`方法中初始化，如下面的代码所示。我们将把这种方法分成几个部分，以便您更容易理解:

    ```
    def __init__(self, learning_rate = 0.001):
        super().__init__()
        self.learning_rate = learning_rate
        #Inpuut size (256, 3, 64, 64)
        self.conv_layer1 = nn.Conv2d(in_channels=3,out_channels=3,kernel_size=3,stride=1,padding=1)
        #output_shape: (256, 3, 64, 64)
        self.relu1=nn.ReLU()
        #output_shape: (256, 3, 64, 64)
        self.pool=nn.MaxPool2d(kernel_size=2)
        #output_shape: (256, 3, 32, 32)
        self.conv_layer2 = nn.Conv2d(in_channels=3,out_channels=6,kernel_size=3,stride=1,padding=1)
        #output_shape: (256, 3, 32, 32)
        self.relu2=nn.ReLU()
        #output_shape: (256, 6, 32, 32)
        self.fully_connected_1 =nn.Linear(in_features=32 * 32 * 6,out_features=1000)
        self.fully_connected_2 =nn.Linear(in_features=1000,out_features=250)
        self.fully_connected_3 =nn.Linear(in_features=250,out_features=60)
        self.fully_connected_4 =nn.Linear(in_features=60,out_features=2)
        self.loss = nn.CrossEntropyLoss()
    ```

    在前面的代码中，`ImageClassifier`类接受一个参数，学习率，默认值为`0.001`。

2.  接下来，我们将构建两个卷积层。下面是构建两个卷积层以及最大池和激活函数的代码片段:

    ```
    #Input size (256, 3, 64, 64)
    self.conv_layer1 = nn.Conv2d(in_channels=3,out_channels=3,kernel_size=3,stride=1,padding=1)
    #output_shape: (256, 3, 64, 64)
    self.relu1=nn.ReLU()
    #output_shape: (256, 3, 64, 64)
    self.pool=nn.MaxPool2d(kernel_size=2)
    #output_shape: (256, 3, 32, 32)
    self.conv_layer2 = nn.Conv2d(in_channels=3,out_channels=6,kernel_size=3,stride=1,padding=1)
    #output_shape: (256, 6, 32, 32)
    self.relu2=nn.ReLU()
    #output_shape: (256, 6, 32, 32)
    ```

    在前面的代码中，我们主要构建了两个卷积层，`conv_layer1`和`conv_layer2`。我们从数据加载器得到的图像是 256 个一批的，这些图像是彩色的，因此它有 *3* 个输入通道(RGB ),尺寸为 *64 x 64* 。我们的第一个卷积层，称为`conv_layer1`，接受大小为`(256, 3, 64, 64)`的输入，它是宽度和高度为 *64* 的 *256* 个图像和 *3* 个通道(RGB)。再看`conv_layer1`，它是一个二维 CNN，取 *3* 个输入通道，输出 *3* 个通道，内核大小为 *3* ，步长和填充为 *1* 个像素。我们还用内核大小 *2* 初始化了最大池。第二卷积层`conv_layer2`以 *3* 个输入通道为输入，输出 *6* 个通道，内核大小为 *3* ，步长和填充为 *1* 。这里，我们使用两个在变量中初始化的激活函数`ReLU`作为`relu1`和`relu2`。在下一节中，我们将介绍如何在这些层上传递数据。

3.  在下面的代码片段中，我们将构建两个卷积层，后面是其他完全连接的线性层。四个完全线性层以及损失函数的代码如下:

    ```
    self.fully_connected_1 =nn.Linear(in_features=32 * 32 * 6,out_features=1000)
    self.fully_connected_2 =nn.Linear(in_features=1000,out_features=250)
    self.fully_connected_3 =nn.Linear(in_features=250,out_features=60)
    self.fully_connected_4 =nn.Linear(in_features=60,out_features=2)
    self.loss = nn.CrossEntropyLoss()
    ```

    在前面的代码中，我们有四个完全连接的线性图层:

*   第一个线性层，也就是`self.fully_connected_1`，取输入，是`conv_layer2`产生的输出，这个`self.fully_connected_1`层输出 1000 个节点。
*   第二个线性层，即`self.fully_connected_2`，取第一个线性层的输出，输出 250 个节点。
*   类似地，第三线性层，即`self.fully_connected_3`，取第二线性层的输出，输出 60 个节点。
*   最后一层，也就是`self.fully_connected_4`，取第三层的输出，输出两个节点。由于这是二进制分类，这种神经网络结构的输出可以是两个值中的一个。最后我们会初始化损失函数，就是`Cross-Entropy`损失。

1.  我们的架构被定义为 CNN 和完全连接的线性网络的组合，所以是时候从不同的层和激活函数传入数据了。这可以通过覆盖`forward`方法来实现。这是`forward`方法的代码:

    ```
    def forward(self, input):
        output=self.conv_layer1(input)
        output=self.relu1(output)     
        output=self.pool(output)
        output=self.conv_layer2(output)
        output=self.relu2(output)

        output=output.view(-1, 6*32*32)
        output = self.fully_connected_1(output)
        output = self.fully_connected_2(output)
        output = self.fully_connected_3(output)
        output = self.fully_connected_4(output)
        return output
    ```

    与 PyTorch 类似，PyTorch Lightning 中的`forward`方法接受输入数据。在前面的代码中，我们有以下内容:

*   我们在第一个卷积层(`conv_layer1`)传递数据。来自`conv_layer1`的输出被传递给`ReLU`激活函数，来自`ReLU`的输出被传递给 max-pooling 层。
*   一旦输入数据被第一卷积层处理，激活函数经历汇集层。
*   然后，输出被传递到我们的第二卷积层(`conv_layer2`)，并且来自第二卷积层的输出被传递到我们的第二`ReLU`激活函数。
*   通过卷积层传递的数据，这些层的输出是多维的。为了将输出传递到我们的线性层，它被转换为一维形式，这可以使用张量视图方法来实现。
*   一旦数据以一维形式准备就绪，它将通过四个完全连接的层传递，并返回最终输出。

重申一下，在`forward`方法中，输入图像数据首先通过两个卷积层，然后卷积层的输出通过四个完全连接的层。最后，输出被返回。

> **重要提示**
> 
> 超参数可以用一种叫做`save_hyperparameters()`的方法保存。这项技术将在接下来的章节中介绍。

#### 配置优化程序

如前一节所述，配置优化器是 PyTorch Lightning 中任何模型工作所需的生命周期方法之一。我们的`ImageClassifier`模型的`configure_optimizers`生命周期方法的代码如下:

```
def configure_optimizers(self):
  params = self.parameters()
  optimizer = optim.Adam(params=params, lr = self.learning_rate)
  return optimizer
```

在前面的代码中，我们使用了学习率已经在`__init__()`方法中初始化的`Adam`优化器，然后我们从这个方法返回优化器。

`configure_optimizers`方法可以返回多达六个不同的输出。和前面的例子一样，它也可以返回一个列表/元组对象。对于多个优化器，它可以返回两个单独的列表:一个是优化器列表，另一个是学习率调度器列表。

> **重要提示**
> 
> `Configure_optimizers`可以返回六种不同的输出。我们可能不会涵盖本书中的所有案例，但其中一些案例已经在我们即将到来的高级主题章节中使用过。
> 
> 例如，当我们构建一些复杂的神经网络架构时，如**生成对抗网络** ( **甘**)模型，可能需要多个优化器，在某些情况下，我们可能需要一个学习率调度器以及优化器。这可以通过在生命周期方法中配置优化器来解决。

#### 配置培训和测试步骤

在我们已经介绍过的`XOR`模型中，帮助我们在训练数据集上训练模型的生命周期方法之一是`training_step`。类似地，如果我们想在`test`数据集上测试我们的模型，我们有一个生命周期方法叫做`test_step`。

对于我们的`ImageClassifier`模型，我们使用了生命周期方法进行训练和测试。在此模型中，我们将重点关注记录一些额外的指标，并在训练模型时在进度条上显示一些指标。

PyTorch Lightning `training_step`生命周期方法的代码如下:

```
def training_step(self, batch, batch_idx):
  inputs, targets = batch
  outputs = self(inputs) 
  accuracy = self.binary_accuracy(outputs, targets)
  loss = self.loss(outputs, targets)
  self.log('train_accuracy', accuracy, prog_bar=True)
  self.log('train_loss', loss)
  return {"loss":loss, "train_accuracy":accuracy}
```

在前面的代码中，我们有以下内容:

*   在`training_step`生命周期方法中，批量数据作为输入参数被访问，输入数据被传递给模型，并且`self.loss`也被计算。
*   我们使用一个名为`self.binary_accuracy`的效用函数。该方法将实际目标和模型的预测输出作为输入，并计算精度。使用本书的 GitHub 链接可以获得`self.binary_accuracy`方法的完整代码。

我们将在这里执行一些在我们的`XOR`模型中没有完成的额外步骤。我们将使用`self.log`函数并记录一些额外的指标。下面的代码将帮助我们记录我们的`train_accuracy`和`train loss`:

```
self.log('train_accuracy', accuracy, prog_bar=True)
self.log('train_loss', loss)
```

在前面的代码中，`self.log`方法接受指标的键/名称作为第一个参数，第二个参数是指标的值，我们传递的第三个参数默认为`prog_bar`，它总是被设置为`false`。

我们正在记录我们的`train`数据集的准确性和损失。这些记录的值可以在以后用于绘制我们的图表或用于进一步的调查，并将帮助我们调整模型。通过将`prog_bar`参数设置为`true`，它将在训练模型时在进度条上显示每个时期的`train_accuracy`度量。

这种生命周期方法可以返回一个字典作为带有损失和测试准确性的输出。

`test_step`生命周期方法的代码如下:

```
def test_step(self, batch, batch_idx):
  inputs, targets = batch
  outputs = self.forward(inputs)
  accuracy = self.binary_accuracy(outputs,targets)
  loss = self.loss(outputs, targets)
  self.log('test_accuracy', accuracy)
  return {"test_loss":loss, "test_accuracy":accuracy}
```

`test_step`生命周期方法的代码类似于`training_step`生命周期方法。唯一的区别是传递给这个方法的数据是测试数据集。我们将在本章的下一节看到这个方法是如何被触发的。

### 训练模型

一旦我们用所有的生命周期方法建立了我们的模型，这个框架使得训练模型变得简单和容易。在 PyTorch Lightning 中，为了训练模型，我们首先初始化`trainer`类，然后调用`fit`方法来实际训练模型。用于训练我们的`ImageClassifier`模型的代码片段如下:

```
model = ImageClassifier()
trainer = pl.Trainer(max_epochs=100, progress_bar_refresh_rate=30, gpus=1)
trainer.fit(model, train_dataloader=train_dataloader)
```

在前面的代码中，我们首先用默认的学习速率`0.001`初始化我们的`ImageClassifier`模型。然后，我们从 PyTorch Lightning 框架中初始化了`trainer`类对象，使用了`100`个纪元，使用了单个 GPU，并将进度条速率设置为`30`。我们的模型利用 GPU 进行计算，总共运行了`100`个时期。

每当我们训练任何 PyTorch 闪电模型时，主要是在 Jupyter 笔记本上，每个时期的训练进度都在进度条中可视化。这个参数帮助我们控制进度条的更新速度。`fit`方法是我们传递模型和训练数据加载器的地方，这是我们在本节前面创建的。在我们的`training_step`生命周期方法中，来自列车数据加载器的数据被批量访问，这就是我们训练和计算损失的地方。

![Figure 2.11 – Training ImageClassifier for 100 epochs](../media/file10.png)

Figure 2.11 – Training ImageClassifier for 100 epochs

前面的屏幕截图显示了用于培训的指标。

在`training_step`中，我们记录了`train_accuracy`指标，并将`prog_bar`值设置为`true`。这使得`train_accuracy`指标能够显示在每个时期的进度条上，如前面的屏幕截图所示。

此时，我们已经在`100`个时期的`train`数据集上训练了我们的模型，根据进度条上显示的`train_accuracy`指标，训练准确率为 95%。检查我们的模型在`test`数据集上的表现很重要。

### 计算精度

为了计算模型的准确性，我们需要在测试数据加载器中传递测试数据，并在`test`数据集上检查准确性。为了计算模型在`test`数据集上的性能，我们可以利用我们的`trainer`类中的测试方法。下面的代码演示了这一点:

```
trainer.test(test_dataloaders=test_dataloader)
```

在前面的代码中，我们从`trainer`对象调用`test`方法，并传入测试数据加载器。当我们这样做时，PyTorch Lightning 在内部调用我们的`test_step`生命周期方法，并成批传递数据。

上述代码的输出为我们提供了测试精度和损耗值，如下所示:

![Figure 2.12 – Output of the test method](../media/file11.png)

Figure 2.12 – Output of the test method

根据前面的输出，我们的`ImageClassifier`模型在我们的`test`数据集上给出了 60%的准确率。这是我们使用 CNN 的简单`ImageClassifier`模型的结尾。

> **重要提示**
> 
> 您可能已经注意到，该模型在`train`上具有非常好的准确性，但是在`test`数据集的情况下准确性下降。这种行为通常被称为*“过度拟合”*当模型记住了一个训练集，而没有对一个看不见的数据集进行归纳时，通常会发生这种情况。
> 
> 有各种方法可以使模型在`test`数据集上表现得更好，这些方法被称为*“正则化”*方法。批处理规范化、删除等在规范化模型时非常有用。你可以试一试，你会发现测试准确度有所提高。我们还将在以后的章节中使用它们。

### 模型改进练习

*   尝试运行模型历元的模型，看看精度如何提高。
*   尝试调整批量大小并查看结果。在某些情况下，较小的批量可以提供有趣的结果。
*   您还可以尝试改变优化器的学习率，或者甚至使用不同的优化器，比如`AdaGrad`，看看性能是否有变化。通常，较低的学习速率意味着较长的训练时间，但可以避免错误的收敛。
*   您还可以尝试不同的增强方法，如`T.HorizontalFlip()`、`T.VerticalFlip()`或`T.RandomRotate().`数据增强方法通过旋转或翻转图像，从原始图像创建新条目来训练集合。原始图像的这种额外变化使模型学习得更好，并提高了它在看不见的图像上的准确性。
*   稍后，尝试添加第三个卷积层或额外的完全连接层，以查看对模型准确性的影响。

所有这些变化将改进模型。您可能还需要增加启用的 GPU 数量，因为这可能需要更多的计算能力。

## 摘要

在这一章中，我们领略了 MLP 神经网络和 CNN，它们是深度学习的构建模块。我们了解到，通过使用 PyTorch Lightning 框架，我们可以轻松地构建我们的模型。虽然 MLPs 和 CNN 听起来像是基本模型，但它们在商业应用方面相当先进，许多公司只是在为其工业用途热身。神经网络非常广泛地用作结构化数据的分类器，用于预测用户对报价的喜欢或响应倾向，或营销活动优化，以及许多其他事情。CNN 还广泛用于许多工业应用，例如计算图像中对象的数量，识别汽车凹痕以进行保险索赔，面部识别以识别罪犯，等等。

在这一章中，我们看到了如何使用 MLP 模型构建最简单也是最重要的`XOR`操作符。我们进一步扩展了 MLPs 的概念，以构建我们的第一个 CNN 深度学习模型来识别图像。使用 PyTorch Lightning，我们看到了如何用最少的编码和内置函数构建深度学习模型。

虽然深度学习模型非常强大，但它们也非常需要计算。为了达到通常在研究论文中看到的准确率，我们需要针对海量数据扩展模型，并对其进行数千个时期的训练，这反过来需要大量的硬件投资或令人震惊的云计算使用账单。解决这个问题的一个方法是不要从头开始训练深度学习模型，而是使用这些大模型训练的模型中的信息，并将其转移到我们的模型中。这种方法，也被称为**迁移学习**，在这个领域非常流行，因为它有助于节省时间和金钱。

在下一章中，我们将看到如何利用迁移学习在短时间内获得真正好的结果，而无需从头开始进行全面训练。