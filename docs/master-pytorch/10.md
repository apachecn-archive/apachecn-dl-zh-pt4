<link href="Styles/Style01.css" rel="stylesheet" type="text/css"> <link href="Styles/Style00.css" rel="stylesheet" type="text/css"> 

# *第六章*:用 PyTorch 生成音乐和文本

PyTorch 是研究深度学习模型和开发基于深度学习的应用程序的绝佳工具。在前面的章节中，我们看了跨不同领域和模型类型的模型架构。我们使用 PyTorch 从零开始构建这些架构，并使用来自 PyTorch 模型动物园的预训练模型。我们将从这一章开始转换话题，深入探究生成模型。

在前面的章节中，我们的大多数例子和练习都围绕着开发分类模型，这是一项监督学习任务。然而，当涉及到无监督的学习任务时，深度学习模型也被证明是极其有效的。深度生成模型就是这样一个例子。这些模型使用大量未标记的数据进行训练。一旦经过训练，该模型可以生成类似的有意义的数据。它通过学习输入数据中的底层结构和模式来做到这一点。

在本章中，我们将开发文本和音乐生成器。为了开发文本生成器，我们将利用我们在 [*第 5 章*](B12158_05_Final_ASB_ePUB.xhtml#_idTextAnchor106) 、*混合高级模型*中培训的基于 transformer 的语言模型。我们将使用 PyTorch 扩展 transformer 模型，使其作为文本生成器工作。此外，我们将演示如何在 PyTorch 中使用预先训练好的高级 transformer 模型，以便用几行代码建立一个文本生成器。最后，我们将使用 PyTorch 从头开始构建一个在 MIDI 数据集上训练过的音乐生成器模型。

本章结束时，您应该能够在 PyTorch 中创建自己的文本和音乐生成模型。您还将能够应用不同的采样或生成策略来从这些模型中生成数据。本章涵盖以下主题:

*   使用 PyTorch 构建基于 transformer 的文本生成器
*   使用预先训练的 GPT 新协议模型作为文本生成器
*   使用 PyTorch 通过 LSTMs 生成 MIDI 音乐

# 技术要求

我们将在所有练习中使用 Jupyter 笔记本。以下是你需要使用`pip`为本章安装的 Python 库列表；例如，在命令行上运行`pip install torch==1.4.0`:

```
jupyter==1.0.0
```

```
torch==1.4.0
```

```
tqdm==4.43.0
```

```
matplotlib==3.1.2
```

```
torchtext==0.5.0
```

```
transformers==3.0.2
```

```
scikit-image==0.14.2
```

与本章相关的所有代码文件都可以在[https://github . com/packt publishing/Mastering-py torch/tree/master/chapter 06](https://github.com/PacktPublishing/Mastering-PyTorch/tree/master/Chapter06)获得。

# 使用 PyTorch 构建基于 transformer 的文本生成器

在前一章中，我们使用 PyTorch 构建了一个基于 transformer 的语言模型。因为语言模型模拟了特定单词遵循给定单词序列的概率，所以我们在构建自己的文本生成器方面已经完成了一半以上。在本节中，我们将学习如何将这种语言模型扩展为一种深度生成模型，它可以生成任意但有意义的句子，给定一个单词序列形式的初始文本提示。

## 训练基于变压器的语言模型

在前一章中，我们为 5 个时期训练了一个语言模型。在本节中，我们将遵循那些完全相同的步骤，但将对模型进行更长时间的训练；也就是 50 个纪元。这里的目标是获得一个性能更好的语言模型，然后可以生成真实的句子。请注意，模型培训可能需要几个小时。因此，建议在后台对其进行训练；比如隔夜。为了遵循训练语言模型的步骤，请遵循[https://github . com/packt publishing/Mastering-py torch/blob/master/chapter 06/text _ generation . ipynb](https://github.com/PacktPublishing/Mastering-PyTorch/blob/master/Chapter06/text_generation.ipynb)上的完整代码。

经过 50 个时期的训练，我们得到以下输出:

![Figure 6.1 – Language model training logs](Images/B12158_06_01.jpg)

图 6.1–语言模型培训日志

现在我们已经成功地训练了 50 个纪元的 transformer 模型，我们可以继续进行实际练习，在这里我们将扩展这个训练过的语言模型作为文本生成模型。

## 保存和加载语言模型

在这里，一旦训练完成，我们将简单地保存表现最佳的模型检查点。然后，我们可以单独加载这个预训练模型:

1.  一旦训练好模型，最好将其保存在本地，这样您就不必从头开始重新训练它。您可以按如下方式保存:

    ```
    mdl_pth = './transformer.pth'
    torch.save(best_model_so_far.state_dict(), mdl_pth)
    ```

2.  我们现在可以加载保存的模型，这样我们就可以将这个语言模型扩展为文本生成模型:

    ```
    # load the best trained model
    transformer_cached = Transformer(num_tokens, embedding_size, num_heads, num_hidden_params, num_layers, dropout).to(device)
    transformer_cached.load_state_dict(torch.load(mdl_pth))
    ```

在本节中，我们重新实例化了一个 transformer 模型对象，然后将预先训练的模型权重加载到这个新的模型对象中。接下来，我们将使用这个模型来生成文本。

## 使用语言模型生成文本

既然已经保存并加载了模型，我们可以扩展训练好的语言模型来生成文本:

1.  首先，我们必须定义我们想要生成的单词的目标数量，并提供一个初始单词序列作为模型的提示:

    ```
    ln = 10
    sntc = 'It will _'
    sntc_split = sntc.split()
    ```

2.  Finally, we can generate the words one by one in a loop. At each iteration, we can append the predicted word in that iteration to the input sequence. This extended sequence becomes the input to the model in the next iteration and so on. The random seed is added to ensure consistency. By changing the seed, we can generate different texts, as shown in the following code block:

    ```
    torch.manual_seed(799)
    with torch.no_grad():
        for i in range(ln):
            sntc = ' '.join(sntc_split)
            txt_ds = TEXT.numericalize([sntc_split])
            num_b = txt_ds.size(0)
            txt_ds = txt_ds.narrow(0, 0, num_b)
            txt_ds = txt_ds.view(1, -1).t().contiguous().to(device)
            ev_X, _ = return_batch(txt_ds, i+1)
            op = transformer_cached(ev_X)
            op_flat = op.view(-1, num_tokens)
            res = TEXT.vocab.itos[op_flat.argmax(1)[0]]
            sntc_split.insert(-1, res)
    print(sntc[:-2])
    ```

    这将输出以下内容:

![Figure 6.2 – Transformer generated text](Images/B12158_06_02.jpg)

图 6.2–变压器生成的文本

正如我们所看到的，使用 PyTorch，我们可以训练一个语言模型(在本例中是一个基于 transformer 的模型)，然后用它生成带有几行额外代码的文本。生成的文本似乎有道理。这种文本生成器的结果受限于基础语言模型被训练的数据量，以及语言模型有多强大。在这一节中，我们基本上从头开始构建了一个文本生成器。

在下一节中，我们将加载预先训练的语言模型，并将其用作文本生成器。我们将使用变形金刚模型的高级继承者——**生成式预训练变形金刚** ( **GPT** -2)。我们将演示如何使用 PyTorch 用不到 10 行代码构建一个开箱即用的高级文本生成器。我们还将看看从语言模型中生成文本所涉及的一些策略。

# 使用预先训练的 GPT-2 模型作为文本生成器

将`transformers`库与 PyTorch 一起使用，我们可以加载大多数最新的高级 transformer 模型来执行各种任务，例如语言建模、文本分类、机器翻译等等。我们在第五章[](B12158_05_Final_ASB_ePUB.xhtml#_idTextAnchor106)**混合动力高级车型*中演示了如何做到这一点。*

 *在本节中，我们将加载预训练的基于新 GPT 协议的语言模型。然后，我们将扩展这个模型，以便我们可以将它用作文本生成器。然后，我们将探索从预先训练的语言模型生成文本时可以遵循的各种策略，并使用 PyTorch 来演示这些策略。

## 使用 GPT-2 系统生成现成的文本

在练习的形式中，我们将使用 transformers 库加载预训练的 GPT-2 语言模型，并将该语言模型扩展为文本生成模型，以生成任意但有意义的文本。出于演示目的，我们将只展示代码的重要部分。为了访问完整代码，请访问[https://github . com/packt publishing/Mastering-py torch/blob/master/chapter 06/text _ generation _ out _ of _ the _ box . ipynb](https://github.com/PacktPublishing/Mastering-PyTorch/blob/master/Chapter06/text_generation_out_of_the_box.ipynb)。请遵循以下步骤:

1.  First, we need to import the necessary libraries:

    ```
    from transformers import GPT2LMHeadModel, GPT2Tokenizer
    import torch
    ```

    我们将导入 GPT-2 多头语言模型和相应的标记器来生成词汇表。

2.  接下来，我们将实例化`GPT2Tokenizer`和语言模型。设置随机种子将确保可重复的结果。我们可以改变种子，每次生成不同的文本。最后，我们将提供一组初始单词作为模型的提示，如下:

    ```
    torch.manual_seed(799)
    tkz = GPT2Tokenizer.from_pretrained("gpt2")
    mdl = GPT2LMHeadModel.from_pretrained('gpt2')
    ln = 10
    cue = "It will"
    gen = tkz.encode(cue)
    ctx = torch.tensor([gen])
    ```

3.  Finally, we will iteratively predict the next word for a given input sequence of words using the language model. At each iteration, the predicted word is appended to the input sequence of words for the next iteration:

    ```
    prv=None
    for i in range(ln):
        op, prv = mdl(ctx, past=prv)
        tkn = torch.argmax(op[..., -1, :])
        gen += [tkn.tolist()]
        ctx = tkn.unsqueeze(0)
    seq = tkz.decode(gen)
    print(seq)
    ```

    输出应该如下所示:

![Figure 6.3 – GPT-2 generated text](Images/B12158_06_03.jpg)

图 6.3-新 GPT 协议生成的文本

这种生成文本的方式也被称为**贪婪搜索**。在的下一节，我们将更详细地研究贪婪搜索和其他一些文本生成策略。

## 使用 PyTorch 的文本生成策略

当我们使用经过训练的文本生成模型来生成文本时，我们通常会一个字一个字地进行预测。然后，我们将预测单词的结果序列合并为预测文本。当我们在一个循环中迭代单词预测时，我们需要指定一种方法，在给定先前的 *k* 预测的情况下，找到/预测下一个单词。这些方法也被称为文本生成策略，我们将在本节讨论一些众所周知的策略。

### 贪婪搜索

名字 *greedy* 是合理的，因为模型选择在当前迭代中具有最大概率的单词，而不管它们在前面多少时间步。使用这种策略，模型可能会遗漏隐藏在低概率单词后面(在时间上更靠前)的高概率单词，这仅仅是因为模型没有追踪低概率单词。下图展示了贪婪搜索策略，展示了上一个练习的*步骤 3* 中可能发生的情况。在每个时间步，文本生成模型输出可能的单词及其概率:

![Figure 6.4 – Greedy search](Images/B12158_06_04.jpg)

图 6.4–贪婪搜索

我们可以看到，在每一步，在文本生成的贪婪搜索策略下，概率最高的单词被模型挑选出来。请注意倒数第二步，该模型以大致相等的概率预测了单词**系统**、**人**和**未来**。使用贪婪搜索，**系统**被选为下一个单词，因为它比其他单词的概率稍高。然而，你可以争辩说**人**或**未来**本可以产生更好或更有意义的文本。

这是贪婪搜索方法的核心限制。此外，由于缺乏随机性，贪婪搜索还会导致重复的结果。如果有人想艺术地使用这样的文本生成器，贪婪搜索并不是最好的方法，仅仅是因为它的单调性。

在上一节中，我们手工编写了文本生成循环。感谢`transformers`库，我们可以用三行代码编写文本生成步骤:

```
ip_ids = tkz.encode(cue, return_tensors='pt')
```

```
op_greedy = mdl.generate(ip_ids, max_length=ln)
```

```
seq = tkz.decode(op_greedy[0], skip_special_tokens=True)
```

```
print(seq)
```

这将输出以下内容:

![Figure 6.5 – GPT-2 generated text (concise)](Images/B12158_06_05.jpg)

图 6.5-新 GPT 协议生成的文本(简明)

请注意，图 6.5 中生成的句子比图 6.3 中生成的句子少了一个单词。这种差异是因为在后一段代码中，`max_length`参数包含提示词。所以，如果我们有一个线索词，九个新词将被预测。如果我们有两个提示词，将预测八个新词(如这里的情况)，等等。

### 波束搜索

贪婪搜索不是生成文本的唯一方式。 **Beam search** 是贪婪搜索方法的发展，其中我们基于整体预测的序列概率，而不仅仅是下一个单词的概率，维护潜在候选序列的列表。要追踪的候选序列的数量是沿着字预测树的波束的数量。

下图展示了波束大小为三的波束搜索将如何被用于产生三个候选序列(根据总序列概率排序),每个序列五个字:

![Figure 6.6 – Beam search](Images/B12158_06_06.jpg)

图 6.6–波束搜索

在这个波束搜索例子中的每次迭代中，保持三个最可能的候选序列。随着我们在序列中继续深入，候选序列的可能数量呈指数增长。然而，我们只对前三个序列感兴趣。这样，我们就不会像贪婪搜索那样错过潜在的更好的序列。

在 PyTorch 中，我们可以在一行代码中使用现成的光束搜索。下面的代码演示了基于波束搜索的文本生成，使用三个波束生成三个最可能的句子，每个句子包含五个单词:

```
op_beam = mdl.generate(
```

```
    ip_ids, 
```

```
    max_length=5, 
```

```
    num_beams=3, 
```

```
    num_return_sequences=3, 
```

```
)
```

```
for op_beam_cur in op_beam:
```

```
    print(tkz.decode(op_beam_cur, skip_special_tokens=True))
```

这为我们提供了以下输出:

![Figure 6.7 – Beam search results](Images/B12158_06_07.jpg)

图 6.7–波束搜索结果

波束搜索仍然存在重复性或单调性的问题。不同的运行将导致相同的结果集，因为它确定性地寻找具有最大总体概率的序列。在下一节中，我们将研究一些使生成的文本更加不可预测或更有创造性的方法。

### Top-k 和 top-p 采样

我们可以根据下一个单词的相对概率，从下一个单词的可能集合中随机抽取下一个单词，而不是总是挑选概率最高的下一个单词。例如在*图 6.6* 中，**是**、**知道**、**这三个词分别表示**有 **0.7** 、 **0.2** 、 **0.1** 的概率。我们可以根据概率随机抽取这三个词中的任何一个，而不是总是挑选 **be** 对抗 **know** 和 **show** 。如果我们重复这个练习 10 次以生成 10 个单独的文本，则 **be** 将被选择大约 7 次，而 **know** 和 **show** 将分别被选择 2 次和 1 次。这给了我们太多不同的单词组合，这是 beam 或 greedy search 永远不会产生的。

使用采样技术生成文本的两种最流行的方式被称为 **top-k** 和 **top-p** 采样。在 top-k 采样下，我们预定义了一个参数， *k* ，它是在采样下一个单词时应该考虑的候选单词的数量。所有其他单词都被丢弃，并且概率在前 *k* 个单词中被标准化。在我们之前的示例中，如果 *k* 为 2，那么单词 **show** 将被丢弃，单词 **be** 和 **know** 将分别将其概率( **0.7** 和 **0.2** )归一化为 **0.78** 和 **0.22** 。

下面的代码演示了 top-k 文本生成方法:

```
for i in range(3):
```

```
    torch.manual_seed(i)
```

```
    op = mdl.generate(
```

```
        ip_ids, 
```

```
        do_sample=True, 
```

```
        max_length=5, 
```

```
        top_k=2
```

```
    )
```

```
    seq = tkz.decode(op[0], skip_special_tokens=True)
```

```
    print(seq)
```

这将生成以下输出:

![Figure 6.8 – Top-k search results](Images/B12158_06_08.jpg)

图 6.8-前 k 名搜索结果

为了从所有可能的单词中取样，而不仅仅是前 k 个单词，我们应该在代码中将参数`top-k`设置为`0`。如前面的屏幕截图所示，不同的运行会产生不同的结果，而贪婪搜索会在每次运行时产生完全相同的结果，如下面的屏幕截图所示:

![Figure 6.9 – Repetitive greedy search results](Images/B12158_06_09.jpg)

图 6.9-重复的贪婪搜索结果

在top-p 采样策略下，我们可以定义一个累积概率阈值( *p* )然后保留其概率总计为 *p* 的单词，而不是定义顶部 *k* 单词的来查看。在我们的例子中，如果 *p* 在 **0.7** 和 **0.9** 之间，那么我们丢弃**知道**和**显示**，如果 *p* 在 **0.9** 和 **1.0** 之间，那么我们丢弃**显示**，如果 *p* 是**即**为**，**知**，**显**。**

在概率分布平坦的情况下，top-k 策略有时可能不公平。这是因为它剪掉了那些几乎和被保留的词一样可能的词。在这些情况下，top-p 策略将保留较大的词池进行采样，而在概率分布相当尖锐的情况下，将保留较小的词池。

下面的代码演示了 top-p 采样方法:

```
for i in range(3):
```

```
    torch.manual_seed(i)
```

```
    op = mdl.generate(
```

```
        ip_ids, 
```

```
        do_sample=True, 
```

```
        max_length=5, 
```

```
        top_p=0.75, 
```

```
        top_k=0
```

```
    )
```

```
    seq = tkz.decode(op[0], skip_special_tokens=True)
```

```
    print(seq)
```

这将输出以下内容:

![Figure 6.10 – Top-p search results](Images/B12158_06_10.jpg)

图 6.10-前 p 名搜索结果

我们可以同时设置 top-k 和 top-p 策略。在这个例子中，我们已经将`top-k`设置为`0`以实质上禁用 top-k 策略，并且`p`被设置为`0.75`。再一次，这导致不同的句子，并且可以引导我们更有创造性地生成文本，而不是贪婪或光束搜索。有更多的文本生成策略可用，并且在这个领域正在进行大量的研究。我们鼓励您进一步跟进此事。

一个很好的起点是试验一下`transformers`库中可用的文本生成策略。你可以通过这个库的创建者的博文来了解更多信息:[https://huggingface.co/blog/how-to-generate](https://huggingface.co/blog/how-to-generate)。

这结束了我们使用 PyTorch 生成文本的探索。在的下一部分，我们将进行类似的练习，但这次是音乐而不是文本。这个想法是在音乐数据集上训练一个无监督的模型，并使用训练的模型生成与训练数据集中的旋律相似的旋律。

# 使用 PyTorch 通过 LSTMs 生成 MIDI 音乐

从文本继续，在这一部分，我们将使用 PyTorch 创建一个可以创作类似古典音乐的机器学习模型。在上一节中，我们使用了转换器来生成文本。这里，我们将使用 LSTM 模型来处理连续的音乐数据。我们将根据莫扎特的古典音乐作品训练模特。

每首音乐作品本质上都将被分解成一系列的钢琴音符。我们将以**乐器数字接口** ( **MIDI** )文件的形式读取音乐数据，这是一种众所周知的常用格式，可以方便地在各种设备和环境中读取和写入音乐数据。

在将 MIDI 文件转换成钢琴音符序列(我们称之为钢琴卷帘窗)之后，我们将使用它们来训练下一个钢琴音符检测系统。在这个系统中，我们将建立一个基于 LSTM 的分类器，它将为给定的前面的钢琴音符序列预测下一个钢琴音符，总共有 88 个(按照标准的 88 个钢琴键)。

我们现在将以练习的形式演示构建 AI 音乐作曲者的整个过程。我们的重点是 PyTorch 代码，它用于数据加载、模型训练和生成音乐样本。请注意，模型培训过程可能需要几个小时，因此建议在后台运行培训过程；比如隔夜。为了保持文本简短，这里给出的代码已经被缩减。

处理 MIDI 音乐文件的细节超出了本书的范围，尽管我们鼓励您探索完整的代码，这些代码可从 https://github . com/packt publishing/Mastering-py torch/blob/master/chapter 06/music _ generation . ipynb 获得。

## 载入 MIDI 音乐数据

首先，我们将演示如何加载 MIDI 格式的音乐数据。我们将简要提及处理 MIDI 数据的代码，然后举例说明如何用它制作 PyTorch 数据加载器。让我们开始吧:

1.  As always, we will begin by importing the important libraries. Some of the new ones we'll be using in this exercise are as follows:

    ```
    import skimage.io as io
    from struct import pack, unpack
    from io import StringIO, BytesIO
    ```

    `skimage`用于可视化模型生成的音乐样本序列。`struct`和`io`用于处理将 MIDI 音乐数据转换成钢琴卷帘窗的过程。

2.  接下来，我们将编写助手类和函数来加载 MIDI 文件，并将它们转换成可以输入到 LSTM 模型的钢琴音符序列(矩阵)。首先，我们定义一些 MIDI 常量来配置各种音乐控件，如音高、通道、序列开始、序列结束等等:

    ```
    NOTE_MIDI_OFF = 0x80
    NOTE_MIDI_ON = 0x90
    CHNL_PRESS = 0xD0
    MIDI_PITCH_BND = 0xE0
    ...
    ```

3.  然后，我们将定义一系列处理 MIDI 数据输入和输出流的类，MIDI 数据解析器，等等，如下:

    ```
    class MOStrm:
    # MIDI Output Stream
    ...
    class MIFl: 
    # MIDI Input File Reader
    ...
    class MOFl(MOStrm):
    # MIDI Output File Writer
    ...
    class RIStrFl:
    # Raw Input Stream File Reader
    ...
    class ROStrFl:
    # Raw Output Stream File Writer
    ...
    class MFlPrsr:
    # MIDI File Parser
    ...
    class EvtDspch:
    # Event Dispatcher
    ...
    class MidiDataRead(MOStrm):
    # MIDI Data Reader
    ...
    ```

4.  处理完所有与 MIDI 数据 I/O 相关的代码后，我们就可以实例化我们自己的 PyTorch 数据集类了。在此之前，我们必须定义两个关键的函数——一个用于将读取的 MIDI 文件转换成钢琴卷帘窗，一个用于用空音符填充钢琴卷帘窗。这将标准化数据集上音乐片段的长度:

    ```
    def md_fl_to_pio_rl(md_fl):
        md_d = MidiDataRead(md_fl, dtm=0.3)
        pio_rl = md_d.pio_rl.transpose()
        pio_rl[pio_rl > 0] = 1    
        return pio_rl
    def pd_pio_rl(pio_rl, mx_l=132333, pd_v=0):        
        orig_rol_len = pio_rl.shape[1]    
        pdd_rol = np.zeros((88, mx_l))
        pdd_rol[:] = pd_v    
        pdd_rol[:, - orig_rol_len:] = pio_rl
        return pdd_rol
    ```

5.  现在，我们可以定义我们的 PyTorch 数据集类，如下:

    ```
    class NtGenDataset(data.Dataset):    
        def __init__(self, md_pth, mx_seq_ln=1491):        
            ...    
        def mx_len_upd(self):        
            ...   
        def __len__(self):        
            return len(self.md_fnames_ful)    
        def __getitem__(self, index):        
            md_fname_ful = self.md_fnames_ful[index]        
            pio_rl = md_fl_to_pio_rl(md_fname_ful)
            seq_len = pio_rl.shape[1] - 1
            ip_seq = pio_rl[:, :-1]
            gt_seq = pio_rl[:, 1:]
            ...
            return (torch.FloatTensor(ip_seq_pad),
                    torch.LongTensor(gt_seq_pad), torch.LongTensor([seq_len]))
    ```

6.  除了 dataset 类，我们还必须添加另一个辅助函数，将一批训练数据中的音乐序列后处理到三个单独的列表中。这些将是输入序列、输出序列和序列长度，按序列长度降序排列:

    ```
    def pos_proc_seq(btch):
        ip_seqs, op_seqs, lens = btch    
        ...
        ord_tr_data_tups = sorted(tr_data_tups,
                                             key=lambda c: int(c[2]),
                                             reverse=True)
        ip_seq_splt_btch, op_seq_splt_btch, btch_splt_lens = zip(*ord_tr_data_tups)
        ...  
        return tps_ip_seq_btch, ord_op_seq_btch, list(ord_btch_lens_l)
    ```

7.  For this exercise, we will be using a set of Mozart's compositions. You can download the dataset from here: [http://www.piano-midi.de/mozart.htm](http://www.piano-midi.de/mozart.htm) . The downloaded folder consists of 21 MIDI files, which we will split into 18 training and three validation set files. The downloaded data is stored under `./mozart/train` and `./mozart/valid`. Once downloaded, we can read the data and instantiate our own training and validation dataset loaders:

    ```
    training_dataset = NtGenDataset('./mozart/train', mx_seq_ln=None)
    training_datasetloader = data.DataLoader(training_dataset, batch_size=5,shuffle=True, drop_last=True)
    validation_dataset = NtGenDataset('./mozart/valid/', mx_seq_ln=None)
    validation_datasetloader = data.DataLoader(validation_dataset, batch_size=3, shuffle=False, drop_last=False)
    X_validation = next(iter(validation_datasetloader))
    X_validation[0].shape
    ```

    这将为我们提供以下输出:

![Figure 6.11 – Sample music data dimensions](Images/B12158_06_11.jpg)

图 6.11–样本音乐数据维度

正如我们所看到的，第一批验证包含三个长度为 1，587(音符)的序列，其中每个序列被编码为一个 88 大小的向量，88 是钢琴键的总数。对于那些训练有素的音乐家，这里有一张音乐表，相当于验证集音乐文件中 e 上的前几个音符:

![Figure 6.12 – Music sheet of a Mozart composition](Images/B12158_06_12.jpg)

图 6.12-莫扎特作品的音乐风格

或者，我们可以把音符序列想象成一个 88 行的矩阵，每个琴键一行。以下是前面 melody(1，587 个音符中的前 300 个)的直观矩阵表示:

![Figure 6.13 – Matrix representation of a Mozart composition](Images/B12158_06_13.jpg)

图 6.13-莫扎特作品的矩阵表示

数据集引用

Bernd Krueger 的 MIDI、音频(MP3、OGG)和视频文件在 CC BY-SA Germany 许可证下获得许可。姓名:Bernd Krueger 来源:[http://www . piano-midi . de](http://www.piano-midi.de) 这些文件的分发或公开播放只允许在完全相同的许可条件下进行。乐谱是开源的。

我们现在将定义 LSTM 模型和训练程序。

## 定义 LSTM 模型和训练程序

到目前为止，我们已经成功地加载了一个 MIDI 数据集，并用它来创建我们自己的训练和验证数据加载器。在本节中，我们将定义 LSTM 模型架构，以及在模型训练循环中运行的训练和评估例程。让我们开始吧:

1.  First, we must define the model architecture. As we mentioned earlier, we will use an LSTM model that consists of an encoder layer that encodes the 88-dimensional representation of the input data at each time step of the sequence into a 512-dimensional hidden layer representation. The encoder is followed by two LSTM layers, followed by a fully connected layer that finally softmaxes into the 88 classes.

    根据我们在 [*第四章*](B12158_04_Final_ASB_ePUB.xhtml#_idTextAnchor074) *【深度递归模型架构】中讨论的不同类型的**递归神经网络** ( **RNNs** ，这是一个多对一序列分类任务，其中输入是从时间步 *0* 到时间步 *t* 的整个序列，输出是时间步 *t+1* 的 88 个类中的一个，如下*

    ```
    class MusicLSTM(nn.Module):    
        def __init__(self, ip_sz, hd_sz, n_cls, lyrs=2):        
            ...       
            self.nts_enc = nn.Linear(in_features=ip_sz, out_features=hd_sz)        
            self.bn_layer = nn.BatchNorm1d(hd_sz)        
            self.lstm_layer = nn.LSTM(hd_sz, hd_sz, lyrs)        
            self.fc_layer = nn.Linear(hd_sz, n_cls)

        def forward(self, ip_seqs, ip_seqs_len, hd=None):
            ...
            pkd = torch.nn.utils.rnn.pack_padded_sequence(nts_enc_ful, ip_seqs_len)
            op, hd = self.lstm_layer(pkd, hd)
            ...
            lgts = self.fc_layer(op_nrm_drp.permute(2,0,1))
            ...
            zero_one_lgts = torch.stack((lgts, rev_lgts), dim=3).contiguous()
            flt_lgts = zero_one_lgts.view(-1, 2)
            return flt_lgts, hd
    ```

2.  一旦定义了模型架构，我们就可以指定模型训练例程。我们将使用 Adam 优化器和渐变剪裁来避免过度拟合。另一个已经到位的对抗过拟合的措施是使用脱落层，如前一步所述:

    ```
    def lstm_model_training(lstm_model, lr, ep=10, val_loss_best=float("inf")):
        ...
        for curr_ep in range(ep):
            ...
            for batch in training_datasetloader:
                ...
                lgts, _ = lstm_model(ip_seq_b_v, seq_l)
                loss = loss_func(lgts, op_seq_b_v)
                ...
            if vl_ep_cur < val_loss_best:
                torch.save(lstm_model.state_dict(), 'best_model.pth')
                val_loss_best = vl_ep_cur
        return val_loss_best, lstm_model
    ```

3.  类似地，我们将定义模型评估例程，其中正向传递在模型上运行，其参数保持不变:

    ```
    def evaluate_model(lstm_model):
        ...
        for batch in validation_datasetloader:
            ...
            lgts, _ = lstm_model(ip_seq_b_v, seq_l)
            loss = loss_func(lgts, op_seq_b_v)
            vl_loss_full += loss.item()
            seq_len += sum(seq_l)
        return vl_loss_full/(seq_len*88)
    ```

现在，让我们训练和测试音乐生成模型。

## 训练和测试音乐生成模型

在这个最后部分，我们将实际训练 LSTM 模型。然后，我们将使用经过训练的音乐生成模型来生成 w e 可以收听和分析的音乐样本。让我们开始吧:

1.  We are all set to instantiate our model and start training it. We have used categorical cross-entropy as the loss function for this classification task. We are training the model with a learning rate of `0.01` for `10` epochs:

    ```
    loss_func = nn.CrossEntropyLoss().cpu()
    lstm_model = MusicLSTM(ip_sz=88, hd_sz=512, n_cls=88).cpu()
    val_loss_best, lstm_model = lstm_model_training(lstm_model, lr=0.01, ep=10)
    ```

    这将输出以下内容:

    ![Figure 6\. 14 – Music LSTM training logs](Images/B12158_06_14.jpg)

    图 6。14–LSTM 音乐培训日志

2.  Here comes the fun part. Once we have a next-musical-note-predictor, we can use it as a music generator. All we need to do is simply initiate the prediction process by providing an initial note as a cue. The model can then recursively make predictions for the next note at each time step, wherein the predictions at time step *t* are appended to the input sequence at time *t+1*.

    在这里，我们将编写一个音乐生成函数，它接受经过训练的模型对象、要生成的音乐的预期长度、序列的开始音符以及温度。温度是对分类层的`softmax`函数的标准数学运算。它用于通过扩大或缩小软最大化的概率分布来操纵软最大化概率的分布。代码如下:

    ```
    def generate_music(lstm_model, ln=100, tmp=1, seq_st=None):
        ...
        for i in range(ln):
            op, hd = lstm_model(seq_ip_cur, [1], hd)
            probs = nn.functional.softmax(op.div(tmp), dim=1)
            ...
        gen_seq = torch.cat(op_seq, dim=0).cpu().numpy()
        return gen_seq
    ```

    最后，我们可以使用这个功能来创建一个全新的音乐作品:

    ```
    seq = generate_music(lstm_model, ln=100, tmp=1, seq_st=None)
    midiwrite('generated_music.mid', seq, dtm=0.2)
    ```

    这将创建音乐作品，并将其作为 MIDI 文件保存在当前目录中。我们可以打开文件并播放它，听听模型产生了什么。尽管如此，我们还可以查看制作的音乐的视觉矩阵表示:

    ```
    io.imshow(seq)
    ```

    这个应该给我们以下的输出:

![Figure 6.15 – Matrix representation of an AI generated music sample](Images/B12158_06_15.jpg)

图 6.15–人工智能生成的音乐样本的矩阵表示

此外，这是生成的音乐看起来像一张乐谱:

![Figure 6.16 – Music sheet of an AI generated music sample](Images/B12158_06_16.jpg)

图 6.16–人工智能生成的音乐样本的音乐片段

在这里，我们可以看到生成的旋律似乎不像莫扎特的原创作品那么悠扬。尽管如此，您可以在模型学习到的一些组合键中看到一致性。此外，生成的音乐质量可以通过在更多数据上训练模型，以及在更多时期训练它来增强。

我们使用机器学习生成音乐的练习到此结束。在本节中，我们演示了如何使用现有的音乐数据从头开始训练音符预测模型，并使用训练好的模型生成音乐。事实上，您可以扩展使用生成模型来生成任何类型的数据样本的想法。PyTorch 是一个非常有效的工具，尤其是由于它直接的 API 用于数据加载、模型构建/训练/测试，以及使用训练好的模型作为数据生成器。我们鼓励您在不同的用例和数据类型上尝试更多这样的任务。

# 总结

在这一章中，我们使用 PyTorch 探索了生成模型。从文本生成开始，我们利用在前一章中构建的基于 transformer 的语言模型来开发一个文本生成器。我们演示了如何使用 PyTorch 将一个在没有监督的情况下训练的模型(在本例中是一个语言模型)转换成一个数据生成器。之后，我们开发了 transformers 库下的预训练高级 transformer 模型，并将它们用作文本生成器。我们讨论了各种文本生成策略，比如贪婪搜索、波束搜索以及 top-k 和 top-p 采样。

接下来，我们从零开始构建了一个 AI 音乐作曲家。使用莫扎特的钢琴作品，我们训练了一个 LSTM 模型来预测由前面的钢琴音符序列给出的下一个钢琴音符。之后，我们使用我们在没有监督的情况下训练的分类器作为数据生成器来创建音乐。文本和音乐生成器的结果都很有希望，并显示 PyTorch 作为开发艺术人工智能生成模型的资源是多么强大。

本着同样的艺术精神，在下一章中，我们将学习如何使用机器学习将一幅图像的风格转移到另一幅图像上。在 PyTorch 的帮助下，我们将使用 CNN 从各种图像中学习艺术风格，并将这些风格应用到不同的图像中——这一任务更为人所知的是神经风格转移。*